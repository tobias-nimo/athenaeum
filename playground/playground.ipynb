{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Athenaeum Playground\n",
    "\n",
    "This notebook walks through the core features of **Athenaeum** — a Python library for building searchable knowledge bases from documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Use one of the following:\n",
    "\n",
    "#!pip install -q athenaeum-kb[mistral]\n",
    "!uv pip install '../dist/athenaeum_kb-0.2.1-py3-none-any.whl[mistral]' -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "MISTRAL_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass('OPENAI_API_KEY: ')\n",
    "MISTRAL_API_KEY = getpass.getpass('MISTRAL_API_KEY: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DOCS_DIR = Path(\"knowledge-base\")\n",
    "STORAGE_DIR = Path(\".athenaeum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "from athenaeum import Athenaeum, AthenaeumConfig, get_ocr_provider\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import tiktoken\n",
    "\n",
    "config = AthenaeumConfig(\n",
    "    storage_dir=STORAGE_DIR,\n",
    "    rrf_k=70,                   # RRF constant for hybrid search\n",
    "    default_strategy=\"hybrid\",  # Default search strategy\n",
    ")\n",
    "\n",
    "# Custom token-based splitter\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "token_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    Language.MARKDOWN,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=64,\n",
    "    length_function=lambda text: len(enc.encode(text)),\n",
    ")\n",
    "\n",
    "# Embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "# OCR model\n",
    "ocr = get_ocr_provider(\"mistral\", api_key=MISTRAL_API_KEY)\n",
    "\n",
    "kb = Athenaeum(\n",
    "    config=config,\n",
    "    ocr_provider=ocr,\n",
    "    embeddings=embeddings,\n",
    "    text_splitter=token_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded Attention Is All You Need.pdf with ID a37ca72e7343\n",
      "[INFO] Loaded BERT.pdf with ID a93623c887f3\n",
      "[INFO] Loaded XLNet.pdf with ID 398c7fcbc1bb\n",
      "[INFO] Loaded Language Models are Few-Shot Learners.pdf with ID 558dfc0f6033\n",
      "[INFO] Loaded LORA.pdf with ID 56397b5e8269\n",
      "[INFO] Loaded RAG.pdf with ID 30ea5715ee9d\n",
      "[INFO] Loaded ViT.pdf with ID f83a251d3a8d\n",
      "[INFO] Loaded GANs.pdf with ID 8053b5f17066\n",
      "[INFO] Loaded VAE.pdf with ID 77f686645c39\n",
      "[INFO] Loaded DDPM.pdf with ID 7e2a0c030dc2\n",
      "[INFO] Loaded High-Resolution Image Synthesis with Latent Diffusion Models.pdf with ID f3ea54df7f8d\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF papers from `DOCS_DIR`, assigning tags by research area so we can filter later\n",
    "\n",
    "papers = {\n",
    "    \"Attention Is All You Need.pdf\": {\"nlp\", \"transformers\", \"architecture\"},\n",
    "    \"BERT.pdf\":                      {\"nlp\", \"transformers\", \"pretraining\"},\n",
    "    \"XLNet.pdf\":                     {\"nlp\", \"transformers\", \"pretraining\"},\n",
    "    \"Language Models are Few-Shot Learners.pdf\": {\"nlp\", \"transformers\", \"pretraining\", \"generative\"},\n",
    "    \"LORA.pdf\":                      {\"nlp\", \"transformers\", \"fine-tuning\"},\n",
    "    \"RAG.pdf\":                       {\"nlp\", \"transformers\", \"retrieval\"},\n",
    "    \"ViT.pdf\":                       {\"vision\", \"transformers\", \"architecture\"},\n",
    "    \"GANs.pdf\":                      {\"vision\", \"generative\"},\n",
    "    \"VAE.pdf\":                       {\"vision\", \"generative\"},\n",
    "    \"DDPM.pdf\":                      {\"vision\", \"generative\", \"diffusion\"},\n",
    "    \"High-Resolution Image Synthesis with Latent Diffusion Models.pdf\": {\"vision\", \"generative\", \"diffusion\"},\n",
    "}\n",
    "\n",
    "for filename, tags in papers.items():\n",
    "    path = DOCS_DIR / filename\n",
    "    if not path.exists():\n",
    "        print(f\"Skipping (not found): {filename}\")\n",
    "        continue\n",
    "    doc_id = kb.load_doc(str(path), tags=tags)\n",
    "    print(f\"[INFO] Loaded {filename} with ID {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a37ca72e7343  Attention Is All You Need.pdf                               \n",
      "a93623c887f3  BERT.pdf                                                    \n",
      "398c7fcbc1bb  XLNet.pdf                                                   \n",
      "558dfc0f6033  Language Models are Few-Shot Learners.pdf                   \n",
      "56397b5e8269  LORA.pdf                                                    \n",
      "30ea5715ee9d  RAG.pdf                                                     \n",
      "f83a251d3a8d  ViT.pdf                                                     \n",
      "8053b5f17066  GANs.pdf                                                    \n",
      "77f686645c39  VAE.pdf                                                     \n",
      "7e2a0c030dc2  DDPM.pdf                                                    \n",
      "f3ea54df7f8d  High-Resolution Image Synthesis with Latent Diffusion Models.pdf\n"
     ]
    }
   ],
   "source": [
    "# List all documents in kb\n",
    "\n",
    "docs = kb.list_docs()\n",
    "for doc in docs:\n",
    "    print(f\"{doc.id}  {doc.name:<60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion papers:\n",
      "  - DDPM.pdf\n",
      "  - High-Resolution Image Synthesis with Latent Diffusion Models.pdf\n",
      "\n",
      " NLP papers:\n",
      "  - Attention Is All You Need.pdf\n",
      "  - BERT.pdf\n",
      "  - XLNet.pdf\n",
      "  - Language Models are Few-Shot Learners.pdf\n",
      "  - LORA.pdf\n",
      "  - RAG.pdf\n",
      "\n",
      "Vision + fine-tuning:\n",
      "  - LORA.pdf  tags={'transformers', 'fine-tuning', 'nlp'}\n",
      "  - ViT.pdf  tags={'architecture', 'transformers', 'vision'}\n",
      "  - GANs.pdf  tags={'generative', 'vision'}\n",
      "  - VAE.pdf  tags={'generative', 'vision'}\n",
      "  - DDPM.pdf  tags={'generative', 'vision', 'diffusion'}\n",
      "  - High-Resolution Image Synthesis with Latent Diffusion Models.pdf  tags={'generative', 'vision', 'diffusion'}\n"
     ]
    }
   ],
   "source": [
    "# Filter by tags\n",
    "# Tags use OR semantics — any document matching at least one of the given tags is returned.\n",
    "\n",
    "print(\"Diffusion papers:\")\n",
    "for doc in kb.list_docs(tags={\"diffusion\"}):\n",
    "    print(f\"  - {doc.name}\")\n",
    "\n",
    "print()\n",
    "print(\" NLP papers:\")\n",
    "for doc in kb.list_docs(tags={\"nlp\"}):\n",
    "    print(f\"  - {doc.name}\")\n",
    "\n",
    "print()\n",
    "print(\"Vision + fine-tuning:\")\n",
    "for doc in kb.list_docs(tags={\"vision\", \"fine-tuning\"}):\n",
    "    print(f\"  - {doc.name}  tags={doc.tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags:\n",
      " - architecture\n",
      " - diffusion\n",
      " - fine-tuning\n",
      " - generative\n",
      " - nlp\n",
      " - pretraining\n",
      " - retrieval\n",
      " - transformers\n",
      " - vision\n"
     ]
    }
   ],
   "source": [
    "# List all tags in the kb\n",
    "\n",
    "tags = kb.list_tags()\n",
    "\n",
    "print(\"Tags:\")\n",
    "for tag in sorted(tags):\n",
    "    print(f\" - {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.708] Attention Is All You Need.pdf\n",
      "## 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU *[16]*...\n",
      "\n",
      "[8.615] BERT.pdf\n",
      "# 3.2 Fine-tuning BERT\n",
      "\n",
      "Fine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT...\n",
      "\n",
      "[5.722] High-Resolution Image Synthesis with Latent Diffusion Models.pdf\n",
      "Table 15. Hyperparameters for the conditional LDMs from Sec. 4. All models trained on a single NVIDIA A100 except for th...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BM25 (keyword) search\n",
    "query = \"self-attention mechanism\"\n",
    "results = kb.search_docs(query, top_k=3, strategy=\"bm25\")\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.3f}] {hit.name}\")\n",
    "    if hit.snippet:\n",
    "        print(f\"{hit.snippet[:120]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.367] High-Resolution Image Synthesis with Latent Diffusion Models.pdf\n",
      "# 1. Introduction\n",
      "\n",
      "Image synthesis is one of the computer vision fields with the most spectacular recent development, bu...\n",
      "\n",
      "[0.239] DDPM.pdf\n",
      "# 4.4 Interpolation\n",
      "\n",
      "We can interpolate source images  $\\mathbf{x}_0, \\mathbf{x}_0' \\sim q(\\mathbf{x}_0)$  in latent spa...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vector (semantic) search\n",
    "query = \"how to generate realistic images?\"\n",
    "results = kb.search_docs(query, top_k=3, strategy=\"vector\")\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.3f}] {hit.name}\")\n",
    "    if hit.snippet:\n",
    "        print(f\"{hit.snippet[:120]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0280] LORA.pdf\n",
      "# LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\n",
      "\n",
      "Edward Hu* Yelong Shen* Phillip Wallis Zeyuan Allen-Zhu\n",
      "\n",
      "Yuanzhi L...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid search (default)\n",
    "# Combines BM25 and vector search with Reciprocal Rank Fusion.\n",
    "query = \"low-rank adaptation for large language models\"\n",
    "results = kb.search_docs(query, top_k=3)\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.4f}] {hit.name}\")\n",
    "    if hit.snippet:\n",
    "        print(f\"{hit.snippet[:120]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0280] Language Models are Few-Shot Learners.pdf\n",
      "# 3.9.4 News Article Generation\n",
      "\n",
      "Previous work on generative language models qualitatively tested their ability to gener...\n",
      "\n",
      "[0.0267] RAG.pdf\n",
      "- [47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller....\n",
      "\n",
      "[0.0137] LORA.pdf\n",
      "- Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Trai...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search with tag filtering\n",
    "# Restrict search to documents matching specific tags.\n",
    "query = \"generative models\"\n",
    "results = kb.search_docs(query, top_k=3, tags={\"nlp\"})\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.4f}] {hit.name}\")\n",
    "    if hit.snippet:\n",
    "        print(f\"{hit.snippet[:120]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5000] BERT.pdf\n"
     ]
    }
   ],
   "source": [
    "# Search by name\n",
    "query = \"BERT\"\n",
    "results = kb.search_docs(query, scope=\"names\", strategy=\"bm25\")\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.4f}] {hit.name}\")\n",
    "    if hit.snippet:\n",
    "        print(f\"{hit.snippet[:120]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search within a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.014] 408-429\n",
      "#### Masked LM and the Masking Procedure\n",
      "\n",
      "Assuming the unlabeled sentence is my dog is hairy, and du...\n",
      "\n",
      "[0.014] 139-147\n",
      "# 4.2 SQuAD v1.1\n",
      "\n",
      "The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd...\n",
      "\n",
      "[0.014] 475-491\n",
      "### A.5 Illustrations of Fine-tuning on Different Tasks\n",
      "\n",
      "The illustration of fine-tuning BERT on dif...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a specific paper and search for content inside it\n",
    "\n",
    "doc_ids = {doc.name: doc.id for doc in docs}\n",
    "attention_id = doc_ids[\"BERT.pdf\"]\n",
    "query = \"What is SQuAD?\"\n",
    "\n",
    "results = kb.search_doc_contents(attention_id, query, top_k=3)\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.3f}] {hit.line_range[0]}-{hit.line_range[1]}\")\n",
    "    print(f\"{hit.text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.028] 58-70\n",
      "# 3 BERT\n",
      "\n",
      "We introduce BERT and its detailed implementation in this section. There are two steps in ...\n",
      "\n",
      "[0.028] 1-11\n",
      "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "\n",
      "Jacob Devlin Min...\n",
      "\n",
      "[0.014] 103-115\n",
      "# 3.2 Fine-tuning BERT\n",
      "\n",
      "Fine-tuning is straightforward since the self-attention mechanism in the Tra...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a specific paper and search for content inside it\n",
    "\n",
    "doc_ids = {doc.name: doc.id for doc in docs}\n",
    "attention_id = doc_ids[\"BERT.pdf\"]\n",
    "query = \"Describe BERT’s model architecture\"\n",
    "\n",
    "results = kb.search_doc_contents(attention_id, query, top_k=3)\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.3f}] {hit.line_range[0]}-{hit.line_range[1]}\")\n",
    "    print(f\"{hit.text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read specific excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines 1-20 of 1791\n",
      "\n",
      "# Language Models are Few-Shot Learners\n",
      "\n",
      "|  Tom B. Brown* |   | Benjamin Mann* |   | Nick Ryder* |   | Melanie Subbiah*  |   |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "|  Jared Kaplan† | Prafulla Dhariwal | Arvind Neelakantan | Pranav Shyam | Girish Sastry |  |  |   |\n",
      "|  Amanda Askell | Sandhini Agarwal | Ariel Herbert-Voss | Gretchen Krueger | Tom Henighan |  |  |   |\n",
      "|  Rewon Child | Aditya Ramesh | Daniel M. Ziegler | Jeffrey Wu | Clemens Winter |  |  |   |\n",
      "|  Christopher Hesse | Mark Chen | Eric Sigler | Mateusz Litwin | Scott Gray |  |  |   |\n",
      "|  Benjamin Chess |   | Jack Clark |   | Christopher Berner |   |  |   |\n",
      "|  Sam McCandlish |   | Alec Radford | Ilya Sutskever | Dario Amodei |   |  |   |\n",
      "\n",
      "OpenAI\n",
      "\n",
      "# Abstract\n",
      "\n",
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters,  $10\\mathrm{x}$  more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n",
      "\n",
      "Author contributions listed at end of paper.\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Read the first 30 lines of a paper\n",
    "\n",
    "gpt3_id = doc_ids[\"Language Models are Few-Shot Learners.pdf\"]\n",
    "excerpt = kb.read_doc(gpt3_id, start_line=1, end_line=20)\n",
    "\n",
    "print(f\"Lines {excerpt.line_range[0]}-{excerpt.line_range[1]} of {excerpt.total_lines}\\n\")\n",
    "print(excerpt.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use table of contents to navigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS [lines 1-16]\n",
      "- ABSTRACT [lines 17-20]\n",
      "- 1 INTRODUCTION [lines 21-86]\n",
      "        - Terminologies and Conventions [lines 41-44]\n",
      "  - 2 Problem Statement [lines 45-62]\n",
      "  - 3 Aren’t Existing Solutions Good Enough? [lines 63-86]\n",
      "      - Adapter Layers Introduce Inference Latency [lines 67-72]\n",
      "      - Directly Optimizing the Prompt is Hard [lines 73-86]\n",
      "- 4 OUR METHOD [lines 87-90]\n",
      "- 4.1 LOW-RANK-PARAMETRIZED UPDATE MATRICES [lines 91-173]\n",
      "    - 4.2 Applying LoRA to Transformer [lines 107-116]\n",
      "      - Practical Benefits and Limitations. [lines 111-116]\n",
      "  - 5 Empirical Experiments [lines 117-173]\n",
      "    - 5.1 Baselines [lines 121-173]\n",
      "- 5.2 ROBERTA BASE/LARGE [lines 174-177]\n",
      "- 5.3 DEBERTA XXL [lines 178-181]\n",
      "- 5.4 GPT-2 MEDIUM/LARGE [lines 182-199]\n",
      "- 5.5 SCALING UP TO GPT-3 175B [lines 200-210]\n",
      "- 6 RELATED WORKS [lines 211-238]\n",
      "      - Prompt Engineering and Fine-Tuning. [lines 217-220]\n",
      "      - Parameter-Efficient Adaptation. [lines 221-224]\n",
      "      - Low-Rank Structures in Deep Learning. [lines 225-228]\n",
      "  - 7 Understanding the Low-Rank Updates [lines 229-238]\n",
      "- 7.1 WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO? [lines 239-250]\n",
      "- 7.2 WHAT IS THE OPTIMAL RANK  $r$  FOR LORA? [lines 251-311]\n",
      "    - 7.3 How does the Adaptation Matrix $\\Delta W$ compare to $W$? [lines 291-311]\n",
      "- 8 CONCLUSION AND FUTURE WORK [lines 312-396]\n",
      "  - References [lines 320-385]\n",
      "  - Appendix A Large Language Models Still Need Parameter Updates [lines 386-396]\n",
      "- B INFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS [lines 397-405]\n",
      "- C DATASET DETAILS [lines 406-477]\n",
      "  - Appendix D Hyperparameters Used in Experiments [lines 422-477]\n",
      "    - D.1 RoBERTa [lines 424-427]\n",
      "    - D.2 DeBERTa [lines 428-477]\n",
      "- D.3 GPT-2 [lines 478-481]\n",
      "- D.4 GPT-3 [lines 482-524]\n",
      "- E COMBINING LORA WITH Prefix TUNING [lines 525-545]\n",
      "- F ADDITIONAL EMPIRICAL EXPERIMENTS [lines 546-547]\n",
      "- F.1 ADDITIONAL EXPERIMENTS ON GPT-2 [lines 548-591]\n",
      "- F.2 ADDITIONAL EXPERIMENTS ON GPT-3 [lines 592-595]\n",
      "- F.3 LOW-DATA REGIME [lines 596-601]\n",
      "- G MEASURING SIMILARITY BETWEEN SUBSPACES [lines 602-681]\n",
      "- H ADDITIONAL EXPERIMENTS ON LOW-RANK MATRICES [lines 682-685]\n",
      "- H.1 CORRELATION BETWEEN LORA MODULES [lines 686-689]\n",
      "- H.2 EFFECT OF  $r$  ON GPT-2 [lines 690-693]\n",
      "- H.3 CORRELATION BETWEEN  $W$  AND  $\\Delta W$ [lines 694-704]\n",
      "- H.4 AMPLIFICATION FACTOR [lines 705-733]\n"
     ]
    }
   ],
   "source": [
    "lora_id = doc_ids[\"LORA.pdf\"]\n",
    "lora_docs = [d for d in kb.list_docs() if d.id == lora_id]\n",
    "\n",
    "print(lora_docs[0].table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale [lines 1-48]\n",
      "          - Abstract [lines 10-13]\n",
      "  - 1 Introduction [lines 14-27]\n",
      "  - 2 Related Work [lines 28-48]\n",
      "- 3 METHOD [lines 49-52]\n",
      "- 3.1 VISION TRANSFORMER (VIT) [lines 53-109]\n",
      "      - Inductive bias. [lines 70-73]\n",
      "      - Hybrid Architecture. [lines 74-77]\n",
      "    - 3.2 Fine-tuning and Higher Resolution [lines 78-81]\n",
      "  - 4 Experiments [lines 82-109]\n",
      "    - 4.1 Setup [lines 86-109]\n",
      "- 4.2 COMPARISON TO STATE OF THE ART [lines 110-137]\n",
      "- 4.3 PRE-TRAINING DATA REQUIREMENTS [lines 138-164]\n",
      "- 4.4 SCALING STUDY [lines 165-170]\n",
      "- 4.5 INSPECTING VISION TRANSFORMER [lines 171-183]\n",
      "- 4.6 SELF-SUPERVISION [lines 184-199]\n",
      "- 5 CONCLUSION [lines 200-205]\n",
      "- ACKNOWLEDGEMENTS [lines 206-209]\n",
      "- REFERENCES [lines 210-328]\n",
      "- APPENDIX [lines 329-330]\n",
      "- A MULTIHEAD SELF-ATTENTION [lines 331-352]\n",
      "- B EXPERIMENT DETAILS [lines 353-354]\n",
      "- B.1 TRAINING [lines 355-358]\n",
      "- B.1.1 FINE-TUNING [lines 359-381]\n",
      "- B.1.2 SELF-SUPERVISION [lines 382-389]\n",
      "- C ADDITIONAL RESULTS [lines 390-443]\n",
      "- D ADDITIONAL ANALYSES [lines 444-445]\n",
      "- D.1 SGD VS. ADAM FOR RESNETS [lines 446-470]\n",
      "- D.2 TRANSFORMER SHAPE [lines 471-474]\n",
      "- D.3 HEAD TYPE AND CLASSTOKEN [lines 475-496]\n",
      "- D.4 POSITIONAL EMBEDDING [lines 497-525]\n",
      "- D.5 EMPIRICAL COMPUTATIONAL COSTS [lines 526-540]\n",
      "- D.6 AXIAL ATTENTION [lines 541-557]\n",
      "- D.7 ATTENTION DISTANCE [lines 558-561]\n",
      "- D.8 ATTENTION MAPS [lines 562-565]\n",
      "- D.9 OBJECTNET RESULTS [lines 566-569]\n",
      "- D.10 VTAB BREAKDOWN [lines 570-587]\n"
     ]
    }
   ],
   "source": [
    "lora_id = doc_ids[\"ViT.pdf\"]\n",
    "lora_docs = [d for d in kb.list_docs() if d.id == lora_id]\n",
    "\n",
    "print(lora_docs[0].table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage tags\n",
    "\n",
    "Tags can be added or removed after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a tag\n",
    "kb.tag_doc(attention_id, {\"seminal\", \"google\"})\n",
    "\n",
    "# Remove a tag\n",
    "kb.untag_doc(attention_id, {\"google\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "The knowledge base persists in `STORAGE_DIR`. Delete it to start fresh:\n",
    "\n",
    "```python\n",
    "import shutil\n",
    "shutil.rmtree(STORAGE_DIR)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
