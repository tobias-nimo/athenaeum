{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Athenaeum Playground\n",
    "\n",
    "This notebook walks through the core features of **Athenaeum** — a Python library for building searchable knowledge bases from documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```python\n",
    "pip install -q athenaeum-kb[mistral]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install '../dist/athenaeum_kb-0.2.0-py3-none-any.whl[mistral]' -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass('OPENAI_API_KEY: ')\n",
    "MISTRAL_API_KEY = getpass.getpass('MISTRAL_API_KEY: ')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ['MISTRAL_API_KEY'] = MISTRAL_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DOCS_DIR = Path(\"knowledge-base\")\n",
    "STORAGE_DIR = Path(\".athenaeum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athenaeum import Athenaeum, AthenaeumConfig, get_ocr_provider\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "config = AthenaeumConfig(\n",
    "    storage_dir=STORAGE_DIR,\n",
    "    chunk_size=750,\n",
    "    chunk_overlap=75,\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=OPENAI_API_KEY)\n",
    "ocr = get_ocr_provider(\"mistral\")\n",
    "\n",
    "kb = Athenaeum(\n",
    "    embeddings=embeddings,\n",
    "    ocr_provider=ocr,\n",
    "    config=config\n",
    ") # Athenaeum knowledge-base is ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents\n",
    "\n",
    "We load the PDF papers from `DOCS_DIR`, assigning tags by research area so we can filter later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Attention Is All You Need.pdf -> ac66f9901c89  (tags: {'transformers', 'architecture', 'nlp'})\n",
      "Loaded: BERT.pdf -> 4af474727685  (tags: {'transformers', 'pretraining', 'nlp'})\n",
      "Loaded: XLNet.pdf -> 3c2293657e6c  (tags: {'transformers', 'pretraining', 'nlp'})\n",
      "Loaded: Language Models are Few-Shot Learners.pdf -> 6525a2245e91  (tags: {'transformers', 'pretraining', 'generative', 'nlp'})\n",
      "Loaded: LORA.pdf -> e6f9a937828a  (tags: {'transformers', 'fine-tuning', 'nlp'})\n",
      "Loaded: RAG.pdf -> d610388cdccd  (tags: {'transformers', 'retrieval', 'nlp'})\n",
      "Loaded: ViT.pdf -> c7a13d650516  (tags: {'vision', 'architecture', 'transformers'})\n",
      "Loaded: GANs.pdf -> 4f67de6b50af  (tags: {'vision', 'generative'})\n",
      "Loaded: VAE.pdf -> e502d1a1cb3d  (tags: {'vision', 'generative'})\n",
      "Loaded: DDPM.pdf -> c60d566af74b  (tags: {'vision', 'generative', 'diffusion'})\n",
      "Loaded: High-Resolution Image Synthesis with Latent Diffusion Models.pdf -> 09d7abb38688  (tags: {'vision', 'generative', 'diffusion'})\n",
      "\n",
      "Total documents: 11\n"
     ]
    }
   ],
   "source": [
    "papers = {\n",
    "    \"Attention Is All You Need.pdf\": {\"nlp\", \"transformers\", \"architecture\"},\n",
    "    \"BERT.pdf\":                      {\"nlp\", \"transformers\", \"pretraining\"},\n",
    "    \"XLNet.pdf\":                     {\"nlp\", \"transformers\", \"pretraining\"},\n",
    "    \"Language Models are Few-Shot Learners.pdf\": {\"nlp\", \"transformers\", \"pretraining\", \"generative\"},\n",
    "    \"LORA.pdf\":                      {\"nlp\", \"transformers\", \"fine-tuning\"},\n",
    "    \"RAG.pdf\":                       {\"nlp\", \"transformers\", \"retrieval\"},\n",
    "    \"ViT.pdf\":                       {\"vision\", \"transformers\", \"architecture\"},\n",
    "    \"GANs.pdf\":                      {\"vision\", \"generative\"},\n",
    "    \"VAE.pdf\":                       {\"vision\", \"generative\"},\n",
    "    \"DDPM.pdf\":                      {\"vision\", \"generative\", \"diffusion\"},\n",
    "    \"High-Resolution Image Synthesis with Latent Diffusion Models.pdf\": {\"vision\", \"generative\", \"diffusion\"},\n",
    "}\n",
    "\n",
    "for filename, tags in papers.items():\n",
    "    path = DOCS_DIR / filename\n",
    "    if not path.exists():\n",
    "        print(f\"Skipping (not found): {filename}\")\n",
    "        continue\n",
    "    doc_id = kb.load_doc(str(path), tags=tags)\n",
    "    print(f\"[INFO] Loaded {filename} with ID {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ac66f9901c89  Attention Is All You Need.pdf                               \n",
      "4af474727685  BERT.pdf                                                    \n",
      "3c2293657e6c  XLNet.pdf                                                   \n",
      "6525a2245e91  Language Models are Few-Shot Learners.pdf                   \n",
      "e6f9a937828a  LORA.pdf                                                    \n",
      "d610388cdccd  RAG.pdf                                                     \n",
      "c7a13d650516  ViT.pdf                                                     \n",
      "4f67de6b50af  GANs.pdf                                                    \n",
      "e502d1a1cb3d  VAE.pdf                                                     \n",
      "c60d566af74b  DDPM.pdf                                                    \n",
      "09d7abb38688  High-Resolution Image Synthesis with Latent Diffusion Models.pdf\n"
     ]
    }
   ],
   "source": [
    "# All documents\n",
    "docs = kb.list_docs()\n",
    "for doc in docs:\n",
    "    print(f\"{doc.id}  {doc.name:<60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion papers:\n",
      "  - DDPM.pdf\n",
      "  - High-Resolution Image Synthesis with Latent Diffusion Models.pdf\n",
      "\n",
      " NLP papers:\n",
      "  - Attention Is All You Need.pdf\n",
      "  - BERT.pdf\n",
      "  - XLNet.pdf\n",
      "  - Language Models are Few-Shot Learners.pdf\n",
      "  - LORA.pdf\n",
      "  - RAG.pdf\n",
      "\n",
      "Vision + fine-tuning:\n",
      "  - LORA.pdf  tags={'transformers', 'fine-tuning', 'nlp'}\n",
      "  - ViT.pdf  tags={'vision', 'architecture', 'transformers'}\n",
      "  - GANs.pdf  tags={'vision', 'generative'}\n",
      "  - VAE.pdf  tags={'vision', 'generative'}\n",
      "  - DDPM.pdf  tags={'vision', 'generative', 'diffusion'}\n",
      "  - High-Resolution Image Synthesis with Latent Diffusion Models.pdf  tags={'vision', 'generative', 'diffusion'}\n"
     ]
    }
   ],
   "source": [
    "# Filter by tags\n",
    "# Tags use OR semantics — any document matching at least one of the given tags is returned.\n",
    "\n",
    "print(\"Diffusion papers:\")\n",
    "for doc in kb.list_docs(tags={\"diffusion\"}):\n",
    "    print(f\"  - {doc.name}\")\n",
    "\n",
    "print()\n",
    "print(\" NLP papers:\")\n",
    "for doc in kb.list_docs(tags={\"nlp\"}):\n",
    "    print(f\"  - {doc.name}\")\n",
    "\n",
    "print()\n",
    "print(\"Vision + fine-tuning:\")\n",
    "for doc in kb.list_docs(tags={\"vision\", \"fine-tuning\"}):\n",
    "    print(f\"  - {doc.name}  tags={doc.tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags:\n",
      " - architecture\n",
      " - diffusion\n",
      " - fine-tuning\n",
      " - generative\n",
      " - nlp\n",
      " - pretraining\n",
      " - retrieval\n",
      " - seminal\n",
      " - transformers\n",
      " - vision\n"
     ]
    }
   ],
   "source": [
    "# List all tags in the knowledge base\n",
    "\n",
    "tags = kb.list_tags()\n",
    "\n",
    "print(\"Tags:\")\n",
    "for tag in sorted(tags):\n",
    "    print(f\" - {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.972] Attention Is All You Need.pdf\n",
      "# Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani*\n",
      "\n",
      "Google Brain\n",
      "\n",
      "avaswani@google.com\n",
      "\n",
      "Noam Shazeer*\n",
      "\n",
      "Google Brain\n",
      "\n",
      "noam@googl...\n",
      "\n",
      "[1.761] XLNet.pdf\n",
      "# XLNet: Generalized Autoregressive Pretraining for Language Understanding\n",
      "\n",
      "Zhilin Yang^{∗1}, Zihang Dai^{∗12}, Yiming Y...\n",
      "\n",
      "[1.583] BERT.pdf\n",
      "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "\n",
      "Jacob Devlin Ming-Wei Chang Kenton L...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BM25 (keyword) search\n",
    "query = \"self-attention mechanism\"\n",
    "results = kb.search_docs(query, top_k=3, strategy=\"bm25\")\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.3f}] {hit.name}\")\n",
    "    if hit.snippet:\n",
    "        print(f\"{hit.snippet[:120]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.103] High-Resolution Image Synthesis with Latent Diffusion Models.pdf\n",
      "# High-Resolution Image Synthesis with Latent Diffusion Models\n",
      "\n",
      "Robin Rombach $^{1*}$  Andreas Blattmann $^{1*}$  Domini...\n",
      "[0.045] GANs.pdf\n",
      "# Generative Adversarial Nets\n",
      "\n",
      "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\n",
      "Sherjil ...\n",
      "[-0.000] DDPM.pdf\n",
      "# Denoising Diffusion Probabilistic Models\n",
      "\n",
      "Jonathan Ho\n",
      "\n",
      "UC Berkeley\n",
      "\n",
      "jonathanho@berkeley.edu\n",
      "\n",
      "Ajay Jain\n",
      "\n",
      "UC Berkeley\n",
      "\n",
      "a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasnimo/Documents/athenaeum/playground/.venv/lib/python3.13/site-packages/athenaeum/search/vector.py:61: UserWarning: Relevance scores must be between 0 and 1, got [(Document(id='09d7abb38688:0', metadata={'start_line': 1, 'end_line': 750, 'doc_id': '09d7abb38688', 'text': '# High-Resolution Image Synthesis with Latent Diffusion Models\\n\\nRobin Rombach $^{1*}$  Andreas Blattmann $^{1*}$  Dominik Lorenz $^{1}$  Patrick Esser $^{\\\\text{圆}}$  Björn Ommer $^{1}$\\n\\n$^{1}$ Ludwig Maximilian University of Munich &amp; IWR, Heidelberg University, Germany  $\\\\text{圆}$ Runway ML\\n\\nhttps://github.com/CompVis/latent-diffusion\\n\\n# Abstract\\n\\nBy decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.\\n\\n# 1. Introduction\\n\\nImage synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands. Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66,67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1. Boosting the upper bound on achievable quality with less aggressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at  $512^{2}$  px. We denote the spatial downsampling factor by  $f$ . Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\\n\\nresults in image synthesis [30,85] and beyond [7,45,48,57], and define the state-of-the-art in class-conditional image synthesis [15,31] and super-resolution [72]. Moreover, even unconditional DMs can readily be applied to tasks such as inpainting and colorization [85] or stroke-based synthesis [53], in contrast to other types of generative models [19,46,69]. Being likelihood-based models, they do not exhibit mode-collapse and training instabilities as GANs and, by heavily exploiting parameter sharing, they can model highly complex distributions of natural images without involving billions of parameters as in AR models [67].\\n\\nDemocratizing High-Resolution Image Synthesis DMs belong to the class of likelihood-based models, whose mode-covering behavior makes them prone to spend excessive amounts of capacity (and thus compute resources) on modeling imperceptible details of the data [16, 73]. Although the reweighted variational objective [30] aims to address this by undersampling the initial denoising steps, DMs are still computationally demanding, since training and evaluating such a model requires repeated function evaluations (and gradient computations) in the high-dimensional space of RGB images. As an example, training the most powerful DMs often takes hundreds of GPU days (e.g. 150 - 1000 V100 days in [15]) and repeated evaluations on a noisy version of the input space render also inference expensive,\\n\\nso that producing 50k samples takes approximately 5 days [15] on a single A100 GPU. This has two consequences for the research community and users in general: Firstly, training such a model requires massive computational resources only available to a small fraction of the field, and leaves a huge carbon footprint [65,86]. Secondly, evaluating an already trained model is also expensive in time and memory, since the same model architecture must run sequentially for a large number of steps (e.g. 25 - 1000 steps in [15]).\\n\\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\\n\\nDeparture to Latent Space Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a perceptual compression stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (semantic compression). We thus aim to first find a perceptually equivalent, but computationally more suitable space, in which we will train diffusion models for high-resolution image synthesis.\\n\\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class Latent Diffusion Models (LDMs).\\n\\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\\'s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\\n\\nIn sum, our work makes the following contributions:\\n\\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2. Illustrating perceptual and semantic compression: Most bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details. Data and images from [30].\\n\\napplied to high-resolution synthesis of megapixel images.\\n\\n(ii) We achieve competitive performance on multiple tasks (unconditional image synthesis, inpainting, stochastic super-resolution) and datasets while significantly lowering computational costs. Compared to pixel-based diffusion approaches, we also significantly decrease inference costs.\\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of  $\\\\sim 1024^2$  px.\\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81].\\n\\n# 2. Related Work\\n\\nGenerative Models for Image Synthesis The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) [27] allow for efficient sampling of high resolution images with good perceptual quality [3, 42], but are diffi\\n\\nult to optimize *[2, 28, 54]* and struggle to capture the full data distribution *[55]*. In contrast, likelihood-based methods emphasize good density estimation which renders optimization more well-behaved. Variational autoencoders (VAE) *[46]* and flow-based models *[18, 19]* enable efficient synthesis of high resolution images *[9, 44, 92]*, but sample quality is not on par with GANs. While autoregressive models (ARM) *[6, 10, 94, 95]* achieve strong performance in density estimation, computationally demanding architectures *[97]* and a sequential sampling process limit them to low resolution images. Because pixel based representations of images contain barely perceptible, high-frequency details *[16, 73]*, maximum-likelihood training spends a disproportionate amount of capacity on modeling them, resulting in long training times. To scale to higher resolutions, several two-stage approaches *[23, 67, 101, 103]* use ARMs to model a compressed latent image space instead of raw pixels.\\n\\nRecently, Diffusion Probabilistic Models (DM) *[82]*, have achieved state-of-the-art results in density estimation *[45]* as well as in sample quality *[15]*. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet *[15, 30, 71, 85]*. The best synthesis quality is usually achieved when a reweighted objective *[30]* is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies *[47, 75, 84]* and hierarchical approaches *[31, 93]*, training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed LDMs, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).\\n\\nTwo-Stage Image Synthesis To mitigate the shortcomings of individual generative approaches, a lot of research *[11, 23, 67, 70, 101, 103]* has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs *[67, 101]* use autoregressive models to learn an expressive prior over a discretized latent space. *[66]* extend this approach to text-to-image generation by learning a joint distribution over discretized image and text representations. More generally, *[70]* uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs *[23, 103]* employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters *[23, 66]*, limit the overall performance of such approaches and less compression comes at the price of high computational cost *[23, 66]*. Our work prevents such trade-offs, as our proposed LDMs scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing high-fidelity reconstructions (see Fig. 1).\\n\\nWhile approaches to jointly *[93]* or separately *[80]* learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities *[11]* and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces.\\n\\n## 3 Method\\n\\nTo lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms *[30]*, they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources.\\n\\nWe propose to circumvent this drawback by introducing an explicit separation of the compressive from the generative learning phase (see Fig. 2). To achieve this, we utilize an autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly reduced computational complexity.\\n\\nSuch an approach offers several advantages: (i) By leaving the high-dimensional image space, we obtain DMs which are computationally much more efficient because sampling is performed on a low-dimensional space. (ii) We exploit the inductive bias of DMs inherited from their UNet architecture *[71]*, which makes them particularly effective for data with spatial structure and therefore alleviates the need for aggressive, quality-reducing compression levels as required by previous approaches *[23, 66]*. (iii) Finally, we obtain general-purpose compression models whose latent space can be used to train multiple generative models and which can also be utilized for other downstream applications such as single-image CLIP-guided synthesis *[25]*.\\n\\n### 3.1 Perceptual Image Compression\\n\\nOur perceptual compression model is based on previous work *[23]* and consists of an autoencoder trained by combination of a perceptual loss *[106]* and a patch-based *[33]* adversarial objective *[20, 23, 103]*. This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids bluriness introduced by relying solely on pixel-space losses such as $L_{2}$ or $L_{1}$ objectives.\\n\\nMore precisely, given an image $x\\\\in\\\\mathbb{R}^{H\\\\times W\\\\times 3}$ in RGB space, the encoder $\\\\mathcal{E}$ encodes $x$ into a latent representa\\n\\ntion $z=\\\\mathcal{E}(x)$, and the decoder $\\\\mathcal{D}$ reconstructs the image from the latent, giving $\\\\tilde{x}=\\\\mathcal{D}(z)=\\\\mathcal{D}(\\\\mathcal{E}(x))$, where $z\\\\in\\\\mathbb{R}^{h\\\\times w\\\\times c}$. Importantly, the encoder *downsamples* the image by a factor $f=H/h=W/w$, and we investigate different downsampling factors $f=2^{m}$, with $m\\\\in\\\\mathbb{N}$.\\n\\nIn order to avoid arbitrarily high-variance latent spaces, we experiment with two different kinds of regularizations. The first variant, *KL-reg.*, imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE *[46, 69]*, whereas *VQ-reg.* uses a vector quantization layer *[96]* within the decoder. This model can be interpreted as a VQGAN *[23]* but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space $z=\\\\mathcal{E}(x)$, we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works *[23, 66]*, which relied on an arbitrary 1D ordering of the learned space $z$ to model its distribution autoregressively and thereby ignored much of the inherent structure of $z$. Hence, our compression model preserves details of $x$ better (see Tab. 8). The full objective and training details can be found in the supplement.\\n\\n### 3.2 Latent Diffusion Models\\n\\nDiffusion Models *[82]* are probabilistic models designed to learn a data distribution $p(x)$ by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length $T$. For image synthesis, the most successful models *[15, 30, 72]* rely on a reweighted variant of the variational lower bound on $p(x)$, which mirrors denoising score-matching *[85]*. These models can be interpreted as an equally weighted sequence of denoising autoencoders $\\\\epsilon_{\\\\theta}(x_{t},t);\\\\,t=1\\\\dots T$, which are trained to predict a denoised variant of their input $x_{t}$, where $x_{t}$ is a noisy version of the input $x$. The corresponding objective can be simplified to (Sec. B)\\n\\n$L_{DM}=\\\\mathbb{E}_{x,\\\\epsilon\\\\sim\\\\mathcal{N}(0,1),t}\\\\Big{[}\\\\|\\\\epsilon-\\\\epsilon_{\\\\theta}(x_{t},t)\\\\|_{2}^{2}\\\\Big{]}\\\\,,$ (1)\\n\\nwith $t$ uniformly sampled from $\\\\{1,\\\\dots,T\\\\}$.\\n\\nGenerative Modeling of Latent Representations With our trained perceptual compression models consisting of $\\\\mathcal{E}$ and $\\\\mathcal{D}$, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.\\n\\nUnlike previous work that relied on autoregressive, attention-based transformer models in a highly compressed, discrete latent space *[23, 66, 103]*, we can take advantage of image-specific inductive biases that our model offers. This includes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads\\n\\n$L_{LDM}:=\\\\mathbb{E}_{\\\\mathcal{E}(x),\\\\epsilon\\\\sim\\\\mathcal{N}(0,1),t}\\\\Big{[}\\\\|\\\\epsilon-\\\\epsilon_{\\\\theta}(z_{t},t)\\\\|_{2}^{2}\\\\Big{]}\\\\,.$ (2)\\n\\nThe neural backbone $\\\\epsilon_{\\\\theta}(\\\\circ,t)$ of our model is realized as a time-conditional UNet *[71]*. Since the forward process is fixed, $z_{t}$ can be efficiently obtained from $\\\\mathcal{E}$ during training, and samples from $p(z)$ can be decoded to image space with a single pass through $\\\\mathcal{D}$.\\n\\n### 3.3 Conditioning Mechanisms\\n\\nSimilar to other types of generative models *[56, 83]*, diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\\\epsilon_{\\\\theta}(z_{t},t,y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text *[68]*, semantic maps *[33, 61]* or other image-to-image translation tasks *[34]*.\\n\\nIn the context of image synthesis, however, combining the generative power of DMs with other types of conditionings beyond class-labels *[15]* or blurred variants of the input image *[72]* is so far an under-explored area of research.\\n\\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism *[97]*, which is effective for learning attention-based models of various input modalities *[35, 36]*. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\\\tau_{\\\\theta}$ that projects $y$ to an intermediate representation $\\\\tau_{\\\\theta}(y)\\\\in\\\\mathbb{R}^{M\\\\times d_{\\\\tau}}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing $\\\\text{Attention}(Q,K,V)=\\\\text{softmax}\\\\left(\\\\frac{QK^{T}}{\\\\sqrt{d}}\\\\right)\\\\cdot V$, with\\n\\n$Q=W_{Q}^{(i)}\\\\cdot\\\\varphi_{i}(z_{t}),\\\\;K=W_{K}^{(i)}\\\\cdot\\\\tau_{\\\\theta}(y),\\\\;V=W_{V}^{(i)}\\\\cdot\\\\tau_{\\\\theta}(y).$\\n\\nHere, $\\\\varphi_{i}(z_{t})\\\\in\\\\mathbb{R}^{N\\\\times d_{\\\\epsilon}^{i}}$ denotes a (flattened) intermediate representation of the UNet implementing $\\\\epsilon_{\\\\theta}$ and $W_{V}^{(i)}\\\\in\\\\mathbb{R}^{N\\\\times d_{\\\\epsilon}^{i}}$\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 4. Samples from LDMs trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of  $256 \\\\times 256$ . Best viewed when zoomed in. For more samples cf. the supplement.\\n\\n$\\\\mathbb{R}^{d\\\\times d_r^i}$ ,  $W_{Q}^{(i)}\\\\in \\\\mathbb{R}^{d\\\\times d_{r}}$  &amp;  $W_{K}^{(i)}\\\\in \\\\mathbb{R}^{d\\\\times d_{r}}$  are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\\n\\nBased on image-conditioning pairs, we then learn the conditional LDM via\\n\\n$$\\nL _ {L D M} := \\\\mathbb {E} _ {\\\\mathcal {E} (x), y, \\\\epsilon \\\\sim \\\\mathcal {N} (0, 1), t} \\\\left[ \\\\| \\\\epsilon - \\\\epsilon_ {\\\\theta} \\\\left(z _ {t}, t, \\\\tau_ {\\\\theta} (y)\\\\right) \\\\| _ {2} ^ {2} \\\\right], \\\\tag {3}\\n$$\\n\\nwhere both  $\\\\tau_{\\\\theta}$  and  $\\\\epsilon_{\\\\theta}$  are jointly optimized via Eq. 3. This conditioning mechanism is flexible as  $\\\\tau_{\\\\theta}$  can be parameterized with domain-specific experts, e.g. (unmasked) transformers [97] when  $y$  are text prompts (see Sec. 4.3.1)\\n\\n# 4. Experiments\\n\\nLDMs provide means to flexible and computationally tractable diffusion based image synthesis of various image modalities, which we empirically show in the following. Firstly, however, we analyze the gains of our models compared to pixel-based diffusion models in both training and inference. Interestingly, we find that LDMs trained in  $VQ$ -regularized latent spaces sometimes achieve better sample quality, even though the reconstruction capabilities of  $VQ$ -regularized first stage models slightly fall behind those of their continuous counterparts, cf. Tab. 8. A visual comparison between the effects of first stage regularization schemes on LDM training and their generalization abilities to resolutions  $&gt;256^2$  can be found in Appendix D.1. In E.2 we list details on architecture, implementation, training and evaluation for all results presented in this section.\\n\\n# 4.1. On Perceptual Compression Tradeoffs\\n\\nThis section analyzes the behavior of our LDMs with different downsampling factors  $f \\\\in \\\\{1,2,4,8,16,32\\\\}$  (abbreviated as  $LDM - f$ , where  $LDM - 1$  corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters.\\n\\nTab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the LDMs com\\n\\npared in this section. Fig. 6 shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet [12] dataset. We see that, i) small downsampling factors for  $LDM - \\\\{1,2\\\\}$  result in slow training progress, whereas ii) overly large values of  $f$  cause stagnating fidelity after comparably few training steps. Revisiting the analysis above (Fig. 1 and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality.  $LDM - \\\\{4 - 16\\\\}$  strike a good balance between efficiency and perceptually faithful results, which manifests in a significant FID [29] gap of 38 between pixel-based diffusion ( $LDM - 1$ ) and  $LDM - 8$  after 2M training steps.\\n\\nIn Fig. 7, we compare models trained on CelebAHQ [39] and ImageNet in terms of sampling speed for different numbers of denoising steps with the DDIM sampler [84] and plot it against FID-scores [29].  $LDM - \\\\{4 - 8\\\\}$  outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based  $LDM - 1$ , they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. In summary,  $LDM - 4$  and  $-8$  offer the best conditions for achieving high-quality synthesis results.\\n\\n# 4.2. Image Generation with Latent Diffusion\\n\\nWe train unconditional models of  $256^{2}$  images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space\\n\\nText-to-Image Synthesis on LAION. 1.45B Model.\\n\\n|  \\'A street sign that reads \"Latent Diffusion\" \\' | \\'A zombie in the style of Picasso\\' | \\'An image of an animal half mouse half octopus\\' | \\'An illustration of a slightly conscious neural network\\' | \\'A painting of a squirrel eating a burger\\' | \\'A watercolor painting of a chair that looks like an octopus\\' | \\'A shirt with the inscription: \"I love generative models!\" \\'  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  LATENT DIFFUSION |  |  |  |  |  |   |\\n|  LATETEN DIFFUSION |  |  |  |  |  |   |\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 5. Samples for user-defined text prompts from our model for text-to-image synthesis, LDM-8 (KL), which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and  $\\\\eta = 1.0$ . We use unconditional guidance [32] with  $s = 10.0$ .\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 6. Analyzing the training of class-conditional LDMs with different downsampling factors  $f$  over 2M train steps on the ImageNet dataset. Pixel-based LDM-1 requires substantially larger train times compared to models with larger downsampling factors (LDM-{4-16}). Too much perceptual compression as in LDM-32 limits the overall sample quality. All models are trained on a single NVIDIA A100 with the same computational budget. Results obtained with 100 DDIM steps [84] and  $\\\\kappa = 0$ .\\nFigure 7. Comparing LDMs with varying compression on the CelebA-HQ (left) and ImageNet (right) datasets. Different markers indicate  $\\\\{10,20,50,100,200\\\\}$  sampling steps using DDIM, from right to left along each line. The dashed line shows the FID scores for 200 steps, indicating the strong performance of LDM- $\\\\{4-8\\\\}$ . FID scores assessed on 5000 samples. All models were trained for  $500\\\\mathrm{k}$  (CelebA)/2M (ImageNet) steps on an A100.\\n\\nand avoid the difficulty of weighing reconstruction quality against learning the prior over the latent space, see Fig. 1-2.\\n\\nWe outperform prior diffusion based approaches on all but the LSUN-Bedrooms dataset, where our score is close to ADM [15], despite utilizing half its parameters and requiring 4-times less train resources (see Appendix E.3.5).\\n\\n|  CelebA-HQ 256 × 256 |   |   |   | FFHQ 256 × 256  |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Method | FID ↓ | Prec. ↑ | Recall ↑ | Method | FID ↓ | Prec. ↑ | Recall ↑  |\\n|  DC-VAE [63] | 15.8 | - | - | ImageBART [21] | 9.57 | - | -  |\\n|  VQGAN-T [23] (n=400) | 10.2 | - | - | U-Net GAN (vaug) [73] | 10.9 (7.6) | - | -  |\\n|  PGGAN [39] | 8.0 | - | - | UDM [43] | 5.54 | - | -  |\\n|  LSUM [85] | 7.22 | - | - | StyleGAN [41] | 6.16 | 0.71 | 0.46  |\\n|  UDM [43] | 7.16 | - | - | ProjectedGAN [70] | 3.88 | 0.63 | 0.46  |\\n|  LDM-4 (ours, 500-s†) | 5.11 | 0.72 | 0.49 | LDM-4 (ours, 200-s) | 4.98 | 0.73 | 0.50  |\\n|  LSUN-Chutches 256 × 256 |   |   |   | LSUN-Bedrooms 256 × 256  |   |   |   |\\n|  Method | FID ↓ | Prec. ↑ | Recall ↑ | Method | FID ↓ | Prec. ↑ | Recall ↑  |\\n|  DDPM [30] | 7.89 | - | - | ImageBART [21] | 5.51 | - | -  |\\n|  ImageBART [21] | 7.32 | - | - | DDPM [30] | 4.9 | - | -  |\\n|  PGGAN [39] | 6.42 | - | - | UDM [43] | 4.57 | - | -  |\\n|  StyleGAN [41] | 4.21 | - | - | StyleGAN [41] | 2.35 | 0.59 | 0.48  |\\n|  StyleGAN2 [42] | 3.86 | - | - | ADM [15] | 1.90 | 0.66 | 0.51  |\\n|  ProjectedGAN [70] | 1.59 | 0.61 | 0.44 | ProjectedGAN [70] | 1.52 | 0.61 | 0.34  |\\n|  LDM-8* (ours, 200-s) | 4.02 | 0.64 | 0.52 | LDM-4 (ours, 200-s) | 2.95 | 0.66 | 0.48  |\\n\\nTable 1. Evaluation metrics for unconditional image synthesis. CelebA-HQ results reproduced from [43, 63, 100], FFHQ from [42, 43].  $\\\\dagger$ :  $N$ -s refers to  $N$  sampling steps with the DDIM [84] sampler.  $\\\\star$ : trained in KL-regularized latent space. Additional results can be found in the supplementary.\\n\\n|  Text-Conditional Image Synthesis  |   |   |   |   |\\n| --- | --- | --- | --- | --- |\\n|  Method | FID ↓ | IS↑ | Nparams |   |\\n|  CogView† [17] | 27.10 | 18.20 | 4B | self-ranking, rejection rate 0.017  |\\n|  LAFITE† [109] | 26.94 | 26.02 | 75M |   |\\n|  GLIDE* [59] | 12.24 | - | 6B | 277 DDIM steps, c.f.g. [32] s = 3  |\\n|  Make-A-Scene* [26] | 11.84 | - | 4B | c.f.g for AR models [58] s = 5  |\\n|  LDM-KL-8 | 23.31 | 20.03±0.31 | 1.45B | 250 DDIM steps  |\\n|  LDM-KL-8-G* | 12.63 | 30.29±0.01 | 1.45B | 250 DDIM steps, c.f.g. [32] s = 1.5  |\\n\\nTable 2. Evaluation of text-conditional image synthesis on the  $256 \\\\times 256$ -sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters.  $\\\\dagger / \\\\star$ :Numbers from [109]/[26]\\n\\nMoreover, LDMs consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset.\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 8. Layout-to-image synthesis with an LDM on COCO [4], see Sec. 4.3.1. Quantitative evaluation in the supplement D.3.\\n\\n# 4.3. Conditional Latent Diffusion\\n\\n# 4.3.1 Transformer Encoders for LDMs\\n\\nBy introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For text-to-image image modeling, we train a 1.45B parameter  $KL$ -regularized LDM conditioned on language prompts on LAION-400M [78]. We employ the BERT-tokenizer [14] and implement  $\\\\tau_{\\\\theta}$  as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) cross-attention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, cf. Fig. 8 and 5. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [51] validation set, where our model improves upon powerful AR [17,66] and GAN-based [109] methods, cf. Tab. 2. We note that applying classifier-free diffusion guidance [32] greatly boosts sample quality, such that the guided LDM-KL-8-G is on par with the recent state-of-the-art AR [26] and diffusion models [59] for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on semantic layouts on OpenImages [49], and finetune on COCO [4], see Fig. 8. See Sec. D.3 for the quantitative evaluation and implementation details.\\n\\nLastly, following prior work [3, 15, 21, 23], we evaluate our best-performing class-conditional ImageNet models with  $f \\\\in \\\\{4,8\\\\}$  from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. D.4. Here we outperform the state of the art diffusion model ADM [15] while significantly reducing computational requirements and parameter count, cf. Tab 18.\\n\\n# 4.3.2 Convolutional Sampling Beyond  $256^{2}$\\n\\nBy concatenating spatially aligned conditioning information to the input of  $\\\\epsilon_{\\\\theta}$ ,  $LDMs$  can serve as efficient general-\\n\\n|  Method | FID↓ | IST | Precision↑ | Recall↑ | Nparams |   |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  BigGan-deep [3] | 6.95 | 203.6±14 | 0.87 | 0.28 | 340M | -  |\\n|  ADM [15] | 10.94 | 100.98 | 0.69 | 0.63 | 554M | 250 DDIM steps  |\\n|  ADM-G [15] | 4.59 | 186.7 | 0.82 | 0.52 | 600M | 250 DDIM steps  |\\n|  LDM-4 (ours) | 10.56 | 103.49±1.24 | 0.71 | 0.62 | 400M | 250 DDIM steps  |\\n|  LDM-4-G (ours) | 3.60 | 247.67±0.99 | 0.87 | 0.48 | 400M | 250 steps, c.f.g [32], s = 1.5  |\\n\\nTable 3. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on ImageNet [12]. A more detailed comparison with additional baselines can be found in D.4, Tab. 10 and F. c.f.g. denotes classifier-free guidance with a scale  $s$  as proposed in [32].\\n\\npurpose image-to-image translation models. We use this to train models for semantic synthesis, super-resolution (Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthesis, we use images of landscapes paired with semantic maps [23, 61] and concatenate downsampled versions of the semantic maps with the latent image representation of a  $f = 4$  model (VQ-reg., see Tab. 8). We train on an input resolution of  $256^2$  (crops from  $384^2$ ) but find that our model generalizes to larger resolutions and can generate images up to the megapixel regime when evaluated in a convolutional manner (see Fig. 9). We exploit this behavior to also apply the super-resolution models in Sec. 4.4 and the inpainting models in Sec. 4.5 to generate large images between  $512^2$  and  $1024^2$ . For this application, the signal-to-noise ratio (induced by the scale of the latent space) significantly affects the results. In Sec. D.1 we illustrate this when learning an LDM on (i) the latent space as provided by a  $f = 4$  model (KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by the component-wise standard deviation.\\n\\nThe latter, in combination with classifier-free guidance [32], also enables the direct synthesis of  $&gt;256^2$  images for the text-conditional LDM-KL-8-G as in Fig. 13.\\n\\n![img-6.jpeg](img-6.jpeg)\\nFigure 9. A LDM trained on  $256^{2}$  resolution can generalize to larger resolution (here:  $512 \\\\times 1024$ ) for spatially conditioned tasks such as semantic synthesis of landscape images. See Sec. 4.3.2.\\n\\n# 4.4. Super-Resolution with Latent Diffusion\\n\\nLDMs can be efficiently trained for super-resolution by directly conditioning on low-resolution images via concatenation (cf. Sec. 3.3). In a first experiment, we follow SR3\\n\\n![img-7.jpeg](img-7.jpeg)\\nFigure 10. ImageNet  $64\\\\rightarrow 256$  super-resolution on ImageNet-Val. LDM-SR has advantages at rendering realistic textures but SR3 can synthesize more coherent fine structures. See appendix for additional samples and cropouts. SR3 results from [72].\\n\\n[72] and fix the image degradation to a bicubic interpolation with  $4 \\\\times$ -downsampling and train on ImageNet following SR3\\'s data processing pipeline. We use the  $f = 4$  autoencoding model pretrained on OpenImages (VQ-reg., cf. Tab. 8) and concatenate the low-resolution conditioning  $y$  and the inputs to the UNet, i.e.  $\\\\tau_{\\\\theta}$  is the identity. Our qualitative and quantitative results (see Fig. 10 and Tab. 5) show competitive performance and LDM-SR outperforms SR3 in FID while SR3 has a better IS. A simple image regression model achieves the highest PSNR and SSIM scores; however these metrics do not align well with human perception [106] and favor blurriness over imperfectly aligned high frequency details [72]. Further, we conduct a user study comparing the pixel-baseline with LDM-SR. We follow SR3 [72] where human subjects were shown a low-res image in between two high-res images and asked for preference. The results in Tab. 4 affirm the good performance of LDM-SR. PSNR and SSIM can be pushed by using a post-hoc guiding mechanism [15] and we implement this image-based guider via a perceptual loss, see Sec. D.6.\\n\\n|  User Study | SR on ImageNet |   | Inpainting on Places  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  Pixel-DM (f1) | LDM-4 | LAMA [88] | LDM-4  |\\n|  Task 1: Preference vs GT ↑ | 16.0% | 30.4% | 13.6% | 21.0%  |\\n|  Task 2: Preference Score ↑ | 29.4% | 70.6% | 31.9% | 68.1%  |\\n\\nSince the bicubic degradation process does not generalize well to images which do not follow this pre-processing, we also train a generic model, LDM-BSR, by using more diverse degradation. The results are shown in Sec. D.6.1.\\n\\nTable 4. Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in E.3.6\\n\\n|  Method | FID ↓ | IS ↑ | PSNR ↑ | SSIM ↑ | Nparents ↓  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Image Regression [72] | 15.2 | 121.1 | 27.9 | 0.801 | 625M  |\\n|  SR3 [72] | 5.2 | 180.1 | 26.4 | 0.762 | 625M  |\\n|  LDM-4 (ours, 100 steps) | 2.8†/4.8‡ | 166.3 | 24.4±3.6 | 0.69±0.44 | 169M  |\\n|  empGLDM-4 (ours, big, 100 steps) | 2.4†/4.3‡ | 174.9 | 24.7±4.1 | 0.71±0.15 | 552M  |\\n|  LDM-4 (ours, 50 steps, guiding) | 4.4†/6.4‡ | 153.7 | 25.8±3.7 | 0.74±0.21 | 184M  |\\n\\nTable 5.  $\\\\times 4$  upscaling results on ImageNet-Val.  $(256^{2})$ ; †: FID features computed on validation split, ‡: FID features computed on train split; *: Assessed on a NVIDIA A100\\n\\n|  Model (reg.-type) | train throughput samples/sec. | sampling throughput† @256 | @512 | train+val hours/epoch | FID@2k epoch 6  |\\n| --- | --- | --- | --- | --- | --- |\\n|  LDM-1 (no first stage) | 0.11 | 0.26 | 0.07 | 20.66 | 24.74  |\\n|  LDM-4 (KL, w/ attn) | 0.32 | 0.97 | 0.34 | 7.66 | 15.21  |\\n|  LDM-4 (VQ, w/ attn) | 0.33 | 0.97 | 0.34 | 7.04 | 14.99  |\\n|  LDM-4 (VQ, w/o attn) | 0.35 | 0.99 | 0.36 | 6.66 | 15.95  |\\n\\nTable 6. Assessing inpainting efficiency. †: Deviations from Fig. 7 due to varying GPU settings/batch sizes cf. the supplement.\\n\\n# 4.5. Inpainting with Latent Diffusion\\n\\nInpainting is the task of filling masked regions of an image with new content either because parts of the image are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa [88], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8]. The exact training &amp; evaluation protocol on Places [108] is described in Sec. E.2.2.\\n\\nWe first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of LDM-1 (i.e. a pixel-based conditional DM) with LDM-4, for both KL and VQ regularizations, as well as VQ-LDM-4 without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution  $256^2$  and  $512^2$ , the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least  $2.7 \\\\times$  between pixel- and latent-based diffusion models while improving FID scores by a factor of at least  $1.6 \\\\times$ .\\n\\nThe comparison with other inpainting approaches in Tab. 7 shows that our model with attention improves the overall image quality as measured by FID over that of [88]. LPIPS between the unmasked images and our samples is slightly higher than that of [88]. We attribute this to [88] only producing a single result which tends to recover more of an average image compared to the diverse results produced by our LDM cf. Fig. 21. Additionally in a user study (Tab. 4) human subjects favor our results over those of [88].\\n\\nBased on these initial results, we also trained a larger diffusion model (big in Tab. 7) in the latent space of the  $VQ$ -regularized first stage without attention. Following [15], the UNet of this diffusion model uses attention layers on three levels of its feature hierarchy, the BigGAN [3] residual block for up- and downsampling and has 387M parameters\\n\\n![img-8.jpeg](img-8.jpeg)\\nFigure 11. Qualitative results on object removal with our big, w/ ft inpainting model. For more results, see Fig. 22.\\n\\ninstead of 215M. After training, we noticed a discrepancy in the quality of samples produced at resolutions  $256^2$  and  $512^2$ , which we hypothesize to be caused by the additional attention modules. However, fine-tuning the model for half an epoch at resolution  $512^2$  allows the model to adjust to the new feature statistics and sets a new state of the art FID on image inpainting (big, w/o attn, w/ft in Tab. 7, Fig. 11.).\\n\\n# 5. Limitations &amp; Societal Impact\\n\\nLimitations While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our  $f = 4$  autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.\\n\\nSocietal Impact Generative models for media like imagery are a double-edged sword: On the one hand, they\\n\\n|  Method | 40-50% masked |   | All samples  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  FID ↓ | LPIPS ↓ | FID ↓ | LPIPS ↓  |\\n|  LDM-4 (ours, big, w/ ft) | 9.39 | 0.246±0.042 | 1.50 | 0.137±0.080  |\\n|  LDM-4 (ours, big, w/o ft) | 12.89 | 0.257±0.047 | 2.40 | 0.142±0.085  |\\n|  LDM-4 (ours, w/ attn) | 11.87 | 0.257±0.042 | 2.15 | 0.144±0.084  |\\n|  LDM-4 (ours, w/o attn) | 12.60 | 0.259±0.041 | 2.37 | 0.145±0.084  |\\n|  LaMa [88]† | 12.31 | 0.243±0.038 | 2.23 | 0.134±0.080  |\\n|  LaMa [88] | 12.0 | 0.24 | 2.21 | 0.14  |\\n|  CoModGAN [107] | 10.4 | 0.26 | 1.82 | 0.15  |\\n|  RegionWise [52] | 21.3 | 0.27 | 4.75 | 0.15  |\\n|  DeepFill v2 [104] | 22.1 | 0.28 | 5.20 | 0.16  |\\n|  EdgeConnect [58] | 30.5 | 0.28 | 8.37 | 0.16  |\\n\\nTable 7. Comparison of inpainting performance on 30k crops of size  $512 \\\\times 512$  from test images of Places [108]. The column  $40-50\\\\%$  reports metrics computed over hard examples where  $40-50\\\\%$  of the image region have to be inpainted.  $\\\\dagger$  recomputed on our test set, since the original test set used in [88] was not available.\\n\\nenable various creative applications, and in particular approaches like ours that reduce the cost of training and inference have the potential to facilitate access to this technology and democratize its exploration. On the other hand, it also means that it becomes easier to create and disseminate manipulated data or spread misinformation and spam. In particular, the deliberate manipulation of images (\"deep fakes\") is a common problem in this context, and women in particular are disproportionately affected by it [13, 24].\\n\\nGenerative models can also reveal their training data [5, 90], which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.\\n\\nFinally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data [22, 38, 91]. While diffusion models achieve better coverage of the data distribution than e.g. GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.\\n\\nFor a more general, detailed discussion of the ethical considerations of deep generative models, see e.g. [13].\\n\\n# 6. Conclusion\\n\\nWe have presented latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and our cross-attention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of conditional image synthesis tasks without task-specific architectures.\\n\\nReferences\\n\\n- [1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 challenge on single image super-resolution: Dataset and study. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1122–1131. IEEE Computer Society, 2017.\\n- [2] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan, 2017.\\n- [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In Int. Conf. Learn. Represent., 2019.\\n- [4] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1209–1218. Computer Vision Foundation / IEEE Computer Society, 2018.\\n- [5] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633–2650, 2021.\\n- [6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pre-training from pixels. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 1691–1703. PMLR, 2020.\\n- [7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In ICLR. OpenReview.net, 2021.\\n- [8] Lu Chi, Borui Jiang, and Yadong Mu. Fast fourier convolution. In NeurIPS, 2020.\\n- [9] Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. CoRR, abs/2011.10650, 2020.\\n- [10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019.\\n- [11] Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In ICLR (Poster). OpenReview.net, 2019.\\n- [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248–255. IEEE Computer Society, 2009.\\n- [13] Emily Denton. Ethical considerations of generative ai. AI for Content Creation Workshop, CVPR, 2021.\\n- [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.\\n- [15] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. CoRR, abs/2105.05233, 2021.\\n- [16] Sander Dieleman. Musings on typicality, 2020.\\n- [17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. CoRR, abs/2105.13290, 2021.\\n- [18] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation, 2015.\\n- [19] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\\n- [20] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Adv. Neural Inform. Process. Syst., pages 658–666, 2016.\\n- [21] Patrick Esser, Robin Rombach, Andreas Blattmann, and Björn Ommer. Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. CoRR, abs/2108.08827, 2021.\\n- [22] Patrick Esser, Robin Rombach, and Björn Ommer. A note on data biases in generative models. arXiv preprint arXiv:2012.02516, 2020.\\n- [23] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. CoRR, abs/2012.09841, 2020.\\n- [24] Mary Anne Franks and Ari Ezra Waldman. Sex, lies, and videotape: Deep fakes and free speech delusions. Md. L. Rev., 78:892, 2018.\\n- [25] Kevin Frans, Lisa B. Soros, and Olaf Witkowski. Clipdraw: Exploring text-to-drawing synthesis through language-image encoders. ArXiv, abs/2106.14843, 2021.\\n- [26] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. CoRR, abs/2203.13131, 2022.\\n- [27] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, 2014.\\n- [28] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans, 2017.\\n- [29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Adv. Neural Inform. Process. Syst., pages 6626–6637, 2017.\\n- [30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.\\n- [31] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. CoRR, abs/2106.15282, 2021.\\n\\n[32] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.\\n- [33] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 5967–5976. IEEE Computer Society, 2017.\\n- [34] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5967–5976, 2017.\\n- [35] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and João Carreira. Perceiver IO: A general architecture for structured inputs &outputs. CoRR, abs/2107.14795, 2021.\\n- [36] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and João Carreira. Perceiver: General perception with iterative attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4651–4664. PMLR, 2021.\\n- [37] Manuel Jahn, Robin Rombach, and Björn Ommer. High-resolution complex scene synthesis with transformers. CoRR, abs/2105.06458, 2021.\\n- [38] Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda, and Subbarao Kambhampati. Imperfect imaganation: Implications of gans exacerbating biases on facial data augmentation and snapchat selfie lenses. arXiv preprint arXiv:2001.09528, 2020.\\n- [39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017.\\n- [40] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4401–4410, 2019.\\n- [41] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n- [42] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. CoRR, abs/1912.04958, 2019.\\n- [43] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Score matching model for unbounded data score. CoRR, abs/2106.05527, 2021.\\n- [44] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, 2018.\\n- [45] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. CoRR, abs/2107.00630, 2021.\\n- [46] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR, 2014.\\n- [47] Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. CoRR, abs/2106.00132, 2021.\\n- [48] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In ICLR. OpenReview.net, 2021.\\n- [49] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio Ferrari. The open images dataset V4: unified image classification, object detection, and visual relationship detection at scale. CoRR, abs/1811.00982, 2018.\\n- [50] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. CoRR, abs/1904.06991, 2019.\\n- [51] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014.\\n- [52] Yuqing Ma, Xianglong Liu, Shihao Bai, Le-Yi Wang, Aishan Liu, Dacheng Tao, and Edwin Hancock. Region-wise generative adversarial image inpainting for large missing areas. ArXiv, abs/1909.12507, 2019.\\n- [53] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and editing with stochastic differential equations. CoRR, abs/2108.01073, 2021.\\n- [54] Lars M. Mescheder. On the convergence properties of GAN training. CoRR, abs/1801.04406, 2018.\\n- [55] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\\n- [56] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014.\\n- [57] Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with diffusion models. CoRR, abs/2103.16091, 2021.\\n- [58] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, and Mehran Ebrahimi. Edgeconnect: Generative image inpainting with adversarial edge learning. ArXiv, abs/1901.00212, 2019.\\n- [59] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. CoRR, abs/2112.10741, 2021.\\n- [60] Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin.\\n\\nHigh-fidelity performance metrics for generative models in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zenodo.4957738.\\n- [61] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\\n- [62] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n- [63] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 823–832. Computer Vision Foundation / IEEE, 2021.\\n- [64] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising subtleties in fid calculation. arXiv preprint arXiv:2104.11222, 2021.\\n- [65] David A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. CoRR, abs/2104.10350, 2021.\\n- [66] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021.\\n- [67] Ali Razavi, Aäron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In NeurIPS, pages 14837–14847, 2019.\\n- [68] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In ICML, 2016.\\n- [69] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on International Conference on Machine Learning, ICML, 2014.\\n- [70] Robin Rombach, Patrick Esser, and Björn Ommer. Network-to-network translation with conditional invertible neural networks. In NeurIPS, 2020.\\n- [71] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI (3), volume 9351 of Lecture Notes in Computer Science, pages 234–241. Springer, 2015.\\n- [72] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. CoRR, abs/2104.07636, 2021.\\n- [73] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. CoRR, abs/1701.05517, 2017.\\n- [74] Dave Salvator. NVIDIA Developer Blog. https://developer.nvidia.com/blog/getting-immediate-speedups-with-a100-tf32, 2020.\\n- [75] Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion models. CoRR, abs/2104.02600, 2021.\\n- [76] Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger. Projected gans converge faster. CoRR, abs/2111.01007, 2021.\\n- [77] Edgar Schönfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative adversarial networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8204–8213. Computer Vision Foundation / IEEE, 2020.\\n- [78] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs, 2021.\\n- [79] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, Int. Conf. Learn. Represent., 2015.\\n- [80] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano Ermon. D2C: diffusion-denoising models for few-shot conditional generation. CoRR, abs/2106.06819, 2021.\\n- [81] Charlie Snell. Alien Dreams: An Emerging Art Scene. https://ml.berkeley.edu/blog/posts/clip-art/, 2021. [Online; accessed November-2021].\\n- [82] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. CoRR, abs/1503.03585, 2015.\\n- [83] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.\\n- [84] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR. OpenReview.net, 2021.\\n- [85] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. CoRR, abs/2011.13456, 2020.\\n- [86] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 13693–13696. AAAI Press, 2020.\\n\\n[87] Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. CoRR, abs/2003.11571, 2020.\\n- [88] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S. Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. ArXiv, abs/2109.07161, 2021.\\n- [89] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 2647–2655. AAAI Press, 2021.\\n- [90] Patrick Tinsley, Adam Czajka, and Patrick Flynn. This face does not exist… but it might be yours! identity leakage in generative models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1320–1328, 2021.\\n- [91] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521–1528. IEEE, 2011.\\n- [92] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In NeurIPS, 2020.\\n- [93] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. CoRR, abs/2106.05931, 2021.\\n- [94] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, 2016.\\n- [95] Aäron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. CoRR, abs/1601.06759, 2016.\\n- [96] Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NIPS, pages 6306–6315, 2017.\\n- [97] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 5998–6008, 2017.\\n- [98] Rivers Have Wings. Tweet on Classifier-free guidance for autoregressive models. https://twitter.com/RiversHaveWings/status/1478093658716966912, 2022.\\n- [99] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019.\\n- [100] Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. VAEBM: A symbiosis between variational autoencoders and energy-based models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\\n- [101] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. CoRR, abs/2104.10157, 2021.\\n- [102] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015.\\n- [103] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan, 2021.\\n- [104] Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. Free-form image inpainting with gated convolution. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4470–4479, 2019.\\n- [105] K. Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model for deep blind image super-resolution. ArXiv, abs/2103.14006, 2021.\\n- [106] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\\n- [107] Shengyu Zhao, Jianwei Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I-Chao Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. ArXiv, abs/2103.10428, 2021.\\n- [108] Bolei Zhou, Àgata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:1452–1464, 2018.\\n- [109] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. LAFITE: towards language-free training for text-to-image generation. CoRR, abs/2111.13792, 2021.\\n\\n# Appendix\\n\\n![img-9.jpeg](img-9.jpeg)\\n\\n![img-10.jpeg](img-10.jpeg)\\n\\n![img-11.jpeg](img-11.jpeg)\\nFigure 12. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on  $512^{2}$  images.\\n\\n\\'A painting of the last supper by Picasso.\\'\\n\\n![img-12.jpeg](img-12.jpeg)\\n\\'An oil painting of a latent space.\\'\\n\\n\\'An epic painting of Gandalf the Black summoning thunder and lightning in the mountains.\\'\\n\\n![img-13.jpeg](img-13.jpeg)\\n\\'A sunset over a mountain range, vector image.\\'\\n\\n![img-14.jpeg](img-14.jpeg)\\n\\n![img-15.jpeg](img-15.jpeg)\\nFigure 13. Combining classifier free diffusion guidance with the convolutional sampling strategy from Sec. 4.3.2, our 1.45B parameter text-to-image model can be used for rendering images larger than the native  $256^2$  resolution the model was trained on.\\n\\nA. Changelog\\n\\nHere we list changes between this version (https://arxiv.org/abs/2112.10752v2) of the paper and the previous version, i.e. https://arxiv.org/abs/2112.10752v1.\\n\\n- We updated the results on text-to-image synthesis in Sec. 4.3 which were obtained by training a new, larger model (1.45B parameters). This also includes a new comparison to very recent competing methods on this task that were published on arXiv at the same time as (*[59, 109]*) or after (*[26]*) the publication of our work.\\n- We updated results on class-conditional synthesis on ImageNet in Sec. 4.1, Tab. 3 (see also Sec. D.4) obtained by retraining the model with a larger batch size. The corresponding qualitative results in Fig. 26 and Fig. 27 were also updated. Both the updated text-to-image and the class-conditional model now use classifier-free guidance *[32]* as a measure to increase visual fidelity.\\n- We conducted a user study (following the scheme suggested by Saharia et al *[72]*) which provides additional evaluation for our inpainting (Sec. 4.5) and superresolution models (Sec. 4.4).\\n- Added Fig. 5 to the main paper, moved Fig. 18 to the appendix, added Fig. 13 to the appendix.\\n\\n## Appendix B Detailed Information on Denoising Diffusion Models\\n\\nDiffusion models can be specified in terms of a signal-to-noise ratio $\\\\text{SNR}(t)=\\\\frac{\\\\alpha_{t}^{2}}{\\\\sigma_{t}^{2}}$ consisting of sequences $(\\\\alpha_{t})_{t=1}^{T}$ and $(\\\\sigma_{t})_{t=1}^{T}$ which, starting from a data sample $x_{0}$, define a forward diffusion process $q$ as\\n\\n$q(x_{t}|x_{0})=\\\\mathcal{N}(x_{t}|\\\\alpha_{t}x_{0},\\\\sigma_{t}^{2}\\\\mathbb{I})$ (4)\\n\\nwith the Markov structure for $s<t$:\\n\\n$q(x_{t}|x_{s})$ $=\\\\mathcal{N}(x_{t}|\\\\alpha_{t|s}x_{s},\\\\sigma_{t|s}^{2}\\\\mathbb{I})$ (5)\\n$\\\\alpha_{t|s}$ $=\\\\frac{\\\\alpha_{t}}{\\\\alpha_{s}}$ (6)\\n$\\\\sigma_{t|s}^{2}$ $=\\\\sigma_{t}^{2}-\\\\alpha_{t|s}^{2}\\\\sigma_{s}^{2}$ (7)\\n\\nDenoising diffusion models are generative models $p(x_{0})$ which revert this process with a similar Markov structure running backward in time, i.e. they are specified as\\n\\n$p(x_{0})=\\\\int_{z}p(x_{T})\\\\prod_{t=1}^{T}p(x_{t-1}|x_{t})$ (8)\\n\\nThe evidence lower bound (ELBO) associated with this model then decomposes over the discrete time steps as\\n\\n$-\\\\log p(x_{0})\\\\leq\\\\mathbb{KL}(q(x_{T}|x_{0})|p(x_{T}))+\\\\sum_{t=1}^{T}\\\\mathbb{E}_{q(x_{t}|x_{0})}\\\\mathbb{KL}(q(x_{t-1}|x_{t},x_{0})|p(x_{t-1}|x_{t}))$ (9)\\n\\nThe prior $p(x_{T})$ is typically choosen as a standard normal distribution and the first term of the ELBO then depends only on the final signal-to-noise ratio $\\\\text{SNR}(T)$. To minimize the remaining terms, a common choice to parameterize $p(x_{t-1}|x_{t})$ is to specify it in terms of the true posterior $q(x_{t-1}|x_{t},x_{0})$ but with the unknown $x_{0}$ replaced by an estimate $x_{\\\\theta}(x_{t},t)$ based on the current step $x_{t}$. This gives *[45]*\\n\\n$p(x_{t-1}|x_{t})$ $\\\\coloneqq q(x_{t-1}|x_{t},x_{\\\\theta}(x_{t},t))$ (10)\\n$=\\\\mathcal{N}(x_{t-1}|\\\\mu_{\\\\theta}(x_{t},t),\\\\sigma_{t|t-1}^{2}\\\\frac{\\\\sigma_{t-1}^{2}}{\\\\sigma_{t}^{2}}\\\\mathbb{I}),$ (11)\\n\\nwhere the mean can be expressed as\\n\\n$\\\\mu_{\\\\theta}(x_{t},t)=\\\\frac{\\\\alpha_{t|t-1}\\\\sigma_{t-1}^{2}}{\\\\sigma_{t}^{2}}x_{t}+\\\\frac{\\\\alpha_{t-1}\\\\sigma_{t|t-1}^{2}}{\\\\sigma_{t}^{2}}x_{\\\\theta}(x_{t},t).$ (12)\\n\\nIn this case, the sum of the ELBO simplify to\\n\\n$\\\\sum_{t=1}^{T}\\\\mathbb{E}_{q(x_{t}|x_{0})}\\\\mathbb{KL}(q(x_{t-1}|x_{t},x_{0})|p(x_{t-1})=\\\\sum_{t=1}^{T}\\\\mathbb{E}_{\\\\mathcal{N}(\\\\epsilon|0,\\\\mathbb{I})}\\\\frac{1}{2}(\\\\text{SNR}(t-1)-\\\\text{SNR}(t))\\\\|x_{0}-x_{\\\\theta}(\\\\alpha_{t}x_{0}+\\\\sigma_{t}\\\\epsilon,t)\\\\|^{2}$ (13)\\n\\nFollowing *[30]*, we use the reparameterization\\n\\n$\\\\epsilon_{\\\\theta}(x_{t},t)=(x_{t}-\\\\alpha_{t}x_{\\\\theta}(x_{t},t))/\\\\sigma_{t}$ (14)\\n\\nto express the reconstruction term as a denoising objective,\\n\\n$\\\\|x_{0}-x_{\\\\theta}(\\\\alpha_{t}x_{0}+\\\\sigma_{t}\\\\epsilon,t)\\\\|^{2}=\\\\frac{\\\\sigma_{t}^{2}}{\\\\alpha_{t}^{2}}\\\\|\\\\epsilon-\\\\epsilon_{\\\\theta}(\\\\alpha_{t}x_{0}+\\\\sigma_{t}\\\\epsilon,t)\\\\|^{2}$ (15)\\n\\nand the reweighting, which assigns each of the terms the same weight and results in Eq. (1).\\n\\n# C. Image Guiding Mechanisms\\n\\n|  Samples 2562 | Guided Convolutional Samples 5122 | Convolutional Samples 5122  |\\n| --- | --- | --- |\\n|  |   |   |\\n|  |   |   |\\n|  |   |   |\\n|  |   |   |\\n|  |   |   |\\n\\nFigure 14. On landscapes, convolutional sampling with unconditional models can lead to homogeneous and incoherent global structures (see column 2).  $L_{2}$ -guiding with a low resolution image can help to reestablish coherent global structures.\\n\\nAn intriguing feature of diffusion models is that unconditional models can be conditioned at test-time [15, 82, 85]. In particular, [15] presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset with a classifier  $\\\\log p_{\\\\Phi}(y|x_t)$ , trained on each  $x_t$  of the diffusion process. We directly build on this formulation and introduce post-hoc image-guiding:\\n\\nFor an epsilon-parameterized model with fixed variance, the guiding algorithm as introduced in [15] reads:\\n\\n$$\\n\\\\hat {\\\\epsilon} \\\\leftarrow \\\\epsilon_ {\\\\theta} \\\\left(z _ {t}, t\\\\right) + \\\\sqrt {1 - \\\\alpha_ {t} ^ {2}} \\\\nabla_ {z _ {t}} \\\\log p _ {\\\\Phi} (y | z _ {t}). \\\\tag {16}\\n$$\\n\\nThis can be interpreted as an update correcting the \"score\"  $\\\\epsilon_{\\\\theta}$  with a conditional distribution  $\\\\log p_{\\\\Phi}(y|z_t)$ .\\n\\nSo far, this scenario has only been applied to single-class classification models. We re-interpret the guiding distribution  $p_{\\\\Phi}(y|T(\\\\mathcal{D}(z_0(z_t))))$  as a general purpose image-to-image translation task given a target image  $y$ , where  $T$  can be any differentiable transformation adopted to the image-to-image translation task at hand, such as the identity, a downsampling operation or similar.\\n\\nAs an example, we can assume a Gaussian guider with fixed variance $\\\\sigma^{2}=1$, such that\\n\\n$\\\\log p_{\\\\Phi}(y|z_{t})=-\\\\frac{1}{2}\\\\|y-T(\\\\mathcal{D}(z_{0}(z_{t})))\\\\|_{2}^{2}$ (17)\\n\\nbecomes a $L_{2}$ regression objective.\\n\\nFig. 14 demonstrates how this formulation can serve as an upsampling mechanism of an unconditional model trained on $256^{2}$ images, where unconditional samples of size $256^{2}$ guide the convolutional synthesis of $512^{2}$ images and $T$ is a $2\\\\times$ bicubic downsampling. Following this motivation, we also experiment with a perceptual similarity guiding and replace the $L_{2}$ objective with the LPIPS *[106]* metric, see Sec. 4.4.\\n\\n# D. Additional Results\\n\\n# D.1. Choosing the Signal-to-Noise Ratio for High-Resolution Synthesis\\n\\n![img-16.jpeg](img-16.jpeg)\\nFigure 15. Illustrating the effect of latent space rescaling on convolutional sampling, here for semantic image synthesis on landscapes. See Sec. 4.3.2 and Sec. D.1.\\n\\nAs discussed in Sec. 4.3.2, the signal-to-noise ratio induced by the variance of the latent space (i.e.  $\\\\mathrm{Var}(z) / \\\\sigma_t^2$ ) significantly affects the results for convolutional sampling. For example, when training a LDM directly in the latent space of a KL-regularized model (see Tab. 8), this ratio is very high, such that the model allocates a lot of semantic detail early on in the reverse denoising process. In contrast, when rescaling the latent space by the component-wise standard deviation of the latents as described in Sec. G, the SNR is decreased. We illustrate the effect on convolutional sampling for semantic image synthesis in Fig. 15. Note that the VQ-regularized space has a variance close to 1, such that it does not have to be rescaled.\\n\\n# D.2. Full List of all First Stage Models\\n\\nWe provide a complete list of various autoenconding models trained on the OpenImages dataset in Tab. 8.\\n\\n# D.3. Layout-to-Image Synthesis\\n\\nHere we provide the quantitative evaluation and additional samples for our layout-to-image models from Sec. 4.3.1. We train a model on the COCO [4] and one on the OpenImages [49] dataset, which we subsequently additionally finetune on COCO. Tab 9 shows the result. Our COCO model reaches the performance of recent state-of-the-art models in layout-to-image synthesis, when following their training and evaluation protocol [89]. When finetuning from the OpenImages model, we surpass these works. Our OpenImages model surpasses the results of Jahn et al [37] by a margin of nearly 11 in terms of FID. In Fig. 16 we show additional samples of the model finetuned on COCO.\\n\\n# D.4. Class-Conditional Image Synthesis on ImageNet\\n\\nTab. 10 contains the results for our class-conditional LDM measured in FID and Inception score (IS). LDM-8 requires significantly fewer parameters and compute requirements (see Tab. 18) to achieve very competitive performance. Similar to previous work, we can further boost the performance by training a classifier on each noise scale and guiding with it,\\n\\n|  f | |Z| | c | R-FID ↓ | R-IS ↑ | PSNR ↑ | PSIM ↓ | SSIM ↑  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  16 VQGAN [23] | 16384 | 256 | 4.98 | - | 19.9 ±3.4 | 1.83 ±0.42 | 0.51 ±0.18  |\\n|  16 VQGAN [23] | 1024 | 256 | 7.94 | - | 19.4 ±3.3 | 1.98 ±0.43 | 0.50 ±0.18  |\\n|  8 DALL-E [66] | 8192 | - | 32.01 | - | 22.8 ±2.1 | 1.95 ±0.51 | 0.73 ±0.13  |\\n|  32 | 16384 | 16 | 31.83 | 40.40 ±1.07 | 17.45 ±2.90 | 2.58 ±0.48 | 0.41 ±0.18  |\\n|  16 | 16384 | 8 | 5.15 | 144.55 ±3.74 | 20.83 ±3.61 | 1.73 ±0.43 | 0.54 ±0.18  |\\n|  8 | 16384 | 4 | 1.14 | 201.92 ±3.97 | 23.07 ±3.99 | 1.17 ±0.36 | 0.65 ±0.16  |\\n|  8 | 256 | 4 | 1.49 | 194.20 ±3.87 | 22.35 ±3.81 | 1.26 ±0.37 | 0.62 ±0.16  |\\n|  4 | 8192 | 3 | 0.58 | 224.78 ±5.35 | 27.43 ±4.26 | 0.53 ±0.21 | 0.82 ±0.10  |\\n|  4† | 8192 | 3 | 1.06 | 221.94 ±4.58 | 25.21 ±4.17 | 0.72 ±0.26 | 0.76 ±0.12  |\\n|  4 | 256 | 3 | 0.47 | 223.81 ±4.58 | 26.43 ±4.22 | 0.62 ±0.24 | 0.80 ±0.11  |\\n|  2 | 2048 | 2 | 0.16 | 232.75 ±5.09 | 30.85 ±4.12 | 0.27 ±0.12 | 0.91 ±0.05  |\\n|  2 | 64 | 2 | 0.40 | 226.62 ±4.83 | 29.13 ±3.46 | 0.38 ±0.13 | 0.90 ±0.05  |\\n|  32 | KL | 64 | 2.04 | 189.53 ±3.68 | 22.27 ±3.93 | 1.41 ±0.40 | 0.61 ±0.17  |\\n|  32 | KL | 16 | 7.3 | 132.75 ±2.71 | 20.38 ±3.56 | 1.88 ±0.45 | 0.53 ±0.18  |\\n|  16 | KL | 16 | 0.87 | 210.31 ±3.97 | 24.08 ±4.22 | 1.07 ±0.36 | 0.68 ±0.15  |\\n|  16 | KL | 8 | 2.63 | 178.68 ±4.08 | 21.94 ±3.92 | 1.49 ±0.42 | 0.59 ±0.17  |\\n|  8 | KL | 4 | 0.90 | 209.90 ±4.92 | 24.19 ±4.19 | 1.02 ±0.35 | 0.69 ±0.15  |\\n|  4 | KL | 3 | 0.27 | 227.57 ±4.89 | 27.53 ±4.54 | 0.55 ±0.24 | 0.82 ±0.11  |\\n|  2 | KL | 2 | 0.086 | 232.66 ±5.16 | 32.47 ±4.19 | 0.20 ±0.09 | 0.93 ±0.04  |\\n\\nTable 8. Complete autoencoder zoo trained on OpenImages, evaluated on ImageNet-Val. † denotes an attention-free autoencoder.\\n\\n![img-17.jpeg](img-17.jpeg)\\nlayout-to-image synthesis on the COCO dataset\\nFigure 16. More samples from our best model for layout-to-image synthesis,  $LDM-4$ , which was trained on the OpenImages dataset and finetuned on the COCO dataset. Samples generated with 100 DDIM steps and  $\\\\eta = 0$ . Layouts are from the COCO validation set.\\n\\nsee Sec. C. Unlike the pixel-based methods, this classifier is trained very cheaply in latent space. For additional qualitative results, see Fig. 26 and Fig. 27.\\n\\n|  Method | COCO256 × 256 | OpenImages 256 × 256 | OpenImages 512 × 512  |\\n| --- | --- | --- | --- |\\n|   |  FID↓ | FID↓ | FID↓  |\\n|  LostGAN-V2 [87] | 42.55 | - | -  |\\n|  OC-GAN [89] | 41.65 | - | -  |\\n|  SPADE [62] | 41.11 | - | -  |\\n|  VQGAN+T [37] | 56.58 | 45.33 | 48.11  |\\n|  LDM-8 (100 steps, ours) | 42.06† | - | -  |\\n|  LDM-4 (200 steps, ours) | 40.91* | 32.02 | 35.80  |\\n\\nTable 9. Quantitative comparison of our layout-to-image models on the COCO [4] and OpenImages [49] datasets. †: Training from scratch on COCO; *: Finetuning from OpenImages.\\n\\n|  Method | FID↓ | IS↑ | Precision↑ | Recall↑ | Nparams |   |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  SR3 [72] | 11.30 | - | - | - | 625M | -  |\\n|  ImageBART [21] | 21.19 | - | - | - | 3.5B | -  |\\n|  ImageBART [21] | 7.44 | - | - | - | 3.5B | 0.05 acc. rate*  |\\n|  VQGAN+T [23] | 17.04 | 70.6±1.8 | - | - | 1.3B | -  |\\n|  VQGAN+T [23] | 5.88 | 304.8±3.6 | - | - | 1.3B | 0.05 acc. rate*  |\\n|  BigGan-deep [3] | 6.95 | 203.6±2.6 | 0.87 | 0.28 | 340M |   |\\n|  ADM [15] | 10.94 | 100.98 | 0.69 | 0.63 | 554M | 250 DDIM steps  |\\n|  ADM-G [15] | 4.59 | 186.7 | 0.82 | 0.52 | 608M | 250 DDIM steps  |\\n|  ADM-G,ADM-U [15] | 3.85 | 221.72 | 0.84 | 0.53 | n/a | 2 × 250 DDIM steps  |\\n|  CDM [31] | 4.88 | 158.71±3.26 | - | - | n/a | 2 × 100 DDIM steps  |\\n|  LDM-8 (ours) | 17.41 | 72.92±2.6 | 0.65 | 0.62 | 395M | 200 DDIM steps, 2.9M train steps, batch size 64  |\\n|  LDM-8-G (ours) | 8.11 | 190.43±2.60 | 0.83 | 0.36 | 506M | 200 DDIM steps, classifier scale 10, 2.9M train steps, batch size 64  |\\n|  LDM-8 (ours) | 15.51 | 79.03±1.03 | 0.65 | 0.63 | 395M | 200 DDIM steps, 4.8M train steps, batch size 64  |\\n|  LDM-8-G (ours) | 7.76 | 209.52±4.24 | 0.84 | 0.35 | 506M | 200 DDIM steps, classifier scale 10, 4.8M train steps, batch size 64  |\\n|  LDM-4 (ours) | 10.56 | 103.49±1.24 | 0.71 | 0.62 | 400M | 250 DDIM steps, 178K train steps, batch size 1200  |\\n|  LDM-4-G (ours) | 3.95 | 178.22±2.43 | 0.81 | 0.55 | 400M | 250 DDIM steps, unconditional guidance [32] scale 1.25, 178K train steps, batch size 1200  |\\n|  LDM-4-G (ours) | 3.60 | 247.67±5.59 | 0.87 | 0.48 | 400M | 250 DDIM steps, unconditional guidance [32] scale 1.5, 178K train steps, batch size 1200  |\\n\\nTable 10. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on the ImageNet [12] dataset.*: Classifier rejection sampling with the given rejection rate as proposed in [67].\\n\\n# D.5. Sample Quality vs. V100 Days (Continued from Sec. 4.1)\\n\\n![img-18.jpeg](img-18.jpeg)\\nFigure 17. For completeness we also report the training progress of class-conditional  $LDMs$  on the ImageNet dataset for a fixed number of 35 V100 days. Results obtained with 100 DDIM steps [84] and  $\\\\kappa = 0$ . FIDs computed on 5000 samples for efficiency reasons.\\n\\n![img-19.jpeg](img-19.jpeg)\\n\\nFor the assessment of sample quality over the training progress in Sec. 4.1, we reported FID and IS scores as a function of train steps. Another possibility is to report these metrics over the used resources in V100 days. Such an analysis is additionally provided in Fig. 17, showing qualitatively similar results.\\n\\n|  Method | FID ↓ | IS ↑ | PSNR ↑ | SSIM ↑  |\\n| --- | --- | --- | --- | --- |\\n|  Image Regression [72] | 15.2 | 121.1 | 27.9 | 0.801  |\\n|  SR3 [72] | 5.2 | 180.1 | 26.4 | 0.762  |\\n|  LDM-4 (ours, 100 steps) | 2.8†/4.8‡ | 166.3 | 24.4±3.8 | 0.69±0.14  |\\n|  LDM-4 (ours, 50 steps, guiding) | 4.4†/6.4‡ | 153.7 | 25.8±3.7 | 0.74±0.12  |\\n|  LDM-4 (ours, 100 steps, guiding) | 4.4†/6.4‡ | 154.1 | 25.7±3.7 | 0.73±0.12  |\\n|  LDM-4 (ours, 100 steps, +15 ep.) | 2.6†/4.6‡ | 169.76±5.03 | 24.4±3.8 | 0.69±0.14  |\\n|  Pixel-DM (100 steps, +15 ep.) | 5.1†/7.1‡ | 163.06±4.67 | 24.1±3.3 | 0.59±0.12  |\\n\\nTable 11.  $\\\\times 4$  upscaling results on ImageNet-Val.  $(256^{2})$ ;  $\\\\dagger$ : FID features computed on validation split,  $\\\\ddagger$ : FID features computed on train split. We also include a pixel-space baseline that receives the same amount of compute as LDM-4. The last two rows received 15 epochs of additional training compared to the former results.\\n\\n# D.6. Super-Resolution\\n\\nFor better comparability between LDMs and diffusion models in pixel space, we extend our analysis from Tab. 5 by comparing a diffusion model trained for the same number of steps and with a comparable number  ${}^{1}$  of parameters to our LDM. The results of this comparison are shown in the last two rows of Tab. 11 and demonstrate that LDM achieves better performance while allowing for significantly faster sampling. A qualitative comparison is given in Fig. 20 which shows random samples from both LDM and the diffusion model in pixel space.\\n\\n# D.6.1 LDM-BSR: General Purpose SR Model via Diverse Image Degradation\\n\\n![img-20.jpeg](img-20.jpeg)\\nFigure 18. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from a class-conditional LDM (image cf. Fig. 4) to  $1024^2$  resolution. In contrast, using a fixed degradation process (see Sec. 4.4) hinders generalization.\\n\\nTo evaluate generalization of our LDM-SR, we apply it both on synthetic LDM samples from a class-conditional ImageNet model (Sec. 4.1) and images crawled from the internet. Interestingly, we observe that LDM-SR, trained only with a bicubicly downsampled conditioning as in [72], does not generalize well to images which do not follow this pre-processing. Hence, to obtain a superresolution model for a wide range of real world images, which can contain complex superpositions of camera noise, compression artifacts, blurr and interpolations, we replace the bicubic downsampling operation in LDM-SR with the degradation pipeline from [105]. The BSR-degradation process is a degradation pipeline which applies JPEG compressions noise, camera sensor noise, different image interpolations for downsampling, Gaussian blur kernels and Gaussian noise in a random order to an image. We found that using the bsr-degradation process with the original parameters as in [105] leads to a very strong degradation process. Since a more moderate degradation process seemed appropriate for our application, we adapted the parameters of the bsr-degradation (our adapted degradation process can be found in our code base at https://github.com/CompVis/latent-diffusion). Fig. 18 illustrates the effectiveness of this approach by directly comparing LDM-SR with LDM-BSR. The latter produces images much sharper than the models confined to a fixed preprocessing, making it suitable for real-world applications. Further results of LDM-BSR are shown on LSUN-cows in Fig. 19.\\n\\n# E. Implementation Details and Hyperparameters\\n\\n# E.1. Hyperparameters\\n\\nWe provide an overview of the hyperparameters of all trained LDM models in Tab. 12, Tab. 13, Tab. 14 and Tab. 15.\\n\\n|   | CelebA-HQ 256 × 256 | FFHQ 256 × 256 | LSUN-Churches 256 × 256 | LSUN-Bedrooms 256 × 256  |\\n| --- | --- | --- | --- | --- |\\n|  f | 4 | 4 | 8 | 4  |\\n|  z-shape | 64 × 64 × 3 | 64 × 64 × 3 | - | 64 × 64 × 3  |\\n|  |Z | 8192 | 8192 | - | 8192  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear  |\\n|  Nparams | 274M | 274M | 294M | 274M  |\\n|  Channels | 224 | 224 | 192 | 224  |\\n|  Depth | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,2,3,4 | 1,2,3,4 | 1,2,2,4,4 | 1,2,3,4  |\\n|  Attention resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8, 4 | 32, 16, 8  |\\n|  Head Channels | 32 | 32 | 24 | 32  |\\n|  Batch Size | 48 | 42 | 96 | 48  |\\n|  Iterations* | 410k | 635k | 500k | 1.9M  |\\n|  Learning Rate | 9.6e-5 | 8.4e-5 | 5.e-5 | 9.6e-5  |\\n\\nTable 12. Hyperparameters for the unconditional LDMs producing the numbers shown in Tab. 1. All models trained on a single NVIDIA A100.\\n\\n|   | LDM-1 | LDM-2 | LDM-4 | LDM-8 | LDM-16 | LDM-32  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  z-shape | 256 × 256 × 3 | 128 × 128 × 2 | 64 × 64 × 3 | 32 × 32 × 4 | 16 × 16 × 8 | 88 × 8 × 32  |\\n|  |Z | - | 2048 | 8192 | 16384 | 16384 | 16384  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear | linear | linear  |\\n|  Model Size | 396M | 391M | 391M | 395M | 395M | 395M  |\\n|  Channels | 192 | 192 | 192 | 256 | 256 | 256  |\\n|  Depth | 2 | 2 | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,1,2,2,4,4 | 1,2,2,4,4 | 1,2,3,5 | 1,2,4 | 1,2,4 | 1,2,4  |\\n|  Number of Heads | 1 | 1 | 1 | 1 | 1 | 1  |\\n|  Batch Size | 7 | 9 | 40 | 64 | 112 | 112  |\\n|  Iterations | 2M | 2M | 2M | 2M | 2M | 2M  |\\n|  Learning Rate | 4.9e-5 | 6.3e-5 | 8e-5 | 6.4e-5 | 4.5e-5 | 4.5e-5  |\\n|  Conditioning | CA | CA | CA | CA | CA | CA  |\\n|  CA-resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 16, 8, 4 | 8, 4, 2  |\\n|  Embedding Dimension | 512 | 512 | 512 | 512 | 512 | 512  |\\n|  Transformers Depth | 1 | 1 | 1 | 1 | 1 | 1  |\\n\\nTable 13. Hyperparameters for the conditional LDMs trained on the ImageNet dataset for the analysis in Sec. 4.1. All models trained on a single NVIDIA A100.\\n\\n# E.2. Implementation Details\\n\\n# E.2.1 Implementations of  $\\\\tau_{\\\\theta}$  for conditional LDMs\\n\\nFor the experiments on text-to-image and layout-to-image (Sec. 4.3.1) synthesis, we implement the conditioner  $\\\\tau_{\\\\theta}$  as an unmasked transformer which processes a tokenized version of the input  $y$  and produces an output  $\\\\zeta \\\\coloneqq \\\\tau_{\\\\theta}(y)$ , where  $\\\\zeta \\\\in \\\\mathbb{R}^{M\\\\times d_{\\\\tau}}$ . More specifically, the transformer is implemented from  $N$  transformer blocks consisting of global self-attention layers, layer-normalization and position-wise MLPs as follows:\\n\\n|   | LDM-1 | LDM-2 | LDM-4 | LDM-8 | LDM-16 | LDM-32  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  z-shape | 256 × 256 × 3 | 128 × 128 × 2 | 64 × 64 × 3 | 32 × 32 × 4 | 16 × 16 × 8 | 88 × 8 × 32  |\\n|  |Z | - | 2048 | 8192 | 16384 | 16384 | 16384  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear | linear | linear  |\\n|  Model Size | 270M | 265M | 274M | 258M | 260M | 258M  |\\n|  Channels | 192 | 192 | 224 | 256 | 256 | 256  |\\n|  Depth | 2 | 2 | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,1,2,2,4,4 | 1,2,2,4,4 | 1,2,3,4 | 1,2,4 | 1,2,4 | 1,2,4  |\\n|  Attention resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 16, 8, 4 | 8, 4, 2  |\\n|  Head Channels | 32 | 32 | 32 | 32 | 32 | 32  |\\n|  Batch Size | 9 | 11 | 48 | 96 | 128 | 128  |\\n|  Iterations* | 500k | 500k | 500k | 500k | 500k | 500k  |\\n|  Learning Rate | 9e-5 | 1.1e-4 | 9.6e-5 | 9.6e-5 | 1.3e-4 | 1.3e-4  |\\n\\nTable 14. Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in Fig. 7. All models trained on a single NVIDIA A100. *: All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the provided FID scores.\\n\\n|  Task | Text-to-Image | Layout-to-Image |   | Class-Label-to-Image | Super Resolution | Inpainting | Semantic-Map-to-Image  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Dataset | LAION | OpenImages | COCO | ImageNet | ImageNet | Places | Landscapes  |\\n|  f | 8 | 4 | 8 | 4 | 4 | 4 | 8  |\\n|  z-shape | 32 × 32 × 4 | 64 × 64 × 3 | 32 × 32 × 4 | 64 × 64 × 3 | 64 × 64 × 3 | 64 × 64 × 3 | 32 × 32 × 4  |\\n|  |Z | - | 8192 | 16384 | 8192 | 8192 | 8192 | 16384  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear | linear | linear | linear  |\\n|  Model Size | 1.45B | 306M | 345M | 395M | 169M | 215M | 215M  |\\n|  Channels | 320 | 128 | 192 | 192 | 160 | 128 | 128  |\\n|  Depth | 2 | 2 | 2 | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,2,4,4 | 1,2,3,4 | 1,2,4 | 1,2,3,5 | 1,2,2,4 | 1,4,8 | 1,4,8  |\\n|  Number of Heads | 8 | 1 | 1 | 1 | 1 | 1 | 1  |\\n|  Dropout | - | - | 0.1 | - | - | - | -  |\\n|  Batch Size | 680 | 24 | 48 | 1200 | 64 | 128 | 48  |\\n|  Iterations | 390K | 4.4M | 170K | 178K | 860K | 360K | 360K  |\\n|  Learning Rate | 1.0e-4 | 4.8e-5 | 4.8e-5 | 1.0e-4 | 6.4e-5 | 1.0e-6 | 4.8e-5  |\\n|  Conditioning | CA | CA | CA | CA | concat | concat | concat  |\\n|  (C)A-resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | - | - | -  |\\n|  Embedding Dimension | 1280 | 512 | 512 | 512 | - | - | -  |\\n|  Transformer Depth | 1 | 3 | 2 | 1 | - | - | -  |\\n\\nTable 15. Hyperparameters for the conditional LDMs from Sec. 4. All models trained on a single NVIDIA A100 except for the inpainting model which was trained on eight V100.\\n\\n$\\\\zeta \\\\gets \\\\mathrm{TokEmb}(y) + \\\\mathrm{PosEmb}(y)$  (18)\\n\\nfor  $i = 1,\\\\dots ,N$  ..\\n\\n$\\\\zeta_{1}\\\\gets \\\\mathrm{LayerNorm}(\\\\zeta)$  (19)\\n', 'chunk_index': 0}, page_content='# High-Resolution Image Synthesis with Latent Diffusion Models\\n\\nRobin Rombach $^{1*}$  Andreas Blattmann $^{1*}$  Dominik Lorenz $^{1}$  Patrick Esser $^{\\\\text{圆}}$  Björn Ommer $^{1}$\\n\\n$^{1}$ Ludwig Maximilian University of Munich &amp; IWR, Heidelberg University, Germany  $\\\\text{圆}$ Runway ML\\n\\nhttps://github.com/CompVis/latent-diffusion\\n\\n# Abstract\\n\\nBy decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.\\n\\n# 1. Introduction\\n\\nImage synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands. Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66,67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1. Boosting the upper bound on achievable quality with less aggressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at  $512^{2}$  px. We denote the spatial downsampling factor by  $f$ . Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8.\\n\\nresults in image synthesis [30,85] and beyond [7,45,48,57], and define the state-of-the-art in class-conditional image synthesis [15,31] and super-resolution [72]. Moreover, even unconditional DMs can readily be applied to tasks such as inpainting and colorization [85] or stroke-based synthesis [53], in contrast to other types of generative models [19,46,69]. Being likelihood-based models, they do not exhibit mode-collapse and training instabilities as GANs and, by heavily exploiting parameter sharing, they can model highly complex distributions of natural images without involving billions of parameters as in AR models [67].\\n\\nDemocratizing High-Resolution Image Synthesis DMs belong to the class of likelihood-based models, whose mode-covering behavior makes them prone to spend excessive amounts of capacity (and thus compute resources) on modeling imperceptible details of the data [16, 73]. Although the reweighted variational objective [30] aims to address this by undersampling the initial denoising steps, DMs are still computationally demanding, since training and evaluating such a model requires repeated function evaluations (and gradient computations) in the high-dimensional space of RGB images. As an example, training the most powerful DMs often takes hundreds of GPU days (e.g. 150 - 1000 V100 days in [15]) and repeated evaluations on a noisy version of the input space render also inference expensive,\\n\\nso that producing 50k samples takes approximately 5 days [15] on a single A100 GPU. This has two consequences for the research community and users in general: Firstly, training such a model requires massive computational resources only available to a small fraction of the field, and leaves a huge carbon footprint [65,86]. Secondly, evaluating an already trained model is also expensive in time and memory, since the same model architecture must run sequentially for a large number of steps (e.g. 25 - 1000 steps in [15]).\\n\\nTo increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.\\n\\nDeparture to Latent Space Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a perceptual compression stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (semantic compression). We thus aim to first find a perceptually equivalent, but computationally more suitable space, in which we will train diffusion models for high-resolution image synthesis.\\n\\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23, 66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class Latent Diffusion Models (LDMs).\\n\\nA notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM\\'s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.\\n\\nIn sum, our work makes the following contributions:\\n\\n(i) In contrast to purely transformer-based approaches [23, 66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2. Illustrating perceptual and semantic compression: Most bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details. Data and images from [30].\\n\\napplied to high-resolution synthesis of megapixel images.\\n\\n(ii) We achieve competitive performance on multiple tasks (unconditional image synthesis, inpainting, stochastic super-resolution) and datasets while significantly lowering computational costs. Compared to pixel-based diffusion approaches, we also significantly decrease inference costs.\\n(iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.\\n(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of  $\\\\sim 1024^2$  px.\\n(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models.\\n(vi) Finally, we release pretrained latent diffusion and autoencoding models at https://github.com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81].\\n\\n# 2. Related Work\\n\\nGenerative Models for Image Synthesis The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN) [27] allow for efficient sampling of high resolution images with good perceptual quality [3, 42], but are diffi\\n\\nult to optimize *[2, 28, 54]* and struggle to capture the full data distribution *[55]*. In contrast, likelihood-based methods emphasize good density estimation which renders optimization more well-behaved. Variational autoencoders (VAE) *[46]* and flow-based models *[18, 19]* enable efficient synthesis of high resolution images *[9, 44, 92]*, but sample quality is not on par with GANs. While autoregressive models (ARM) *[6, 10, 94, 95]* achieve strong performance in density estimation, computationally demanding architectures *[97]* and a sequential sampling process limit them to low resolution images. Because pixel based representations of images contain barely perceptible, high-frequency details *[16, 73]*, maximum-likelihood training spends a disproportionate amount of capacity on modeling them, resulting in long training times. To scale to higher resolutions, several two-stage approaches *[23, 67, 101, 103]* use ARMs to model a compressed latent image space instead of raw pixels.\\n\\nRecently, Diffusion Probabilistic Models (DM) *[82]*, have achieved state-of-the-art results in density estimation *[45]* as well as in sample quality *[15]*. The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet *[15, 30, 71, 85]*. The best synthesis quality is usually achieved when a reweighted objective *[30]* is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies *[47, 75, 84]* and hierarchical approaches *[31, 93]*, training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed LDMs, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1).\\n\\nTwo-Stage Image Synthesis To mitigate the shortcomings of individual generative approaches, a lot of research *[11, 23, 67, 70, 101, 103]* has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ-VAEs *[67, 101]* use autoregressive models to learn an expressive prior over a discretized latent space. *[66]* extend this approach to text-to-image generation by learning a joint distribution over discretized image and text representations. More generally, *[70]* uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs *[23, 103]* employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters *[23, 66]*, limit the overall performance of such approaches and less compression comes at the price of high computational cost *[23, 66]*. Our work prevents such trade-offs, as our proposed LDMs scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing high-fidelity reconstructions (see Fig. 1).\\n\\nWhile approaches to jointly *[93]* or separately *[80]* learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities *[11]* and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces.\\n\\n## 3 Method\\n\\nTo lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms *[30]*, they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources.\\n\\nWe propose to circumvent this drawback by introducing an explicit separation of the compressive from the generative learning phase (see Fig. 2). To achieve this, we utilize an autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly reduced computational complexity.\\n\\nSuch an approach offers several advantages: (i) By leaving the high-dimensional image space, we obtain DMs which are computationally much more efficient because sampling is performed on a low-dimensional space. (ii) We exploit the inductive bias of DMs inherited from their UNet architecture *[71]*, which makes them particularly effective for data with spatial structure and therefore alleviates the need for aggressive, quality-reducing compression levels as required by previous approaches *[23, 66]*. (iii) Finally, we obtain general-purpose compression models whose latent space can be used to train multiple generative models and which can also be utilized for other downstream applications such as single-image CLIP-guided synthesis *[25]*.\\n\\n### 3.1 Perceptual Image Compression\\n\\nOur perceptual compression model is based on previous work *[23]* and consists of an autoencoder trained by combination of a perceptual loss *[106]* and a patch-based *[33]* adversarial objective *[20, 23, 103]*. This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids bluriness introduced by relying solely on pixel-space losses such as $L_{2}$ or $L_{1}$ objectives.\\n\\nMore precisely, given an image $x\\\\in\\\\mathbb{R}^{H\\\\times W\\\\times 3}$ in RGB space, the encoder $\\\\mathcal{E}$ encodes $x$ into a latent representa\\n\\ntion $z=\\\\mathcal{E}(x)$, and the decoder $\\\\mathcal{D}$ reconstructs the image from the latent, giving $\\\\tilde{x}=\\\\mathcal{D}(z)=\\\\mathcal{D}(\\\\mathcal{E}(x))$, where $z\\\\in\\\\mathbb{R}^{h\\\\times w\\\\times c}$. Importantly, the encoder *downsamples* the image by a factor $f=H/h=W/w$, and we investigate different downsampling factors $f=2^{m}$, with $m\\\\in\\\\mathbb{N}$.\\n\\nIn order to avoid arbitrarily high-variance latent spaces, we experiment with two different kinds of regularizations. The first variant, *KL-reg.*, imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE *[46, 69]*, whereas *VQ-reg.* uses a vector quantization layer *[96]* within the decoder. This model can be interpreted as a VQGAN *[23]* but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space $z=\\\\mathcal{E}(x)$, we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works *[23, 66]*, which relied on an arbitrary 1D ordering of the learned space $z$ to model its distribution autoregressively and thereby ignored much of the inherent structure of $z$. Hence, our compression model preserves details of $x$ better (see Tab. 8). The full objective and training details can be found in the supplement.\\n\\n### 3.2 Latent Diffusion Models\\n\\nDiffusion Models *[82]* are probabilistic models designed to learn a data distribution $p(x)$ by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length $T$. For image synthesis, the most successful models *[15, 30, 72]* rely on a reweighted variant of the variational lower bound on $p(x)$, which mirrors denoising score-matching *[85]*. These models can be interpreted as an equally weighted sequence of denoising autoencoders $\\\\epsilon_{\\\\theta}(x_{t},t);\\\\,t=1\\\\dots T$, which are trained to predict a denoised variant of their input $x_{t}$, where $x_{t}$ is a noisy version of the input $x$. The corresponding objective can be simplified to (Sec. B)\\n\\n$L_{DM}=\\\\mathbb{E}_{x,\\\\epsilon\\\\sim\\\\mathcal{N}(0,1),t}\\\\Big{[}\\\\|\\\\epsilon-\\\\epsilon_{\\\\theta}(x_{t},t)\\\\|_{2}^{2}\\\\Big{]}\\\\,,$ (1)\\n\\nwith $t$ uniformly sampled from $\\\\{1,\\\\dots,T\\\\}$.\\n\\nGenerative Modeling of Latent Representations With our trained perceptual compression models consisting of $\\\\mathcal{E}$ and $\\\\mathcal{D}$, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.\\n\\nUnlike previous work that relied on autoregressive, attention-based transformer models in a highly compressed, discrete latent space *[23, 66, 103]*, we can take advantage of image-specific inductive biases that our model offers. This includes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads\\n\\n$L_{LDM}:=\\\\mathbb{E}_{\\\\mathcal{E}(x),\\\\epsilon\\\\sim\\\\mathcal{N}(0,1),t}\\\\Big{[}\\\\|\\\\epsilon-\\\\epsilon_{\\\\theta}(z_{t},t)\\\\|_{2}^{2}\\\\Big{]}\\\\,.$ (2)\\n\\nThe neural backbone $\\\\epsilon_{\\\\theta}(\\\\circ,t)$ of our model is realized as a time-conditional UNet *[71]*. Since the forward process is fixed, $z_{t}$ can be efficiently obtained from $\\\\mathcal{E}$ during training, and samples from $p(z)$ can be decoded to image space with a single pass through $\\\\mathcal{D}$.\\n\\n### 3.3 Conditioning Mechanisms\\n\\nSimilar to other types of generative models *[56, 83]*, diffusion models are in principle capable of modeling conditional distributions of the form $p(z|y)$. This can be implemented with a conditional denoising autoencoder $\\\\epsilon_{\\\\theta}(z_{t},t,y)$ and paves the way to controlling the synthesis process through inputs $y$ such as text *[68]*, semantic maps *[33, 61]* or other image-to-image translation tasks *[34]*.\\n\\nIn the context of image synthesis, however, combining the generative power of DMs with other types of conditionings beyond class-labels *[15]* or blurred variants of the input image *[72]* is so far an under-explored area of research.\\n\\nWe turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism *[97]*, which is effective for learning attention-based models of various input modalities *[35, 36]*. To pre-process $y$ from various modalities (such as language prompts) we introduce a domain specific encoder $\\\\tau_{\\\\theta}$ that projects $y$ to an intermediate representation $\\\\tau_{\\\\theta}(y)\\\\in\\\\mathbb{R}^{M\\\\times d_{\\\\tau}}$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing $\\\\text{Attention}(Q,K,V)=\\\\text{softmax}\\\\left(\\\\frac{QK^{T}}{\\\\sqrt{d}}\\\\right)\\\\cdot V$, with\\n\\n$Q=W_{Q}^{(i)}\\\\cdot\\\\varphi_{i}(z_{t}),\\\\;K=W_{K}^{(i)}\\\\cdot\\\\tau_{\\\\theta}(y),\\\\;V=W_{V}^{(i)}\\\\cdot\\\\tau_{\\\\theta}(y).$\\n\\nHere, $\\\\varphi_{i}(z_{t})\\\\in\\\\mathbb{R}^{N\\\\times d_{\\\\epsilon}^{i}}$ denotes a (flattened) intermediate representation of the UNet implementing $\\\\epsilon_{\\\\theta}$ and $W_{V}^{(i)}\\\\in\\\\mathbb{R}^{N\\\\times d_{\\\\epsilon}^{i}}$\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 4. Samples from LDMs trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of  $256 \\\\times 256$ . Best viewed when zoomed in. For more samples cf. the supplement.\\n\\n$\\\\mathbb{R}^{d\\\\times d_r^i}$ ,  $W_{Q}^{(i)}\\\\in \\\\mathbb{R}^{d\\\\times d_{r}}$  &amp;  $W_{K}^{(i)}\\\\in \\\\mathbb{R}^{d\\\\times d_{r}}$  are learnable projection matrices [36, 97]. See Fig. 3 for a visual depiction.\\n\\nBased on image-conditioning pairs, we then learn the conditional LDM via\\n\\n$$\\nL _ {L D M} := \\\\mathbb {E} _ {\\\\mathcal {E} (x), y, \\\\epsilon \\\\sim \\\\mathcal {N} (0, 1), t} \\\\left[ \\\\| \\\\epsilon - \\\\epsilon_ {\\\\theta} \\\\left(z _ {t}, t, \\\\tau_ {\\\\theta} (y)\\\\right) \\\\| _ {2} ^ {2} \\\\right], \\\\tag {3}\\n$$\\n\\nwhere both  $\\\\tau_{\\\\theta}$  and  $\\\\epsilon_{\\\\theta}$  are jointly optimized via Eq. 3. This conditioning mechanism is flexible as  $\\\\tau_{\\\\theta}$  can be parameterized with domain-specific experts, e.g. (unmasked) transformers [97] when  $y$  are text prompts (see Sec. 4.3.1)\\n\\n# 4. Experiments\\n\\nLDMs provide means to flexible and computationally tractable diffusion based image synthesis of various image modalities, which we empirically show in the following. Firstly, however, we analyze the gains of our models compared to pixel-based diffusion models in both training and inference. Interestingly, we find that LDMs trained in  $VQ$ -regularized latent spaces sometimes achieve better sample quality, even though the reconstruction capabilities of  $VQ$ -regularized first stage models slightly fall behind those of their continuous counterparts, cf. Tab. 8. A visual comparison between the effects of first stage regularization schemes on LDM training and their generalization abilities to resolutions  $&gt;256^2$  can be found in Appendix D.1. In E.2 we list details on architecture, implementation, training and evaluation for all results presented in this section.\\n\\n# 4.1. On Perceptual Compression Tradeoffs\\n\\nThis section analyzes the behavior of our LDMs with different downsampling factors  $f \\\\in \\\\{1,2,4,8,16,32\\\\}$  (abbreviated as  $LDM - f$ , where  $LDM - 1$  corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters.\\n\\nTab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the LDMs com\\n\\npared in this section. Fig. 6 shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet [12] dataset. We see that, i) small downsampling factors for  $LDM - \\\\{1,2\\\\}$  result in slow training progress, whereas ii) overly large values of  $f$  cause stagnating fidelity after comparably few training steps. Revisiting the analysis above (Fig. 1 and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality.  $LDM - \\\\{4 - 16\\\\}$  strike a good balance between efficiency and perceptually faithful results, which manifests in a significant FID [29] gap of 38 between pixel-based diffusion ( $LDM - 1$ ) and  $LDM - 8$  after 2M training steps.\\n\\nIn Fig. 7, we compare models trained on CelebAHQ [39] and ImageNet in terms of sampling speed for different numbers of denoising steps with the DDIM sampler [84] and plot it against FID-scores [29].  $LDM - \\\\{4 - 8\\\\}$  outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based  $LDM - 1$ , they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. In summary,  $LDM - 4$  and  $-8$  offer the best conditions for achieving high-quality synthesis results.\\n\\n# 4.2. Image Generation with Latent Diffusion\\n\\nWe train unconditional models of  $256^{2}$  images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space\\n\\nText-to-Image Synthesis on LAION. 1.45B Model.\\n\\n|  \\'A street sign that reads \"Latent Diffusion\" \\' | \\'A zombie in the style of Picasso\\' | \\'An image of an animal half mouse half octopus\\' | \\'An illustration of a slightly conscious neural network\\' | \\'A painting of a squirrel eating a burger\\' | \\'A watercolor painting of a chair that looks like an octopus\\' | \\'A shirt with the inscription: \"I love generative models!\" \\'  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  LATENT DIFFUSION |  |  |  |  |  |   |\\n|  LATETEN DIFFUSION |  |  |  |  |  |   |\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 5. Samples for user-defined text prompts from our model for text-to-image synthesis, LDM-8 (KL), which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and  $\\\\eta = 1.0$ . We use unconditional guidance [32] with  $s = 10.0$ .\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 6. Analyzing the training of class-conditional LDMs with different downsampling factors  $f$  over 2M train steps on the ImageNet dataset. Pixel-based LDM-1 requires substantially larger train times compared to models with larger downsampling factors (LDM-{4-16}). Too much perceptual compression as in LDM-32 limits the overall sample quality. All models are trained on a single NVIDIA A100 with the same computational budget. Results obtained with 100 DDIM steps [84] and  $\\\\kappa = 0$ .\\nFigure 7. Comparing LDMs with varying compression on the CelebA-HQ (left) and ImageNet (right) datasets. Different markers indicate  $\\\\{10,20,50,100,200\\\\}$  sampling steps using DDIM, from right to left along each line. The dashed line shows the FID scores for 200 steps, indicating the strong performance of LDM- $\\\\{4-8\\\\}$ . FID scores assessed on 5000 samples. All models were trained for  $500\\\\mathrm{k}$  (CelebA)/2M (ImageNet) steps on an A100.\\n\\nand avoid the difficulty of weighing reconstruction quality against learning the prior over the latent space, see Fig. 1-2.\\n\\nWe outperform prior diffusion based approaches on all but the LSUN-Bedrooms dataset, where our score is close to ADM [15], despite utilizing half its parameters and requiring 4-times less train resources (see Appendix E.3.5).\\n\\n|  CelebA-HQ 256 × 256 |   |   |   | FFHQ 256 × 256  |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Method | FID ↓ | Prec. ↑ | Recall ↑ | Method | FID ↓ | Prec. ↑ | Recall ↑  |\\n|  DC-VAE [63] | 15.8 | - | - | ImageBART [21] | 9.57 | - | -  |\\n|  VQGAN-T [23] (n=400) | 10.2 | - | - | U-Net GAN (vaug) [73] | 10.9 (7.6) | - | -  |\\n|  PGGAN [39] | 8.0 | - | - | UDM [43] | 5.54 | - | -  |\\n|  LSUM [85] | 7.22 | - | - | StyleGAN [41] | 6.16 | 0.71 | 0.46  |\\n|  UDM [43] | 7.16 | - | - | ProjectedGAN [70] | 3.88 | 0.63 | 0.46  |\\n|  LDM-4 (ours, 500-s†) | 5.11 | 0.72 | 0.49 | LDM-4 (ours, 200-s) | 4.98 | 0.73 | 0.50  |\\n|  LSUN-Chutches 256 × 256 |   |   |   | LSUN-Bedrooms 256 × 256  |   |   |   |\\n|  Method | FID ↓ | Prec. ↑ | Recall ↑ | Method | FID ↓ | Prec. ↑ | Recall ↑  |\\n|  DDPM [30] | 7.89 | - | - | ImageBART [21] | 5.51 | - | -  |\\n|  ImageBART [21] | 7.32 | - | - | DDPM [30] | 4.9 | - | -  |\\n|  PGGAN [39] | 6.42 | - | - | UDM [43] | 4.57 | - | -  |\\n|  StyleGAN [41] | 4.21 | - | - | StyleGAN [41] | 2.35 | 0.59 | 0.48  |\\n|  StyleGAN2 [42] | 3.86 | - | - | ADM [15] | 1.90 | 0.66 | 0.51  |\\n|  ProjectedGAN [70] | 1.59 | 0.61 | 0.44 | ProjectedGAN [70] | 1.52 | 0.61 | 0.34  |\\n|  LDM-8* (ours, 200-s) | 4.02 | 0.64 | 0.52 | LDM-4 (ours, 200-s) | 2.95 | 0.66 | 0.48  |\\n\\nTable 1. Evaluation metrics for unconditional image synthesis. CelebA-HQ results reproduced from [43, 63, 100], FFHQ from [42, 43].  $\\\\dagger$ :  $N$ -s refers to  $N$  sampling steps with the DDIM [84] sampler.  $\\\\star$ : trained in KL-regularized latent space. Additional results can be found in the supplementary.\\n\\n|  Text-Conditional Image Synthesis  |   |   |   |   |\\n| --- | --- | --- | --- | --- |\\n|  Method | FID ↓ | IS↑ | Nparams |   |\\n|  CogView† [17] | 27.10 | 18.20 | 4B | self-ranking, rejection rate 0.017  |\\n|  LAFITE† [109] | 26.94 | 26.02 | 75M |   |\\n|  GLIDE* [59] | 12.24 | - | 6B | 277 DDIM steps, c.f.g. [32] s = 3  |\\n|  Make-A-Scene* [26] | 11.84 | - | 4B | c.f.g for AR models [58] s = 5  |\\n|  LDM-KL-8 | 23.31 | 20.03±0.31 | 1.45B | 250 DDIM steps  |\\n|  LDM-KL-8-G* | 12.63 | 30.29±0.01 | 1.45B | 250 DDIM steps, c.f.g. [32] s = 1.5  |\\n\\nTable 2. Evaluation of text-conditional image synthesis on the  $256 \\\\times 256$ -sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters.  $\\\\dagger / \\\\star$ :Numbers from [109]/[26]\\n\\nMoreover, LDMs consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset.\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 8. Layout-to-image synthesis with an LDM on COCO [4], see Sec. 4.3.1. Quantitative evaluation in the supplement D.3.\\n\\n# 4.3. Conditional Latent Diffusion\\n\\n# 4.3.1 Transformer Encoders for LDMs\\n\\nBy introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For text-to-image image modeling, we train a 1.45B parameter  $KL$ -regularized LDM conditioned on language prompts on LAION-400M [78]. We employ the BERT-tokenizer [14] and implement  $\\\\tau_{\\\\theta}$  as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) cross-attention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, cf. Fig. 8 and 5. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [51] validation set, where our model improves upon powerful AR [17,66] and GAN-based [109] methods, cf. Tab. 2. We note that applying classifier-free diffusion guidance [32] greatly boosts sample quality, such that the guided LDM-KL-8-G is on par with the recent state-of-the-art AR [26] and diffusion models [59] for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on semantic layouts on OpenImages [49], and finetune on COCO [4], see Fig. 8. See Sec. D.3 for the quantitative evaluation and implementation details.\\n\\nLastly, following prior work [3, 15, 21, 23], we evaluate our best-performing class-conditional ImageNet models with  $f \\\\in \\\\{4,8\\\\}$  from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. D.4. Here we outperform the state of the art diffusion model ADM [15] while significantly reducing computational requirements and parameter count, cf. Tab 18.\\n\\n# 4.3.2 Convolutional Sampling Beyond  $256^{2}$\\n\\nBy concatenating spatially aligned conditioning information to the input of  $\\\\epsilon_{\\\\theta}$ ,  $LDMs$  can serve as efficient general-\\n\\n|  Method | FID↓ | IST | Precision↑ | Recall↑ | Nparams |   |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  BigGan-deep [3] | 6.95 | 203.6±14 | 0.87 | 0.28 | 340M | -  |\\n|  ADM [15] | 10.94 | 100.98 | 0.69 | 0.63 | 554M | 250 DDIM steps  |\\n|  ADM-G [15] | 4.59 | 186.7 | 0.82 | 0.52 | 600M | 250 DDIM steps  |\\n|  LDM-4 (ours) | 10.56 | 103.49±1.24 | 0.71 | 0.62 | 400M | 250 DDIM steps  |\\n|  LDM-4-G (ours) | 3.60 | 247.67±0.99 | 0.87 | 0.48 | 400M | 250 steps, c.f.g [32], s = 1.5  |\\n\\nTable 3. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on ImageNet [12]. A more detailed comparison with additional baselines can be found in D.4, Tab. 10 and F. c.f.g. denotes classifier-free guidance with a scale  $s$  as proposed in [32].\\n\\npurpose image-to-image translation models. We use this to train models for semantic synthesis, super-resolution (Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthesis, we use images of landscapes paired with semantic maps [23, 61] and concatenate downsampled versions of the semantic maps with the latent image representation of a  $f = 4$  model (VQ-reg., see Tab. 8). We train on an input resolution of  $256^2$  (crops from  $384^2$ ) but find that our model generalizes to larger resolutions and can generate images up to the megapixel regime when evaluated in a convolutional manner (see Fig. 9). We exploit this behavior to also apply the super-resolution models in Sec. 4.4 and the inpainting models in Sec. 4.5 to generate large images between  $512^2$  and  $1024^2$ . For this application, the signal-to-noise ratio (induced by the scale of the latent space) significantly affects the results. In Sec. D.1 we illustrate this when learning an LDM on (i) the latent space as provided by a  $f = 4$  model (KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by the component-wise standard deviation.\\n\\nThe latter, in combination with classifier-free guidance [32], also enables the direct synthesis of  $&gt;256^2$  images for the text-conditional LDM-KL-8-G as in Fig. 13.\\n\\n![img-6.jpeg](img-6.jpeg)\\nFigure 9. A LDM trained on  $256^{2}$  resolution can generalize to larger resolution (here:  $512 \\\\times 1024$ ) for spatially conditioned tasks such as semantic synthesis of landscape images. See Sec. 4.3.2.\\n\\n# 4.4. Super-Resolution with Latent Diffusion\\n\\nLDMs can be efficiently trained for super-resolution by directly conditioning on low-resolution images via concatenation (cf. Sec. 3.3). In a first experiment, we follow SR3\\n\\n![img-7.jpeg](img-7.jpeg)\\nFigure 10. ImageNet  $64\\\\rightarrow 256$  super-resolution on ImageNet-Val. LDM-SR has advantages at rendering realistic textures but SR3 can synthesize more coherent fine structures. See appendix for additional samples and cropouts. SR3 results from [72].\\n\\n[72] and fix the image degradation to a bicubic interpolation with  $4 \\\\times$ -downsampling and train on ImageNet following SR3\\'s data processing pipeline. We use the  $f = 4$  autoencoding model pretrained on OpenImages (VQ-reg., cf. Tab. 8) and concatenate the low-resolution conditioning  $y$  and the inputs to the UNet, i.e.  $\\\\tau_{\\\\theta}$  is the identity. Our qualitative and quantitative results (see Fig. 10 and Tab. 5) show competitive performance and LDM-SR outperforms SR3 in FID while SR3 has a better IS. A simple image regression model achieves the highest PSNR and SSIM scores; however these metrics do not align well with human perception [106] and favor blurriness over imperfectly aligned high frequency details [72]. Further, we conduct a user study comparing the pixel-baseline with LDM-SR. We follow SR3 [72] where human subjects were shown a low-res image in between two high-res images and asked for preference. The results in Tab. 4 affirm the good performance of LDM-SR. PSNR and SSIM can be pushed by using a post-hoc guiding mechanism [15] and we implement this image-based guider via a perceptual loss, see Sec. D.6.\\n\\n|  User Study | SR on ImageNet |   | Inpainting on Places  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  Pixel-DM (f1) | LDM-4 | LAMA [88] | LDM-4  |\\n|  Task 1: Preference vs GT ↑ | 16.0% | 30.4% | 13.6% | 21.0%  |\\n|  Task 2: Preference Score ↑ | 29.4% | 70.6% | 31.9% | 68.1%  |\\n\\nSince the bicubic degradation process does not generalize well to images which do not follow this pre-processing, we also train a generic model, LDM-BSR, by using more diverse degradation. The results are shown in Sec. D.6.1.\\n\\nTable 4. Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in E.3.6\\n\\n|  Method | FID ↓ | IS ↑ | PSNR ↑ | SSIM ↑ | Nparents ↓  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Image Regression [72] | 15.2 | 121.1 | 27.9 | 0.801 | 625M  |\\n|  SR3 [72] | 5.2 | 180.1 | 26.4 | 0.762 | 625M  |\\n|  LDM-4 (ours, 100 steps) | 2.8†/4.8‡ | 166.3 | 24.4±3.6 | 0.69±0.44 | 169M  |\\n|  empGLDM-4 (ours, big, 100 steps) | 2.4†/4.3‡ | 174.9 | 24.7±4.1 | 0.71±0.15 | 552M  |\\n|  LDM-4 (ours, 50 steps, guiding) | 4.4†/6.4‡ | 153.7 | 25.8±3.7 | 0.74±0.21 | 184M  |\\n\\nTable 5.  $\\\\times 4$  upscaling results on ImageNet-Val.  $(256^{2})$ ; †: FID features computed on validation split, ‡: FID features computed on train split; *: Assessed on a NVIDIA A100\\n\\n|  Model (reg.-type) | train throughput samples/sec. | sampling throughput† @256 | @512 | train+val hours/epoch | FID@2k epoch 6  |\\n| --- | --- | --- | --- | --- | --- |\\n|  LDM-1 (no first stage) | 0.11 | 0.26 | 0.07 | 20.66 | 24.74  |\\n|  LDM-4 (KL, w/ attn) | 0.32 | 0.97 | 0.34 | 7.66 | 15.21  |\\n|  LDM-4 (VQ, w/ attn) | 0.33 | 0.97 | 0.34 | 7.04 | 14.99  |\\n|  LDM-4 (VQ, w/o attn) | 0.35 | 0.99 | 0.36 | 6.66 | 15.95  |\\n\\nTable 6. Assessing inpainting efficiency. †: Deviations from Fig. 7 due to varying GPU settings/batch sizes cf. the supplement.\\n\\n# 4.5. Inpainting with Latent Diffusion\\n\\nInpainting is the task of filling masked regions of an image with new content either because parts of the image are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa [88], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8]. The exact training &amp; evaluation protocol on Places [108] is described in Sec. E.2.2.\\n\\nWe first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of LDM-1 (i.e. a pixel-based conditional DM) with LDM-4, for both KL and VQ regularizations, as well as VQ-LDM-4 without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution  $256^2$  and  $512^2$ , the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least  $2.7 \\\\times$  between pixel- and latent-based diffusion models while improving FID scores by a factor of at least  $1.6 \\\\times$ .\\n\\nThe comparison with other inpainting approaches in Tab. 7 shows that our model with attention improves the overall image quality as measured by FID over that of [88]. LPIPS between the unmasked images and our samples is slightly higher than that of [88]. We attribute this to [88] only producing a single result which tends to recover more of an average image compared to the diverse results produced by our LDM cf. Fig. 21. Additionally in a user study (Tab. 4) human subjects favor our results over those of [88].\\n\\nBased on these initial results, we also trained a larger diffusion model (big in Tab. 7) in the latent space of the  $VQ$ -regularized first stage without attention. Following [15], the UNet of this diffusion model uses attention layers on three levels of its feature hierarchy, the BigGAN [3] residual block for up- and downsampling and has 387M parameters\\n\\n![img-8.jpeg](img-8.jpeg)\\nFigure 11. Qualitative results on object removal with our big, w/ ft inpainting model. For more results, see Fig. 22.\\n\\ninstead of 215M. After training, we noticed a discrepancy in the quality of samples produced at resolutions  $256^2$  and  $512^2$ , which we hypothesize to be caused by the additional attention modules. However, fine-tuning the model for half an epoch at resolution  $512^2$  allows the model to adjust to the new feature statistics and sets a new state of the art FID on image inpainting (big, w/o attn, w/ft in Tab. 7, Fig. 11.).\\n\\n# 5. Limitations &amp; Societal Impact\\n\\nLimitations While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our  $f = 4$  autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect.\\n\\nSocietal Impact Generative models for media like imagery are a double-edged sword: On the one hand, they\\n\\n|  Method | 40-50% masked |   | All samples  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  FID ↓ | LPIPS ↓ | FID ↓ | LPIPS ↓  |\\n|  LDM-4 (ours, big, w/ ft) | 9.39 | 0.246±0.042 | 1.50 | 0.137±0.080  |\\n|  LDM-4 (ours, big, w/o ft) | 12.89 | 0.257±0.047 | 2.40 | 0.142±0.085  |\\n|  LDM-4 (ours, w/ attn) | 11.87 | 0.257±0.042 | 2.15 | 0.144±0.084  |\\n|  LDM-4 (ours, w/o attn) | 12.60 | 0.259±0.041 | 2.37 | 0.145±0.084  |\\n|  LaMa [88]† | 12.31 | 0.243±0.038 | 2.23 | 0.134±0.080  |\\n|  LaMa [88] | 12.0 | 0.24 | 2.21 | 0.14  |\\n|  CoModGAN [107] | 10.4 | 0.26 | 1.82 | 0.15  |\\n|  RegionWise [52] | 21.3 | 0.27 | 4.75 | 0.15  |\\n|  DeepFill v2 [104] | 22.1 | 0.28 | 5.20 | 0.16  |\\n|  EdgeConnect [58] | 30.5 | 0.28 | 8.37 | 0.16  |\\n\\nTable 7. Comparison of inpainting performance on 30k crops of size  $512 \\\\times 512$  from test images of Places [108]. The column  $40-50\\\\%$  reports metrics computed over hard examples where  $40-50\\\\%$  of the image region have to be inpainted.  $\\\\dagger$  recomputed on our test set, since the original test set used in [88] was not available.\\n\\nenable various creative applications, and in particular approaches like ours that reduce the cost of training and inference have the potential to facilitate access to this technology and democratize its exploration. On the other hand, it also means that it becomes easier to create and disseminate manipulated data or spread misinformation and spam. In particular, the deliberate manipulation of images (\"deep fakes\") is a common problem in this context, and women in particular are disproportionately affected by it [13, 24].\\n\\nGenerative models can also reveal their training data [5, 90], which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood.\\n\\nFinally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data [22, 38, 91]. While diffusion models achieve better coverage of the data distribution than e.g. GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question.\\n\\nFor a more general, detailed discussion of the ethical considerations of deep generative models, see e.g. [13].\\n\\n# 6. Conclusion\\n\\nWe have presented latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and our cross-attention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of conditional image synthesis tasks without task-specific architectures.\\n\\nReferences\\n\\n- [1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 challenge on single image super-resolution: Dataset and study. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1122–1131. IEEE Computer Society, 2017.\\n- [2] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan, 2017.\\n- [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In Int. Conf. Learn. Represent., 2019.\\n- [4] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1209–1218. Computer Vision Foundation / IEEE Computer Society, 2018.\\n- [5] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633–2650, 2021.\\n- [6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pre-training from pixels. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 1691–1703. PMLR, 2020.\\n- [7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In ICLR. OpenReview.net, 2021.\\n- [8] Lu Chi, Borui Jiang, and Yadong Mu. Fast fourier convolution. In NeurIPS, 2020.\\n- [9] Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. CoRR, abs/2011.10650, 2020.\\n- [10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019.\\n- [11] Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In ICLR (Poster). OpenReview.net, 2019.\\n- [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248–255. IEEE Computer Society, 2009.\\n- [13] Emily Denton. Ethical considerations of generative ai. AI for Content Creation Workshop, CVPR, 2021.\\n- [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.\\n- [15] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. CoRR, abs/2105.05233, 2021.\\n- [16] Sander Dieleman. Musings on typicality, 2020.\\n- [17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. CoRR, abs/2105.13290, 2021.\\n- [18] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation, 2015.\\n- [19] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\\n- [20] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Adv. Neural Inform. Process. Syst., pages 658–666, 2016.\\n- [21] Patrick Esser, Robin Rombach, Andreas Blattmann, and Björn Ommer. Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. CoRR, abs/2108.08827, 2021.\\n- [22] Patrick Esser, Robin Rombach, and Björn Ommer. A note on data biases in generative models. arXiv preprint arXiv:2012.02516, 2020.\\n- [23] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. CoRR, abs/2012.09841, 2020.\\n- [24] Mary Anne Franks and Ari Ezra Waldman. Sex, lies, and videotape: Deep fakes and free speech delusions. Md. L. Rev., 78:892, 2018.\\n- [25] Kevin Frans, Lisa B. Soros, and Olaf Witkowski. Clipdraw: Exploring text-to-drawing synthesis through language-image encoders. ArXiv, abs/2106.14843, 2021.\\n- [26] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. CoRR, abs/2203.13131, 2022.\\n- [27] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, 2014.\\n- [28] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans, 2017.\\n- [29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Adv. Neural Inform. Process. Syst., pages 6626–6637, 2017.\\n- [30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.\\n- [31] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. CoRR, abs/2106.15282, 2021.\\n\\n[32] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.\\n- [33] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 5967–5976. IEEE Computer Society, 2017.\\n- [34] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5967–5976, 2017.\\n- [35] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and João Carreira. Perceiver IO: A general architecture for structured inputs &outputs. CoRR, abs/2107.14795, 2021.\\n- [36] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and João Carreira. Perceiver: General perception with iterative attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4651–4664. PMLR, 2021.\\n- [37] Manuel Jahn, Robin Rombach, and Björn Ommer. High-resolution complex scene synthesis with transformers. CoRR, abs/2105.06458, 2021.\\n- [38] Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda, and Subbarao Kambhampati. Imperfect imaganation: Implications of gans exacerbating biases on facial data augmentation and snapchat selfie lenses. arXiv preprint arXiv:2001.09528, 2020.\\n- [39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017.\\n- [40] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4401–4410, 2019.\\n- [41] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n- [42] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. CoRR, abs/1912.04958, 2019.\\n- [43] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Score matching model for unbounded data score. CoRR, abs/2106.05527, 2021.\\n- [44] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, 2018.\\n- [45] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. CoRR, abs/2107.00630, 2021.\\n- [46] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR, 2014.\\n- [47] Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. CoRR, abs/2106.00132, 2021.\\n- [48] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In ICLR. OpenReview.net, 2021.\\n- [49] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio Ferrari. The open images dataset V4: unified image classification, object detection, and visual relationship detection at scale. CoRR, abs/1811.00982, 2018.\\n- [50] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. CoRR, abs/1904.06991, 2019.\\n- [51] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014.\\n- [52] Yuqing Ma, Xianglong Liu, Shihao Bai, Le-Yi Wang, Aishan Liu, Dacheng Tao, and Edwin Hancock. Region-wise generative adversarial image inpainting for large missing areas. ArXiv, abs/1909.12507, 2019.\\n- [53] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and editing with stochastic differential equations. CoRR, abs/2108.01073, 2021.\\n- [54] Lars M. Mescheder. On the convergence properties of GAN training. CoRR, abs/1801.04406, 2018.\\n- [55] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\\n- [56] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014.\\n- [57] Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with diffusion models. CoRR, abs/2103.16091, 2021.\\n- [58] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, and Mehran Ebrahimi. Edgeconnect: Generative image inpainting with adversarial edge learning. ArXiv, abs/1901.00212, 2019.\\n- [59] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. CoRR, abs/2112.10741, 2021.\\n- [60] Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin.\\n\\nHigh-fidelity performance metrics for generative models in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zenodo.4957738.\\n- [61] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\\n- [62] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n- [63] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 823–832. Computer Vision Foundation / IEEE, 2021.\\n- [64] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising subtleties in fid calculation. arXiv preprint arXiv:2104.11222, 2021.\\n- [65] David A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. CoRR, abs/2104.10350, 2021.\\n- [66] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021.\\n- [67] Ali Razavi, Aäron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In NeurIPS, pages 14837–14847, 2019.\\n- [68] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In ICML, 2016.\\n- [69] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on International Conference on Machine Learning, ICML, 2014.\\n- [70] Robin Rombach, Patrick Esser, and Björn Ommer. Network-to-network translation with conditional invertible neural networks. In NeurIPS, 2020.\\n- [71] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI (3), volume 9351 of Lecture Notes in Computer Science, pages 234–241. Springer, 2015.\\n- [72] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. CoRR, abs/2104.07636, 2021.\\n- [73] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. CoRR, abs/1701.05517, 2017.\\n- [74] Dave Salvator. NVIDIA Developer Blog. https://developer.nvidia.com/blog/getting-immediate-speedups-with-a100-tf32, 2020.\\n- [75] Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion models. CoRR, abs/2104.02600, 2021.\\n- [76] Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger. Projected gans converge faster. CoRR, abs/2111.01007, 2021.\\n- [77] Edgar Schönfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative adversarial networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8204–8213. Computer Vision Foundation / IEEE, 2020.\\n- [78] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs, 2021.\\n- [79] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, Int. Conf. Learn. Represent., 2015.\\n- [80] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano Ermon. D2C: diffusion-denoising models for few-shot conditional generation. CoRR, abs/2106.06819, 2021.\\n- [81] Charlie Snell. Alien Dreams: An Emerging Art Scene. https://ml.berkeley.edu/blog/posts/clip-art/, 2021. [Online; accessed November-2021].\\n- [82] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. CoRR, abs/1503.03585, 2015.\\n- [83] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.\\n- [84] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR. OpenReview.net, 2021.\\n- [85] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. CoRR, abs/2011.13456, 2020.\\n- [86] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 13693–13696. AAAI Press, 2020.\\n\\n[87] Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. CoRR, abs/2003.11571, 2020.\\n- [88] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S. Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. ArXiv, abs/2109.07161, 2021.\\n- [89] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 2647–2655. AAAI Press, 2021.\\n- [90] Patrick Tinsley, Adam Czajka, and Patrick Flynn. This face does not exist… but it might be yours! identity leakage in generative models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1320–1328, 2021.\\n- [91] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521–1528. IEEE, 2011.\\n- [92] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In NeurIPS, 2020.\\n- [93] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. CoRR, abs/2106.05931, 2021.\\n- [94] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, 2016.\\n- [95] Aäron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. CoRR, abs/1601.06759, 2016.\\n- [96] Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NIPS, pages 6306–6315, 2017.\\n- [97] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 5998–6008, 2017.\\n- [98] Rivers Have Wings. Tweet on Classifier-free guidance for autoregressive models. https://twitter.com/RiversHaveWings/status/1478093658716966912, 2022.\\n- [99] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019.\\n- [100] Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. VAEBM: A symbiosis between variational autoencoders and energy-based models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\\n- [101] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. CoRR, abs/2104.10157, 2021.\\n- [102] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015.\\n- [103] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan, 2021.\\n- [104] Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. Free-form image inpainting with gated convolution. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4470–4479, 2019.\\n- [105] K. Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model for deep blind image super-resolution. ArXiv, abs/2103.14006, 2021.\\n- [106] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\\n- [107] Shengyu Zhao, Jianwei Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I-Chao Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. ArXiv, abs/2103.10428, 2021.\\n- [108] Bolei Zhou, Àgata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:1452–1464, 2018.\\n- [109] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. LAFITE: towards language-free training for text-to-image generation. CoRR, abs/2111.13792, 2021.\\n\\n# Appendix\\n\\n![img-9.jpeg](img-9.jpeg)\\n\\n![img-10.jpeg](img-10.jpeg)\\n\\n![img-11.jpeg](img-11.jpeg)\\nFigure 12. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on  $512^{2}$  images.\\n\\n\\'A painting of the last supper by Picasso.\\'\\n\\n![img-12.jpeg](img-12.jpeg)\\n\\'An oil painting of a latent space.\\'\\n\\n\\'An epic painting of Gandalf the Black summoning thunder and lightning in the mountains.\\'\\n\\n![img-13.jpeg](img-13.jpeg)\\n\\'A sunset over a mountain range, vector image.\\'\\n\\n![img-14.jpeg](img-14.jpeg)\\n\\n![img-15.jpeg](img-15.jpeg)\\nFigure 13. Combining classifier free diffusion guidance with the convolutional sampling strategy from Sec. 4.3.2, our 1.45B parameter text-to-image model can be used for rendering images larger than the native  $256^2$  resolution the model was trained on.\\n\\nA. Changelog\\n\\nHere we list changes between this version (https://arxiv.org/abs/2112.10752v2) of the paper and the previous version, i.e. https://arxiv.org/abs/2112.10752v1.\\n\\n- We updated the results on text-to-image synthesis in Sec. 4.3 which were obtained by training a new, larger model (1.45B parameters). This also includes a new comparison to very recent competing methods on this task that were published on arXiv at the same time as (*[59, 109]*) or after (*[26]*) the publication of our work.\\n- We updated results on class-conditional synthesis on ImageNet in Sec. 4.1, Tab. 3 (see also Sec. D.4) obtained by retraining the model with a larger batch size. The corresponding qualitative results in Fig. 26 and Fig. 27 were also updated. Both the updated text-to-image and the class-conditional model now use classifier-free guidance *[32]* as a measure to increase visual fidelity.\\n- We conducted a user study (following the scheme suggested by Saharia et al *[72]*) which provides additional evaluation for our inpainting (Sec. 4.5) and superresolution models (Sec. 4.4).\\n- Added Fig. 5 to the main paper, moved Fig. 18 to the appendix, added Fig. 13 to the appendix.\\n\\n## Appendix B Detailed Information on Denoising Diffusion Models\\n\\nDiffusion models can be specified in terms of a signal-to-noise ratio $\\\\text{SNR}(t)=\\\\frac{\\\\alpha_{t}^{2}}{\\\\sigma_{t}^{2}}$ consisting of sequences $(\\\\alpha_{t})_{t=1}^{T}$ and $(\\\\sigma_{t})_{t=1}^{T}$ which, starting from a data sample $x_{0}$, define a forward diffusion process $q$ as\\n\\n$q(x_{t}|x_{0})=\\\\mathcal{N}(x_{t}|\\\\alpha_{t}x_{0},\\\\sigma_{t}^{2}\\\\mathbb{I})$ (4)\\n\\nwith the Markov structure for $s<t$:\\n\\n$q(x_{t}|x_{s})$ $=\\\\mathcal{N}(x_{t}|\\\\alpha_{t|s}x_{s},\\\\sigma_{t|s}^{2}\\\\mathbb{I})$ (5)\\n$\\\\alpha_{t|s}$ $=\\\\frac{\\\\alpha_{t}}{\\\\alpha_{s}}$ (6)\\n$\\\\sigma_{t|s}^{2}$ $=\\\\sigma_{t}^{2}-\\\\alpha_{t|s}^{2}\\\\sigma_{s}^{2}$ (7)\\n\\nDenoising diffusion models are generative models $p(x_{0})$ which revert this process with a similar Markov structure running backward in time, i.e. they are specified as\\n\\n$p(x_{0})=\\\\int_{z}p(x_{T})\\\\prod_{t=1}^{T}p(x_{t-1}|x_{t})$ (8)\\n\\nThe evidence lower bound (ELBO) associated with this model then decomposes over the discrete time steps as\\n\\n$-\\\\log p(x_{0})\\\\leq\\\\mathbb{KL}(q(x_{T}|x_{0})|p(x_{T}))+\\\\sum_{t=1}^{T}\\\\mathbb{E}_{q(x_{t}|x_{0})}\\\\mathbb{KL}(q(x_{t-1}|x_{t},x_{0})|p(x_{t-1}|x_{t}))$ (9)\\n\\nThe prior $p(x_{T})$ is typically choosen as a standard normal distribution and the first term of the ELBO then depends only on the final signal-to-noise ratio $\\\\text{SNR}(T)$. To minimize the remaining terms, a common choice to parameterize $p(x_{t-1}|x_{t})$ is to specify it in terms of the true posterior $q(x_{t-1}|x_{t},x_{0})$ but with the unknown $x_{0}$ replaced by an estimate $x_{\\\\theta}(x_{t},t)$ based on the current step $x_{t}$. This gives *[45]*\\n\\n$p(x_{t-1}|x_{t})$ $\\\\coloneqq q(x_{t-1}|x_{t},x_{\\\\theta}(x_{t},t))$ (10)\\n$=\\\\mathcal{N}(x_{t-1}|\\\\mu_{\\\\theta}(x_{t},t),\\\\sigma_{t|t-1}^{2}\\\\frac{\\\\sigma_{t-1}^{2}}{\\\\sigma_{t}^{2}}\\\\mathbb{I}),$ (11)\\n\\nwhere the mean can be expressed as\\n\\n$\\\\mu_{\\\\theta}(x_{t},t)=\\\\frac{\\\\alpha_{t|t-1}\\\\sigma_{t-1}^{2}}{\\\\sigma_{t}^{2}}x_{t}+\\\\frac{\\\\alpha_{t-1}\\\\sigma_{t|t-1}^{2}}{\\\\sigma_{t}^{2}}x_{\\\\theta}(x_{t},t).$ (12)\\n\\nIn this case, the sum of the ELBO simplify to\\n\\n$\\\\sum_{t=1}^{T}\\\\mathbb{E}_{q(x_{t}|x_{0})}\\\\mathbb{KL}(q(x_{t-1}|x_{t},x_{0})|p(x_{t-1})=\\\\sum_{t=1}^{T}\\\\mathbb{E}_{\\\\mathcal{N}(\\\\epsilon|0,\\\\mathbb{I})}\\\\frac{1}{2}(\\\\text{SNR}(t-1)-\\\\text{SNR}(t))\\\\|x_{0}-x_{\\\\theta}(\\\\alpha_{t}x_{0}+\\\\sigma_{t}\\\\epsilon,t)\\\\|^{2}$ (13)\\n\\nFollowing *[30]*, we use the reparameterization\\n\\n$\\\\epsilon_{\\\\theta}(x_{t},t)=(x_{t}-\\\\alpha_{t}x_{\\\\theta}(x_{t},t))/\\\\sigma_{t}$ (14)\\n\\nto express the reconstruction term as a denoising objective,\\n\\n$\\\\|x_{0}-x_{\\\\theta}(\\\\alpha_{t}x_{0}+\\\\sigma_{t}\\\\epsilon,t)\\\\|^{2}=\\\\frac{\\\\sigma_{t}^{2}}{\\\\alpha_{t}^{2}}\\\\|\\\\epsilon-\\\\epsilon_{\\\\theta}(\\\\alpha_{t}x_{0}+\\\\sigma_{t}\\\\epsilon,t)\\\\|^{2}$ (15)\\n\\nand the reweighting, which assigns each of the terms the same weight and results in Eq. (1).\\n\\n# C. Image Guiding Mechanisms\\n\\n|  Samples 2562 | Guided Convolutional Samples 5122 | Convolutional Samples 5122  |\\n| --- | --- | --- |\\n|  |   |   |\\n|  |   |   |\\n|  |   |   |\\n|  |   |   |\\n|  |   |   |\\n\\nFigure 14. On landscapes, convolutional sampling with unconditional models can lead to homogeneous and incoherent global structures (see column 2).  $L_{2}$ -guiding with a low resolution image can help to reestablish coherent global structures.\\n\\nAn intriguing feature of diffusion models is that unconditional models can be conditioned at test-time [15, 82, 85]. In particular, [15] presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset with a classifier  $\\\\log p_{\\\\Phi}(y|x_t)$ , trained on each  $x_t$  of the diffusion process. We directly build on this formulation and introduce post-hoc image-guiding:\\n\\nFor an epsilon-parameterized model with fixed variance, the guiding algorithm as introduced in [15] reads:\\n\\n$$\\n\\\\hat {\\\\epsilon} \\\\leftarrow \\\\epsilon_ {\\\\theta} \\\\left(z _ {t}, t\\\\right) + \\\\sqrt {1 - \\\\alpha_ {t} ^ {2}} \\\\nabla_ {z _ {t}} \\\\log p _ {\\\\Phi} (y | z _ {t}). \\\\tag {16}\\n$$\\n\\nThis can be interpreted as an update correcting the \"score\"  $\\\\epsilon_{\\\\theta}$  with a conditional distribution  $\\\\log p_{\\\\Phi}(y|z_t)$ .\\n\\nSo far, this scenario has only been applied to single-class classification models. We re-interpret the guiding distribution  $p_{\\\\Phi}(y|T(\\\\mathcal{D}(z_0(z_t))))$  as a general purpose image-to-image translation task given a target image  $y$ , where  $T$  can be any differentiable transformation adopted to the image-to-image translation task at hand, such as the identity, a downsampling operation or similar.\\n\\nAs an example, we can assume a Gaussian guider with fixed variance $\\\\sigma^{2}=1$, such that\\n\\n$\\\\log p_{\\\\Phi}(y|z_{t})=-\\\\frac{1}{2}\\\\|y-T(\\\\mathcal{D}(z_{0}(z_{t})))\\\\|_{2}^{2}$ (17)\\n\\nbecomes a $L_{2}$ regression objective.\\n\\nFig. 14 demonstrates how this formulation can serve as an upsampling mechanism of an unconditional model trained on $256^{2}$ images, where unconditional samples of size $256^{2}$ guide the convolutional synthesis of $512^{2}$ images and $T$ is a $2\\\\times$ bicubic downsampling. Following this motivation, we also experiment with a perceptual similarity guiding and replace the $L_{2}$ objective with the LPIPS *[106]* metric, see Sec. 4.4.\\n\\n# D. Additional Results\\n\\n# D.1. Choosing the Signal-to-Noise Ratio for High-Resolution Synthesis\\n\\n![img-16.jpeg](img-16.jpeg)\\nFigure 15. Illustrating the effect of latent space rescaling on convolutional sampling, here for semantic image synthesis on landscapes. See Sec. 4.3.2 and Sec. D.1.\\n\\nAs discussed in Sec. 4.3.2, the signal-to-noise ratio induced by the variance of the latent space (i.e.  $\\\\mathrm{Var}(z) / \\\\sigma_t^2$ ) significantly affects the results for convolutional sampling. For example, when training a LDM directly in the latent space of a KL-regularized model (see Tab. 8), this ratio is very high, such that the model allocates a lot of semantic detail early on in the reverse denoising process. In contrast, when rescaling the latent space by the component-wise standard deviation of the latents as described in Sec. G, the SNR is decreased. We illustrate the effect on convolutional sampling for semantic image synthesis in Fig. 15. Note that the VQ-regularized space has a variance close to 1, such that it does not have to be rescaled.\\n\\n# D.2. Full List of all First Stage Models\\n\\nWe provide a complete list of various autoenconding models trained on the OpenImages dataset in Tab. 8.\\n\\n# D.3. Layout-to-Image Synthesis\\n\\nHere we provide the quantitative evaluation and additional samples for our layout-to-image models from Sec. 4.3.1. We train a model on the COCO [4] and one on the OpenImages [49] dataset, which we subsequently additionally finetune on COCO. Tab 9 shows the result. Our COCO model reaches the performance of recent state-of-the-art models in layout-to-image synthesis, when following their training and evaluation protocol [89]. When finetuning from the OpenImages model, we surpass these works. Our OpenImages model surpasses the results of Jahn et al [37] by a margin of nearly 11 in terms of FID. In Fig. 16 we show additional samples of the model finetuned on COCO.\\n\\n# D.4. Class-Conditional Image Synthesis on ImageNet\\n\\nTab. 10 contains the results for our class-conditional LDM measured in FID and Inception score (IS). LDM-8 requires significantly fewer parameters and compute requirements (see Tab. 18) to achieve very competitive performance. Similar to previous work, we can further boost the performance by training a classifier on each noise scale and guiding with it,\\n\\n|  f | |Z| | c | R-FID ↓ | R-IS ↑ | PSNR ↑ | PSIM ↓ | SSIM ↑  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  16 VQGAN [23] | 16384 | 256 | 4.98 | - | 19.9 ±3.4 | 1.83 ±0.42 | 0.51 ±0.18  |\\n|  16 VQGAN [23] | 1024 | 256 | 7.94 | - | 19.4 ±3.3 | 1.98 ±0.43 | 0.50 ±0.18  |\\n|  8 DALL-E [66] | 8192 | - | 32.01 | - | 22.8 ±2.1 | 1.95 ±0.51 | 0.73 ±0.13  |\\n|  32 | 16384 | 16 | 31.83 | 40.40 ±1.07 | 17.45 ±2.90 | 2.58 ±0.48 | 0.41 ±0.18  |\\n|  16 | 16384 | 8 | 5.15 | 144.55 ±3.74 | 20.83 ±3.61 | 1.73 ±0.43 | 0.54 ±0.18  |\\n|  8 | 16384 | 4 | 1.14 | 201.92 ±3.97 | 23.07 ±3.99 | 1.17 ±0.36 | 0.65 ±0.16  |\\n|  8 | 256 | 4 | 1.49 | 194.20 ±3.87 | 22.35 ±3.81 | 1.26 ±0.37 | 0.62 ±0.16  |\\n|  4 | 8192 | 3 | 0.58 | 224.78 ±5.35 | 27.43 ±4.26 | 0.53 ±0.21 | 0.82 ±0.10  |\\n|  4† | 8192 | 3 | 1.06 | 221.94 ±4.58 | 25.21 ±4.17 | 0.72 ±0.26 | 0.76 ±0.12  |\\n|  4 | 256 | 3 | 0.47 | 223.81 ±4.58 | 26.43 ±4.22 | 0.62 ±0.24 | 0.80 ±0.11  |\\n|  2 | 2048 | 2 | 0.16 | 232.75 ±5.09 | 30.85 ±4.12 | 0.27 ±0.12 | 0.91 ±0.05  |\\n|  2 | 64 | 2 | 0.40 | 226.62 ±4.83 | 29.13 ±3.46 | 0.38 ±0.13 | 0.90 ±0.05  |\\n|  32 | KL | 64 | 2.04 | 189.53 ±3.68 | 22.27 ±3.93 | 1.41 ±0.40 | 0.61 ±0.17  |\\n|  32 | KL | 16 | 7.3 | 132.75 ±2.71 | 20.38 ±3.56 | 1.88 ±0.45 | 0.53 ±0.18  |\\n|  16 | KL | 16 | 0.87 | 210.31 ±3.97 | 24.08 ±4.22 | 1.07 ±0.36 | 0.68 ±0.15  |\\n|  16 | KL | 8 | 2.63 | 178.68 ±4.08 | 21.94 ±3.92 | 1.49 ±0.42 | 0.59 ±0.17  |\\n|  8 | KL | 4 | 0.90 | 209.90 ±4.92 | 24.19 ±4.19 | 1.02 ±0.35 | 0.69 ±0.15  |\\n|  4 | KL | 3 | 0.27 | 227.57 ±4.89 | 27.53 ±4.54 | 0.55 ±0.24 | 0.82 ±0.11  |\\n|  2 | KL | 2 | 0.086 | 232.66 ±5.16 | 32.47 ±4.19 | 0.20 ±0.09 | 0.93 ±0.04  |\\n\\nTable 8. Complete autoencoder zoo trained on OpenImages, evaluated on ImageNet-Val. † denotes an attention-free autoencoder.\\n\\n![img-17.jpeg](img-17.jpeg)\\nlayout-to-image synthesis on the COCO dataset\\nFigure 16. More samples from our best model for layout-to-image synthesis,  $LDM-4$ , which was trained on the OpenImages dataset and finetuned on the COCO dataset. Samples generated with 100 DDIM steps and  $\\\\eta = 0$ . Layouts are from the COCO validation set.\\n\\nsee Sec. C. Unlike the pixel-based methods, this classifier is trained very cheaply in latent space. For additional qualitative results, see Fig. 26 and Fig. 27.\\n\\n|  Method | COCO256 × 256 | OpenImages 256 × 256 | OpenImages 512 × 512  |\\n| --- | --- | --- | --- |\\n|   |  FID↓ | FID↓ | FID↓  |\\n|  LostGAN-V2 [87] | 42.55 | - | -  |\\n|  OC-GAN [89] | 41.65 | - | -  |\\n|  SPADE [62] | 41.11 | - | -  |\\n|  VQGAN+T [37] | 56.58 | 45.33 | 48.11  |\\n|  LDM-8 (100 steps, ours) | 42.06† | - | -  |\\n|  LDM-4 (200 steps, ours) | 40.91* | 32.02 | 35.80  |\\n\\nTable 9. Quantitative comparison of our layout-to-image models on the COCO [4] and OpenImages [49] datasets. †: Training from scratch on COCO; *: Finetuning from OpenImages.\\n\\n|  Method | FID↓ | IS↑ | Precision↑ | Recall↑ | Nparams |   |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  SR3 [72] | 11.30 | - | - | - | 625M | -  |\\n|  ImageBART [21] | 21.19 | - | - | - | 3.5B | -  |\\n|  ImageBART [21] | 7.44 | - | - | - | 3.5B | 0.05 acc. rate*  |\\n|  VQGAN+T [23] | 17.04 | 70.6±1.8 | - | - | 1.3B | -  |\\n|  VQGAN+T [23] | 5.88 | 304.8±3.6 | - | - | 1.3B | 0.05 acc. rate*  |\\n|  BigGan-deep [3] | 6.95 | 203.6±2.6 | 0.87 | 0.28 | 340M |   |\\n|  ADM [15] | 10.94 | 100.98 | 0.69 | 0.63 | 554M | 250 DDIM steps  |\\n|  ADM-G [15] | 4.59 | 186.7 | 0.82 | 0.52 | 608M | 250 DDIM steps  |\\n|  ADM-G,ADM-U [15] | 3.85 | 221.72 | 0.84 | 0.53 | n/a | 2 × 250 DDIM steps  |\\n|  CDM [31] | 4.88 | 158.71±3.26 | - | - | n/a | 2 × 100 DDIM steps  |\\n|  LDM-8 (ours) | 17.41 | 72.92±2.6 | 0.65 | 0.62 | 395M | 200 DDIM steps, 2.9M train steps, batch size 64  |\\n|  LDM-8-G (ours) | 8.11 | 190.43±2.60 | 0.83 | 0.36 | 506M | 200 DDIM steps, classifier scale 10, 2.9M train steps, batch size 64  |\\n|  LDM-8 (ours) | 15.51 | 79.03±1.03 | 0.65 | 0.63 | 395M | 200 DDIM steps, 4.8M train steps, batch size 64  |\\n|  LDM-8-G (ours) | 7.76 | 209.52±4.24 | 0.84 | 0.35 | 506M | 200 DDIM steps, classifier scale 10, 4.8M train steps, batch size 64  |\\n|  LDM-4 (ours) | 10.56 | 103.49±1.24 | 0.71 | 0.62 | 400M | 250 DDIM steps, 178K train steps, batch size 1200  |\\n|  LDM-4-G (ours) | 3.95 | 178.22±2.43 | 0.81 | 0.55 | 400M | 250 DDIM steps, unconditional guidance [32] scale 1.25, 178K train steps, batch size 1200  |\\n|  LDM-4-G (ours) | 3.60 | 247.67±5.59 | 0.87 | 0.48 | 400M | 250 DDIM steps, unconditional guidance [32] scale 1.5, 178K train steps, batch size 1200  |\\n\\nTable 10. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on the ImageNet [12] dataset.*: Classifier rejection sampling with the given rejection rate as proposed in [67].\\n\\n# D.5. Sample Quality vs. V100 Days (Continued from Sec. 4.1)\\n\\n![img-18.jpeg](img-18.jpeg)\\nFigure 17. For completeness we also report the training progress of class-conditional  $LDMs$  on the ImageNet dataset for a fixed number of 35 V100 days. Results obtained with 100 DDIM steps [84] and  $\\\\kappa = 0$ . FIDs computed on 5000 samples for efficiency reasons.\\n\\n![img-19.jpeg](img-19.jpeg)\\n\\nFor the assessment of sample quality over the training progress in Sec. 4.1, we reported FID and IS scores as a function of train steps. Another possibility is to report these metrics over the used resources in V100 days. Such an analysis is additionally provided in Fig. 17, showing qualitatively similar results.\\n\\n|  Method | FID ↓ | IS ↑ | PSNR ↑ | SSIM ↑  |\\n| --- | --- | --- | --- | --- |\\n|  Image Regression [72] | 15.2 | 121.1 | 27.9 | 0.801  |\\n|  SR3 [72] | 5.2 | 180.1 | 26.4 | 0.762  |\\n|  LDM-4 (ours, 100 steps) | 2.8†/4.8‡ | 166.3 | 24.4±3.8 | 0.69±0.14  |\\n|  LDM-4 (ours, 50 steps, guiding) | 4.4†/6.4‡ | 153.7 | 25.8±3.7 | 0.74±0.12  |\\n|  LDM-4 (ours, 100 steps, guiding) | 4.4†/6.4‡ | 154.1 | 25.7±3.7 | 0.73±0.12  |\\n|  LDM-4 (ours, 100 steps, +15 ep.) | 2.6†/4.6‡ | 169.76±5.03 | 24.4±3.8 | 0.69±0.14  |\\n|  Pixel-DM (100 steps, +15 ep.) | 5.1†/7.1‡ | 163.06±4.67 | 24.1±3.3 | 0.59±0.12  |\\n\\nTable 11.  $\\\\times 4$  upscaling results on ImageNet-Val.  $(256^{2})$ ;  $\\\\dagger$ : FID features computed on validation split,  $\\\\ddagger$ : FID features computed on train split. We also include a pixel-space baseline that receives the same amount of compute as LDM-4. The last two rows received 15 epochs of additional training compared to the former results.\\n\\n# D.6. Super-Resolution\\n\\nFor better comparability between LDMs and diffusion models in pixel space, we extend our analysis from Tab. 5 by comparing a diffusion model trained for the same number of steps and with a comparable number  ${}^{1}$  of parameters to our LDM. The results of this comparison are shown in the last two rows of Tab. 11 and demonstrate that LDM achieves better performance while allowing for significantly faster sampling. A qualitative comparison is given in Fig. 20 which shows random samples from both LDM and the diffusion model in pixel space.\\n\\n# D.6.1 LDM-BSR: General Purpose SR Model via Diverse Image Degradation\\n\\n![img-20.jpeg](img-20.jpeg)\\nFigure 18. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from a class-conditional LDM (image cf. Fig. 4) to  $1024^2$  resolution. In contrast, using a fixed degradation process (see Sec. 4.4) hinders generalization.\\n\\nTo evaluate generalization of our LDM-SR, we apply it both on synthetic LDM samples from a class-conditional ImageNet model (Sec. 4.1) and images crawled from the internet. Interestingly, we observe that LDM-SR, trained only with a bicubicly downsampled conditioning as in [72], does not generalize well to images which do not follow this pre-processing. Hence, to obtain a superresolution model for a wide range of real world images, which can contain complex superpositions of camera noise, compression artifacts, blurr and interpolations, we replace the bicubic downsampling operation in LDM-SR with the degradation pipeline from [105]. The BSR-degradation process is a degradation pipeline which applies JPEG compressions noise, camera sensor noise, different image interpolations for downsampling, Gaussian blur kernels and Gaussian noise in a random order to an image. We found that using the bsr-degradation process with the original parameters as in [105] leads to a very strong degradation process. Since a more moderate degradation process seemed appropriate for our application, we adapted the parameters of the bsr-degradation (our adapted degradation process can be found in our code base at https://github.com/CompVis/latent-diffusion). Fig. 18 illustrates the effectiveness of this approach by directly comparing LDM-SR with LDM-BSR. The latter produces images much sharper than the models confined to a fixed preprocessing, making it suitable for real-world applications. Further results of LDM-BSR are shown on LSUN-cows in Fig. 19.\\n\\n# E. Implementation Details and Hyperparameters\\n\\n# E.1. Hyperparameters\\n\\nWe provide an overview of the hyperparameters of all trained LDM models in Tab. 12, Tab. 13, Tab. 14 and Tab. 15.\\n\\n|   | CelebA-HQ 256 × 256 | FFHQ 256 × 256 | LSUN-Churches 256 × 256 | LSUN-Bedrooms 256 × 256  |\\n| --- | --- | --- | --- | --- |\\n|  f | 4 | 4 | 8 | 4  |\\n|  z-shape | 64 × 64 × 3 | 64 × 64 × 3 | - | 64 × 64 × 3  |\\n|  |Z | 8192 | 8192 | - | 8192  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear  |\\n|  Nparams | 274M | 274M | 294M | 274M  |\\n|  Channels | 224 | 224 | 192 | 224  |\\n|  Depth | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,2,3,4 | 1,2,3,4 | 1,2,2,4,4 | 1,2,3,4  |\\n|  Attention resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8, 4 | 32, 16, 8  |\\n|  Head Channels | 32 | 32 | 24 | 32  |\\n|  Batch Size | 48 | 42 | 96 | 48  |\\n|  Iterations* | 410k | 635k | 500k | 1.9M  |\\n|  Learning Rate | 9.6e-5 | 8.4e-5 | 5.e-5 | 9.6e-5  |\\n\\nTable 12. Hyperparameters for the unconditional LDMs producing the numbers shown in Tab. 1. All models trained on a single NVIDIA A100.\\n\\n|   | LDM-1 | LDM-2 | LDM-4 | LDM-8 | LDM-16 | LDM-32  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  z-shape | 256 × 256 × 3 | 128 × 128 × 2 | 64 × 64 × 3 | 32 × 32 × 4 | 16 × 16 × 8 | 88 × 8 × 32  |\\n|  |Z | - | 2048 | 8192 | 16384 | 16384 | 16384  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear | linear | linear  |\\n|  Model Size | 396M | 391M | 391M | 395M | 395M | 395M  |\\n|  Channels | 192 | 192 | 192 | 256 | 256 | 256  |\\n|  Depth | 2 | 2 | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,1,2,2,4,4 | 1,2,2,4,4 | 1,2,3,5 | 1,2,4 | 1,2,4 | 1,2,4  |\\n|  Number of Heads | 1 | 1 | 1 | 1 | 1 | 1  |\\n|  Batch Size | 7 | 9 | 40 | 64 | 112 | 112  |\\n|  Iterations | 2M | 2M | 2M | 2M | 2M | 2M  |\\n|  Learning Rate | 4.9e-5 | 6.3e-5 | 8e-5 | 6.4e-5 | 4.5e-5 | 4.5e-5  |\\n|  Conditioning | CA | CA | CA | CA | CA | CA  |\\n|  CA-resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 16, 8, 4 | 8, 4, 2  |\\n|  Embedding Dimension | 512 | 512 | 512 | 512 | 512 | 512  |\\n|  Transformers Depth | 1 | 1 | 1 | 1 | 1 | 1  |\\n\\nTable 13. Hyperparameters for the conditional LDMs trained on the ImageNet dataset for the analysis in Sec. 4.1. All models trained on a single NVIDIA A100.\\n\\n# E.2. Implementation Details\\n\\n# E.2.1 Implementations of  $\\\\tau_{\\\\theta}$  for conditional LDMs\\n\\nFor the experiments on text-to-image and layout-to-image (Sec. 4.3.1) synthesis, we implement the conditioner  $\\\\tau_{\\\\theta}$  as an unmasked transformer which processes a tokenized version of the input  $y$  and produces an output  $\\\\zeta \\\\coloneqq \\\\tau_{\\\\theta}(y)$ , where  $\\\\zeta \\\\in \\\\mathbb{R}^{M\\\\times d_{\\\\tau}}$ . More specifically, the transformer is implemented from  $N$  transformer blocks consisting of global self-attention layers, layer-normalization and position-wise MLPs as follows:\\n\\n|   | LDM-1 | LDM-2 | LDM-4 | LDM-8 | LDM-16 | LDM-32  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  z-shape | 256 × 256 × 3 | 128 × 128 × 2 | 64 × 64 × 3 | 32 × 32 × 4 | 16 × 16 × 8 | 88 × 8 × 32  |\\n|  |Z | - | 2048 | 8192 | 16384 | 16384 | 16384  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear | linear | linear  |\\n|  Model Size | 270M | 265M | 274M | 258M | 260M | 258M  |\\n|  Channels | 192 | 192 | 224 | 256 | 256 | 256  |\\n|  Depth | 2 | 2 | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,1,2,2,4,4 | 1,2,2,4,4 | 1,2,3,4 | 1,2,4 | 1,2,4 | 1,2,4  |\\n|  Attention resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 16, 8, 4 | 8, 4, 2  |\\n|  Head Channels | 32 | 32 | 32 | 32 | 32 | 32  |\\n|  Batch Size | 9 | 11 | 48 | 96 | 128 | 128  |\\n|  Iterations* | 500k | 500k | 500k | 500k | 500k | 500k  |\\n|  Learning Rate | 9e-5 | 1.1e-4 | 9.6e-5 | 9.6e-5 | 1.3e-4 | 1.3e-4  |\\n\\nTable 14. Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in Fig. 7. All models trained on a single NVIDIA A100. *: All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the provided FID scores.\\n\\n|  Task | Text-to-Image | Layout-to-Image |   | Class-Label-to-Image | Super Resolution | Inpainting | Semantic-Map-to-Image  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Dataset | LAION | OpenImages | COCO | ImageNet | ImageNet | Places | Landscapes  |\\n|  f | 8 | 4 | 8 | 4 | 4 | 4 | 8  |\\n|  z-shape | 32 × 32 × 4 | 64 × 64 × 3 | 32 × 32 × 4 | 64 × 64 × 3 | 64 × 64 × 3 | 64 × 64 × 3 | 32 × 32 × 4  |\\n|  |Z | - | 8192 | 16384 | 8192 | 8192 | 8192 | 16384  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear | linear | linear | linear  |\\n|  Model Size | 1.45B | 306M | 345M | 395M | 169M | 215M | 215M  |\\n|  Channels | 320 | 128 | 192 | 192 | 160 | 128 | 128  |\\n|  Depth | 2 | 2 | 2 | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,2,4,4 | 1,2,3,4 | 1,2,4 | 1,2,3,5 | 1,2,2,4 | 1,4,8 | 1,4,8  |\\n|  Number of Heads | 8 | 1 | 1 | 1 | 1 | 1 | 1  |\\n|  Dropout | - | - | 0.1 | - | - | - | -  |\\n|  Batch Size | 680 | 24 | 48 | 1200 | 64 | 128 | 48  |\\n|  Iterations | 390K | 4.4M | 170K | 178K | 860K | 360K | 360K  |\\n|  Learning Rate | 1.0e-4 | 4.8e-5 | 4.8e-5 | 1.0e-4 | 6.4e-5 | 1.0e-6 | 4.8e-5  |\\n|  Conditioning | CA | CA | CA | CA | concat | concat | concat  |\\n|  (C)A-resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | - | - | -  |\\n|  Embedding Dimension | 1280 | 512 | 512 | 512 | - | - | -  |\\n|  Transformer Depth | 1 | 3 | 2 | 1 | - | - | -  |\\n\\nTable 15. Hyperparameters for the conditional LDMs from Sec. 4. All models trained on a single NVIDIA A100 except for the inpainting model which was trained on eight V100.\\n\\n$\\\\zeta \\\\gets \\\\mathrm{TokEmb}(y) + \\\\mathrm{PosEmb}(y)$  (18)\\n\\nfor  $i = 1,\\\\dots ,N$  ..\\n\\n$\\\\zeta_{1}\\\\gets \\\\mathrm{LayerNorm}(\\\\zeta)$  (19)\\n'), 0.10336887200990763), (Document(id='4f67de6b50af:0', metadata={'start_line': 1, 'end_line': 255, 'chunk_index': 0, 'doc_id': '4f67de6b50af', 'text': '# Generative Adversarial Nets\\n\\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\\nSherjil Ozair, Aaron Courville, Yoshua Bengio\\nDépartement d’informatique et de recherche opérationnelle\\nUniversité de Montréal\\nMontréal, QC H3C 3J7\\n\\n###### Abstract\\n\\nWe propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$. The training procedure for $G$ is to maximize the probability of $D$ making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions $G$ and $D$, a unique solution exists, with $G$ recovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$ everywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\\n\\n## 1 Introduction\\n\\nThe promise of deep learning is to discover rich, hierarchical models *[2]* that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora. So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label *[14, 22]*. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units *[19, 9, 10]* which have a particularly well-behaved gradient . Deep generative models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context. We propose a new generative model estimation procedure that sidesteps these difficulties.\\n\\nIn the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\\n\\n*Jean Pouget-Abadie is visiting Université de Montréal from Ecole Polytechnique.\\n\\n†Sherjil Ozair is visiting Université de Montréal from Indian Institute of Technology Delhi\\n\\n†Yoshua Bengio is a CIFAR Senior Fellow.\\n\\n†All code and hyperparameters available at http://www.github.com/goodfeli/adversarial\\n\\nThis framework can yield specific training algorithms for many kinds of model and optimization algorithm. In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as adversarial nets. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms *[17]* and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary.\\n\\n## 2 Related work\\n\\nAn alternative to directed graphical models with latent variables are undirected graphical models with latent variables, such as restricted Boltzmann machines (RBMs) *[27, 16]*, deep Boltzmann machines (DBMs) *[26]* and their numerous variants. The interactions within such models are represented as the product of unnormalized potential functions, normalized by a global summation/integration over all states of the random variables. This quantity (the partition function) and its gradient are intractable for all but the most trivial instances, although they can be estimated by Markov chain Monte Carlo (MCMC) methods. Mixing poses a significant problem for learning algorithms that rely on MCMC *[3, 5]*.\\n\\nDeep belief networks (DBNs) *[16]* are hybrid models containing a single undirected layer and several directed layers. While a fast approximate layer-wise training criterion exists, DBNs incur the computational difficulties associated with both undirected and directed models.\\n\\nAlternative criteria that do not approximate or bound the log-likelihood have also been proposed, such as score matching *[18]* and noise-contrastive estimation (NCE) *[13]*. Both of these require the learned probability density to be analytically specified up to a normalization constant. Note that in many interesting generative models with several layers of latent variables (such as DBNs and DBMs), it is not even possible to derive a tractable unnormalized probability density. Some models such as denoising auto-encoders *[30]* and contractive autoencoders have learning rules very similar to score matching applied to RBMs. In NCE, as in this work, a discriminative training criterion is employed to fit a generative model. However, rather than fitting a separate discriminative model, the generative model itself is used to discriminate generated data from samples a fixed noise distribution. Because NCE uses a fixed noise distribution, learning slows dramatically after the model has learned even an approximately correct distribution over a small subset of the observed variables.\\n\\nFinally, some techniques do not involve defining a probability distribution explicitly, but rather train a generative machine to draw samples from the desired distribution. This approach has the advantage that such machines can be designed to be trained by back-propagation. Prominent recent work in this area includes the generative stochastic network (GSN) framework *[5]*, which extends generalized denoising auto-encoders *[4]*: both can be seen as defining a parameterized Markov chain, i.e., one learns the parameters of a machine that performs one step of a generative Markov chain. Compared to GSNs, the adversarial nets framework does not require a Markov chain for sampling. Because adversarial nets do not require feedback loops during generation, they are better able to leverage piecewise linear units *[19, 9, 10]*, which improve the performance of backpropagation but have problems with unbounded activation when used ina feedback loop. More recent examples of training a generative machine by back-propagating into it include recent work on auto-encoding variational Bayes *[20]* and stochastic backpropagation *[24]*.\\n\\n## 3 Adversarial nets\\n\\nThe adversarial modeling framework is most straightforward to apply when the models are both multilayer perceptrons. To learn the generator’s distribution $p_{g}$ over data $\\\\bm{x}$, we define a prior on input noise variables $p_{\\\\bm{z}}(\\\\bm{z})$, then represent a mapping to data space as $G(\\\\bm{z};\\\\theta_{g})$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $\\\\theta_{g}$. We also define a second multilayer perceptron $D(\\\\bm{x};\\\\theta_{d})$ that outputs a single scalar. $D(\\\\bm{x})$ represents the probability that $\\\\bm{x}$ came from the data rather than $p_{g}$. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $\\\\log(1-D(G(\\\\bm{z})))$:\\n\\nIn other words, $D$ and $G$ play the following two-player minimax game with value function $V(G,D)$:\\n\\n$\\\\min_{G}\\\\max_{D}V(D,G)=\\\\mathbb{E}_{\\\\boldsymbol{x}\\\\sim p_{\\\\text{data}}(\\\\boldsymbol{x})}[\\\\log D(\\\\boldsymbol{x})]+\\\\mathbb{E}_{\\\\boldsymbol{z}\\\\sim p_{\\\\boldsymbol{z}}(\\\\boldsymbol{z})}[\\\\log(1-D(G(\\\\boldsymbol{z})))].$ (1)\\n\\nIn the next section, we present a theoretical analysis of adversarial nets, essentially showing that the training criterion allows one to recover the data generating distribution as $G$ and $D$ are given enough capacity, i.e., in the non-parametric limit. See Figure 1 for a less formal, more pedagogical explanation of the approach. In practice, we must implement the game using an iterative, numerical approach. Optimizing $D$ to completion in the inner loop of training is computationally prohibitive, and on finite datasets would result in overfitting. Instead, we alternate between $k$ steps of optimizing $D$ and one step of optimizing $G$. This results in $D$ being maintained near its optimal solution, so long as $G$ changes slowly enough. This strategy is analogous to the way that SML/PCD *[31, 29]* training maintains samples from a Markov chain from one learning step to the next in order to avoid burning in a Markov chain as part of the inner loop of learning. The procedure is formally presented in Algorithm 1.\\n\\nIn practice, equation 1 may not provide sufficient gradient for $G$ to learn well. Early in learning, when $G$ is poor, $D$ can reject samples with high confidence because they are clearly different from the training data. In this case, $\\\\log(1-D(G(\\\\boldsymbol{z})))$ saturates. Rather than training $G$ to minimize $\\\\log(1-D(G(\\\\boldsymbol{z})))$ we can train $G$ to maximize $\\\\log D(G(\\\\boldsymbol{z}))$. This objective function results in the same fixed point of the dynamics of $G$ and $D$ but provides much stronger gradients early in learning.\\n\\n![img-0.jpeg](img-0.jpeg)\\n(a)\\n\\n![img-1.jpeg](img-1.jpeg)\\n(b)\\n\\n![img-2.jpeg](img-2.jpeg)\\n(c)\\n\\n![img-3.jpeg](img-3.jpeg)\\n(d)\\n\\nFigure 1: Generative adversarial nets are trained by simultaneously updating the discriminative distribution ($D$, blue, dashed line) so that it discriminates between samples from the data generating distribution (black, dotted line) $p_{\\\\boldsymbol{x}}$ from those of the generative distribution $p_{g}$ (G) (green, solid line). The lower horizontal line is the domain from which $\\\\boldsymbol{z}$ is sampled, in this case uniformly. The horizontal line above is part of the domain of $\\\\boldsymbol{x}$. The upward arrows show how the mapping $\\\\boldsymbol{x}=G(\\\\boldsymbol{z})$ imposes the non-uniform distribution $p_{g}$ on transformed samples. $G$ contracts in regions of high density and expands in regions of low density of $p_{g}$. (a) Consider an adversarial pair near convergence: $p_{g}$ is similar to $p_{\\\\text{data}}$ and $D$ is a partially accurate classifier. (b) In the inner loop of the algorithm $D$ is trained to discriminate samples from data, converging to $D^{*}(\\\\boldsymbol{x})=\\\\frac{p_{\\\\text{data}}(\\\\boldsymbol{x})}{p_{\\\\text{data}}(\\\\boldsymbol{x})+p_{g}(\\\\boldsymbol{x})}$. (c) After an update to $G$, gradient of $D$ has guided $G(\\\\boldsymbol{z})$ to flow to regions that are more likely to be classified as data. (d) After several steps of training, if $G$ and $D$ have enough capacity, they will reach a point at which both cannot improve because $p_{g}=p_{\\\\text{data}}$. The discriminator is unable to differentiate between the two distributions, i.e. $D(\\\\boldsymbol{x})=\\\\frac{1}{2}$.\\n\\n## 4 Theoretical Results\\n\\nThe generator $G$ implicitly defines a probability distribution $p_{g}$ as the distribution of the samples $G(\\\\boldsymbol{z})$ obtained when $\\\\boldsymbol{z}\\\\sim p_{\\\\boldsymbol{z}}$. Therefore, we would like Algorithm 1 to converge to a good estimator of $p_{\\\\text{data}}$, if given enough capacity and training time. The results of this section are done in a non-parametric setting, e.g. we represent a model with infinite capacity by studying convergence in the space of probability density functions.\\n\\nWe will show in section 4.1 that this minimax game has a global optimum for $p_{g}=p_{\\\\text{data}}$. We will then show in section 4.2 that Algorithm 1 optimizes Eq 1, thus obtaining the desired result.\\n\\nAlgorithm 1 Minibatch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, $k$, is a hyperparameter. We used $k=1$, the least expensive option, in our experiments.\\n\\nfor number of training iterations do\\nfor $k$ steps do\\n$\\\\bullet$ Sample minibatch of $m$ noise samples $\\\\{\\\\bm{z}^{(1)},\\\\ldots,\\\\bm{z}^{(m)}\\\\}$ from noise prior $p_{g}(\\\\bm{z})$.\\n$\\\\bullet$ Sample minibatch of $m$ examples $\\\\{\\\\bm{x}^{(1)},\\\\ldots,\\\\bm{x}^{(m)}\\\\}$ from data generating distribution $p_{\\\\text{data}}(\\\\bm{x})$.\\n$\\\\bullet$ Update the discriminator by ascending its stochastic gradient:\\n\\n$\\\\nabla_{\\\\theta_{g}}\\\\frac{1}{m}\\\\sum_{i=1}^{m}\\\\left[\\\\log D\\\\left(\\\\bm{x}^{(i)}\\\\right)+\\\\log\\\\left(1-D\\\\left(G\\\\left(\\\\bm{z}^{(i)}\\\\right)\\\\right)\\\\right)\\\\right].$\\n\\nend for\\n$\\\\bullet$ Sample minibatch of $m$ noise samples $\\\\{\\\\bm{z}^{(1)},\\\\ldots,\\\\bm{z}^{(m)}\\\\}$ from noise prior $p_{g}(\\\\bm{z})$.\\n$\\\\bullet$ Update the generator by descending its stochastic gradient:\\n\\n$\\\\nabla_{\\\\theta_{g}}\\\\frac{1}{m}\\\\sum_{i=1}^{m}\\\\log\\\\left(1-D\\\\left(G\\\\left(\\\\bm{z}^{(i)}\\\\right)\\\\right)\\\\right).$\\n\\nend for\\nThe gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments.\\n\\n### 4.1 Global Optimality of $p_{g}=p_{\\\\text{data}}$\\n\\nWe first consider the optimal discriminator $D$ for any given generator $G$.\\n\\n###### Proposition 1.\\n\\nFor $G$ fixed, the optimal discriminator $D$ is\\n\\n$D_{G}^{*}(\\\\bm{x})=\\\\frac{p_{\\\\text{data}}(\\\\bm{x})}{p_{\\\\text{data}}(\\\\bm{x})+p_{g}(\\\\bm{x})}$ (2)\\n\\n###### Proof.\\n\\nThe training criterion for the discriminator D, given any generator $G$, is to maximize the quantity $V(G,D)$\\n\\n$V(G,D)$ $=\\\\int_{\\\\bm{x}}p_{\\\\text{data}}(\\\\bm{x})\\\\log(D(\\\\bm{x}))dx+\\\\int_{z}p_{\\\\bm{z}}(\\\\bm{z})\\\\log(1-D(g(\\\\bm{z})))dz$\\n$=\\\\int_{\\\\bm{x}}p_{\\\\text{data}}(\\\\bm{x})\\\\log(D(\\\\bm{x}))+p_{g}(\\\\bm{x})\\\\log(1-D(\\\\bm{x}))dx$ (3)\\n\\nFor any $(a,b)\\\\in\\\\mathbb{R}^{2}\\\\setminus\\\\{0,0\\\\}$, the function $y\\\\rightarrow a\\\\log(y)+b\\\\log(1-y)$ achieves its maximum in $[0,1]$ at $\\\\frac{a}{a+b}$. The discriminator does not need to be defined outside of $Supp(p_{\\\\text{data}})\\\\cup Supp(p_{g})$, concluding the proof. ∎\\n\\nNote that the training objective for $D$ can be interpreted as maximizing the log-likelihood for estimating the conditional probability $P(Y=y|\\\\bm{x})$, where $Y$ indicates whether $\\\\bm{x}$ comes from $p_{\\\\text{data}}$ (with $y=1$) or from $p_{g}$ (with $y=0$). The minimax game in Eq. 1 can now be reformulated as:\\n\\n$C(G)=$ $\\\\max_{D}V(G,D)$\\n$=$ $\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{\\\\text{data}}}[\\\\log D_{G}^{*}(\\\\bm{x})]+\\\\mathbb{E}_{\\\\bm{z}\\\\sim p_{\\\\bm{z}}}[\\\\log(1-D_{G}^{*}(G(\\\\bm{z})))]$ (4)\\n$=$ $\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{\\\\text{data}}}[\\\\log D_{G}^{*}(\\\\bm{x})]+\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{g}}[\\\\log(1-D_{G}^{*}(\\\\bm{x}))]$\\n$=$ $\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{\\\\text{data}}}\\\\left[\\\\log\\\\frac{p_{\\\\text{data}}(\\\\bm{x})}{P_{\\\\text{data}}(\\\\bm{x})+p_{g}(\\\\bm{x})}\\\\right]+\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{g}}\\\\left[\\\\log\\\\frac{p_{g}(\\\\bm{x})}{p_{\\\\text{data}}(\\\\bm{x})+p_{g}(\\\\bm{x})}\\\\right]$\\n\\n###### Theorem 1.\\n\\nThe global minimum of the virtual training criterion $C(G)$ is achieved if and only if $p_{g}=p_{\\\\text{data}}.$ At that point, $C(G)$ achieves the value $-\\\\log 4.$\\n\\n###### Proof.\\n\\nFor $p_{g}=p_{\\\\text{data}}$, $D_{G}^{*}(\\\\bm{x})=\\\\frac{1}{2}$, (consider Eq. 2). Hence, by inspecting Eq. 4 at $D_{G}^{*}(\\\\bm{x})=\\\\frac{1}{2}$, we find $C(G)=\\\\log\\\\frac{1}{2}+\\\\log\\\\frac{1}{2}=-\\\\log 4$. To see that this is the best possible value of $C(G)$, reached only for $p_{g}=p_{\\\\text{data}}$, observe that\\n\\n$\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{\\\\text{data}}}\\\\,[-\\\\log 2]+\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{g}}\\\\,[-\\\\log 2]=-\\\\log 4$\\n\\nand that by subtracting this expression from $C(G)=V(D_{G}^{*},G)$, we obtain:\\n\\n$C(G)=-\\\\log(4)+KL\\\\left(p_{\\\\text{data}}\\\\left\\\\|\\\\frac{p_{\\\\text{data}}+p_{g}}{2}\\\\right)+KL\\\\left(p_{g}\\\\left\\\\|\\\\frac{p_{\\\\text{data}}+p_{g}}{2}\\\\right.\\\\right)\\\\right.$ (5)\\n\\nwhere KL is the Kullback–Leibler divergence. We recognize in the previous expression the Jensen–Shannon divergence between the model’s distribution and the data generating process:\\n\\n$C(G)=-\\\\log(4)+2\\\\cdot JSD\\\\left(p_{\\\\text{data}}\\\\left\\\\|p_{g}\\\\right.\\\\right)$ (6)\\n\\nSince the Jensen–Shannon divergence between two distributions is always non-negative and zero only when they are equal, we have shown that $C^{*}=-\\\\log(4)$ is the global minimum of $C(G)$ and that the only solution is $p_{g}=p_{\\\\text{data}}$, i.e., the generative model perfectly replicating the data generating process. ∎\\n\\n### 4.2 Convergence of Algorithm 1\\n\\n###### Proposition 2.\\n\\nIf $G$ and $D$ have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given $G$, and $p_{g}$ is updated so as to improve the criterion\\n\\n$\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{\\\\text{data}}}[\\\\log D_{G}^{*}(\\\\bm{x})]+\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{g}}[\\\\log(1-D_{G}^{*}(\\\\bm{x}))]$\\n\\nthen $p_{g}$ converges to $p_{\\\\text{data}}$\\n\\n###### Proof.\\n\\nConsider $V(G,D)=U(p_{g},D)$ as a function of $p_{g}$ as done in the above criterion. Note that $U(p_{g},D)$ is convex in $p_{g}$. The subderivatives of a supremum of convex functions include the derivative of the function at the point where the maximum is attained. In other words, if $f(x)=\\\\sup_{\\\\alpha\\\\in\\\\mathcal{A}}f_{\\\\alpha}(x)$ and $f_{\\\\alpha}(x)$ is convex in $x$ for every $\\\\alpha$, then $\\\\partial f_{\\\\beta}(x)\\\\in\\\\partial f$ if $\\\\beta=\\\\arg\\\\sup_{\\\\alpha\\\\in\\\\mathcal{A}}f_{\\\\alpha}(x)$. This is equivalent to computing a gradient descent update for $p_{g}$ at the optimal $D$ given the corresponding $G$. $\\\\sup_{D}U(p_{g},D)$ is convex in $p_{g}$ with a unique global optima as proven in Thm 1, therefore with sufficiently small updates of $p_{g}$, $p_{g}$ converges to $p_{x}$, concluding the proof. ∎\\n\\nIn practice, adversarial nets represent a limited family of $p_{g}$ distributions via the function $G(\\\\bm{z};\\\\theta_{g})$, and we optimize $\\\\theta_{g}$ rather than $p_{g}$ itself. Using a multilayer perceptron to define $G$ introduces multiple critical points in parameter space. However, the excellent performance of multilayer perceptrons in practice suggests that they are a reasonable model to use despite their lack of theoretical guarantees.\\n\\n## 5 Experiments\\n\\nWe trained adversarial nets an a range of datasets including MNIST*[23]*, the Toronto Face Database (TFD) *[28]*, and CIFAR-10 *[21]*. The generator nets used a mixture of rectifier linear activations *[19, 9]* and sigmoid activations, while the discriminator net used maxout *[10]* activations. Dropout *[17]* was applied in training the discriminator net. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network.\\n\\nWe estimate probability of the test set data under $p_{g}$ by fitting a Gaussian Parzen window to the samples generated with $G$ and reporting the log-likelihood under this distribution. The $\\\\sigma$ parameter\\n\\n|  Model | MNIST | TFD  |\\n| --- | --- | --- |\\n|  DBN [3] | 138 ± 2 | 1909 ± 66  |\\n|  Stacked CAE [3] | 121 ± 1.6 | 2110 ± 50  |\\n|  Deep GSN [6] | 214 ± 1.1 | 1890 ± 29  |\\n|  Adversarial nets | 225 ± 2 | 2057 ± 26  |\\n\\nTable 1: Parzen window-based log-likelihood estimates. The reported numbers on MNIST are the mean log-likelihood of samples on test set, with the standard error of the mean computed across examples. On TFD, we computed the standard error across folds of the dataset, with a different  $\\\\sigma$  chosen using the validation set of each fold. On TFD,  $\\\\sigma$  was cross validated on each fold and mean log-likelihood on each fold were computed. For MNIST we compare against other models of the real-valued (rather than binary) version of dataset.\\n\\nof the Gaussians was obtained by cross validation on the validation set. This procedure was introduced in Breuleux et al. [8] and used for various generative models for which the exact likelihood is not tractable [25, 3, 5]. Results are reported in Table 1. This method of estimating the likelihood has somewhat high variance and does not perform well in high dimensional spaces but it is the best method available to our knowledge. Advances in generative models that can sample but not estimate likelihood directly motivate further research into how to evaluate such models.\\n\\nIn Figures 2 and 3 we show samples drawn from the generator net after training. While we make no claim that these samples are better than samples generated by existing methods, we believe that these samples are at least competitive with the better generative models in the literature and highlight the potential of the adversarial framework.\\n\\n![img-4.jpeg](img-4.jpeg)\\na)\\n\\n![img-5.jpeg](img-5.jpeg)\\nb)\\n\\n![img-6.jpeg](img-6.jpeg)\\nc)\\n\\n![img-7.jpeg](img-7.jpeg)\\nd)\\nFigure 2: Visualization of samples from the model. Rightmost column shows the nearest training example of the neighboring sample, in order to demonstrate that the model has not memorized the training set. Samples are fair random draws, not cherry-picked. Unlike most other visualizations of deep generative models, these images show actual samples from the model distributions, not conditional means given samples of hidden units. Moreover, these samples are uncorrelated because the sampling process does not depend on Markov chain mixing. a) MNIST b) TFD c) CIFAR-10 (fully connected model) d) CIFAR-10 (convolutional discriminator and \"deconvolutional\" generator)\\n\\n111111111111111111111111111111111111111111\\n\\nFigure 3: Digits obtained by linearly interpolating between coordinates in  $z$  space of the full model.\\n\\n|   | Deep directed graphical models | Deep undirected graphical models | Generative autoencoders | Adversarial models  |\\n| --- | --- | --- | --- | --- |\\n|  Training | Inference needed during training. | Inference needed during training. MCMC needed to approximate partition function gradient. | Enforced tradeoff between mixing and power of reconstruction generation | Synchronizing the discriminator with the generator. Helvetica.  |\\n|  Inference | Learned approximate inference | Variational inference | MCMC-based inference | Learned approximate inference  |\\n|  Sampling | No difficulties | Requires Markov chain | Requires Markov chain | No difficulties  |\\n|  Evaluating p(x) | Intractable, may be approximated with AIS | Intractable, may be approximated with AIS | Not explicitly represented, may be approximated with Parzen density estimation | Not explicitly represented, may be approximated with Parzen density estimation  |\\n|  Model design | Nearly all models incur extreme difficulty | Careful design needed to ensure multiple properties | Any differentiable function is theoretically permitted | Any differentiable function is theoretically permitted  |\\n\\nTable 2: Challenges in generative modeling: a summary of the difficulties encountered by different approaches to deep generative modeling for each of the major operations involving a model.\\n\\n# 6 Advantages and disadvantages\\n\\nThis new framework comes with advantages and disadvantages relative to previous modeling frameworks. The disadvantages are primarily that there is no explicit representation of  $p_{g}(\\\\pmb{x})$ , and that  $D$  must be synchronized well with  $G$  during training (in particular,  $G$  must not be trained too much without updating  $D$ , in order to avoid \"the Helvetica scenario\" in which  $G$  collapses too many values of  $\\\\mathbf{z}$  to the same value of  $\\\\mathbf{x}$  to have enough diversity to model  $p_{\\\\mathrm{data}}$ ), much as the negative chains of a Boltzmann machine must be kept up to date between learning steps. The advantages are that Markov chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model. Table 2 summarizes the comparison of generative adversarial nets with other generative modeling approaches.\\n\\nThe aforementioned advantages are primarily computational. Adversarial models may also gain some statistical advantage from the generator network not being updated directly with data examples, but only with gradients flowing through the discriminator. This means that components of the input are not copied directly into the generator\\'s parameters. Another advantage of adversarial networks is that they can represent very sharp, even degenerate distributions, while methods based on Markov chains require that the distribution be somewhat blurry in order for the chains to be able to mix between modes.\\n\\n# 7 Conclusions and future work\\n\\nThis framework admits many straightforward extensions:\\n\\n1. A conditional generative model  $p(\\\\pmb{x} \\\\mid \\\\pmb{c})$  can be obtained by adding  $\\\\pmb{c}$  as input to both  $G$  and  $D$ .\\n2. Learned approximate inference can be performed by training an auxiliary network to predict  $z$  given  $x$ . This is similar to the inference net trained by the wake-sleep algorithm [15] but with the advantage that the inference net may be trained for a fixed generator net after the generator net has finished training.\\n\\n3. One can approximately model all conditionals $p(\\\\bm{x}_{S}\\\\mid\\\\bm{x}_{\\\\mathcal{S}})$ where $S$ is a subset of the indices of $\\\\bm{x}$ by training a family of conditional models that share parameters. Essentially, one can use adversarial nets to implement a stochastic extension of the deterministic MP-DBM *[11]*.\\n4. Semi-supervised learning: features from the discriminator or inference net could improve performance of classifiers when limited labeled data is available.\\n5. Efficiency improvements: training could be accelerated greatly by divising better methods for coordinating $G$ and $D$ or determining better distributions to sample $\\\\mathbf{z}$ from during training.\\n\\nThis paper has demonstrated the viability of the adversarial modeling framework, suggesting that these research directions could prove useful.\\n\\n### Acknowledgments\\n\\nWe would like to acknowledge Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for helpful discussions. Yann Dauphin shared his Parzen window evaluation code with us. We would like to thank the developers of Pylearn2 *[12]* and Theano *[7, 1]*, particularly Frédéric Bastien who rushed a Theano feature specifically to benefit this project. Arnaud Bergeron provided much-needed support with LaTeX typesetting. We would also like to thank CIFAR, and Canada Research Chairs for funding, and Compute Canada, and Calcul Québec for providing computational resources. Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning. Finally, we would like to thank Les Trois Brasseurs for stimulating our creativity.\\n\\n## References\\n\\n- [1] Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.\\n- [2] Bengio, Y. (2009). Learning deep architectures for AI. Now Publishers.\\n- [3] Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013a). Better mixing via deep representations. In ICML’13.\\n- [4] Bengio, Y., Yao, L., Alain, G., and Vincent, P. (2013b). Generalized denoising auto-encoders as generative models. In NIPS26. Nips Foundation.\\n- [5] Bengio, Y., Thibodeau-Laufer, E., and Yosinski, J. (2014a). Deep generative stochastic networks trainable by backprop. In ICML’14.\\n- [6] Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014b). Deep generative stochastic networks trainable by backprop. In Proceedings of the 30th International Conference on Machine Learning (ICML’14).\\n- [7] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation.\\n- [8] Breuleux, O., Bengio, Y., and Vincent, P. (2011). Quickly generating representative samples from an RBM-derived process. Neural Computation, 23(8), 2053–2073.\\n- [9] Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep sparse rectifier neural networks. In AISTATS’2011.\\n- [10] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013a). Maxout networks. In ICML’2013.\\n- [11] Goodfellow, I. J., Mirza, M., Courville, A., and Bengio, Y. (2013b). Multi-prediction deep Boltzmann machines. In NIPS’2013.\\n- [12] Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., and Bengio, Y. (2013c). Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214.\\n- [13] Gutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS’2010.\\n- [14] Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. (2012a). Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6), 82–97.\\n- [15] Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995). The wake-sleep algorithm for unsupervised neural networks. Science, 268, 1558–1161.\\n-\\n\\n[16] Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18, 1527–1554.\\n- [17] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012b). Improving neural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.\\n- [18] Hyvärinen, A. (2005). Estimation of non-normalized statistical models using score matching. J. Machine Learning Res., 6.\\n- [19] Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pages 2146–2153. IEEE.\\n- [20] Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations (ICLR).\\n- [21] Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images. Technical report, University of Toronto.\\n- [22] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In NIPS’2012.\\n- [23] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.\\n- [24] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. Technical report, arXiv:1401.4082.\\n- [25] Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012). A generative process for sampling contractive auto-encoders. In ICML’12.\\n- [26] Salakhutdinov, R. and Hinton, G. E. (2009). Deep Boltzmann machines. In AISTATS’2009, pages 448–455.\\n- [27] Smolensky, P. (1986). Information processing in dynamical systems: Foundations of harmony theory. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing, volume 1, chapter 6, pages 194–281. MIT Press, Cambridge.\\n- [28] Susskind, J., Anderson, A., and Hinton, G. E. (2010). The Toronto face dataset. Technical Report UTML TR 2010-001, U. Toronto.\\n- [29] Tieleman, T. (2008). Training restricted Boltzmann machines using approximations to the likelihood gradient. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, ICML 2008, pages 1064–1071. ACM.\\n- [30] Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and composing robust features with denoising autoencoders. In ICML 2008.\\n- [31] Younes, L. (1999). On the convergence of Markovian stochastic algorithms with rapidly decreasing ergodicity rates. Stochastics and Stochastic Reports, 65(3), 177–228.'}, page_content='# Generative Adversarial Nets\\n\\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\\nSherjil Ozair, Aaron Courville, Yoshua Bengio\\nDépartement d’informatique et de recherche opérationnelle\\nUniversité de Montréal\\nMontréal, QC H3C 3J7\\n\\n###### Abstract\\n\\nWe propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$. The training procedure for $G$ is to maximize the probability of $D$ making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions $G$ and $D$, a unique solution exists, with $G$ recovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$ everywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\\n\\n## 1 Introduction\\n\\nThe promise of deep learning is to discover rich, hierarchical models *[2]* that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora. So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label *[14, 22]*. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units *[19, 9, 10]* which have a particularly well-behaved gradient . Deep generative models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context. We propose a new generative model estimation procedure that sidesteps these difficulties.\\n\\nIn the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\\n\\n*Jean Pouget-Abadie is visiting Université de Montréal from Ecole Polytechnique.\\n\\n†Sherjil Ozair is visiting Université de Montréal from Indian Institute of Technology Delhi\\n\\n†Yoshua Bengio is a CIFAR Senior Fellow.\\n\\n†All code and hyperparameters available at http://www.github.com/goodfeli/adversarial\\n\\nThis framework can yield specific training algorithms for many kinds of model and optimization algorithm. In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as adversarial nets. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms *[17]* and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary.\\n\\n## 2 Related work\\n\\nAn alternative to directed graphical models with latent variables are undirected graphical models with latent variables, such as restricted Boltzmann machines (RBMs) *[27, 16]*, deep Boltzmann machines (DBMs) *[26]* and their numerous variants. The interactions within such models are represented as the product of unnormalized potential functions, normalized by a global summation/integration over all states of the random variables. This quantity (the partition function) and its gradient are intractable for all but the most trivial instances, although they can be estimated by Markov chain Monte Carlo (MCMC) methods. Mixing poses a significant problem for learning algorithms that rely on MCMC *[3, 5]*.\\n\\nDeep belief networks (DBNs) *[16]* are hybrid models containing a single undirected layer and several directed layers. While a fast approximate layer-wise training criterion exists, DBNs incur the computational difficulties associated with both undirected and directed models.\\n\\nAlternative criteria that do not approximate or bound the log-likelihood have also been proposed, such as score matching *[18]* and noise-contrastive estimation (NCE) *[13]*. Both of these require the learned probability density to be analytically specified up to a normalization constant. Note that in many interesting generative models with several layers of latent variables (such as DBNs and DBMs), it is not even possible to derive a tractable unnormalized probability density. Some models such as denoising auto-encoders *[30]* and contractive autoencoders have learning rules very similar to score matching applied to RBMs. In NCE, as in this work, a discriminative training criterion is employed to fit a generative model. However, rather than fitting a separate discriminative model, the generative model itself is used to discriminate generated data from samples a fixed noise distribution. Because NCE uses a fixed noise distribution, learning slows dramatically after the model has learned even an approximately correct distribution over a small subset of the observed variables.\\n\\nFinally, some techniques do not involve defining a probability distribution explicitly, but rather train a generative machine to draw samples from the desired distribution. This approach has the advantage that such machines can be designed to be trained by back-propagation. Prominent recent work in this area includes the generative stochastic network (GSN) framework *[5]*, which extends generalized denoising auto-encoders *[4]*: both can be seen as defining a parameterized Markov chain, i.e., one learns the parameters of a machine that performs one step of a generative Markov chain. Compared to GSNs, the adversarial nets framework does not require a Markov chain for sampling. Because adversarial nets do not require feedback loops during generation, they are better able to leverage piecewise linear units *[19, 9, 10]*, which improve the performance of backpropagation but have problems with unbounded activation when used ina feedback loop. More recent examples of training a generative machine by back-propagating into it include recent work on auto-encoding variational Bayes *[20]* and stochastic backpropagation *[24]*.\\n\\n## 3 Adversarial nets\\n\\nThe adversarial modeling framework is most straightforward to apply when the models are both multilayer perceptrons. To learn the generator’s distribution $p_{g}$ over data $\\\\bm{x}$, we define a prior on input noise variables $p_{\\\\bm{z}}(\\\\bm{z})$, then represent a mapping to data space as $G(\\\\bm{z};\\\\theta_{g})$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $\\\\theta_{g}$. We also define a second multilayer perceptron $D(\\\\bm{x};\\\\theta_{d})$ that outputs a single scalar. $D(\\\\bm{x})$ represents the probability that $\\\\bm{x}$ came from the data rather than $p_{g}$. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $\\\\log(1-D(G(\\\\bm{z})))$:\\n\\nIn other words, $D$ and $G$ play the following two-player minimax game with value function $V(G,D)$:\\n\\n$\\\\min_{G}\\\\max_{D}V(D,G)=\\\\mathbb{E}_{\\\\boldsymbol{x}\\\\sim p_{\\\\text{data}}(\\\\boldsymbol{x})}[\\\\log D(\\\\boldsymbol{x})]+\\\\mathbb{E}_{\\\\boldsymbol{z}\\\\sim p_{\\\\boldsymbol{z}}(\\\\boldsymbol{z})}[\\\\log(1-D(G(\\\\boldsymbol{z})))].$ (1)\\n\\nIn the next section, we present a theoretical analysis of adversarial nets, essentially showing that the training criterion allows one to recover the data generating distribution as $G$ and $D$ are given enough capacity, i.e., in the non-parametric limit. See Figure 1 for a less formal, more pedagogical explanation of the approach. In practice, we must implement the game using an iterative, numerical approach. Optimizing $D$ to completion in the inner loop of training is computationally prohibitive, and on finite datasets would result in overfitting. Instead, we alternate between $k$ steps of optimizing $D$ and one step of optimizing $G$. This results in $D$ being maintained near its optimal solution, so long as $G$ changes slowly enough. This strategy is analogous to the way that SML/PCD *[31, 29]* training maintains samples from a Markov chain from one learning step to the next in order to avoid burning in a Markov chain as part of the inner loop of learning. The procedure is formally presented in Algorithm 1.\\n\\nIn practice, equation 1 may not provide sufficient gradient for $G$ to learn well. Early in learning, when $G$ is poor, $D$ can reject samples with high confidence because they are clearly different from the training data. In this case, $\\\\log(1-D(G(\\\\boldsymbol{z})))$ saturates. Rather than training $G$ to minimize $\\\\log(1-D(G(\\\\boldsymbol{z})))$ we can train $G$ to maximize $\\\\log D(G(\\\\boldsymbol{z}))$. This objective function results in the same fixed point of the dynamics of $G$ and $D$ but provides much stronger gradients early in learning.\\n\\n![img-0.jpeg](img-0.jpeg)\\n(a)\\n\\n![img-1.jpeg](img-1.jpeg)\\n(b)\\n\\n![img-2.jpeg](img-2.jpeg)\\n(c)\\n\\n![img-3.jpeg](img-3.jpeg)\\n(d)\\n\\nFigure 1: Generative adversarial nets are trained by simultaneously updating the discriminative distribution ($D$, blue, dashed line) so that it discriminates between samples from the data generating distribution (black, dotted line) $p_{\\\\boldsymbol{x}}$ from those of the generative distribution $p_{g}$ (G) (green, solid line). The lower horizontal line is the domain from which $\\\\boldsymbol{z}$ is sampled, in this case uniformly. The horizontal line above is part of the domain of $\\\\boldsymbol{x}$. The upward arrows show how the mapping $\\\\boldsymbol{x}=G(\\\\boldsymbol{z})$ imposes the non-uniform distribution $p_{g}$ on transformed samples. $G$ contracts in regions of high density and expands in regions of low density of $p_{g}$. (a) Consider an adversarial pair near convergence: $p_{g}$ is similar to $p_{\\\\text{data}}$ and $D$ is a partially accurate classifier. (b) In the inner loop of the algorithm $D$ is trained to discriminate samples from data, converging to $D^{*}(\\\\boldsymbol{x})=\\\\frac{p_{\\\\text{data}}(\\\\boldsymbol{x})}{p_{\\\\text{data}}(\\\\boldsymbol{x})+p_{g}(\\\\boldsymbol{x})}$. (c) After an update to $G$, gradient of $D$ has guided $G(\\\\boldsymbol{z})$ to flow to regions that are more likely to be classified as data. (d) After several steps of training, if $G$ and $D$ have enough capacity, they will reach a point at which both cannot improve because $p_{g}=p_{\\\\text{data}}$. The discriminator is unable to differentiate between the two distributions, i.e. $D(\\\\boldsymbol{x})=\\\\frac{1}{2}$.\\n\\n## 4 Theoretical Results\\n\\nThe generator $G$ implicitly defines a probability distribution $p_{g}$ as the distribution of the samples $G(\\\\boldsymbol{z})$ obtained when $\\\\boldsymbol{z}\\\\sim p_{\\\\boldsymbol{z}}$. Therefore, we would like Algorithm 1 to converge to a good estimator of $p_{\\\\text{data}}$, if given enough capacity and training time. The results of this section are done in a non-parametric setting, e.g. we represent a model with infinite capacity by studying convergence in the space of probability density functions.\\n\\nWe will show in section 4.1 that this minimax game has a global optimum for $p_{g}=p_{\\\\text{data}}$. We will then show in section 4.2 that Algorithm 1 optimizes Eq 1, thus obtaining the desired result.\\n\\nAlgorithm 1 Minibatch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, $k$, is a hyperparameter. We used $k=1$, the least expensive option, in our experiments.\\n\\nfor number of training iterations do\\nfor $k$ steps do\\n$\\\\bullet$ Sample minibatch of $m$ noise samples $\\\\{\\\\bm{z}^{(1)},\\\\ldots,\\\\bm{z}^{(m)}\\\\}$ from noise prior $p_{g}(\\\\bm{z})$.\\n$\\\\bullet$ Sample minibatch of $m$ examples $\\\\{\\\\bm{x}^{(1)},\\\\ldots,\\\\bm{x}^{(m)}\\\\}$ from data generating distribution $p_{\\\\text{data}}(\\\\bm{x})$.\\n$\\\\bullet$ Update the discriminator by ascending its stochastic gradient:\\n\\n$\\\\nabla_{\\\\theta_{g}}\\\\frac{1}{m}\\\\sum_{i=1}^{m}\\\\left[\\\\log D\\\\left(\\\\bm{x}^{(i)}\\\\right)+\\\\log\\\\left(1-D\\\\left(G\\\\left(\\\\bm{z}^{(i)}\\\\right)\\\\right)\\\\right)\\\\right].$\\n\\nend for\\n$\\\\bullet$ Sample minibatch of $m$ noise samples $\\\\{\\\\bm{z}^{(1)},\\\\ldots,\\\\bm{z}^{(m)}\\\\}$ from noise prior $p_{g}(\\\\bm{z})$.\\n$\\\\bullet$ Update the generator by descending its stochastic gradient:\\n\\n$\\\\nabla_{\\\\theta_{g}}\\\\frac{1}{m}\\\\sum_{i=1}^{m}\\\\log\\\\left(1-D\\\\left(G\\\\left(\\\\bm{z}^{(i)}\\\\right)\\\\right)\\\\right).$\\n\\nend for\\nThe gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments.\\n\\n### 4.1 Global Optimality of $p_{g}=p_{\\\\text{data}}$\\n\\nWe first consider the optimal discriminator $D$ for any given generator $G$.\\n\\n###### Proposition 1.\\n\\nFor $G$ fixed, the optimal discriminator $D$ is\\n\\n$D_{G}^{*}(\\\\bm{x})=\\\\frac{p_{\\\\text{data}}(\\\\bm{x})}{p_{\\\\text{data}}(\\\\bm{x})+p_{g}(\\\\bm{x})}$ (2)\\n\\n###### Proof.\\n\\nThe training criterion for the discriminator D, given any generator $G$, is to maximize the quantity $V(G,D)$\\n\\n$V(G,D)$ $=\\\\int_{\\\\bm{x}}p_{\\\\text{data}}(\\\\bm{x})\\\\log(D(\\\\bm{x}))dx+\\\\int_{z}p_{\\\\bm{z}}(\\\\bm{z})\\\\log(1-D(g(\\\\bm{z})))dz$\\n$=\\\\int_{\\\\bm{x}}p_{\\\\text{data}}(\\\\bm{x})\\\\log(D(\\\\bm{x}))+p_{g}(\\\\bm{x})\\\\log(1-D(\\\\bm{x}))dx$ (3)\\n\\nFor any $(a,b)\\\\in\\\\mathbb{R}^{2}\\\\setminus\\\\{0,0\\\\}$, the function $y\\\\rightarrow a\\\\log(y)+b\\\\log(1-y)$ achieves its maximum in $[0,1]$ at $\\\\frac{a}{a+b}$. The discriminator does not need to be defined outside of $Supp(p_{\\\\text{data}})\\\\cup Supp(p_{g})$, concluding the proof. ∎\\n\\nNote that the training objective for $D$ can be interpreted as maximizing the log-likelihood for estimating the conditional probability $P(Y=y|\\\\bm{x})$, where $Y$ indicates whether $\\\\bm{x}$ comes from $p_{\\\\text{data}}$ (with $y=1$) or from $p_{g}$ (with $y=0$). The minimax game in Eq. 1 can now be reformulated as:\\n\\n$C(G)=$ $\\\\max_{D}V(G,D)$\\n$=$ $\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{\\\\text{data}}}[\\\\log D_{G}^{*}(\\\\bm{x})]+\\\\mathbb{E}_{\\\\bm{z}\\\\sim p_{\\\\bm{z}}}[\\\\log(1-D_{G}^{*}(G(\\\\bm{z})))]$ (4)\\n$=$ $\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{\\\\text{data}}}[\\\\log D_{G}^{*}(\\\\bm{x})]+\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{g}}[\\\\log(1-D_{G}^{*}(\\\\bm{x}))]$\\n$=$ $\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{\\\\text{data}}}\\\\left[\\\\log\\\\frac{p_{\\\\text{data}}(\\\\bm{x})}{P_{\\\\text{data}}(\\\\bm{x})+p_{g}(\\\\bm{x})}\\\\right]+\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{g}}\\\\left[\\\\log\\\\frac{p_{g}(\\\\bm{x})}{p_{\\\\text{data}}(\\\\bm{x})+p_{g}(\\\\bm{x})}\\\\right]$\\n\\n###### Theorem 1.\\n\\nThe global minimum of the virtual training criterion $C(G)$ is achieved if and only if $p_{g}=p_{\\\\text{data}}.$ At that point, $C(G)$ achieves the value $-\\\\log 4.$\\n\\n###### Proof.\\n\\nFor $p_{g}=p_{\\\\text{data}}$, $D_{G}^{*}(\\\\bm{x})=\\\\frac{1}{2}$, (consider Eq. 2). Hence, by inspecting Eq. 4 at $D_{G}^{*}(\\\\bm{x})=\\\\frac{1}{2}$, we find $C(G)=\\\\log\\\\frac{1}{2}+\\\\log\\\\frac{1}{2}=-\\\\log 4$. To see that this is the best possible value of $C(G)$, reached only for $p_{g}=p_{\\\\text{data}}$, observe that\\n\\n$\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{\\\\text{data}}}\\\\,[-\\\\log 2]+\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{g}}\\\\,[-\\\\log 2]=-\\\\log 4$\\n\\nand that by subtracting this expression from $C(G)=V(D_{G}^{*},G)$, we obtain:\\n\\n$C(G)=-\\\\log(4)+KL\\\\left(p_{\\\\text{data}}\\\\left\\\\|\\\\frac{p_{\\\\text{data}}+p_{g}}{2}\\\\right)+KL\\\\left(p_{g}\\\\left\\\\|\\\\frac{p_{\\\\text{data}}+p_{g}}{2}\\\\right.\\\\right)\\\\right.$ (5)\\n\\nwhere KL is the Kullback–Leibler divergence. We recognize in the previous expression the Jensen–Shannon divergence between the model’s distribution and the data generating process:\\n\\n$C(G)=-\\\\log(4)+2\\\\cdot JSD\\\\left(p_{\\\\text{data}}\\\\left\\\\|p_{g}\\\\right.\\\\right)$ (6)\\n\\nSince the Jensen–Shannon divergence between two distributions is always non-negative and zero only when they are equal, we have shown that $C^{*}=-\\\\log(4)$ is the global minimum of $C(G)$ and that the only solution is $p_{g}=p_{\\\\text{data}}$, i.e., the generative model perfectly replicating the data generating process. ∎\\n\\n### 4.2 Convergence of Algorithm 1\\n\\n###### Proposition 2.\\n\\nIf $G$ and $D$ have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given $G$, and $p_{g}$ is updated so as to improve the criterion\\n\\n$\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{\\\\text{data}}}[\\\\log D_{G}^{*}(\\\\bm{x})]+\\\\mathbb{E}_{\\\\bm{x}\\\\sim p_{g}}[\\\\log(1-D_{G}^{*}(\\\\bm{x}))]$\\n\\nthen $p_{g}$ converges to $p_{\\\\text{data}}$\\n\\n###### Proof.\\n\\nConsider $V(G,D)=U(p_{g},D)$ as a function of $p_{g}$ as done in the above criterion. Note that $U(p_{g},D)$ is convex in $p_{g}$. The subderivatives of a supremum of convex functions include the derivative of the function at the point where the maximum is attained. In other words, if $f(x)=\\\\sup_{\\\\alpha\\\\in\\\\mathcal{A}}f_{\\\\alpha}(x)$ and $f_{\\\\alpha}(x)$ is convex in $x$ for every $\\\\alpha$, then $\\\\partial f_{\\\\beta}(x)\\\\in\\\\partial f$ if $\\\\beta=\\\\arg\\\\sup_{\\\\alpha\\\\in\\\\mathcal{A}}f_{\\\\alpha}(x)$. This is equivalent to computing a gradient descent update for $p_{g}$ at the optimal $D$ given the corresponding $G$. $\\\\sup_{D}U(p_{g},D)$ is convex in $p_{g}$ with a unique global optima as proven in Thm 1, therefore with sufficiently small updates of $p_{g}$, $p_{g}$ converges to $p_{x}$, concluding the proof. ∎\\n\\nIn practice, adversarial nets represent a limited family of $p_{g}$ distributions via the function $G(\\\\bm{z};\\\\theta_{g})$, and we optimize $\\\\theta_{g}$ rather than $p_{g}$ itself. Using a multilayer perceptron to define $G$ introduces multiple critical points in parameter space. However, the excellent performance of multilayer perceptrons in practice suggests that they are a reasonable model to use despite their lack of theoretical guarantees.\\n\\n## 5 Experiments\\n\\nWe trained adversarial nets an a range of datasets including MNIST*[23]*, the Toronto Face Database (TFD) *[28]*, and CIFAR-10 *[21]*. The generator nets used a mixture of rectifier linear activations *[19, 9]* and sigmoid activations, while the discriminator net used maxout *[10]* activations. Dropout *[17]* was applied in training the discriminator net. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network.\\n\\nWe estimate probability of the test set data under $p_{g}$ by fitting a Gaussian Parzen window to the samples generated with $G$ and reporting the log-likelihood under this distribution. The $\\\\sigma$ parameter\\n\\n|  Model | MNIST | TFD  |\\n| --- | --- | --- |\\n|  DBN [3] | 138 ± 2 | 1909 ± 66  |\\n|  Stacked CAE [3] | 121 ± 1.6 | 2110 ± 50  |\\n|  Deep GSN [6] | 214 ± 1.1 | 1890 ± 29  |\\n|  Adversarial nets | 225 ± 2 | 2057 ± 26  |\\n\\nTable 1: Parzen window-based log-likelihood estimates. The reported numbers on MNIST are the mean log-likelihood of samples on test set, with the standard error of the mean computed across examples. On TFD, we computed the standard error across folds of the dataset, with a different  $\\\\sigma$  chosen using the validation set of each fold. On TFD,  $\\\\sigma$  was cross validated on each fold and mean log-likelihood on each fold were computed. For MNIST we compare against other models of the real-valued (rather than binary) version of dataset.\\n\\nof the Gaussians was obtained by cross validation on the validation set. This procedure was introduced in Breuleux et al. [8] and used for various generative models for which the exact likelihood is not tractable [25, 3, 5]. Results are reported in Table 1. This method of estimating the likelihood has somewhat high variance and does not perform well in high dimensional spaces but it is the best method available to our knowledge. Advances in generative models that can sample but not estimate likelihood directly motivate further research into how to evaluate such models.\\n\\nIn Figures 2 and 3 we show samples drawn from the generator net after training. While we make no claim that these samples are better than samples generated by existing methods, we believe that these samples are at least competitive with the better generative models in the literature and highlight the potential of the adversarial framework.\\n\\n![img-4.jpeg](img-4.jpeg)\\na)\\n\\n![img-5.jpeg](img-5.jpeg)\\nb)\\n\\n![img-6.jpeg](img-6.jpeg)\\nc)\\n\\n![img-7.jpeg](img-7.jpeg)\\nd)\\nFigure 2: Visualization of samples from the model. Rightmost column shows the nearest training example of the neighboring sample, in order to demonstrate that the model has not memorized the training set. Samples are fair random draws, not cherry-picked. Unlike most other visualizations of deep generative models, these images show actual samples from the model distributions, not conditional means given samples of hidden units. Moreover, these samples are uncorrelated because the sampling process does not depend on Markov chain mixing. a) MNIST b) TFD c) CIFAR-10 (fully connected model) d) CIFAR-10 (convolutional discriminator and \"deconvolutional\" generator)\\n\\n111111111111111111111111111111111111111111\\n\\nFigure 3: Digits obtained by linearly interpolating between coordinates in  $z$  space of the full model.\\n\\n|   | Deep directed graphical models | Deep undirected graphical models | Generative autoencoders | Adversarial models  |\\n| --- | --- | --- | --- | --- |\\n|  Training | Inference needed during training. | Inference needed during training. MCMC needed to approximate partition function gradient. | Enforced tradeoff between mixing and power of reconstruction generation | Synchronizing the discriminator with the generator. Helvetica.  |\\n|  Inference | Learned approximate inference | Variational inference | MCMC-based inference | Learned approximate inference  |\\n|  Sampling | No difficulties | Requires Markov chain | Requires Markov chain | No difficulties  |\\n|  Evaluating p(x) | Intractable, may be approximated with AIS | Intractable, may be approximated with AIS | Not explicitly represented, may be approximated with Parzen density estimation | Not explicitly represented, may be approximated with Parzen density estimation  |\\n|  Model design | Nearly all models incur extreme difficulty | Careful design needed to ensure multiple properties | Any differentiable function is theoretically permitted | Any differentiable function is theoretically permitted  |\\n\\nTable 2: Challenges in generative modeling: a summary of the difficulties encountered by different approaches to deep generative modeling for each of the major operations involving a model.\\n\\n# 6 Advantages and disadvantages\\n\\nThis new framework comes with advantages and disadvantages relative to previous modeling frameworks. The disadvantages are primarily that there is no explicit representation of  $p_{g}(\\\\pmb{x})$ , and that  $D$  must be synchronized well with  $G$  during training (in particular,  $G$  must not be trained too much without updating  $D$ , in order to avoid \"the Helvetica scenario\" in which  $G$  collapses too many values of  $\\\\mathbf{z}$  to the same value of  $\\\\mathbf{x}$  to have enough diversity to model  $p_{\\\\mathrm{data}}$ ), much as the negative chains of a Boltzmann machine must be kept up to date between learning steps. The advantages are that Markov chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model. Table 2 summarizes the comparison of generative adversarial nets with other generative modeling approaches.\\n\\nThe aforementioned advantages are primarily computational. Adversarial models may also gain some statistical advantage from the generator network not being updated directly with data examples, but only with gradients flowing through the discriminator. This means that components of the input are not copied directly into the generator\\'s parameters. Another advantage of adversarial networks is that they can represent very sharp, even degenerate distributions, while methods based on Markov chains require that the distribution be somewhat blurry in order for the chains to be able to mix between modes.\\n\\n# 7 Conclusions and future work\\n\\nThis framework admits many straightforward extensions:\\n\\n1. A conditional generative model  $p(\\\\pmb{x} \\\\mid \\\\pmb{c})$  can be obtained by adding  $\\\\pmb{c}$  as input to both  $G$  and  $D$ .\\n2. Learned approximate inference can be performed by training an auxiliary network to predict  $z$  given  $x$ . This is similar to the inference net trained by the wake-sleep algorithm [15] but with the advantage that the inference net may be trained for a fixed generator net after the generator net has finished training.\\n\\n3. One can approximately model all conditionals $p(\\\\bm{x}_{S}\\\\mid\\\\bm{x}_{\\\\mathcal{S}})$ where $S$ is a subset of the indices of $\\\\bm{x}$ by training a family of conditional models that share parameters. Essentially, one can use adversarial nets to implement a stochastic extension of the deterministic MP-DBM *[11]*.\\n4. Semi-supervised learning: features from the discriminator or inference net could improve performance of classifiers when limited labeled data is available.\\n5. Efficiency improvements: training could be accelerated greatly by divising better methods for coordinating $G$ and $D$ or determining better distributions to sample $\\\\mathbf{z}$ from during training.\\n\\nThis paper has demonstrated the viability of the adversarial modeling framework, suggesting that these research directions could prove useful.\\n\\n### Acknowledgments\\n\\nWe would like to acknowledge Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for helpful discussions. Yann Dauphin shared his Parzen window evaluation code with us. We would like to thank the developers of Pylearn2 *[12]* and Theano *[7, 1]*, particularly Frédéric Bastien who rushed a Theano feature specifically to benefit this project. Arnaud Bergeron provided much-needed support with LaTeX typesetting. We would also like to thank CIFAR, and Canada Research Chairs for funding, and Compute Canada, and Calcul Québec for providing computational resources. Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning. Finally, we would like to thank Les Trois Brasseurs for stimulating our creativity.\\n\\n## References\\n\\n- [1] Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.\\n- [2] Bengio, Y. (2009). Learning deep architectures for AI. Now Publishers.\\n- [3] Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013a). Better mixing via deep representations. In ICML’13.\\n- [4] Bengio, Y., Yao, L., Alain, G., and Vincent, P. (2013b). Generalized denoising auto-encoders as generative models. In NIPS26. Nips Foundation.\\n- [5] Bengio, Y., Thibodeau-Laufer, E., and Yosinski, J. (2014a). Deep generative stochastic networks trainable by backprop. In ICML’14.\\n- [6] Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014b). Deep generative stochastic networks trainable by backprop. In Proceedings of the 30th International Conference on Machine Learning (ICML’14).\\n- [7] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation.\\n- [8] Breuleux, O., Bengio, Y., and Vincent, P. (2011). Quickly generating representative samples from an RBM-derived process. Neural Computation, 23(8), 2053–2073.\\n- [9] Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep sparse rectifier neural networks. In AISTATS’2011.\\n- [10] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013a). Maxout networks. In ICML’2013.\\n- [11] Goodfellow, I. J., Mirza, M., Courville, A., and Bengio, Y. (2013b). Multi-prediction deep Boltzmann machines. In NIPS’2013.\\n- [12] Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., and Bengio, Y. (2013c). Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214.\\n- [13] Gutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS’2010.\\n- [14] Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. (2012a). Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6), 82–97.\\n- [15] Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995). The wake-sleep algorithm for unsupervised neural networks. Science, 268, 1558–1161.\\n-\\n\\n[16] Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18, 1527–1554.\\n- [17] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012b). Improving neural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.\\n- [18] Hyvärinen, A. (2005). Estimation of non-normalized statistical models using score matching. J. Machine Learning Res., 6.\\n- [19] Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pages 2146–2153. IEEE.\\n- [20] Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations (ICLR).\\n- [21] Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images. Technical report, University of Toronto.\\n- [22] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In NIPS’2012.\\n- [23] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.\\n- [24] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. Technical report, arXiv:1401.4082.\\n- [25] Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012). A generative process for sampling contractive auto-encoders. In ICML’12.\\n- [26] Salakhutdinov, R. and Hinton, G. E. (2009). Deep Boltzmann machines. In AISTATS’2009, pages 448–455.\\n- [27] Smolensky, P. (1986). Information processing in dynamical systems: Foundations of harmony theory. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing, volume 1, chapter 6, pages 194–281. MIT Press, Cambridge.\\n- [28] Susskind, J., Anderson, A., and Hinton, G. E. (2010). The Toronto face dataset. Technical Report UTML TR 2010-001, U. Toronto.\\n- [29] Tieleman, T. (2008). Training restricted Boltzmann machines using approximations to the likelihood gradient. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, ICML 2008, pages 1064–1071. ACM.\\n- [30] Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and composing robust features with denoising autoencoders. In ICML 2008.\\n- [31] Younes, L. (1999). On the convergence of Markovian stochastic algorithms with rapidly decreasing ergodicity rates. Stochastics and Stochastic Reports, 65(3), 177–228.'), 0.04484443251725645), (Document(id='09d7abb38688:1', metadata={'doc_id': '09d7abb38688', 'chunk_index': 1, 'end_line': 965, 'start_line': 697, 'text': '# E.2. Implementation Details\\n\\n# E.2.1 Implementations of  $\\\\tau_{\\\\theta}$  for conditional LDMs\\n\\nFor the experiments on text-to-image and layout-to-image (Sec. 4.3.1) synthesis, we implement the conditioner  $\\\\tau_{\\\\theta}$  as an unmasked transformer which processes a tokenized version of the input  $y$  and produces an output  $\\\\zeta \\\\coloneqq \\\\tau_{\\\\theta}(y)$ , where  $\\\\zeta \\\\in \\\\mathbb{R}^{M\\\\times d_{\\\\tau}}$ . More specifically, the transformer is implemented from  $N$  transformer blocks consisting of global self-attention layers, layer-normalization and position-wise MLPs as follows:\\n\\n|   | LDM-1 | LDM-2 | LDM-4 | LDM-8 | LDM-16 | LDM-32  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  z-shape | 256 × 256 × 3 | 128 × 128 × 2 | 64 × 64 × 3 | 32 × 32 × 4 | 16 × 16 × 8 | 88 × 8 × 32  |\\n|  |Z | - | 2048 | 8192 | 16384 | 16384 | 16384  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear | linear | linear  |\\n|  Model Size | 270M | 265M | 274M | 258M | 260M | 258M  |\\n|  Channels | 192 | 192 | 224 | 256 | 256 | 256  |\\n|  Depth | 2 | 2 | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,1,2,2,4,4 | 1,2,2,4,4 | 1,2,3,4 | 1,2,4 | 1,2,4 | 1,2,4  |\\n|  Attention resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 16, 8, 4 | 8, 4, 2  |\\n|  Head Channels | 32 | 32 | 32 | 32 | 32 | 32  |\\n|  Batch Size | 9 | 11 | 48 | 96 | 128 | 128  |\\n|  Iterations* | 500k | 500k | 500k | 500k | 500k | 500k  |\\n|  Learning Rate | 9e-5 | 1.1e-4 | 9.6e-5 | 9.6e-5 | 1.3e-4 | 1.3e-4  |\\n\\nTable 14. Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in Fig. 7. All models trained on a single NVIDIA A100. *: All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the provided FID scores.\\n\\n|  Task | Text-to-Image | Layout-to-Image |   | Class-Label-to-Image | Super Resolution | Inpainting | Semantic-Map-to-Image  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Dataset | LAION | OpenImages | COCO | ImageNet | ImageNet | Places | Landscapes  |\\n|  f | 8 | 4 | 8 | 4 | 4 | 4 | 8  |\\n|  z-shape | 32 × 32 × 4 | 64 × 64 × 3 | 32 × 32 × 4 | 64 × 64 × 3 | 64 × 64 × 3 | 64 × 64 × 3 | 32 × 32 × 4  |\\n|  |Z | - | 8192 | 16384 | 8192 | 8192 | 8192 | 16384  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear | linear | linear | linear  |\\n|  Model Size | 1.45B | 306M | 345M | 395M | 169M | 215M | 215M  |\\n|  Channels | 320 | 128 | 192 | 192 | 160 | 128 | 128  |\\n|  Depth | 2 | 2 | 2 | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,2,4,4 | 1,2,3,4 | 1,2,4 | 1,2,3,5 | 1,2,2,4 | 1,4,8 | 1,4,8  |\\n|  Number of Heads | 8 | 1 | 1 | 1 | 1 | 1 | 1  |\\n|  Dropout | - | - | 0.1 | - | - | - | -  |\\n|  Batch Size | 680 | 24 | 48 | 1200 | 64 | 128 | 48  |\\n|  Iterations | 390K | 4.4M | 170K | 178K | 860K | 360K | 360K  |\\n|  Learning Rate | 1.0e-4 | 4.8e-5 | 4.8e-5 | 1.0e-4 | 6.4e-5 | 1.0e-6 | 4.8e-5  |\\n|  Conditioning | CA | CA | CA | CA | concat | concat | concat  |\\n|  (C)A-resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | - | - | -  |\\n|  Embedding Dimension | 1280 | 512 | 512 | 512 | - | - | -  |\\n|  Transformer Depth | 1 | 3 | 2 | 1 | - | - | -  |\\n\\nTable 15. Hyperparameters for the conditional LDMs from Sec. 4. All models trained on a single NVIDIA A100 except for the inpainting model which was trained on eight V100.\\n\\n$\\\\zeta \\\\gets \\\\mathrm{TokEmb}(y) + \\\\mathrm{PosEmb}(y)$  (18)\\n\\nfor  $i = 1,\\\\dots ,N$  ..\\n\\n$\\\\zeta_{1}\\\\gets \\\\mathrm{LayerNorm}(\\\\zeta)$  (19)\\n\\n$\\\\zeta_{2}\\\\gets \\\\mathrm{MultiHeadSelfAttention}(\\\\zeta_{1}) + \\\\zeta$  (20)\\n\\n$\\\\zeta_{3}\\\\gets \\\\mathrm{LayerNorm}(\\\\zeta_{2})$  (21)\\n\\n$\\\\zeta \\\\gets \\\\mathrm{MLP}(\\\\zeta_3) + \\\\zeta_2$  (22)\\n\\n$\\\\zeta \\\\gets \\\\mathrm{LayerNorm}(\\\\zeta)$  (23)\\n\\n(24)\\n\\nWith  $\\\\zeta$  available, the conditioning is mapped into the UNet via the cross-attention mechanism as depicted in Fig. 3. We modify the \"ablated UNet\" [15] architecture and replace the self-attention layer with a shallow (unmasked) transformer consisting of  $T$  blocks with alternating layers of (i) self-attention, (ii) a position-wise MLP and (iii) a cross-attention layer;\\n\\nsee Tab. 16. Note that without (ii) and (iii), this architecture is equivalent to the \"ablated UNet\".\\n\\nWhile it would be possible to increase the representational power of  $\\\\tau_{\\\\theta}$  by additionally conditioning on the time step  $t$ , we do not pursue this choice as it reduces the speed of inference. We leave a more detailed analysis of this modification to future work.\\n\\nFor the text-to-image model, we rely on a publicly available $^3$  tokenizer [99]. The layout-to-image model discretizes the spatial locations of the bounding boxes and encodes each box as a  $(l,b,c)$ -tuple, where  $l$  denotes the (discrete) top-left and  $b$  the bottom-right position. Class information is contained in  $c$ .\\n\\nSee Tab. 17 for the hyperparameters of  $\\\\tau_{\\\\theta}$  and Tab. 13 for those of the UNet for both of the above tasks.\\n\\nNote that the class-conditional model as described in Sec. 4.1 is also implemented via cross-attention, where  $\\\\tau_{\\\\theta}$  is a single learnable embedding layer with a dimensionality of 512, mapping classes  $y$  to  $\\\\zeta \\\\in \\\\mathbb{R}^{1\\\\times 512}$ .\\n\\n|  input | Rh×w×c  |\\n| --- | --- |\\n|  LayerNorm | Rh×w×c  |\\n|  Conv1x1 | Rh×w×d·nh  |\\n|  Reshape | Rh·w×d·nh  |\\n|  SelfAttention | Rh·w×d·nh  |\\n|  MLP | Rh·w×d·nh  |\\n|  CrossAttention | Rh·w×d·nh  |\\n|  Reshape | Rh×w×d·nh  |\\n|  Conv1x1 | Rh×w×c  |\\n\\nTable 16. Architecture of a transformer block as described in Sec. E.2.1, replacing the self-attention layer of the standard \"ablated UNet\" architecture [15]. Here,  $n_h$  denotes the number of attention heads and  $d$  the dimensionality per head.\\n\\n|   | Text-to-Image | Layout-to-Image  |\\n| --- | --- | --- |\\n|  seq-length | 77 | 92  |\\n|  depth N | 32 | 16  |\\n|  dim | 1280 | 512  |\\n\\nTable 17. Hyperparameters for the experiments with transformer encoders in Sec. 4.3.\\n\\n# E.2.2 Inpainting\\n\\nFor our experiments on image-inpainting in Sec. 4.5, we used the code of [88] to generate synthetic masks. We use a fixed set of  $2\\\\mathrm{k}$  validation and  $30\\\\mathrm{k}$  testing samples from Places [108]. During training, we use random crops of size  $256 \\\\times 256$  and evaluate on crops of size  $512 \\\\times 512$ . This follows the training and testing protocol in [88] and reproduces their reported metrics (see  $\\\\dagger$  in Tab. 7). We include additional qualitative results of LDM-4, w/attn in Fig. 21 and of LDM-4, w/o attn, big, w/ft in Fig. 22.\\n\\n# E.3. Evaluation Details\\n\\nThis section provides additional details on evaluation for the experiments shown in Sec. 4.\\n\\n# E.3.1 Quantitative Results in Unconditional and Class-Conditional Image Synthesis\\n\\nWe follow common practice and estimate the statistics for calculating the FID-, Precision- and Recall-scores [29,50] shown in Tab. 1 and 10 based on 50k samples from our models and the entire training set of each of the shown datasets. For calculating FID scores we use the torch-fidelity package [60]. However, since different data processing pipelines might lead to different results [64], we also evaluate our models with the script provided by Dhariwal and Nichol [15]. We find that results\\n\\nmainly coincide, except for the ImageNet and LSUN-Bedrooms datasets, where we notice slightly varying scores of 7.76 (torch-fidelity) vs. 7.77 (Nichol and Dhariwal) and 2.95 vs 3.0. For the future we emphasize the importance of a unified procedure for sample quality assessment. Precision and Recall are also computed by using the script provided by Nichol and Dhariwal.\\n\\n#### E.3.2 Text-to-Image Synthesis\\n\\nFollowing the evaluation protocol of *[66]* we compute FID and Inception Score for the Text-to-Image models from Tab. 2 by comparing generated samples with 30000 samples from the validation set of the MS-COCO dataset *[51]*. FID and Inception Scores are computed with torch-fidelity.\\n\\n#### E.3.3 Layout-to-Image Synthesis\\n\\nFor assessing the sample quality of our Layout-to-Image models from Tab. 9 on the COCO dataset, we follow common practice *[87, 89, 37]* and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split. To obtain better comparability, we use the exact same samples as in *[37]*. For the OpenImages dataset we similarly follow their protocol and use 2048 center-cropped test images from the validation set.\\n\\n#### E.3.4 Super Resolution\\n\\nWe evaluate the super-resolution models on ImageNet following the pipeline suggested in *[72]*, *i.e*. images with a shorter size less than $256$ px are removed (both for training and evaluation). On ImageNet, the low-resolution images are produced using bicubic interpolation with anti-aliasing. FIDs are evaluated using torch-fidelity *[60]*, and we produce samples on the validation split. For FID scores, we additionally compare to reference features computed on the train split, see Tab. 5 and Tab. 11.\\n\\n#### E.3.5 Efficiency Analysis\\n\\nFor efficiency reasons we compute the sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples. Therefore, the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the learning rates slightly vary between different runs *cf*. Tab. 13 and 14.\\n\\n#### E.3.6 User Study\\n\\nFor the results of the user study presented in Tab. 4 we followed the protocoll of *[72]* and and use the 2-alternative force-choice paradigm to assess human preference scores for two distinct tasks. In Task-1 subjects were shown a low resolution/masked image between the corresponding ground truth high resolution/unmasked version and a synthesized image, which was generated by using the middle image as conditioning. For SuperResolution subjects were asked: *’Which of the two images is a better high quality version of the low resolution image in the middle?’*. For Inpainting we asked *’Which of the two images contains more realistic inpainted regions of the image in the middle?’*. In Task-2, humans were similarly shown the low-res/masked version and asked for preference between two corresponding images generated by the two competing methods. As in *[72]* humans viewed the images for 3 seconds before responding.\\n\\n# F. Computational Requirements\\n\\n|  Method | Generator Compute | Classifier Compute | Overall Compute | Inference Throughput* | Nparams | FID↓ | IS↑ | Precision↑ | Recall↑  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  LSUN Churches 2562  |   |   |   |   |   |   |   |   |   |\\n|  StyleGAN2 [42]† | 64 | - | 64 | - | 59M | 3.86 | - | - | -  |\\n|  LDM-8 (ours, 100 steps, 410K) | 18 | - | 18 | 6.80 | 256M | 4.02 | - | 0.64 | 0.52  |\\n|  LSUN Bedrooms 2563  |   |   |   |   |   |   |   |   |   |\\n|  ADM [15]† (1000 steps) | 232 | - | 232 | 0.03 | 552M | 1.9 | - | 0.66 | 0.51  |\\n|  LDM-4 (ours, 200 steps, 1.9M) | 60 | - | 55 | 1.07 | 274M | 2.95 | - | 0.66 | 0.48  |\\n|  CelebA-HQ 2562  |   |   |   |   |   |   |   |   |   |\\n|  LDM-4 (ours, 500 steps, 410K) | 14.4 | - | 14.4 | 0.43 | 274M | 5.11 | - | 0.72 | 0.49  |\\n|  FFHQ 2562  |   |   |   |   |   |   |   |   |   |\\n|  StyleGAN2 [42] | 32.13‡ | - | 32.13‡ | - | 59M | 3.8 | - | - | -  |\\n|  LDM-4 (ours, 200 steps, 635K) | 26 | - | 26 | 1.07 | 274M | 4.98 | - | 0.73 | 0.50  |\\n|  ImageNet 2562  |   |   |   |   |   |   |   |   |   |\\n|  VQGAN-f-4 (ours, first stage) | 29 | - | 29 | - | 55M | 0.58†† | - | - | -  |\\n|  VQGAN-f-8 (ours, first stage) | 66 | - | 66 | - | 68M | 1.14†† | - | - | -  |\\n|  BigGAN-deep [3]† | 128-256 |  | 128-256 | - | 340M | 6.95 | 203.6±1.0 | 0.87 | 0.28  |\\n|  ADM [15] (250 steps)† | 916 | - | 916 | 0.12 | 554M | 10.94 | 100.98 | 0.69 | 0.63  |\\n|  ADM-G [15] (25 steps)† | 916 | 46 | 962 | 0.7 | 608M | 5.58 | - | 0.81 | 0.49  |\\n|  ADM-G [15] (250 steps)† | 916 | 46 | 962 | 0.07 | 608M | 4.59 | 186.7 | 0.82 | 0.52  |\\n|  ADM-G.ADM-G [15] (250 steps)† | 329 | 30 | 349 | n/a | n/a | 3.85 | 221.72 | 0.84 | 0.53  |\\n|  LDM-8-G (ours, 100, 2.9M) | 79 | 12 | 91 | 1.93 | 506M | 8.11 | 190.4±2.0 | 0.83 | 0.36  |\\n|  LDM-8 (ours, 200 ddim steps 2.9M, batch size 64) | 79 | - | 79 | 1.9 | 395M | 17.41 | 72.92 | 0.65 | 0.62  |\\n|  LDM-4 (ours, 250 ddim steps 178K, batch size 1200) | 271 | - | 271 | 0.7 | 400M | 10.56 | 103.49±1.10 | 0.71 | 0.62  |\\n|  LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classifier-free guidance [32] scale 1.25) | 271 | - | 271 | 0.4 | 400M | 3.95 | 178.22±1.41 | 0.81 | 0.55  |\\n|  LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classifier-free guidance [32] scale 1.5) | 271 | - | 271 | 0.4 | 400M | 3.60 | 247.67±3.50 | 0.87 | 0.48  |\\n\\nTable 18. Comparing compute requirements during training and inference throughput with state-of-the-art generative models. Compute during training in V100-days, numbers of competing methods taken from [15] unless stated differently;*: Throughput measured in samples/sec on a single NVIDIA A100;†: Numbers taken from [15];‡: Assumed to be trained on 25M train examples; ††: R-FID vs. ImageNet validation set\\n\\nIn Tab 18 we provide a more detailed analysis on our used compute ressources and compare our best performing models on the CelebA-HQ, FFHQ, LSUN and ImageNet datasets with the recent state of the art models by using their provided numbers, cf. [15]. As they report their used compute in V100 days and we train all our models on a single NVIDIA A100 GPU, we convert the A100 days to V100 days by assuming a  $\\\\times 2.2$  speedup of A100 vs V100 [74] $^4$ . To assess sample quality, we additionally report FID scores on the reported datasets. We closely reach the performance of state of the art methods as StyleGAN2 [42] and ADM [15] while significantly reducing the required compute resources.\\n\\nG Details on Autoencoder Models\\n\\nWe train all our autoencoder models in an adversarial manner following *[23]*, such that a patch-based discriminator $D_{\\\\psi}$ is optimized to differentiate original images from reconstructions $\\\\mathcal{D}(\\\\mathcal{E}(x))$. To avoid arbitrarily scaled latent spaces, we regularize the latent $z$ to be zero centered and obtain small variance by introducing an regularizing loss term $L_{reg}$.\\nWe investigate two different regularization methods: (i) a low-weighted Kullback-Leibler-term between $q_{\\\\mathcal{E}}(z|x)=\\\\mathcal{N}(z;\\\\mathcal{E}_{\\\\mu},\\\\mathcal{E}_{\\\\sigma^{2}})$ and a standard normal distribution $\\\\mathcal{N}(z;0,1)$ as in a standard variational autoencoder *[69, 46]*, and, (ii) regularizing the latent space with a vector quantization layer by learning a codebook of $|\\\\mathcal{Z}|$ different exemplars *[96]*.\\nTo obtain high-fidelity reconstructions we only use a very small regularization for both scenarios, *i.e*. we either weight the $\\\\mathbb{KL}$ term by a factor $\\\\sim 10^{-6}$ or choose a high codebook dimensionality $|\\\\mathcal{Z}|$.\\n\\nThe full objective to train the autoencoding model $(\\\\mathcal{E},\\\\mathcal{D})$ reads:\\n\\n$L_{\\\\text{Autoencoder}}=\\\\min_{\\\\mathcal{E},\\\\mathcal{D}}\\\\max_{\\\\psi}\\\\Big{(}L_{rec}(x,\\\\mathcal{D}(\\\\mathcal{E}(x)))-L_{adv}(\\\\mathcal{D}(\\\\mathcal{E}(x)))+\\\\log D_{\\\\psi}(x)+L_{reg}(x;\\\\mathcal{E},\\\\mathcal{D})\\\\Big{)}$ (25)\\n\\n##### DM Training in Latent Space\\n\\nNote that for training diffusion models on the learned latent space, we again distinguish two cases when learning $p(z)$ or $p(z|y)$ (Sec. 4.3): (i) For a KL-regularized latent space, we sample $z=\\\\mathcal{E}_{\\\\mu}(x)+\\\\mathcal{E}_{\\\\sigma}(x)\\\\cdot\\\\varepsilon=:\\\\mathcal{E}(x)$, where $\\\\varepsilon\\\\sim\\\\mathcal{N}(0,1)$. When rescaling the latent, we estimate the component-wise variance\\n\\n$\\\\hat{\\\\sigma}^{2}=\\\\frac{1}{bchw}\\\\sum_{b,c,h,w}(z^{b,c,h,w}-\\\\hat{\\\\mu})^{2}$\\n\\nfrom the first batch in the data, where $\\\\hat{\\\\mu}=\\\\frac{1}{bchw}\\\\sum_{b,c,h,w}z^{b,c,h,w}$. The output of $\\\\mathcal{E}$ is scaled such that the rescaled latent has unit standard deviation, *i.e*. $z\\\\leftarrow\\\\frac{z}{\\\\hat{\\\\sigma}}=\\\\frac{\\\\mathcal{E}(x)}{\\\\hat{\\\\sigma}}$. (ii) For a VQ-regularized latent space, we extract $z$ *before* the quantization layer and absorb the quantization operation into the decoder, *i.e*. it can be interpreted as the first layer of $\\\\mathcal{D}$.\\n\\n## Appendix H Additional Qualitative Results\\n\\nFinally, we provide additional qualitative results for our landscapes model (Fig. 12, 23, 24 and 25), our class-conditional ImageNet model (Fig. 26 - 27) and our unconditional models for the CelebA-HQ, FFHQ and LSUN datasets (Fig. 28 - 31). Similar as for the inpainting model in Sec. 4.5 we also fine-tuned the semantic landscapes model from Sec. 4.3.2 directly on $512^{2}$ images and depict qualitative results in Fig. 12 and Fig. 23. For our those models trained on comparably small datasets, we additionally show nearest neighbors in VGG *[79]* feature space for samples from our models in Fig. 32 - 34.\\n\\n![img-21.jpeg](img-21.jpeg)\\nbicubic\\n\\n![img-22.jpeg](img-22.jpeg)\\nLDM-BSR\\n\\n![img-23.jpeg](img-23.jpeg)\\n\\n![img-24.jpeg](img-24.jpeg)\\n\\n![img-25.jpeg](img-25.jpeg)\\nFigure 19. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from the LSUN-Cows dataset to  $1024^{2}$  resolution.\\n\\n![img-26.jpeg](img-26.jpeg)\\n\\n![img-27.jpeg](img-27.jpeg)\\nFigure 20. Qualitative superresolution comparison of two random samples between LDM-SR and baseline-diffusionmodel in Pixelspace. Evaluated on imagenet validation-set after same amount of training steps.\\n\\n![img-28.jpeg](img-28.jpeg)\\nFigure 21. Qualitative results on image inpainting. In contrast to [88], our generative approach enables generation of multiple diverse samples for a given input.\\n\\n![img-29.jpeg](img-29.jpeg)\\nFigure 22. More qualitative results on object removal as in Fig. 11.\\n\\n![img-30.jpeg](img-30.jpeg)\\nSemantic Synthesis on Flickr-Landscapes [23] (512 $^2$  finetuning)\\n\\n![img-31.jpeg](img-31.jpeg)\\n\\n![img-32.jpeg](img-32.jpeg)\\nFigure 23. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on  $512^{2}$  images.\\n\\n![img-33.jpeg](img-33.jpeg)\\nFigure 24. A LDM trained on  $256^{2}$  resolution can generalize to larger resolution for spatially conditioned tasks such as semantic synthesis of landscape images. See Sec. 4.3.2.\\n\\nSemantic Synthesis on Flickr-Landscapes [23]\\n\\n![img-34.jpeg](img-34.jpeg)\\n\\n![img-35.jpeg](img-35.jpeg)\\n\\n![img-36.jpeg](img-36.jpeg)\\n\\n![img-37.jpeg](img-37.jpeg)\\nFigure 25. When provided a semantic map as conditioning, our LDMs generalize to substantially larger resolutions than those seen during training. Although this model was trained on inputs of size  $256^2$  it can be used to create high-resolution samples as the ones shown here, which are of resolution  $1024 \\\\times 384$ .\\n\\nRandom class conditional samples on the ImageNet dataset\\n\\n![img-38.jpeg](img-38.jpeg)\\nFigure 26. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance [32] scale  $s = 5.0$  and 200 DDIM steps with  $\\\\eta = 1.0$ .\\n\\n![img-39.jpeg](img-39.jpeg)\\nRandom class conditional samples on the ImageNet dataset\\nFigure 27. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance [32] scale  $s = 3.0$  and 200 DDIM steps with  $\\\\eta = 1.0$ .\\n\\n![img-40.jpeg](img-40.jpeg)\\nRandom samples on the CelebA-HQ dataset\\nFigure 28. Random samples of our best performing model LDM-4 on the CelebA-HQ dataset. Sampled with 500 DDIM steps and  $\\\\eta = 0$  (FID = 5.15).\\n\\n![img-41.jpeg](img-41.jpeg)\\nRandom samples on the FFHQ dataset\\nFigure 29. Random samples of our best performing model LDM-4 on the FFHQ dataset. Sampled with 200 DDIM steps and  $\\\\eta = 1$  (FID  $= 4.98$ ).\\n\\n![img-42.jpeg](img-42.jpeg)\\nRandom samples on the LSUN-Churches dataset\\nFigure 30. Random samples of our best performing model LDM-8 on the LSUN-Churches dataset. Sampled with 200 DDIM steps and  $\\\\eta = 0$  (FID = 4.48).\\n\\n![img-43.jpeg](img-43.jpeg)\\nRandom samples on the LSUN-Bedrooms dataset\\nFigure 31. Random samples of our best performing model LDM-4 on the LSUN-Bedrooms dataset. Sampled with 200 DDIM steps and  $\\\\eta = 1$  (FID = 2.95).\\n\\n# Nearest Neighbors on the CelebA-HQ dataset\\n\\n![img-44.jpeg](img-44.jpeg)\\nFigure 32. Nearest neighbors of our best CelebA-HQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors.\\n\\n![img-45.jpeg](img-45.jpeg)\\nNearest Neighbors on the FFHQ dataset\\nFigure 33. Nearest neighbors of our best FFHQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors.\\n\\n![img-46.jpeg](img-46.jpeg)\\nNearest Neighbors on the LSUN-Churches dataset\\nFigure 34. Nearest neighbors of our best LSUN-Churches model, computed in the feature space of a VGG-16 [79]. The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors.'}, page_content='# E.2. Implementation Details\\n\\n# E.2.1 Implementations of  $\\\\tau_{\\\\theta}$  for conditional LDMs\\n\\nFor the experiments on text-to-image and layout-to-image (Sec. 4.3.1) synthesis, we implement the conditioner  $\\\\tau_{\\\\theta}$  as an unmasked transformer which processes a tokenized version of the input  $y$  and produces an output  $\\\\zeta \\\\coloneqq \\\\tau_{\\\\theta}(y)$ , where  $\\\\zeta \\\\in \\\\mathbb{R}^{M\\\\times d_{\\\\tau}}$ . More specifically, the transformer is implemented from  $N$  transformer blocks consisting of global self-attention layers, layer-normalization and position-wise MLPs as follows:\\n\\n|   | LDM-1 | LDM-2 | LDM-4 | LDM-8 | LDM-16 | LDM-32  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  z-shape | 256 × 256 × 3 | 128 × 128 × 2 | 64 × 64 × 3 | 32 × 32 × 4 | 16 × 16 × 8 | 88 × 8 × 32  |\\n|  |Z | - | 2048 | 8192 | 16384 | 16384 | 16384  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear | linear | linear  |\\n|  Model Size | 270M | 265M | 274M | 258M | 260M | 258M  |\\n|  Channels | 192 | 192 | 224 | 256 | 256 | 256  |\\n|  Depth | 2 | 2 | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,1,2,2,4,4 | 1,2,2,4,4 | 1,2,3,4 | 1,2,4 | 1,2,4 | 1,2,4  |\\n|  Attention resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 16, 8, 4 | 8, 4, 2  |\\n|  Head Channels | 32 | 32 | 32 | 32 | 32 | 32  |\\n|  Batch Size | 9 | 11 | 48 | 96 | 128 | 128  |\\n|  Iterations* | 500k | 500k | 500k | 500k | 500k | 500k  |\\n|  Learning Rate | 9e-5 | 1.1e-4 | 9.6e-5 | 9.6e-5 | 1.3e-4 | 1.3e-4  |\\n\\nTable 14. Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in Fig. 7. All models trained on a single NVIDIA A100. *: All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the provided FID scores.\\n\\n|  Task | Text-to-Image | Layout-to-Image |   | Class-Label-to-Image | Super Resolution | Inpainting | Semantic-Map-to-Image  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Dataset | LAION | OpenImages | COCO | ImageNet | ImageNet | Places | Landscapes  |\\n|  f | 8 | 4 | 8 | 4 | 4 | 4 | 8  |\\n|  z-shape | 32 × 32 × 4 | 64 × 64 × 3 | 32 × 32 × 4 | 64 × 64 × 3 | 64 × 64 × 3 | 64 × 64 × 3 | 32 × 32 × 4  |\\n|  |Z | - | 8192 | 16384 | 8192 | 8192 | 8192 | 16384  |\\n|  Diffusion steps | 1000 | 1000 | 1000 | 1000 | 1000 | 1000 | 1000  |\\n|  Noise Schedule | linear | linear | linear | linear | linear | linear | linear  |\\n|  Model Size | 1.45B | 306M | 345M | 395M | 169M | 215M | 215M  |\\n|  Channels | 320 | 128 | 192 | 192 | 160 | 128 | 128  |\\n|  Depth | 2 | 2 | 2 | 2 | 2 | 2 | 2  |\\n|  Channel Multiplier | 1,2,4,4 | 1,2,3,4 | 1,2,4 | 1,2,3,5 | 1,2,2,4 | 1,4,8 | 1,4,8  |\\n|  Number of Heads | 8 | 1 | 1 | 1 | 1 | 1 | 1  |\\n|  Dropout | - | - | 0.1 | - | - | - | -  |\\n|  Batch Size | 680 | 24 | 48 | 1200 | 64 | 128 | 48  |\\n|  Iterations | 390K | 4.4M | 170K | 178K | 860K | 360K | 360K  |\\n|  Learning Rate | 1.0e-4 | 4.8e-5 | 4.8e-5 | 1.0e-4 | 6.4e-5 | 1.0e-6 | 4.8e-5  |\\n|  Conditioning | CA | CA | CA | CA | concat | concat | concat  |\\n|  (C)A-resolutions | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | 32, 16, 8 | - | - | -  |\\n|  Embedding Dimension | 1280 | 512 | 512 | 512 | - | - | -  |\\n|  Transformer Depth | 1 | 3 | 2 | 1 | - | - | -  |\\n\\nTable 15. Hyperparameters for the conditional LDMs from Sec. 4. All models trained on a single NVIDIA A100 except for the inpainting model which was trained on eight V100.\\n\\n$\\\\zeta \\\\gets \\\\mathrm{TokEmb}(y) + \\\\mathrm{PosEmb}(y)$  (18)\\n\\nfor  $i = 1,\\\\dots ,N$  ..\\n\\n$\\\\zeta_{1}\\\\gets \\\\mathrm{LayerNorm}(\\\\zeta)$  (19)\\n\\n$\\\\zeta_{2}\\\\gets \\\\mathrm{MultiHeadSelfAttention}(\\\\zeta_{1}) + \\\\zeta$  (20)\\n\\n$\\\\zeta_{3}\\\\gets \\\\mathrm{LayerNorm}(\\\\zeta_{2})$  (21)\\n\\n$\\\\zeta \\\\gets \\\\mathrm{MLP}(\\\\zeta_3) + \\\\zeta_2$  (22)\\n\\n$\\\\zeta \\\\gets \\\\mathrm{LayerNorm}(\\\\zeta)$  (23)\\n\\n(24)\\n\\nWith  $\\\\zeta$  available, the conditioning is mapped into the UNet via the cross-attention mechanism as depicted in Fig. 3. We modify the \"ablated UNet\" [15] architecture and replace the self-attention layer with a shallow (unmasked) transformer consisting of  $T$  blocks with alternating layers of (i) self-attention, (ii) a position-wise MLP and (iii) a cross-attention layer;\\n\\nsee Tab. 16. Note that without (ii) and (iii), this architecture is equivalent to the \"ablated UNet\".\\n\\nWhile it would be possible to increase the representational power of  $\\\\tau_{\\\\theta}$  by additionally conditioning on the time step  $t$ , we do not pursue this choice as it reduces the speed of inference. We leave a more detailed analysis of this modification to future work.\\n\\nFor the text-to-image model, we rely on a publicly available $^3$  tokenizer [99]. The layout-to-image model discretizes the spatial locations of the bounding boxes and encodes each box as a  $(l,b,c)$ -tuple, where  $l$  denotes the (discrete) top-left and  $b$  the bottom-right position. Class information is contained in  $c$ .\\n\\nSee Tab. 17 for the hyperparameters of  $\\\\tau_{\\\\theta}$  and Tab. 13 for those of the UNet for both of the above tasks.\\n\\nNote that the class-conditional model as described in Sec. 4.1 is also implemented via cross-attention, where  $\\\\tau_{\\\\theta}$  is a single learnable embedding layer with a dimensionality of 512, mapping classes  $y$  to  $\\\\zeta \\\\in \\\\mathbb{R}^{1\\\\times 512}$ .\\n\\n|  input | Rh×w×c  |\\n| --- | --- |\\n|  LayerNorm | Rh×w×c  |\\n|  Conv1x1 | Rh×w×d·nh  |\\n|  Reshape | Rh·w×d·nh  |\\n|  SelfAttention | Rh·w×d·nh  |\\n|  MLP | Rh·w×d·nh  |\\n|  CrossAttention | Rh·w×d·nh  |\\n|  Reshape | Rh×w×d·nh  |\\n|  Conv1x1 | Rh×w×c  |\\n\\nTable 16. Architecture of a transformer block as described in Sec. E.2.1, replacing the self-attention layer of the standard \"ablated UNet\" architecture [15]. Here,  $n_h$  denotes the number of attention heads and  $d$  the dimensionality per head.\\n\\n|   | Text-to-Image | Layout-to-Image  |\\n| --- | --- | --- |\\n|  seq-length | 77 | 92  |\\n|  depth N | 32 | 16  |\\n|  dim | 1280 | 512  |\\n\\nTable 17. Hyperparameters for the experiments with transformer encoders in Sec. 4.3.\\n\\n# E.2.2 Inpainting\\n\\nFor our experiments on image-inpainting in Sec. 4.5, we used the code of [88] to generate synthetic masks. We use a fixed set of  $2\\\\mathrm{k}$  validation and  $30\\\\mathrm{k}$  testing samples from Places [108]. During training, we use random crops of size  $256 \\\\times 256$  and evaluate on crops of size  $512 \\\\times 512$ . This follows the training and testing protocol in [88] and reproduces their reported metrics (see  $\\\\dagger$  in Tab. 7). We include additional qualitative results of LDM-4, w/attn in Fig. 21 and of LDM-4, w/o attn, big, w/ft in Fig. 22.\\n\\n# E.3. Evaluation Details\\n\\nThis section provides additional details on evaluation for the experiments shown in Sec. 4.\\n\\n# E.3.1 Quantitative Results in Unconditional and Class-Conditional Image Synthesis\\n\\nWe follow common practice and estimate the statistics for calculating the FID-, Precision- and Recall-scores [29,50] shown in Tab. 1 and 10 based on 50k samples from our models and the entire training set of each of the shown datasets. For calculating FID scores we use the torch-fidelity package [60]. However, since different data processing pipelines might lead to different results [64], we also evaluate our models with the script provided by Dhariwal and Nichol [15]. We find that results\\n\\nmainly coincide, except for the ImageNet and LSUN-Bedrooms datasets, where we notice slightly varying scores of 7.76 (torch-fidelity) vs. 7.77 (Nichol and Dhariwal) and 2.95 vs 3.0. For the future we emphasize the importance of a unified procedure for sample quality assessment. Precision and Recall are also computed by using the script provided by Nichol and Dhariwal.\\n\\n#### E.3.2 Text-to-Image Synthesis\\n\\nFollowing the evaluation protocol of *[66]* we compute FID and Inception Score for the Text-to-Image models from Tab. 2 by comparing generated samples with 30000 samples from the validation set of the MS-COCO dataset *[51]*. FID and Inception Scores are computed with torch-fidelity.\\n\\n#### E.3.3 Layout-to-Image Synthesis\\n\\nFor assessing the sample quality of our Layout-to-Image models from Tab. 9 on the COCO dataset, we follow common practice *[87, 89, 37]* and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split. To obtain better comparability, we use the exact same samples as in *[37]*. For the OpenImages dataset we similarly follow their protocol and use 2048 center-cropped test images from the validation set.\\n\\n#### E.3.4 Super Resolution\\n\\nWe evaluate the super-resolution models on ImageNet following the pipeline suggested in *[72]*, *i.e*. images with a shorter size less than $256$ px are removed (both for training and evaluation). On ImageNet, the low-resolution images are produced using bicubic interpolation with anti-aliasing. FIDs are evaluated using torch-fidelity *[60]*, and we produce samples on the validation split. For FID scores, we additionally compare to reference features computed on the train split, see Tab. 5 and Tab. 11.\\n\\n#### E.3.5 Efficiency Analysis\\n\\nFor efficiency reasons we compute the sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples. Therefore, the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the learning rates slightly vary between different runs *cf*. Tab. 13 and 14.\\n\\n#### E.3.6 User Study\\n\\nFor the results of the user study presented in Tab. 4 we followed the protocoll of *[72]* and and use the 2-alternative force-choice paradigm to assess human preference scores for two distinct tasks. In Task-1 subjects were shown a low resolution/masked image between the corresponding ground truth high resolution/unmasked version and a synthesized image, which was generated by using the middle image as conditioning. For SuperResolution subjects were asked: *’Which of the two images is a better high quality version of the low resolution image in the middle?’*. For Inpainting we asked *’Which of the two images contains more realistic inpainted regions of the image in the middle?’*. In Task-2, humans were similarly shown the low-res/masked version and asked for preference between two corresponding images generated by the two competing methods. As in *[72]* humans viewed the images for 3 seconds before responding.\\n\\n# F. Computational Requirements\\n\\n|  Method | Generator Compute | Classifier Compute | Overall Compute | Inference Throughput* | Nparams | FID↓ | IS↑ | Precision↑ | Recall↑  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  LSUN Churches 2562  |   |   |   |   |   |   |   |   |   |\\n|  StyleGAN2 [42]† | 64 | - | 64 | - | 59M | 3.86 | - | - | -  |\\n|  LDM-8 (ours, 100 steps, 410K) | 18 | - | 18 | 6.80 | 256M | 4.02 | - | 0.64 | 0.52  |\\n|  LSUN Bedrooms 2563  |   |   |   |   |   |   |   |   |   |\\n|  ADM [15]† (1000 steps) | 232 | - | 232 | 0.03 | 552M | 1.9 | - | 0.66 | 0.51  |\\n|  LDM-4 (ours, 200 steps, 1.9M) | 60 | - | 55 | 1.07 | 274M | 2.95 | - | 0.66 | 0.48  |\\n|  CelebA-HQ 2562  |   |   |   |   |   |   |   |   |   |\\n|  LDM-4 (ours, 500 steps, 410K) | 14.4 | - | 14.4 | 0.43 | 274M | 5.11 | - | 0.72 | 0.49  |\\n|  FFHQ 2562  |   |   |   |   |   |   |   |   |   |\\n|  StyleGAN2 [42] | 32.13‡ | - | 32.13‡ | - | 59M | 3.8 | - | - | -  |\\n|  LDM-4 (ours, 200 steps, 635K) | 26 | - | 26 | 1.07 | 274M | 4.98 | - | 0.73 | 0.50  |\\n|  ImageNet 2562  |   |   |   |   |   |   |   |   |   |\\n|  VQGAN-f-4 (ours, first stage) | 29 | - | 29 | - | 55M | 0.58†† | - | - | -  |\\n|  VQGAN-f-8 (ours, first stage) | 66 | - | 66 | - | 68M | 1.14†† | - | - | -  |\\n|  BigGAN-deep [3]† | 128-256 |  | 128-256 | - | 340M | 6.95 | 203.6±1.0 | 0.87 | 0.28  |\\n|  ADM [15] (250 steps)† | 916 | - | 916 | 0.12 | 554M | 10.94 | 100.98 | 0.69 | 0.63  |\\n|  ADM-G [15] (25 steps)† | 916 | 46 | 962 | 0.7 | 608M | 5.58 | - | 0.81 | 0.49  |\\n|  ADM-G [15] (250 steps)† | 916 | 46 | 962 | 0.07 | 608M | 4.59 | 186.7 | 0.82 | 0.52  |\\n|  ADM-G.ADM-G [15] (250 steps)† | 329 | 30 | 349 | n/a | n/a | 3.85 | 221.72 | 0.84 | 0.53  |\\n|  LDM-8-G (ours, 100, 2.9M) | 79 | 12 | 91 | 1.93 | 506M | 8.11 | 190.4±2.0 | 0.83 | 0.36  |\\n|  LDM-8 (ours, 200 ddim steps 2.9M, batch size 64) | 79 | - | 79 | 1.9 | 395M | 17.41 | 72.92 | 0.65 | 0.62  |\\n|  LDM-4 (ours, 250 ddim steps 178K, batch size 1200) | 271 | - | 271 | 0.7 | 400M | 10.56 | 103.49±1.10 | 0.71 | 0.62  |\\n|  LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classifier-free guidance [32] scale 1.25) | 271 | - | 271 | 0.4 | 400M | 3.95 | 178.22±1.41 | 0.81 | 0.55  |\\n|  LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classifier-free guidance [32] scale 1.5) | 271 | - | 271 | 0.4 | 400M | 3.60 | 247.67±3.50 | 0.87 | 0.48  |\\n\\nTable 18. Comparing compute requirements during training and inference throughput with state-of-the-art generative models. Compute during training in V100-days, numbers of competing methods taken from [15] unless stated differently;*: Throughput measured in samples/sec on a single NVIDIA A100;†: Numbers taken from [15];‡: Assumed to be trained on 25M train examples; ††: R-FID vs. ImageNet validation set\\n\\nIn Tab 18 we provide a more detailed analysis on our used compute ressources and compare our best performing models on the CelebA-HQ, FFHQ, LSUN and ImageNet datasets with the recent state of the art models by using their provided numbers, cf. [15]. As they report their used compute in V100 days and we train all our models on a single NVIDIA A100 GPU, we convert the A100 days to V100 days by assuming a  $\\\\times 2.2$  speedup of A100 vs V100 [74] $^4$ . To assess sample quality, we additionally report FID scores on the reported datasets. We closely reach the performance of state of the art methods as StyleGAN2 [42] and ADM [15] while significantly reducing the required compute resources.\\n\\nG Details on Autoencoder Models\\n\\nWe train all our autoencoder models in an adversarial manner following *[23]*, such that a patch-based discriminator $D_{\\\\psi}$ is optimized to differentiate original images from reconstructions $\\\\mathcal{D}(\\\\mathcal{E}(x))$. To avoid arbitrarily scaled latent spaces, we regularize the latent $z$ to be zero centered and obtain small variance by introducing an regularizing loss term $L_{reg}$.\\nWe investigate two different regularization methods: (i) a low-weighted Kullback-Leibler-term between $q_{\\\\mathcal{E}}(z|x)=\\\\mathcal{N}(z;\\\\mathcal{E}_{\\\\mu},\\\\mathcal{E}_{\\\\sigma^{2}})$ and a standard normal distribution $\\\\mathcal{N}(z;0,1)$ as in a standard variational autoencoder *[69, 46]*, and, (ii) regularizing the latent space with a vector quantization layer by learning a codebook of $|\\\\mathcal{Z}|$ different exemplars *[96]*.\\nTo obtain high-fidelity reconstructions we only use a very small regularization for both scenarios, *i.e*. we either weight the $\\\\mathbb{KL}$ term by a factor $\\\\sim 10^{-6}$ or choose a high codebook dimensionality $|\\\\mathcal{Z}|$.\\n\\nThe full objective to train the autoencoding model $(\\\\mathcal{E},\\\\mathcal{D})$ reads:\\n\\n$L_{\\\\text{Autoencoder}}=\\\\min_{\\\\mathcal{E},\\\\mathcal{D}}\\\\max_{\\\\psi}\\\\Big{(}L_{rec}(x,\\\\mathcal{D}(\\\\mathcal{E}(x)))-L_{adv}(\\\\mathcal{D}(\\\\mathcal{E}(x)))+\\\\log D_{\\\\psi}(x)+L_{reg}(x;\\\\mathcal{E},\\\\mathcal{D})\\\\Big{)}$ (25)\\n\\n##### DM Training in Latent Space\\n\\nNote that for training diffusion models on the learned latent space, we again distinguish two cases when learning $p(z)$ or $p(z|y)$ (Sec. 4.3): (i) For a KL-regularized latent space, we sample $z=\\\\mathcal{E}_{\\\\mu}(x)+\\\\mathcal{E}_{\\\\sigma}(x)\\\\cdot\\\\varepsilon=:\\\\mathcal{E}(x)$, where $\\\\varepsilon\\\\sim\\\\mathcal{N}(0,1)$. When rescaling the latent, we estimate the component-wise variance\\n\\n$\\\\hat{\\\\sigma}^{2}=\\\\frac{1}{bchw}\\\\sum_{b,c,h,w}(z^{b,c,h,w}-\\\\hat{\\\\mu})^{2}$\\n\\nfrom the first batch in the data, where $\\\\hat{\\\\mu}=\\\\frac{1}{bchw}\\\\sum_{b,c,h,w}z^{b,c,h,w}$. The output of $\\\\mathcal{E}$ is scaled such that the rescaled latent has unit standard deviation, *i.e*. $z\\\\leftarrow\\\\frac{z}{\\\\hat{\\\\sigma}}=\\\\frac{\\\\mathcal{E}(x)}{\\\\hat{\\\\sigma}}$. (ii) For a VQ-regularized latent space, we extract $z$ *before* the quantization layer and absorb the quantization operation into the decoder, *i.e*. it can be interpreted as the first layer of $\\\\mathcal{D}$.\\n\\n## Appendix H Additional Qualitative Results\\n\\nFinally, we provide additional qualitative results for our landscapes model (Fig. 12, 23, 24 and 25), our class-conditional ImageNet model (Fig. 26 - 27) and our unconditional models for the CelebA-HQ, FFHQ and LSUN datasets (Fig. 28 - 31). Similar as for the inpainting model in Sec. 4.5 we also fine-tuned the semantic landscapes model from Sec. 4.3.2 directly on $512^{2}$ images and depict qualitative results in Fig. 12 and Fig. 23. For our those models trained on comparably small datasets, we additionally show nearest neighbors in VGG *[79]* feature space for samples from our models in Fig. 32 - 34.\\n\\n![img-21.jpeg](img-21.jpeg)\\nbicubic\\n\\n![img-22.jpeg](img-22.jpeg)\\nLDM-BSR\\n\\n![img-23.jpeg](img-23.jpeg)\\n\\n![img-24.jpeg](img-24.jpeg)\\n\\n![img-25.jpeg](img-25.jpeg)\\nFigure 19. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from the LSUN-Cows dataset to  $1024^{2}$  resolution.\\n\\n![img-26.jpeg](img-26.jpeg)\\n\\n![img-27.jpeg](img-27.jpeg)\\nFigure 20. Qualitative superresolution comparison of two random samples between LDM-SR and baseline-diffusionmodel in Pixelspace. Evaluated on imagenet validation-set after same amount of training steps.\\n\\n![img-28.jpeg](img-28.jpeg)\\nFigure 21. Qualitative results on image inpainting. In contrast to [88], our generative approach enables generation of multiple diverse samples for a given input.\\n\\n![img-29.jpeg](img-29.jpeg)\\nFigure 22. More qualitative results on object removal as in Fig. 11.\\n\\n![img-30.jpeg](img-30.jpeg)\\nSemantic Synthesis on Flickr-Landscapes [23] (512 $^2$  finetuning)\\n\\n![img-31.jpeg](img-31.jpeg)\\n\\n![img-32.jpeg](img-32.jpeg)\\nFigure 23. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on  $512^{2}$  images.\\n\\n![img-33.jpeg](img-33.jpeg)\\nFigure 24. A LDM trained on  $256^{2}$  resolution can generalize to larger resolution for spatially conditioned tasks such as semantic synthesis of landscape images. See Sec. 4.3.2.\\n\\nSemantic Synthesis on Flickr-Landscapes [23]\\n\\n![img-34.jpeg](img-34.jpeg)\\n\\n![img-35.jpeg](img-35.jpeg)\\n\\n![img-36.jpeg](img-36.jpeg)\\n\\n![img-37.jpeg](img-37.jpeg)\\nFigure 25. When provided a semantic map as conditioning, our LDMs generalize to substantially larger resolutions than those seen during training. Although this model was trained on inputs of size  $256^2$  it can be used to create high-resolution samples as the ones shown here, which are of resolution  $1024 \\\\times 384$ .\\n\\nRandom class conditional samples on the ImageNet dataset\\n\\n![img-38.jpeg](img-38.jpeg)\\nFigure 26. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance [32] scale  $s = 5.0$  and 200 DDIM steps with  $\\\\eta = 1.0$ .\\n\\n![img-39.jpeg](img-39.jpeg)\\nRandom class conditional samples on the ImageNet dataset\\nFigure 27. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance [32] scale  $s = 3.0$  and 200 DDIM steps with  $\\\\eta = 1.0$ .\\n\\n![img-40.jpeg](img-40.jpeg)\\nRandom samples on the CelebA-HQ dataset\\nFigure 28. Random samples of our best performing model LDM-4 on the CelebA-HQ dataset. Sampled with 500 DDIM steps and  $\\\\eta = 0$  (FID = 5.15).\\n\\n![img-41.jpeg](img-41.jpeg)\\nRandom samples on the FFHQ dataset\\nFigure 29. Random samples of our best performing model LDM-4 on the FFHQ dataset. Sampled with 200 DDIM steps and  $\\\\eta = 1$  (FID  $= 4.98$ ).\\n\\n![img-42.jpeg](img-42.jpeg)\\nRandom samples on the LSUN-Churches dataset\\nFigure 30. Random samples of our best performing model LDM-8 on the LSUN-Churches dataset. Sampled with 200 DDIM steps and  $\\\\eta = 0$  (FID = 4.48).\\n\\n![img-43.jpeg](img-43.jpeg)\\nRandom samples on the LSUN-Bedrooms dataset\\nFigure 31. Random samples of our best performing model LDM-4 on the LSUN-Bedrooms dataset. Sampled with 200 DDIM steps and  $\\\\eta = 1$  (FID = 2.95).\\n\\n# Nearest Neighbors on the CelebA-HQ dataset\\n\\n![img-44.jpeg](img-44.jpeg)\\nFigure 32. Nearest neighbors of our best CelebA-HQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors.\\n\\n![img-45.jpeg](img-45.jpeg)\\nNearest Neighbors on the FFHQ dataset\\nFigure 33. Nearest neighbors of our best FFHQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors.\\n\\n![img-46.jpeg](img-46.jpeg)\\nNearest Neighbors on the LSUN-Churches dataset\\nFigure 34. Nearest neighbors of our best LSUN-Churches model, computed in the feature space of a VGG-16 [79]. The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors.'), 0.04446544805544639), (Document(id='c60d566af74b:0', metadata={'end_line': 514, 'text': '# Denoising Diffusion Probabilistic Models\\n\\nJonathan Ho\\n\\nUC Berkeley\\n\\njonathanho@berkeley.edu\\n\\nAjay Jain\\n\\nUC Berkeley\\n\\najayj@berkeley.edu\\n\\nPieter Abbeel\\n\\nUC Berkeley\\n\\npabbeel@cs.berkeley.edu\\n\\n# Abstract\\n\\nWe present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On  $256 \\\\times 256$  LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.\\n\\n# 1 Introduction\\n\\nDeep generative models of all kinds have recently exhibited high quality samples in a wide variety of data modalities. Generative adversarial networks (GANs), autoregressive models, flows, and variational autoencoders (VAEs) have synthesized striking image and audio samples [14, 27, 3, 58, 38, 25, 10, 32, 44, 57, 26, 33, 45], and there have been remarkable advances in energy-based modeling and score matching that have produced images comparable to those of GANs [11, 55].\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Generated samples on CelebA-HQ  $256 \\\\times 256$  (left) and unconditional CIFAR10 (right)\\n\\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2: The directed graphical model considered in this work.\\n\\nThis paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model (which we will call a \"diffusion model\" for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed. When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.\\n\\nDiffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples. We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models (Section 4). In addition, we show that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling (Section 3.2) [55, 61]. We obtained our best sample quality results using this parameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.\\n\\nDespite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models (our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching [11, 55]). We find that the majority of our models\\' lossless codelengths are consumed to describe imperceptible image details (Section 4.3). We present a more refined analysis of this phenomenon in the language of lossy compression, and we show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.\\n\\n# 2 Background\\n\\nDiffusion models [53] are latent variable models of the form  $p_{\\\\theta}(\\\\mathbf{x}_0) \\\\coloneqq \\\\int p_{\\\\theta}(\\\\mathbf{x}_{0:T}) d\\\\mathbf{x}_{1:T}$ , where  $\\\\mathbf{x}_1, \\\\ldots, \\\\mathbf{x}_T$  are latents of the same dimensionality as the data  $\\\\mathbf{x}_0 \\\\sim q(\\\\mathbf{x}_0)$ . The joint distribution  $p_{\\\\theta}(\\\\mathbf{x}_{0:T})$  is called the reverse process, and it is defined as a Markov chain with learned Gaussian transitions starting at  $p(\\\\mathbf{x}_T) = \\\\mathcal{N}(\\\\mathbf{x}_T; \\\\mathbf{0}, \\\\mathbf{I})$ :\\n\\n$$\\np _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0: T}\\\\right) := p \\\\left(\\\\mathbf {x} _ {T}\\\\right) \\\\prod_ {t = 1} ^ {T} p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right), \\\\quad p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right) := \\\\mathcal {N} \\\\left(\\\\mathbf {x} _ {t - 1}; \\\\boldsymbol {\\\\mu} _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t}, t\\\\right), \\\\boldsymbol {\\\\Sigma} _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t}, t\\\\right)\\\\right) \\\\tag {1}\\n$$\\n\\nWhat distinguishes diffusion models from other types of latent variable models is that the approximate posterior  $q(\\\\mathbf{x}_{1:T}|\\\\mathbf{x}_0)$ , called the forward process or diffusion process, is fixed to a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule  $\\\\beta_1,\\\\ldots ,\\\\beta_T$ :\\n\\n$$\\nq \\\\left(\\\\mathbf {x} _ {1: T} \\\\mid \\\\mathbf {x} _ {0}\\\\right) := \\\\prod_ {t = 1} ^ {T} q \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {t - 1}\\\\right), \\\\quad q \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {t - 1}\\\\right) := \\\\mathcal {N} \\\\left(\\\\mathbf {x} _ {t}; \\\\sqrt {1 - \\\\beta_ {t}} \\\\mathbf {x} _ {t - 1}, \\\\beta_ {t} \\\\mathbf {I}\\\\right) \\\\tag {2}\\n$$\\n\\nTraining is performed by optimizing the usual variational bound on negative log likelihood:\\n\\n$$\\n\\\\mathbb {E} \\\\left[ - \\\\log p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0}\\\\right) \\\\right] \\\\leq \\\\mathbb {E} _ {q} \\\\left[ - \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0 : T}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {1 : T} \\\\mid \\\\mathbf {x} _ {0}\\\\right)} \\\\right] = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log p \\\\left(\\\\mathbf {x} _ {T}\\\\right) - \\\\sum_ {t \\\\geq 1} \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {t - 1}\\\\right)} \\\\right] =: L \\\\tag {3}\\n$$\\n\\nThe forward process variances  $\\\\beta_{t}$  can be learned by reparameterization [33] or held constant as hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in  $p_{\\\\theta}(\\\\mathbf{x}_{t - 1}|\\\\mathbf{x}_t)$ , because both processes have the same functional form when  $\\\\beta_{t}$  are small [53]. A notable property of the forward process is that it admits sampling  $\\\\mathbf{x}_t$  at an arbitrary timestep  $t$  in closed form: using the notation  $\\\\alpha_{t}\\\\coloneqq 1 - \\\\beta_{t}$  and  $\\\\bar{\\\\alpha}_{t}\\\\coloneqq \\\\prod_{s = 1}^{t}\\\\alpha_{s}$ , we have\\n\\n$$\\nq \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {0}\\\\right) = \\\\mathcal {N} \\\\left(\\\\mathbf {x} _ {t}; \\\\sqrt {\\\\bar {\\\\alpha} _ {t}} \\\\mathbf {x} _ {0}, (1 - \\\\bar {\\\\alpha} _ {t}) \\\\mathbf {I}\\\\right) \\\\tag {4}\\n$$\\n\\nEfficient training is therefore possible by optimizing random terms of $L$ with stochastic gradient descent. Further improvements come from variance reduction by rewriting $L$ (3) as:\\n\\n$\\\\mathbb{E}_{q}\\\\bigg{[}\\\\underbrace{D_{\\\\text{KL}}(q(\\\\mathbf{x}_{T}|\\\\mathbf{x}_{0})\\\\parallel p(\\\\mathbf{x}_{T}))}_{L_{T}}+\\\\sum_{t>1}\\\\underbrace{D_{\\\\text{KL}}(q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t},\\\\mathbf{x}_{0})\\\\parallel p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t}))}_{L_{t-1}}\\\\underbrace{-\\\\log p_{\\\\theta}(\\\\mathbf{x}_{0}|\\\\mathbf{x}_{1})}_{L_{0}}\\\\bigg{]}$ (5)\\n\\n(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL divergence to directly compare $p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})$ against forward process posteriors, which are tractable when conditioned on $\\\\mathbf{x}_{0}$:\\n\\n$q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t},\\\\mathbf{x}_{0})=\\\\mathcal{N}(\\\\mathbf{x}_{t-1};\\\\tilde{\\\\bm{\\\\mu}}_{t}(\\\\mathbf{x}_{t},\\\\mathbf{x}_{0}),\\\\tilde{\\\\beta}_{t}\\\\mathbf{I}),$ (6)\\n$\\\\text{where}\\\\quad\\\\tilde{\\\\bm{\\\\mu}}_{t}(\\\\mathbf{x}_{t},\\\\mathbf{x}_{0})\\\\coloneqq\\\\frac{\\\\sqrt{\\\\bar{\\\\alpha}_{t-1}}\\\\beta_{t}}{1-\\\\bar{\\\\alpha}_{t}}\\\\mathbf{x}_{0}+\\\\frac{\\\\sqrt{\\\\alpha_{t}}(1-\\\\bar{\\\\alpha}_{t-1})}{1-\\\\bar{\\\\alpha}_{t}}\\\\mathbf{x}_{t}\\\\quad\\\\text{and}\\\\quad\\\\tilde{\\\\beta}_{t}\\\\coloneqq\\\\frac{1-\\\\bar{\\\\alpha}_{t-1}}{1-\\\\bar{\\\\alpha}_{t}}\\\\beta_{t}$ (7)\\n\\nConsequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be calculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance Monte Carlo estimates.\\n\\n## 3 Diffusion models and denoising autoencoders\\n\\nDiffusion models might appear to be a restricted class of latent variable models, but they allow a large number of degrees of freedom in implementation. One must choose the variances $\\\\beta_{t}$ of the forward process and the model architecture and Gaussian distribution parameterization of the reverse process. To guide our choices, we establish a new explicit connection between diffusion models and denoising score matching (Section 3.2) that leads to a simplified, weighted variational bound objective for diffusion models (Section 3.4). Ultimately, our model design is justified by simplicity and empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).\\n\\n### 3.1 Forward process and $L_{T}$\\n\\nWe ignore the fact that the forward process variances $\\\\beta_{t}$ are learnable by reparameterization and instead fix them to constants (see Section 4 for details). Thus, in our implementation, the approximate posterior $q$ has no learnable parameters, so $L_{T}$ is a constant during training and can be ignored.\\n\\n### 3.2 Reverse process and $L_{1:T-1}$\\n\\nNow we discuss our choices in $p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})=\\\\mathcal{N}(\\\\mathbf{x}_{t-1};\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t},t),\\\\bm{\\\\Sigma}_{\\\\theta}(\\\\mathbf{x}_{t},t))$ for $1<t\\\\leq T$. First, we set $\\\\bm{\\\\Sigma}_{\\\\theta}(\\\\mathbf{x}_{t},t)=\\\\sigma_{t}^{2}\\\\mathbf{I}$ to untrained time dependent constants. Experimentally, both $\\\\sigma_{t}^{2}=\\\\beta_{t}$ and $\\\\sigma_{t}^{2}=\\\\tilde{\\\\beta}_{t}=\\\\frac{1-\\\\bar{\\\\alpha}_{t-1}}{1-\\\\bar{\\\\alpha}_{t}}\\\\beta_{t}$ had similar results. The first choice is optimal for $\\\\mathbf{x}_{0}\\\\sim\\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$, and the second is optimal for $\\\\mathbf{x}_{0}$ deterministically set to one point. These are the two extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance *[53]*.\\n\\nSecond, to represent the mean $\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t},t)$, we propose a specific parameterization motivated by the following analysis of $L_{t}$. With $p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})=\\\\mathcal{N}(\\\\mathbf{x}_{t-1};\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t},t),\\\\sigma_{t}^{2}\\\\mathbf{I})$, we can write:\\n\\n$L_{t-1}=\\\\mathbb{E}_{q}\\\\bigg{[}\\\\frac{1}{2\\\\sigma_{t}^{2}}\\\\|\\\\tilde{\\\\bm{\\\\mu}}_{t}(\\\\mathbf{x}_{t},\\\\mathbf{x}_{0})-\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t},t)\\\\|^{2}\\\\bigg{]}+C$ (8)\\n\\nwhere $C$ is a constant that does not depend on $\\\\theta$. So, we see that the most straightforward parameterization of $\\\\bm{\\\\mu}_{\\\\theta}$ is a model that predicts $\\\\tilde{\\\\bm{\\\\mu}}_{t}$, the forward process posterior mean. However, we can expand Eq. (8) further by reparameterizing Eq. (4) as $\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon})=\\\\sqrt{\\\\bar{\\\\alpha}_{t}}\\\\mathbf{x}_{0}+\\\\sqrt{1-\\\\bar{\\\\alpha}_{t}}\\\\bm{\\\\epsilon}$ for $\\\\bm{\\\\epsilon}\\\\sim\\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$ and applying the forward process posterior formula (7):\\n\\n$L_{t-1}-C$ $=\\\\mathbb{E}_{\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon}}\\\\Bigg{[}\\\\frac{1}{2\\\\sigma_{t}^{2}}\\\\left\\\\|\\\\tilde{\\\\bm{\\\\mu}}_{t}\\\\bigg{(}\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon}),\\\\frac{1}{\\\\sqrt{\\\\bar{\\\\alpha}_{t}}}(\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon})-\\\\sqrt{1-\\\\bar{\\\\alpha}_{t}}\\\\bm{\\\\epsilon})\\\\bigg{)}-\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon}),t)\\\\right\\\\|^{2}\\\\Bigg{]}$ (9)\\n$=\\\\mathbb{E}_{\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon}}\\\\Bigg{[}\\\\frac{1}{2\\\\sigma_{t}^{2}}\\\\left\\\\|\\\\frac{1}{\\\\sqrt{\\\\alpha_{t}}}\\\\left(\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon})-\\\\frac{\\\\beta_{t}}{\\\\sqrt{1-\\\\bar{\\\\alpha}_{t}}}\\\\bm{\\\\epsilon}\\\\right)-\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon}),t)\\\\right\\\\|^{2}\\\\Bigg{]}$ (10)\\n\\n###\\n\\n|  Algorithm 1 Training | Algorithm 2 Sampling  |\\n| --- | --- |\\n|  1: repeat | 1: xT ~ N(0, I)  |\\n|  2: x0 ~ q(x0) | 2: for t = T, ..., 1 do  |\\n|  3: t ~ Uniform({1, ..., T}) | 3: z ~ N(0, I) if t > 1, else z = 0  |\\n|  4: ε ~ N(0, I) | 4: xt-1 = 1/√αt (xt - 1-αt/√1-αt εθ(xt, t)) + σtz  |\\n|  5: Take gradient descent step on ∇θ ||ε - εθ(√αt x0 + √1-αt ε, t)||2 | 5: end for  |\\n|  6: until converged | 6: return x0  |\\n\\nEquation (10) reveals that  $\\\\pmb{\\\\mu}_{\\\\theta}$  must predict  $\\\\frac{1}{\\\\sqrt{\\\\alpha_t}}\\\\left(\\\\mathbf{x}_t - \\\\frac{\\\\beta_t}{\\\\sqrt{1 - \\\\bar{\\\\alpha}_t}}\\\\pmb{\\\\epsilon}\\\\right)$  given  $\\\\mathbf{x}_t$ . Since  $\\\\mathbf{x}_t$  is available as input to the model, we may choose the parameterization\\n\\n$$\\n\\\\boldsymbol {\\\\mu} _ {\\\\theta} (\\\\mathbf {x} _ {t}, t) = \\\\tilde {\\\\boldsymbol {\\\\mu}} _ {t} \\\\left(\\\\mathbf {x} _ {t}, \\\\frac {1}{\\\\sqrt {\\\\bar {\\\\alpha} _ {t}}} \\\\left(\\\\mathbf {x} _ {t} - \\\\sqrt {1 - \\\\bar {\\\\alpha} _ {t}} \\\\boldsymbol {\\\\epsilon} _ {\\\\theta} (\\\\mathbf {x} _ {t})\\\\right)\\\\right) = \\\\frac {1}{\\\\sqrt {\\\\alpha_ {t}}} \\\\left(\\\\mathbf {x} _ {t} - \\\\frac {\\\\beta_ {t}}{\\\\sqrt {1 - \\\\bar {\\\\alpha} _ {t}}} \\\\boldsymbol {\\\\epsilon} _ {\\\\theta} (\\\\mathbf {x} _ {t}, t)\\\\right) \\\\tag {11}\\n$$\\n\\nwhere  $\\\\epsilon_{\\\\theta}$  is a function approximator intended to predict  $\\\\epsilon$  from  $\\\\mathbf{x}_t$ . To sample  $\\\\mathbf{x}_{t - 1} \\\\sim p_{\\\\theta}(\\\\mathbf{x}_{t - 1}|\\\\mathbf{x}_t)$  is to compute  $\\\\mathbf{x}_{t - 1} = \\\\frac{1}{\\\\sqrt{\\\\alpha_t}}\\\\left(\\\\mathbf{x}_t - \\\\frac{\\\\beta_t}{\\\\sqrt{1 - \\\\bar{\\\\alpha}_t}}\\\\epsilon_{\\\\theta}(\\\\mathbf{x}_t,t)\\\\right) + \\\\sigma_t\\\\mathbf{z}$ , where  $\\\\mathbf{z} \\\\sim \\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$ . The complete sampling procedure, Algorithm 2, resembles Langevin dynamics with  $\\\\epsilon_{\\\\theta}$  as a learned gradient of the data density. Furthermore, with the parameterization (11), Eq. (10) simplifies to:\\n\\n$$\\n\\\\mathbb {E} _ {\\\\mathbf {x} _ {0}, \\\\epsilon} \\\\left[ \\\\frac {\\\\beta_ {t} ^ {2}}{2 \\\\sigma_ {t} ^ {2} \\\\alpha_ {t} (1 - \\\\bar {\\\\alpha} _ {t})} \\\\left\\\\| \\\\epsilon - \\\\epsilon_ {\\\\theta} \\\\left(\\\\sqrt {\\\\bar {\\\\alpha} _ {t}} \\\\mathbf {x} _ {0} + \\\\sqrt {1 - \\\\bar {\\\\alpha} _ {t}} \\\\epsilon , t\\\\right) \\\\right\\\\| ^ {2} \\\\right] \\\\tag {12}\\n$$\\n\\nwhich resembles denoising score matching over multiple noise scales indexed by  $t$  [55]. As Eq. (12) is equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see that optimizing an objective resembling denoising score matching is equivalent to using variational inference to fit the finite-time marginal of a sampling chain resembling Langevin dynamics.\\n\\nTo summarize, we can train the reverse process mean function approximator  $\\\\mu_{\\\\theta}$  to predict  $\\\\tilde{\\\\mu}_t$ , or by modifying its parameterization, we can train it to predict  $\\\\epsilon$ . (There is also the possibility of predicting  $\\\\mathbf{x}_0$ , but we found this to lead to worse sample quality early in our experiments.) We have shown that the  $\\\\epsilon$ -prediction parameterization both resembles Langevin dynamics and simplifies the diffusion model\\'s variational bound to an objective that resembles denoising score matching. Nonetheless, it is just another parameterization of  $p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t)$ , so we verify its effectiveness in Section 4 in an ablation where we compare predicting  $\\\\epsilon$  against predicting  $\\\\tilde{\\\\mu}_t$ .\\n\\n## 3.3 Data scaling, reverse process decoder, and  $L_{0}$\\n\\nWe assume that image data consists of integers in  $\\\\{0,1,\\\\dots,255\\\\}$  scaled linearly to  $[-1,1]$ . This ensures that the neural network reverse process operates on consistently scaled inputs starting from the standard normal prior  $p(\\\\mathbf{x}_T)$ . To obtain discrete log likelihoods, we set the last term of the reverse process to an independent discrete decoder derived from the Gaussian  $\\\\mathcal{N}(\\\\mathbf{x}_0;\\\\boldsymbol{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_1,1),\\\\sigma_1^2\\\\mathbf{I})$ :\\n\\n$$\\np _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0} \\\\mid \\\\mathbf {x} _ {1}\\\\right) = \\\\prod_ {i = 1} ^ {D} \\\\int_ {\\\\delta_ {-} \\\\left(x _ {0} ^ {i}\\\\right)} ^ {\\\\delta_ {+} \\\\left(x _ {0} ^ {i}\\\\right)} \\\\mathcal {N} \\\\left(x; \\\\mu_ {\\\\theta} ^ {i} \\\\left(\\\\mathbf {x} _ {1}, 1\\\\right), \\\\sigma_ {1} ^ {2}\\\\right) d x \\\\tag {13}\\n$$\\n\\n$$\\n\\\\delta_ {+} (x) = \\\\left\\\\{ \\\\begin{array}{l l} \\\\infty &amp; \\\\text {if} x = 1 \\\\\\\\ x + \\\\frac {1}{2 5 5} &amp; \\\\text {if} x &lt; 1 \\\\end{array} \\\\right. \\\\qquad \\\\delta_ {-} (x) = \\\\left\\\\{ \\\\begin{array}{l l} - \\\\infty &amp; \\\\text {if} x = - 1 \\\\\\\\ x - \\\\frac {1}{2 5 5} &amp; \\\\text {if} x &gt; - 1 \\\\end{array} \\\\right.\\n$$\\n\\nwhere  $D$  is the data dimensionality and the  $i$  superscript indicates extraction of one coordinate. (It would be straightforward to instead incorporate a more powerful decoder like a conditional autoregressive model, but we leave that to future work.) Similar to the discretized continuous distributions used in VAE decoders and autoregressive models [34, 52], our choice here ensures that the variational bound is a lossless codelength of discrete data, without need of adding noise to the data or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of sampling, we display  $\\\\mu_{\\\\theta}(\\\\mathbf{x}_1,1)$  noiselessly.\\n\\n## 3.4 Simplified training objective\\n\\nWith the reverse process and decoder defined above, the variational bound, consisting of terms derived from Eqs. (12) and (13), is clearly differentiable with respect to  $\\\\theta$  and is ready to be employed for\\n\\nTable 1: CIFAR10 results. NLL measured in bits/dim.\\n\\n|  Model | IS | FID | NLL Test (Train)  |\\n| --- | --- | --- | --- |\\n|  Conditional  |   |   |   |\\n|  EBM [11] | 8.30 | 37.9 |   |\\n|  JEM [17] | 8.76 | 38.4 |   |\\n|  BigGAN [3] | 9.22 | 14.73 |   |\\n|  StyleGAN2 + ADA (v1) [29] | 10.06 | 2.67 |   |\\n|  Unconditional  |   |   |   |\\n|  Diffusion (original) [53] |  |  | ≤ 5.40  |\\n|  Gated PixelCNN [59] | 4.60 | 65.93 | 3.03 (2.90)  |\\n|  Sparse Transformer [7] |  |  | 2.80  |\\n|  PixelIQN [43] | 5.29 | 49.46 |   |\\n|  EBM [11] | 6.78 | 38.2 |   |\\n|  NCSNv2 [56] |  | 31.75 |   |\\n|  NCSN [55] | 8.87±0.12 | 25.32 |   |\\n|  SNGAN [39] | 8.22±0.05 | 21.7 |   |\\n|  SNGAN-DDLS [4] | 9.09±0.10 | 15.42 |   |\\n|  StyleGAN2 + ADA (v1) [29] | 9.74 ± 0.05 | 3.26 |   |\\n|  Ours (L, fixed isotropic Σ) | 7.67±0.13 | 13.51 | ≤ 3.70 (3.69)  |\\n|  Ours (Lsimple) | 9.46±0.11 | 3.17 | ≤ 3.75 (3.72)  |\\n\\nTable 2: Unconditional CIFAR10 reverse process parameterization and training objective ablation. Blank entries were unstable to train and generated poor samples with out-of-range scores.\\n\\n|  Objective | IS | FID  |\\n| --- | --- | --- |\\n|  μ prediction (baseline)  |   |   |\\n|  L, learned diagonal Σ | 7.28±0.10 | 23.69  |\\n|  L, fixed isotropic Σ | 8.06±0.09 | 13.22  |\\n|  ||μ - μθ||2 | - | -  |\\n|  ε prediction (ours)  |   |   |\\n|  L, learned diagonal Σ | - | -  |\\n|  L, fixed isotropic Σ | 7.67±0.13 | 13.51  |\\n|  ||ε - εθ||2 (Lsimple) | 9.46±0.11 | 3.17  |\\n\\ntraining. However, we found it beneficial to sample quality (and simpler to implement) to train on the following variant of the variational bound:\\n\\n$$\\nL _ {\\\\text {s i m p l e}} (\\\\theta) := \\\\mathbb {E} _ {t, \\\\mathbf {x} _ {0}, \\\\epsilon} \\\\left[ \\\\left\\\\| \\\\boldsymbol {\\\\epsilon} - \\\\boldsymbol {\\\\epsilon} _ {\\\\theta} \\\\left(\\\\sqrt {\\\\bar {\\\\alpha} _ {t}} \\\\mathbf {x} _ {0} + \\\\sqrt {1 - \\\\bar {\\\\alpha} _ {t}} \\\\boldsymbol {\\\\epsilon}, t\\\\right) \\\\right\\\\| ^ {2} \\\\right] \\\\tag {14}\\n$$\\n\\nwhere  $t$  is uniform between 1 and  $T$ . The  $t = 1$  case corresponds to  $L_{0}$  with the integral in the discrete decoder definition (13) approximated by the Gaussian probability density function times the bin width, ignoring  $\\\\sigma_1^2$  and edge effects. The  $t &gt; 1$  cases correspond to an unweighted version of Eq. (12), analogous to the loss weighting used by the NCSN denoising score matching model [55]. ( $L_{T}$  does not appear because the forward process variances  $\\\\beta_{t}$  are fixed.) Algorithm 1 displays the complete training procedure with this simplified objective.\\n\\nSince our simplified objective (14) discards the weighting in Eq. (12), it is a weighted variational bound that emphasizes different aspects of reconstruction compared to the standard variational bound [18, 22]. In particular, our diffusion process setup in Section 4 causes the simplified objective to down-weight loss terms corresponding to small  $t$ . These terms train the network to denoise data with very small amounts of noise, so it is beneficial to down-weight them so that the network can focus on more difficult denoising tasks at larger  $t$  terms. We will see in our experiments that this reweighting leads to better sample quality.\\n\\n# 4 Experiments\\n\\nWe set  $T = 1000$  for all experiments so that the number of neural network evaluations needed during sampling matches previous work [53, 55]. We set the forward process variances to constants increasing linearly from  $\\\\beta_{1} = 10^{-4}$  to  $\\\\beta_{T} = 0.02$ . These constants were chosen to be small relative to data scaled to  $[-1, 1]$ , ensuring that reverse and forward processes have approximately the same functional form while keeping the signal-to-noise ratio at  $\\\\mathbf{x}_T$  as small as possible ( $L_{T} = D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{T}|\\\\mathbf{x}_{0}) \\\\parallel \\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})) \\\\approx 10^{-5}$  bits per dimension in our experiments).\\n\\nTo represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52, 48] with group normalization throughout [66]. Parameters are shared across time, which is specified to the network using the Transformer sinusoidal position embedding [60]. We use self-attention at the  $16 \\\\times 16$  feature map resolution [63, 60]. Details are in Appendix B.\\n\\n# 4.1 Sample quality\\n\\nTable 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on CIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than most models in the literature, including class conditional models. Our FID score is computed with respect to the training set, as is standard practice; when we compute it with respect to the test set, the score is 5.24, which is still better than many of the training set FID scores in the literature.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 3: LSUN Church samples. FID=7.89\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 4: LSUN Bedroom samples. FID=4.90\\n\\n|  Algorithm 3 Sending x0  |\\n| --- |\\n|  1: Send xT ~ q(xt|x0) using p(xt)  |\\n|  2: for t = T - 1, ..., 2, 1 do  |\\n|  3: Send xt ~ q(xt|xt+1, x0) using pθ(xt|xt+1)  |\\n|  4: end for  |\\n|  5: Send x0 using pθ(x0|x1)  |\\n|  Algorithm 4 Receiving  |\\n| --- |\\n|  1: Receive xT using p(xt)  |\\n|  2: for t = T - 1, ..., 1, 0 do  |\\n|  3: Receive xt using pθ(xt|xt+1)  |\\n|  4: end for  |\\n|  5: return x0  |\\n\\nWe find that training our models on the true variational bound yields better codelengths than training on the simplified objective, as expected, but the latter yields the best sample quality. See Fig. 1 for CIFAR10 and CelebA-HQ  $256 \\\\times 256$  samples, Fig. 3 and Fig. 4 for LSUN  $256 \\\\times 256$  samples [71], and Appendix D for more.\\n\\n# 4.2 Reverse process parameterization and training objective ablation\\n\\nIn Table 2, we show the sample quality effects of reverse process parameterizations and training objectives (Section 3.2). We find that the baseline option of predicting  $\\\\hat{\\\\mu}$  works well only when trained on the true variational bound instead of unweighted mean squared error, a simplified objective akin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized diagonal  $\\\\Sigma_{\\\\theta}(\\\\mathbf{x}_t)$  into the variational bound) leads to unstable training and poorer sample quality compared to fixed variances. Predicting  $\\\\epsilon$ , as we proposed, performs approximately as well as predicting  $\\\\hat{\\\\mu}$  when trained on the variational bound with fixed variances, but much better when trained with our simplified objective.\\n\\n# 4.3 Progressive coding\\n\\nTable 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at most 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based models and indicates that our diffusion model is not overfitting (see Appendix D for nearest neighbor visualizations). Still, while our lossless codelengths are better than the large estimates reported for energy based models and score matching using annealed importance sampling [11], they are not competitive with other types of likelihood-based generative models [7].\\n\\nSince our samples are nonetheless of high quality, we conclude that diffusion models have an inductive bias that makes them excellent lossy compressors. Treating the variational bound terms  $L_{1} + \\\\dots + L_{T}$  as rate and  $L_{0}$  as distortion, our CIFAR10 model with the highest quality samples has a rate of 1.78 bits/dim and a distortion of 1.97 bits/dim, which amounts to a root mean squared error of 0.95 on a scale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.\\n\\nProgressive lossy compression We can probe further into the rate-distortion behavior of our model by introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4, which assume access to a procedure, such as minimal random coding [19, 20], that can transmit a sample  $\\\\mathbf{x} \\\\sim q(\\\\mathbf{x})$  using approximately  $D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}) \\\\parallel p(\\\\mathbf{x}))$  bits on average for any distributions  $p$  and  $q$ , for which only  $p$  is available to the receiver beforehand. When applied to  $\\\\mathbf{x}_0 \\\\sim q(\\\\mathbf{x}_0)$ , Algorithms 3 and 4 transmit  $\\\\mathbf{x}_T, \\\\ldots, \\\\mathbf{x}_0$  in sequence using a total expected codelength equal to Eq. (5). The receiver,\\n\\nat any time $t$, has the partial information $\\\\mathbf{x}_t$ fully available and can progressively estimate:\\n\\n$$\\n\\\\mathbf{x}_0 \\\\approx \\\\hat{\\\\mathbf{x}}_0 = \\\\left(\\\\mathbf{x}_t - \\\\sqrt{1 - \\\\bar{\\\\alpha}_t} \\\\boldsymbol{\\\\epsilon}_{\\\\theta}(\\\\mathbf{x}_t)\\\\right) / \\\\sqrt{\\\\bar{\\\\alpha}_t} \\\\tag{15}\\n$$\\n\\ndue to Eq. (4). (A stochastic reconstruction $\\\\mathbf{x}_0 \\\\sim p_\\\\theta(\\\\mathbf{x}_0|\\\\mathbf{x}_t)$ is also valid, but we do not consider it here because it makes distortion more difficult to evaluate.) Figure 5 shows the resulting rate-distortion plot on the CIFAR10 test set. At each time $t$, the distortion is calculated as the root mean squared error $\\\\sqrt{\\\\|\\\\mathbf{x}_0 - \\\\hat{\\\\mathbf{x}}_0\\\\|^2 / D}$, and the rate is calculated as the cumulative number of bits received so far at time $t$. The distortion decreases steeply in the low-rate region of the rate-distortion plot, indicating that the majority of the bits are indeed allocated to imperceptible distortions.\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared error on a [0, 255] scale. See Table 4 for details.\\n\\n![img-5.jpeg](img-5.jpeg)\\n\\n![img-6.jpeg](img-6.jpeg)\\n\\nProgressive generation We also run a progressive unconditional generation process given by progressive decompression from random bits. In other words, we predict the result of the reverse process, $\\\\hat{\\\\mathbf{x}}_0$, while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the resulting sample quality of $\\\\hat{\\\\mathbf{x}}_0$ over the course of the reverse process. Large scale image features appear first and details appear last. Figure 7 shows stochastic predictions $\\\\mathbf{x}_0 \\\\sim p_\\\\theta(\\\\mathbf{x}_0|\\\\mathbf{x}_t)$ with $\\\\mathbf{x}_t$ frozen for various $t$. When $t$ is small, all but fine details are preserved, and when $t$ is large, only large scale features are preserved. Perhaps these are hints of conceptual compression [18].\\n\\n![img-7.jpeg](img-7.jpeg)\\nFigure 6: Unconditional CIFAR10 progressive generation ($\\\\hat{\\\\mathbf{x}}_0$ over time, from left to right). Extended samples and sample quality metrics over time in the appendix (Figs. 10 and 14).\\n\\n![img-8.jpeg](img-8.jpeg)\\nFigure 7: When conditioned on the same latent, CelebA-HQ $256 \\\\times 256$ samples share high-level attributes. Bottom-right quadrants are $\\\\mathbf{x}_t$, and other quadrants are samples from $p_\\\\theta(\\\\mathbf{x}_0|\\\\mathbf{x}_t)$.\\n\\nConnection to autoregressive decoding Note that the variational bound (5) can be rewritten as:\\n\\n$$\\nL = D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_T) \\\\| p(\\\\mathbf{x}_T)) + \\\\mathbb{E}_q \\\\left[ \\\\sum_{t \\\\geq 1} D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t) \\\\| p_\\\\theta(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t)) \\\\right] + H(\\\\mathbf{x}_0) \\\\tag{16}\\n$$\\n\\n(See Appendix A for a derivation.) Now consider setting the diffusion process length $T$ to the dimensionality of the data, defining the forward process so that $q(\\\\mathbf{x}_t|\\\\mathbf{x}_0)$ places all probability mass on $\\\\mathbf{x}_0$ with the first $t$ coordinates masked out (i.e. $q(\\\\mathbf{x}_t|\\\\mathbf{x}_{t-1})$ masks out the $t^{\\\\text{th}}$ coordinate), setting $p(\\\\mathbf{x}_T)$ to place all mass on a blank image, and, for the sake of argument, taking $p_\\\\theta(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t)$ to\\n\\n![img-9.jpeg](img-9.jpeg)\\nFigure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.\\n\\n![img-10.jpeg](img-10.jpeg)\\n\\nbe a fully expressive conditional distribution. With these choices,  $D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_T) \\\\parallel p(\\\\mathbf{x}_T)) = 0$ , and minimizing  $D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t) \\\\parallel p_\\\\theta(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t))$  trains  $p_\\\\theta$  to copy coordinates  $t + 1, \\\\ldots, T$  unchanged and to predict the  $t^{\\\\text{th}}$  coordinate given  $t + 1, \\\\ldots, T$ . Thus, training  $p_\\\\theta$  with this particular diffusion is training an autoregressive model.\\n\\nWe can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with a generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has shown that such reorderings introduce inductive biases that have an impact on sample quality [38], so we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since Gaussian noise might be more natural to add to images compared to masking noise. Moreover, the Gaussian diffusion length is not restricted to equal the data dimension; for instance, we use  $T = 1000$ , which is less than the dimension of the  $32 \\\\times 32 \\\\times 3$  or  $256 \\\\times 256 \\\\times 3$  images in our experiments. Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.\\n\\n# 4.4 Interpolation\\n\\nWe can interpolate source images  $\\\\mathbf{x}_0, \\\\mathbf{x}_0\\' \\\\sim q(\\\\mathbf{x}_0)$  in latent space using  $q$  as a stochastic encoder,  $\\\\mathbf{x}_t, \\\\mathbf{x}_t\\' \\\\sim q(\\\\mathbf{x}_t | \\\\mathbf{x}_0)$ , then decoding the linearly interpolated latent  $\\\\bar{\\\\mathbf{x}}_t = (1 - \\\\lambda) \\\\mathbf{x}_0 + \\\\lambda \\\\mathbf{x}_0\\'$  into image space by the reverse process,  $\\\\bar{\\\\mathbf{x}}_0 \\\\sim p(\\\\mathbf{x}_0 | \\\\bar{\\\\mathbf{x}}_t)$ . In effect, we use the reverse process to remove artifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8 (left). We fixed the noise for different values of  $\\\\lambda$  so  $\\\\mathbf{x}_t$  and  $\\\\mathbf{x}_t\\'$  remain the same. Fig. 8 (right) shows interpolations and reconstructions of original CelebA-HQ  $256 \\\\times 256$  images ( $t = 500$ ). The reverse process produces high-quality reconstructions, and plausible interpolations that smoothly vary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger  $t$  results in coarser and more varied interpolations, with novel samples at  $t = 1000$  (Appendix Fig. 9).\\n\\n# 5 Related Work\\n\\nWhile diffusion models might resemble flows [9, 46, 10, 32, 5, 16, 23] and VAEs [33, 47, 37], diffusion models are designed so that  $q$  has no parameters and the top-level latent  $\\\\mathbf{x}_T$  has nearly zero mutual information with the data  $\\\\mathbf{x}_0$ . Our  $\\\\epsilon$ -prediction reverse process parameterization establishes a connection between diffusion models and denoising score matching over multiple noise levels with annealed Langevin dynamics for sampling [55, 56]. Diffusion models, however, admit straightforward log likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler using variational inference (see Appendix C for details). The connection also has the reverse implication that a certain weighted form of denoising score matching is the same as variational inference to train a Langevin-like sampler. Other methods for learning transition operators of Markov chains include infusion training [2], variational walkback [15], generative stochastic networks [1], and others [50, 54, 36, 42, 35, 65].\\n\\nBy the known connection between score matching and energy-based modeling, our work could have implications for other recent work on energy-based models [67-69, 12, 70, 13, 11, 41, 17, 8]. Our rate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent of how rate-distortion curves can be computed over distortion penalties in one run of annealed importance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW and related models [18, 40] and may also lead to more general designs for subscale orderings or sampling strategies for autoregressive models [38, 64].\\n\\n6 Conclusion\\n\\nWe have presented high quality image samples using diffusion models, and we have found connections among diffusion models and variational inference for training Markov chains, denoising score matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive models, and progressive lossy compression. Since diffusion models seem to have excellent inductive biases for image data, we look forward to investigating their utility in other data modalities and as components in other types of generative models and machine learning systems.\\n\\n## Broader Impact\\n\\nOur work on diffusion models takes on a similar scope as existing work on other types of deep generative models, such as efforts to improve the sample quality of GANs, flows, autoregressive models, and so forth. Our paper represents progress in making diffusion models a generally useful tool in this family of techniques, so it may serve to amplify any impacts that generative models have had (and will have) on the broader world.\\n\\nUnfortunately, there are numerous well-known malicious uses of generative models. Sample generation techniques can be employed to produce fake images and videos of high profile figures for political purposes. While fake images were manually created long before software tools were available, generative models such as ours make the process easier. Fortunately, CNN-generated images currently have subtle flaws that allow detection *[62]*, but improvements in generative models may make this more difficult. Generative models also reflect the biases in the datasets on which they are trained. As many large datasets are collected from the internet by automated systems, it can be difficult to remove these biases, especially when the images are unlabeled. If samples from generative models trained on these datasets proliferate throughout the internet, then these biases will only be reinforced further.\\n\\nOn the other hand, diffusion models may be useful for data compression, which, as data becomes higher resolution and as global internet traffic increases, might be crucial to ensure accessibility of the internet to wide audiences. Our work might contribute to representation learning on unlabeled raw data for a large range of downstream tasks, from image classification to reinforcement learning, and diffusion models might also become viable for creative uses in art, photography, and music.\\n\\n## Acknowledgments and Disclosure of Funding\\n\\nThis work was supported by ONR PECASE and the NSF Graduate Research Fellowship under grant number DGE-1752814. Google’s TensorFlow Research Cloud (TFRC) provided Cloud TPUs.\\n\\n## References\\n\\n- [1] Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and Pascal Vincent. GSNs: generative stochastic networks. Information and Inference: A Journal of the IMA, 5(2):210–249, 2016.\\n- [2] Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion training. In International Conference on Learning Representations, 2017.\\n- [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019.\\n- [4] Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua Bengio. Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling. arXiv preprint arXiv:2003.06060, 2020.\\n- [5] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Processing Systems, pages 6571–6583, 2018.\\n- [6] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved autoregressive generative model. In International Conference on Machine Learning, pages 863–871, 2018.\\n- [7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.\\n\\n[8] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual energy-based models for text generation. arXiv preprint arXiv:2004.11714, 2020.\\n- [9] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014.\\n- [10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint arXiv:1605.08803, 2016.\\n- [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in Neural Information Processing Systems, pages 3603–3613, 2019.\\n- [12] Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative ConvNets via multi-grid modeling and sampling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9155–9164, 2018.\\n- [13] Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow contrastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7518–7528, 2020.\\n- [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672–2680, 2014.\\n- [15] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net. In Advances in Neural Information Processing Systems, pages 4392–4402, 2017.\\n- [16] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. FFJORD: Free-form continuous dynamics for scalable reversible generative models. In International Conference on Learning Representations, 2019.\\n- [17] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In International Conference on Learning Representations, 2020.\\n- [18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In Advances In Neural Information Processing Systems, pages 3549–3557, 2016.\\n- [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007.\\n- [20] Marton Havasi, Robert Peharz, and José Miguel Hernández-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. In International Conference on Learning Representations, 2019.\\n- [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637, 2017.\\n- [22] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017.\\n- [23] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-based generative models with variational dequantization and architecture design. In International Conference on Machine Learning, 2019.\\n- [24] Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression rates of deep generative models. In International Conference on Machine Learning, 2020.\\n- [25] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks. In International Conference on Machine Learning, pages 1771–1779, 2017.\\n- [26] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. In International Conference on Machine Learning, pages 2410–2419, 2018.\\n- [27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018.\\n- [28] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\\n\\n4401–4410, 2019.\\n- [29] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676v1, 2020.\\n- [30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8110–8119, 2020.\\n- [31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.\\n- [32] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems, pages 10215–10224, 2018.\\n- [33] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.\\n- [34] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pages 4743–4751, 2016.\\n- [35] John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with sampler-induced distributions. In Advances in Neural Information Processing Systems, pages 8501–8513, 2019.\\n- [36] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with neural networks. In International Conference on Learning Representations, 2018.\\n- [37] Lars Maaløe, Marco Fraccaro, Valentin Liévin, and Ole Winther. BIVA: A very deep hierarchy of latent variables for generative modeling. In Advances in Neural Information Processing Systems, pages 6548–6558, 2019.\\n- [38] Jacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. In International Conference on Learning Representations, 2019.\\n- [39] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018.\\n- [40] Alex Nichol. VQ-DRAW: A sequential discrete VAE. arXiv preprint arXiv:2003.01599, 2020.\\n- [41] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of MCMC-based maximum likelihood learning of energy-based models. arXiv preprint arXiv:1903.12370, 2019.\\n- [42] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent short-run MCMC toward energy-based model. In Advances in Neural Information Processing Systems, pages 5233–5243, 2019.\\n- [43] Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative modeling. In International Conference on Machine Learning, pages 3936–3945, 2018.\\n- [44] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. WaveGlow: A flow-based generative network for speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3617–3621. IEEE, 2019.\\n- [45] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In Advances in Neural Information Processing Systems, pages 14837–14847, 2019.\\n- [46] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning, pages 1530–1538, 2015.\\n- [47] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pages 1278–1286, 2014.\\n- [48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234–241. Springer, 2015.\\n- [49] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901–909, 2016.\\n- [50] Tim Salimans, Diederik Kingma, and Max Welling. Markov Chain Monte Carlo and variational inference: Bridging the gap. In International Conference on Machine Learning, pages 1218–1226, 2015.\\n\\n[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234–2242, 2016.\\n- [52] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations, 2017.\\n- [53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265, 2015.\\n- [54] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-NICE-MC: Adversarial training for MCMC. In Advances in Neural Information Processing Systems, pages 5140–5150, 2017.\\n- [55] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pages 11895–11907, 2019.\\n- [56] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. arXiv preprint arXiv:2006.09011, 2020.\\n- [57] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.\\n- [58] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. International Conference on Machine Learning, 2016.\\n- [59] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with PixelCNN decoders. In Advances in Neural Information Processing Systems, pages 4790–4798, 2016.\\n- [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008, 2017.\\n- [61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661–1674, 2011.\\n- [62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot…for now. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.\\n- [63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7794–7803, 2018.\\n- [64] Auke J Wiggers and Emiel Hoogeboom. Predictive sampling with forecasting autoregressive models. arXiv preprint arXiv:2002.09928, 2020.\\n- [65] Hao Wu, Jonas Köhler, and Frank Noé. Stochastic normalizing flows. arXiv preprint arXiv:2002.06707, 2020.\\n- [66] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3–19, 2018.\\n- [67] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In International Conference on Machine Learning, pages 2635–2644, 2016.\\n- [68] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7093–7101, 2017.\\n- [69] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning descriptor networks for 3d shape synthesis and analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8629–8638, 2018.\\n- [70] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning energy-based spatial-temporal generative convnets for dynamic patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\\n- [71] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\\n- [72] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\\n\\n# Extra information\\n\\nLSUN FID scores for LSUN datasets are included in Table 3. Scores marked with * are reported by StyleGAN2 as baselines, and other scores are reported by their respective authors.\\n\\nTable 3: FID scores for LSUN  ${256} \\\\times  {256}$  datasets\\n\\n|  Model | LSUN Bedroom | LSUN Church | LSUN Cat  |\\n| --- | --- | --- | --- |\\n|  ProgressiveGAN [27] | 8.34 | 6.42 | 37.52  |\\n|  StyleGAN [28] | 2.65 | 4.21* | 8.53*  |\\n|  StyleGAN2 [30] | - | 3.86 | 6.93  |\\n|  Ours (Lsimple) | 6.36 | 7.89 | 19.75  |\\n|  Ours (Lsimple, large) | 4.90 | - | -  |\\n\\nProgressive compression Our lossy compression argument in Section 4.3 is only a proof of concept, because Algorithms 3 and 4 depend on a procedure such as minimal random coding [20], which is not tractable for high dimensional data. These algorithms serve as a compression interpretation of the variational bound (5) of Sohl-Dickstein et al. [53], not yet as a practical compression system.\\n\\nTable 4: Unconditional CIFAR10 test set rate-distortion values (accompanies Fig. 5)\\n\\n|  Reverse process time (T-t+1) | Rate (bits/dim) | Distortion (RMSE [0, 255])  |\\n| --- | --- | --- |\\n|  1000 | 1.77581 | 0.95136  |\\n|  900 | 0.11994 | 12.02277  |\\n|  800 | 0.05415 | 18.47482  |\\n|  700 | 0.02866 | 24.43656  |\\n|  600 | 0.01507 | 30.80948  |\\n|  500 | 0.00716 | 38.03236  |\\n|  400 | 0.00282 | 46.12765  |\\n|  300 | 0.00081 | 54.18826  |\\n|  200 | 0.00013 | 60.97170  |\\n|  100 | 0.00000 | 67.60125  |\\n\\n# A Extended derivations\\n\\nBelow is a derivation of Eq. (5), the reduced variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. [53]; we include it here only for completeness.\\n\\n$$\\n\\\\begin{array}{l} L = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0 : T}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {1 : T} \\\\mid \\\\mathbf {x} _ {0}\\\\right)} \\\\right] \\\\tag {17} \\\\\\\\ = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log p \\\\left(\\\\mathbf {x} _ {T}\\\\right) - \\\\sum_ {t \\\\geq 1} \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {t - 1}\\\\right)} \\\\right] \\\\tag {18} \\\\\\\\ = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log p \\\\left(\\\\mathbf {x} _ {T}\\\\right) - \\\\sum_ {t &gt; 1} \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {t - 1}\\\\right)} - \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0} \\\\mid \\\\mathbf {x} _ {1}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {1} \\\\mid \\\\mathbf {x} _ {0}\\\\right)} \\\\right] \\\\tag {19} \\\\\\\\ = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log p (\\\\mathbf {x} _ {T}) - \\\\sum_ {t &gt; 1} \\\\log \\\\frac {p _ {\\\\theta} (\\\\mathbf {x} _ {t - 1} | \\\\mathbf {x} _ {t})}{q (\\\\mathbf {x} _ {t - 1} | \\\\mathbf {x} _ {t} , \\\\mathbf {x} _ {0})} \\\\cdot \\\\frac {q (\\\\mathbf {x} _ {t - 1} | \\\\mathbf {x} _ {0})}{q (\\\\mathbf {x} _ {t} | \\\\mathbf {x} _ {0})} - \\\\log \\\\frac {p _ {\\\\theta} (\\\\mathbf {x} _ {0} | \\\\mathbf {x} _ {1})}{q (\\\\mathbf {x} _ {1} | \\\\mathbf {x} _ {0})} \\\\right] \\\\tag {20} \\\\\\\\ = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log \\\\frac {p \\\\left(\\\\mathbf {x} _ {T}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {T} \\\\mid \\\\mathbf {x} _ {0}\\\\right)} - \\\\sum_ {t &gt; 1} \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t} , \\\\mathbf {x} _ {0}\\\\right)} - \\\\log p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0} \\\\mid \\\\mathbf {x} _ {1}\\\\right) \\\\right] \\\\tag {21} \\\\\\\\ \\\\end{array}\\n$$\\n\\n$=\\\\mathbb{E}_{q}\\\\Bigg{[}D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{T}|\\\\mathbf{x}_{0})\\\\parallel p(\\\\mathbf{x}_{T}))+\\\\sum_{t>1}D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t},\\\\mathbf{x}_{0})\\\\parallel p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t}))-\\\\log p_{\\\\theta}(\\\\mathbf{x}_{0}|\\\\mathbf{x}_{1})\\\\Bigg{]}$ (22)\\n\\nThe following is an alternate version of $L$. It is not tractable to estimate, but it is useful for our discussion in Section 4.3.\\n\\n$L$ $=\\\\mathbb{E}_{q}\\\\Bigg{[}-\\\\log p(\\\\mathbf{x}_{T})-\\\\sum_{t\\\\geq 1}\\\\log\\\\frac{p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})}{q(\\\\mathbf{x}_{t}|\\\\mathbf{x}_{t-1})}\\\\Bigg{]}$ (23)\\n$=\\\\mathbb{E}_{q}\\\\Bigg{[}-\\\\log p(\\\\mathbf{x}_{T})-\\\\sum_{t\\\\geq 1}\\\\log\\\\frac{p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})}{q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})}\\\\cdot\\\\frac{q(\\\\mathbf{x}_{t-1})}{q(\\\\mathbf{x}_{t})}\\\\Bigg{]}$ (24)\\n$=\\\\mathbb{E}_{q}\\\\Bigg{[}-\\\\log\\\\frac{p(\\\\mathbf{x}_{T})}{q(\\\\mathbf{x}_{T})}-\\\\sum_{t\\\\geq 1}\\\\log\\\\frac{p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})}{q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})}-\\\\log q(\\\\mathbf{x}_{0})\\\\Bigg{]}$ (25)\\n$=D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{T})\\\\parallel p(\\\\mathbf{x}_{T}))+\\\\mathbb{E}_{q}\\\\Bigg{[}\\\\sum_{t\\\\geq 1}D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})\\\\parallel p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t}))\\\\Bigg{]}+H(\\\\mathbf{x}_{0})$ (26)\\n\\n## Appendix B Experimental details\\n\\nOur neural network architecture follows the backbone of PixelCNN++ *[52]*, which is a U-Net *[48]* based on a Wide ResNet *[72]*. We replaced weight normalization *[49]* with group normalization *[66]* to make the implementation simpler. Our $32\\\\times 32$ models use four feature map resolutions ($32\\\\times 32$ to $4\\\\times 4$), and our $256\\\\times 256$ models use six. All models have two convolutional residual blocks per resolution level and self-attention blocks at the $16\\\\times 16$ resolution between the convolutional blocks *[6]*. Diffusion time $t$ is specified by adding the Transformer sinusoidal position embedding *[60]* into each residual block. Our CIFAR10 model has 35.7 million parameters, and our LSUN and CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.\\n\\nWe used TPU v3-8 (similar to 8 V100 GPUs) for all experiments. Our CIFAR model trains at 21 steps per second at batch size 128 (10.6 hours to train to completion at 800k steps), and sampling a batch of 256 images takes 17 seconds. Our CelebA-HQ/LSUN (256^{2}) models train at 2.2 steps per second at batch size 64, and sampling a batch of 128 images takes 300 seconds. We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps. The larger LSUN Bedroom model was trained for 1.15M steps.\\n\\nApart from an initial choice of hyperparameters early on to make network size fit within memory constraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample quality, then transferred the resulting settings over to the other datasets:\\n\\n- We chose the $\\\\beta_{t}$ schedule from a set of constant, linear, and quadratic schedules, all constrained so that $L_{T}\\\\approx 0$. We set $T=1000$ without a sweep, and we chose a linear schedule from $\\\\beta_{1}=10^{-4}$ to $\\\\beta_{T}=0.02$.\\n- We set the dropout rate on CIFAR10 to $0.1$ by sweeping over the values $\\\\{0.1,0.2,0.3,0.4\\\\}$. Without dropout on CIFAR10, we obtained poorer samples reminiscent of the overfitting artifacts in an unregularized PixelCNN++ *[52]*. We set dropout rate on the other datasets to zero without sweeping.\\n- We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly. We also used random horizontal flips for all other datasets except LSUN Bedroom.\\n- We tried Adam *[31]* and RMSProp early on in our experimentation process and chose the former. We left the hyperparameters to their standard values. We set the learning rate to $2\\\\times 10^{-4}$ without any sweeping, and we lowered it to $2\\\\times 10^{-5}$ for the $256\\\\times 256$ images, which seemed unstable to train with the larger learning rate.\\n\\n- We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over these values.\\n- We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over this value.\\n\\nFinal experiments were trained once and evaluated throughout training for sample quality. Sample quality scores and log likelihood are reported on the minimum FID value over the course of training. On CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code from the OpenAI *[51]* and TTUR *[21]* repositories, respectively. On LSUN, we calculated FID scores on 50000 samples using code from the StyleGAN2 *[30]* repository. CIFAR10 and CelebA-HQ were loaded as provided by TensorFlow Datasets (https://www.tensorflow.org/datasets), and LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard from the papers that introduced their usage in a generative modeling context. All details can be found in the source code release.\\n\\n## Appendix C Discussion on related work\\n\\nOur model architecture, forward process definition, and prior differ from NCSN *[55, 56]* in subtle but important ways that improve sample quality, and, notably, we directly train our sampler as a latent variable model rather than adding it after training post-hoc. In greater detail:\\n\\n1. We use a U-Net with self-attention; NCSN uses a RefineNet with dilated convolutions. We condition all layers on $t$ by adding in the Transformer sinusoidal position embedding, rather than only in normalization layers (NCSNv1) or only at the output (v2).\\n2. Diffusion models scale down the data with each forward process step (by a $\\\\sqrt{1-\\\\beta_{t}}$ factor) so that variance does not grow when adding noise, thus providing consistently scaled inputs to the neural net reverse process. NCSN omits this scaling factor.\\n3. Unlike NCSN, our forward process destroys signal ($D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{T}|\\\\mathbf{x}_{0})\\\\parallel\\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I}))\\\\approx 0$), ensuring a close match between the prior and aggregate posterior of $\\\\mathbf{x}_{T}$. Also unlike NCSN, our $\\\\beta_{t}$ are very small, which ensures that the forward process is reversible by a Markov chain with conditional Gaussians. Both of these factors prevent distribution shift when sampling.\\n4. Our Langevin-like sampler has coefficients (learning rate, noise scale, etc.) derived rigorously from $\\\\beta_{t}$ in the forward process. Thus, our training procedure directly trains our sampler to match the data distribution after $T$ steps: it trains the sampler as a latent variable model using variational inference. In contrast, NCSN’s sampler coefficients are set by hand post-hoc, and their training procedure is not guaranteed to directly optimize a quality metric of their sampler.\\n\\n## Appendix D Samples\\n\\n##### Additional samples\\n\\nFigure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion models trained on CelebA-HQ, CIFAR10 and LSUN datasets.\\n\\n##### Latent structure and reverse process stochasticity\\n\\nDuring sampling, both the prior $\\\\mathbf{x}_{T}\\\\sim\\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$ and Langevin dynamics are stochastic. To understand the significance of the second source of noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA $256\\\\times 256$ dataset. Figure 7 shows multiple draws from the reverse process $\\\\mathbf{x}_{0}\\\\sim p_{\\\\theta}(\\\\mathbf{x}_{0}|\\\\mathbf{x}_{t})$ that share the latent $\\\\mathbf{x}_{t}$ for $t\\\\in\\\\{1000,750,500,250\\\\}$. To accomplish this, we run a single reverse chain from an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple images. When the chain is split after the prior draw at $\\\\mathbf{x}_{T=1000}$, the samples differ significantly. However, when the chain is split after more steps, samples share high-level attributes like gender, hair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents like $\\\\mathbf{x}_{750}$ encode these attributes, despite their imperceptibility.\\n\\n##### Coarse-to-fine interpolation\\n\\nFigure 9 shows interpolations between a pair of source CelebA $256\\\\times 256$ images as we vary the number of diffusion steps prior to latent space interpolation. Increasing the number of diffusion steps destroys more structure in the source images, which the\\n\\nmodel completes during the reverse process. This allows us to interpolate at both fine granularities and coarse granularities. In the limiting case of 0 diffusion steps, the interpolation mixes source images in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and interpolations are novel samples.\\n\\n![img-11.jpeg](img-11.jpeg)\\nFigure 9: Coarse-to-fine interpolations that vary the number of diffusion steps prior to latent mixing.\\n\\n![img-12.jpeg](img-12.jpeg)\\nFigure 10: Unconditional CIFAR10 progressive sampling quality over time\\n\\n![img-13.jpeg](img-13.jpeg)\\n\\n![img-14.jpeg](img-14.jpeg)\\nFigure 11: CelebA-HQ  $256 \\\\times 256$  generated samples\\n\\n![img-15.jpeg](img-15.jpeg)\\n\\n![img-16.jpeg](img-16.jpeg)\\n(a) Pixel space nearest neighbors\\n(b) Inception feature space nearest neighbors\\nFigure 12: CelebA-HQ  $256 \\\\times 256$  nearest neighbors, computed on a  $100 \\\\times 100$  crop surrounding the faces. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.\\n\\n![img-17.jpeg](img-17.jpeg)\\nFigure 13: Unconditional CIFAR10 generated samples\\n\\n![img-18.jpeg](img-18.jpeg)\\nFigure 14: Unconditional CIFAR10 progressive generation\\n\\n![img-19.jpeg](img-19.jpeg)\\n\\n![img-20.jpeg](img-20.jpeg)\\n(a) Pixel space nearest neighbors\\n(b) Inception feature space nearest neighbors\\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.\\n\\n![img-21.jpeg](img-21.jpeg)\\nFigure 16: LSUN Church generated samples. FID=7.89\\n\\n![img-22.jpeg](img-22.jpeg)\\nFigure 17: LSUN Bedroom generated samples, large model. FID=4.90\\n\\n![img-23.jpeg](img-23.jpeg)\\nFigure 18: LSUN Bedroom generated samples, small model. FID=6.36\\n\\n![img-24.jpeg](img-24.jpeg)\\nFigure 19: LSUN Cat generated samples. FID=19.75', 'doc_id': 'c60d566af74b', 'chunk_index': 0, 'start_line': 1}, page_content='# Denoising Diffusion Probabilistic Models\\n\\nJonathan Ho\\n\\nUC Berkeley\\n\\njonathanho@berkeley.edu\\n\\nAjay Jain\\n\\nUC Berkeley\\n\\najayj@berkeley.edu\\n\\nPieter Abbeel\\n\\nUC Berkeley\\n\\npabbeel@cs.berkeley.edu\\n\\n# Abstract\\n\\nWe present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On  $256 \\\\times 256$  LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.\\n\\n# 1 Introduction\\n\\nDeep generative models of all kinds have recently exhibited high quality samples in a wide variety of data modalities. Generative adversarial networks (GANs), autoregressive models, flows, and variational autoencoders (VAEs) have synthesized striking image and audio samples [14, 27, 3, 58, 38, 25, 10, 32, 44, 57, 26, 33, 45], and there have been remarkable advances in energy-based modeling and score matching that have produced images comparable to those of GANs [11, 55].\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Generated samples on CelebA-HQ  $256 \\\\times 256$  (left) and unconditional CIFAR10 (right)\\n\\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2: The directed graphical model considered in this work.\\n\\nThis paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model (which we will call a \"diffusion model\" for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed. When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.\\n\\nDiffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples. We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models (Section 4). In addition, we show that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling (Section 3.2) [55, 61]. We obtained our best sample quality results using this parameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.\\n\\nDespite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models (our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching [11, 55]). We find that the majority of our models\\' lossless codelengths are consumed to describe imperceptible image details (Section 4.3). We present a more refined analysis of this phenomenon in the language of lossy compression, and we show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.\\n\\n# 2 Background\\n\\nDiffusion models [53] are latent variable models of the form  $p_{\\\\theta}(\\\\mathbf{x}_0) \\\\coloneqq \\\\int p_{\\\\theta}(\\\\mathbf{x}_{0:T}) d\\\\mathbf{x}_{1:T}$ , where  $\\\\mathbf{x}_1, \\\\ldots, \\\\mathbf{x}_T$  are latents of the same dimensionality as the data  $\\\\mathbf{x}_0 \\\\sim q(\\\\mathbf{x}_0)$ . The joint distribution  $p_{\\\\theta}(\\\\mathbf{x}_{0:T})$  is called the reverse process, and it is defined as a Markov chain with learned Gaussian transitions starting at  $p(\\\\mathbf{x}_T) = \\\\mathcal{N}(\\\\mathbf{x}_T; \\\\mathbf{0}, \\\\mathbf{I})$ :\\n\\n$$\\np _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0: T}\\\\right) := p \\\\left(\\\\mathbf {x} _ {T}\\\\right) \\\\prod_ {t = 1} ^ {T} p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right), \\\\quad p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right) := \\\\mathcal {N} \\\\left(\\\\mathbf {x} _ {t - 1}; \\\\boldsymbol {\\\\mu} _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t}, t\\\\right), \\\\boldsymbol {\\\\Sigma} _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t}, t\\\\right)\\\\right) \\\\tag {1}\\n$$\\n\\nWhat distinguishes diffusion models from other types of latent variable models is that the approximate posterior  $q(\\\\mathbf{x}_{1:T}|\\\\mathbf{x}_0)$ , called the forward process or diffusion process, is fixed to a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule  $\\\\beta_1,\\\\ldots ,\\\\beta_T$ :\\n\\n$$\\nq \\\\left(\\\\mathbf {x} _ {1: T} \\\\mid \\\\mathbf {x} _ {0}\\\\right) := \\\\prod_ {t = 1} ^ {T} q \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {t - 1}\\\\right), \\\\quad q \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {t - 1}\\\\right) := \\\\mathcal {N} \\\\left(\\\\mathbf {x} _ {t}; \\\\sqrt {1 - \\\\beta_ {t}} \\\\mathbf {x} _ {t - 1}, \\\\beta_ {t} \\\\mathbf {I}\\\\right) \\\\tag {2}\\n$$\\n\\nTraining is performed by optimizing the usual variational bound on negative log likelihood:\\n\\n$$\\n\\\\mathbb {E} \\\\left[ - \\\\log p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0}\\\\right) \\\\right] \\\\leq \\\\mathbb {E} _ {q} \\\\left[ - \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0 : T}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {1 : T} \\\\mid \\\\mathbf {x} _ {0}\\\\right)} \\\\right] = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log p \\\\left(\\\\mathbf {x} _ {T}\\\\right) - \\\\sum_ {t \\\\geq 1} \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {t - 1}\\\\right)} \\\\right] =: L \\\\tag {3}\\n$$\\n\\nThe forward process variances  $\\\\beta_{t}$  can be learned by reparameterization [33] or held constant as hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in  $p_{\\\\theta}(\\\\mathbf{x}_{t - 1}|\\\\mathbf{x}_t)$ , because both processes have the same functional form when  $\\\\beta_{t}$  are small [53]. A notable property of the forward process is that it admits sampling  $\\\\mathbf{x}_t$  at an arbitrary timestep  $t$  in closed form: using the notation  $\\\\alpha_{t}\\\\coloneqq 1 - \\\\beta_{t}$  and  $\\\\bar{\\\\alpha}_{t}\\\\coloneqq \\\\prod_{s = 1}^{t}\\\\alpha_{s}$ , we have\\n\\n$$\\nq \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {0}\\\\right) = \\\\mathcal {N} \\\\left(\\\\mathbf {x} _ {t}; \\\\sqrt {\\\\bar {\\\\alpha} _ {t}} \\\\mathbf {x} _ {0}, (1 - \\\\bar {\\\\alpha} _ {t}) \\\\mathbf {I}\\\\right) \\\\tag {4}\\n$$\\n\\nEfficient training is therefore possible by optimizing random terms of $L$ with stochastic gradient descent. Further improvements come from variance reduction by rewriting $L$ (3) as:\\n\\n$\\\\mathbb{E}_{q}\\\\bigg{[}\\\\underbrace{D_{\\\\text{KL}}(q(\\\\mathbf{x}_{T}|\\\\mathbf{x}_{0})\\\\parallel p(\\\\mathbf{x}_{T}))}_{L_{T}}+\\\\sum_{t>1}\\\\underbrace{D_{\\\\text{KL}}(q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t},\\\\mathbf{x}_{0})\\\\parallel p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t}))}_{L_{t-1}}\\\\underbrace{-\\\\log p_{\\\\theta}(\\\\mathbf{x}_{0}|\\\\mathbf{x}_{1})}_{L_{0}}\\\\bigg{]}$ (5)\\n\\n(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL divergence to directly compare $p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})$ against forward process posteriors, which are tractable when conditioned on $\\\\mathbf{x}_{0}$:\\n\\n$q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t},\\\\mathbf{x}_{0})=\\\\mathcal{N}(\\\\mathbf{x}_{t-1};\\\\tilde{\\\\bm{\\\\mu}}_{t}(\\\\mathbf{x}_{t},\\\\mathbf{x}_{0}),\\\\tilde{\\\\beta}_{t}\\\\mathbf{I}),$ (6)\\n$\\\\text{where}\\\\quad\\\\tilde{\\\\bm{\\\\mu}}_{t}(\\\\mathbf{x}_{t},\\\\mathbf{x}_{0})\\\\coloneqq\\\\frac{\\\\sqrt{\\\\bar{\\\\alpha}_{t-1}}\\\\beta_{t}}{1-\\\\bar{\\\\alpha}_{t}}\\\\mathbf{x}_{0}+\\\\frac{\\\\sqrt{\\\\alpha_{t}}(1-\\\\bar{\\\\alpha}_{t-1})}{1-\\\\bar{\\\\alpha}_{t}}\\\\mathbf{x}_{t}\\\\quad\\\\text{and}\\\\quad\\\\tilde{\\\\beta}_{t}\\\\coloneqq\\\\frac{1-\\\\bar{\\\\alpha}_{t-1}}{1-\\\\bar{\\\\alpha}_{t}}\\\\beta_{t}$ (7)\\n\\nConsequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be calculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance Monte Carlo estimates.\\n\\n## 3 Diffusion models and denoising autoencoders\\n\\nDiffusion models might appear to be a restricted class of latent variable models, but they allow a large number of degrees of freedom in implementation. One must choose the variances $\\\\beta_{t}$ of the forward process and the model architecture and Gaussian distribution parameterization of the reverse process. To guide our choices, we establish a new explicit connection between diffusion models and denoising score matching (Section 3.2) that leads to a simplified, weighted variational bound objective for diffusion models (Section 3.4). Ultimately, our model design is justified by simplicity and empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).\\n\\n### 3.1 Forward process and $L_{T}$\\n\\nWe ignore the fact that the forward process variances $\\\\beta_{t}$ are learnable by reparameterization and instead fix them to constants (see Section 4 for details). Thus, in our implementation, the approximate posterior $q$ has no learnable parameters, so $L_{T}$ is a constant during training and can be ignored.\\n\\n### 3.2 Reverse process and $L_{1:T-1}$\\n\\nNow we discuss our choices in $p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})=\\\\mathcal{N}(\\\\mathbf{x}_{t-1};\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t},t),\\\\bm{\\\\Sigma}_{\\\\theta}(\\\\mathbf{x}_{t},t))$ for $1<t\\\\leq T$. First, we set $\\\\bm{\\\\Sigma}_{\\\\theta}(\\\\mathbf{x}_{t},t)=\\\\sigma_{t}^{2}\\\\mathbf{I}$ to untrained time dependent constants. Experimentally, both $\\\\sigma_{t}^{2}=\\\\beta_{t}$ and $\\\\sigma_{t}^{2}=\\\\tilde{\\\\beta}_{t}=\\\\frac{1-\\\\bar{\\\\alpha}_{t-1}}{1-\\\\bar{\\\\alpha}_{t}}\\\\beta_{t}$ had similar results. The first choice is optimal for $\\\\mathbf{x}_{0}\\\\sim\\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$, and the second is optimal for $\\\\mathbf{x}_{0}$ deterministically set to one point. These are the two extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance *[53]*.\\n\\nSecond, to represent the mean $\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t},t)$, we propose a specific parameterization motivated by the following analysis of $L_{t}$. With $p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})=\\\\mathcal{N}(\\\\mathbf{x}_{t-1};\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t},t),\\\\sigma_{t}^{2}\\\\mathbf{I})$, we can write:\\n\\n$L_{t-1}=\\\\mathbb{E}_{q}\\\\bigg{[}\\\\frac{1}{2\\\\sigma_{t}^{2}}\\\\|\\\\tilde{\\\\bm{\\\\mu}}_{t}(\\\\mathbf{x}_{t},\\\\mathbf{x}_{0})-\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t},t)\\\\|^{2}\\\\bigg{]}+C$ (8)\\n\\nwhere $C$ is a constant that does not depend on $\\\\theta$. So, we see that the most straightforward parameterization of $\\\\bm{\\\\mu}_{\\\\theta}$ is a model that predicts $\\\\tilde{\\\\bm{\\\\mu}}_{t}$, the forward process posterior mean. However, we can expand Eq. (8) further by reparameterizing Eq. (4) as $\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon})=\\\\sqrt{\\\\bar{\\\\alpha}_{t}}\\\\mathbf{x}_{0}+\\\\sqrt{1-\\\\bar{\\\\alpha}_{t}}\\\\bm{\\\\epsilon}$ for $\\\\bm{\\\\epsilon}\\\\sim\\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$ and applying the forward process posterior formula (7):\\n\\n$L_{t-1}-C$ $=\\\\mathbb{E}_{\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon}}\\\\Bigg{[}\\\\frac{1}{2\\\\sigma_{t}^{2}}\\\\left\\\\|\\\\tilde{\\\\bm{\\\\mu}}_{t}\\\\bigg{(}\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon}),\\\\frac{1}{\\\\sqrt{\\\\bar{\\\\alpha}_{t}}}(\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon})-\\\\sqrt{1-\\\\bar{\\\\alpha}_{t}}\\\\bm{\\\\epsilon})\\\\bigg{)}-\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon}),t)\\\\right\\\\|^{2}\\\\Bigg{]}$ (9)\\n$=\\\\mathbb{E}_{\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon}}\\\\Bigg{[}\\\\frac{1}{2\\\\sigma_{t}^{2}}\\\\left\\\\|\\\\frac{1}{\\\\sqrt{\\\\alpha_{t}}}\\\\left(\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon})-\\\\frac{\\\\beta_{t}}{\\\\sqrt{1-\\\\bar{\\\\alpha}_{t}}}\\\\bm{\\\\epsilon}\\\\right)-\\\\bm{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_{t}(\\\\mathbf{x}_{0},\\\\bm{\\\\epsilon}),t)\\\\right\\\\|^{2}\\\\Bigg{]}$ (10)\\n\\n###\\n\\n|  Algorithm 1 Training | Algorithm 2 Sampling  |\\n| --- | --- |\\n|  1: repeat | 1: xT ~ N(0, I)  |\\n|  2: x0 ~ q(x0) | 2: for t = T, ..., 1 do  |\\n|  3: t ~ Uniform({1, ..., T}) | 3: z ~ N(0, I) if t > 1, else z = 0  |\\n|  4: ε ~ N(0, I) | 4: xt-1 = 1/√αt (xt - 1-αt/√1-αt εθ(xt, t)) + σtz  |\\n|  5: Take gradient descent step on ∇θ ||ε - εθ(√αt x0 + √1-αt ε, t)||2 | 5: end for  |\\n|  6: until converged | 6: return x0  |\\n\\nEquation (10) reveals that  $\\\\pmb{\\\\mu}_{\\\\theta}$  must predict  $\\\\frac{1}{\\\\sqrt{\\\\alpha_t}}\\\\left(\\\\mathbf{x}_t - \\\\frac{\\\\beta_t}{\\\\sqrt{1 - \\\\bar{\\\\alpha}_t}}\\\\pmb{\\\\epsilon}\\\\right)$  given  $\\\\mathbf{x}_t$ . Since  $\\\\mathbf{x}_t$  is available as input to the model, we may choose the parameterization\\n\\n$$\\n\\\\boldsymbol {\\\\mu} _ {\\\\theta} (\\\\mathbf {x} _ {t}, t) = \\\\tilde {\\\\boldsymbol {\\\\mu}} _ {t} \\\\left(\\\\mathbf {x} _ {t}, \\\\frac {1}{\\\\sqrt {\\\\bar {\\\\alpha} _ {t}}} \\\\left(\\\\mathbf {x} _ {t} - \\\\sqrt {1 - \\\\bar {\\\\alpha} _ {t}} \\\\boldsymbol {\\\\epsilon} _ {\\\\theta} (\\\\mathbf {x} _ {t})\\\\right)\\\\right) = \\\\frac {1}{\\\\sqrt {\\\\alpha_ {t}}} \\\\left(\\\\mathbf {x} _ {t} - \\\\frac {\\\\beta_ {t}}{\\\\sqrt {1 - \\\\bar {\\\\alpha} _ {t}}} \\\\boldsymbol {\\\\epsilon} _ {\\\\theta} (\\\\mathbf {x} _ {t}, t)\\\\right) \\\\tag {11}\\n$$\\n\\nwhere  $\\\\epsilon_{\\\\theta}$  is a function approximator intended to predict  $\\\\epsilon$  from  $\\\\mathbf{x}_t$ . To sample  $\\\\mathbf{x}_{t - 1} \\\\sim p_{\\\\theta}(\\\\mathbf{x}_{t - 1}|\\\\mathbf{x}_t)$  is to compute  $\\\\mathbf{x}_{t - 1} = \\\\frac{1}{\\\\sqrt{\\\\alpha_t}}\\\\left(\\\\mathbf{x}_t - \\\\frac{\\\\beta_t}{\\\\sqrt{1 - \\\\bar{\\\\alpha}_t}}\\\\epsilon_{\\\\theta}(\\\\mathbf{x}_t,t)\\\\right) + \\\\sigma_t\\\\mathbf{z}$ , where  $\\\\mathbf{z} \\\\sim \\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$ . The complete sampling procedure, Algorithm 2, resembles Langevin dynamics with  $\\\\epsilon_{\\\\theta}$  as a learned gradient of the data density. Furthermore, with the parameterization (11), Eq. (10) simplifies to:\\n\\n$$\\n\\\\mathbb {E} _ {\\\\mathbf {x} _ {0}, \\\\epsilon} \\\\left[ \\\\frac {\\\\beta_ {t} ^ {2}}{2 \\\\sigma_ {t} ^ {2} \\\\alpha_ {t} (1 - \\\\bar {\\\\alpha} _ {t})} \\\\left\\\\| \\\\epsilon - \\\\epsilon_ {\\\\theta} \\\\left(\\\\sqrt {\\\\bar {\\\\alpha} _ {t}} \\\\mathbf {x} _ {0} + \\\\sqrt {1 - \\\\bar {\\\\alpha} _ {t}} \\\\epsilon , t\\\\right) \\\\right\\\\| ^ {2} \\\\right] \\\\tag {12}\\n$$\\n\\nwhich resembles denoising score matching over multiple noise scales indexed by  $t$  [55]. As Eq. (12) is equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see that optimizing an objective resembling denoising score matching is equivalent to using variational inference to fit the finite-time marginal of a sampling chain resembling Langevin dynamics.\\n\\nTo summarize, we can train the reverse process mean function approximator  $\\\\mu_{\\\\theta}$  to predict  $\\\\tilde{\\\\mu}_t$ , or by modifying its parameterization, we can train it to predict  $\\\\epsilon$ . (There is also the possibility of predicting  $\\\\mathbf{x}_0$ , but we found this to lead to worse sample quality early in our experiments.) We have shown that the  $\\\\epsilon$ -prediction parameterization both resembles Langevin dynamics and simplifies the diffusion model\\'s variational bound to an objective that resembles denoising score matching. Nonetheless, it is just another parameterization of  $p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t)$ , so we verify its effectiveness in Section 4 in an ablation where we compare predicting  $\\\\epsilon$  against predicting  $\\\\tilde{\\\\mu}_t$ .\\n\\n## 3.3 Data scaling, reverse process decoder, and  $L_{0}$\\n\\nWe assume that image data consists of integers in  $\\\\{0,1,\\\\dots,255\\\\}$  scaled linearly to  $[-1,1]$ . This ensures that the neural network reverse process operates on consistently scaled inputs starting from the standard normal prior  $p(\\\\mathbf{x}_T)$ . To obtain discrete log likelihoods, we set the last term of the reverse process to an independent discrete decoder derived from the Gaussian  $\\\\mathcal{N}(\\\\mathbf{x}_0;\\\\boldsymbol{\\\\mu}_{\\\\theta}(\\\\mathbf{x}_1,1),\\\\sigma_1^2\\\\mathbf{I})$ :\\n\\n$$\\np _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0} \\\\mid \\\\mathbf {x} _ {1}\\\\right) = \\\\prod_ {i = 1} ^ {D} \\\\int_ {\\\\delta_ {-} \\\\left(x _ {0} ^ {i}\\\\right)} ^ {\\\\delta_ {+} \\\\left(x _ {0} ^ {i}\\\\right)} \\\\mathcal {N} \\\\left(x; \\\\mu_ {\\\\theta} ^ {i} \\\\left(\\\\mathbf {x} _ {1}, 1\\\\right), \\\\sigma_ {1} ^ {2}\\\\right) d x \\\\tag {13}\\n$$\\n\\n$$\\n\\\\delta_ {+} (x) = \\\\left\\\\{ \\\\begin{array}{l l} \\\\infty &amp; \\\\text {if} x = 1 \\\\\\\\ x + \\\\frac {1}{2 5 5} &amp; \\\\text {if} x &lt; 1 \\\\end{array} \\\\right. \\\\qquad \\\\delta_ {-} (x) = \\\\left\\\\{ \\\\begin{array}{l l} - \\\\infty &amp; \\\\text {if} x = - 1 \\\\\\\\ x - \\\\frac {1}{2 5 5} &amp; \\\\text {if} x &gt; - 1 \\\\end{array} \\\\right.\\n$$\\n\\nwhere  $D$  is the data dimensionality and the  $i$  superscript indicates extraction of one coordinate. (It would be straightforward to instead incorporate a more powerful decoder like a conditional autoregressive model, but we leave that to future work.) Similar to the discretized continuous distributions used in VAE decoders and autoregressive models [34, 52], our choice here ensures that the variational bound is a lossless codelength of discrete data, without need of adding noise to the data or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of sampling, we display  $\\\\mu_{\\\\theta}(\\\\mathbf{x}_1,1)$  noiselessly.\\n\\n## 3.4 Simplified training objective\\n\\nWith the reverse process and decoder defined above, the variational bound, consisting of terms derived from Eqs. (12) and (13), is clearly differentiable with respect to  $\\\\theta$  and is ready to be employed for\\n\\nTable 1: CIFAR10 results. NLL measured in bits/dim.\\n\\n|  Model | IS | FID | NLL Test (Train)  |\\n| --- | --- | --- | --- |\\n|  Conditional  |   |   |   |\\n|  EBM [11] | 8.30 | 37.9 |   |\\n|  JEM [17] | 8.76 | 38.4 |   |\\n|  BigGAN [3] | 9.22 | 14.73 |   |\\n|  StyleGAN2 + ADA (v1) [29] | 10.06 | 2.67 |   |\\n|  Unconditional  |   |   |   |\\n|  Diffusion (original) [53] |  |  | ≤ 5.40  |\\n|  Gated PixelCNN [59] | 4.60 | 65.93 | 3.03 (2.90)  |\\n|  Sparse Transformer [7] |  |  | 2.80  |\\n|  PixelIQN [43] | 5.29 | 49.46 |   |\\n|  EBM [11] | 6.78 | 38.2 |   |\\n|  NCSNv2 [56] |  | 31.75 |   |\\n|  NCSN [55] | 8.87±0.12 | 25.32 |   |\\n|  SNGAN [39] | 8.22±0.05 | 21.7 |   |\\n|  SNGAN-DDLS [4] | 9.09±0.10 | 15.42 |   |\\n|  StyleGAN2 + ADA (v1) [29] | 9.74 ± 0.05 | 3.26 |   |\\n|  Ours (L, fixed isotropic Σ) | 7.67±0.13 | 13.51 | ≤ 3.70 (3.69)  |\\n|  Ours (Lsimple) | 9.46±0.11 | 3.17 | ≤ 3.75 (3.72)  |\\n\\nTable 2: Unconditional CIFAR10 reverse process parameterization and training objective ablation. Blank entries were unstable to train and generated poor samples with out-of-range scores.\\n\\n|  Objective | IS | FID  |\\n| --- | --- | --- |\\n|  μ prediction (baseline)  |   |   |\\n|  L, learned diagonal Σ | 7.28±0.10 | 23.69  |\\n|  L, fixed isotropic Σ | 8.06±0.09 | 13.22  |\\n|  ||μ - μθ||2 | - | -  |\\n|  ε prediction (ours)  |   |   |\\n|  L, learned diagonal Σ | - | -  |\\n|  L, fixed isotropic Σ | 7.67±0.13 | 13.51  |\\n|  ||ε - εθ||2 (Lsimple) | 9.46±0.11 | 3.17  |\\n\\ntraining. However, we found it beneficial to sample quality (and simpler to implement) to train on the following variant of the variational bound:\\n\\n$$\\nL _ {\\\\text {s i m p l e}} (\\\\theta) := \\\\mathbb {E} _ {t, \\\\mathbf {x} _ {0}, \\\\epsilon} \\\\left[ \\\\left\\\\| \\\\boldsymbol {\\\\epsilon} - \\\\boldsymbol {\\\\epsilon} _ {\\\\theta} \\\\left(\\\\sqrt {\\\\bar {\\\\alpha} _ {t}} \\\\mathbf {x} _ {0} + \\\\sqrt {1 - \\\\bar {\\\\alpha} _ {t}} \\\\boldsymbol {\\\\epsilon}, t\\\\right) \\\\right\\\\| ^ {2} \\\\right] \\\\tag {14}\\n$$\\n\\nwhere  $t$  is uniform between 1 and  $T$ . The  $t = 1$  case corresponds to  $L_{0}$  with the integral in the discrete decoder definition (13) approximated by the Gaussian probability density function times the bin width, ignoring  $\\\\sigma_1^2$  and edge effects. The  $t &gt; 1$  cases correspond to an unweighted version of Eq. (12), analogous to the loss weighting used by the NCSN denoising score matching model [55]. ( $L_{T}$  does not appear because the forward process variances  $\\\\beta_{t}$  are fixed.) Algorithm 1 displays the complete training procedure with this simplified objective.\\n\\nSince our simplified objective (14) discards the weighting in Eq. (12), it is a weighted variational bound that emphasizes different aspects of reconstruction compared to the standard variational bound [18, 22]. In particular, our diffusion process setup in Section 4 causes the simplified objective to down-weight loss terms corresponding to small  $t$ . These terms train the network to denoise data with very small amounts of noise, so it is beneficial to down-weight them so that the network can focus on more difficult denoising tasks at larger  $t$  terms. We will see in our experiments that this reweighting leads to better sample quality.\\n\\n# 4 Experiments\\n\\nWe set  $T = 1000$  for all experiments so that the number of neural network evaluations needed during sampling matches previous work [53, 55]. We set the forward process variances to constants increasing linearly from  $\\\\beta_{1} = 10^{-4}$  to  $\\\\beta_{T} = 0.02$ . These constants were chosen to be small relative to data scaled to  $[-1, 1]$ , ensuring that reverse and forward processes have approximately the same functional form while keeping the signal-to-noise ratio at  $\\\\mathbf{x}_T$  as small as possible ( $L_{T} = D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{T}|\\\\mathbf{x}_{0}) \\\\parallel \\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})) \\\\approx 10^{-5}$  bits per dimension in our experiments).\\n\\nTo represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52, 48] with group normalization throughout [66]. Parameters are shared across time, which is specified to the network using the Transformer sinusoidal position embedding [60]. We use self-attention at the  $16 \\\\times 16$  feature map resolution [63, 60]. Details are in Appendix B.\\n\\n# 4.1 Sample quality\\n\\nTable 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on CIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than most models in the literature, including class conditional models. Our FID score is computed with respect to the training set, as is standard practice; when we compute it with respect to the test set, the score is 5.24, which is still better than many of the training set FID scores in the literature.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 3: LSUN Church samples. FID=7.89\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 4: LSUN Bedroom samples. FID=4.90\\n\\n|  Algorithm 3 Sending x0  |\\n| --- |\\n|  1: Send xT ~ q(xt|x0) using p(xt)  |\\n|  2: for t = T - 1, ..., 2, 1 do  |\\n|  3: Send xt ~ q(xt|xt+1, x0) using pθ(xt|xt+1)  |\\n|  4: end for  |\\n|  5: Send x0 using pθ(x0|x1)  |\\n|  Algorithm 4 Receiving  |\\n| --- |\\n|  1: Receive xT using p(xt)  |\\n|  2: for t = T - 1, ..., 1, 0 do  |\\n|  3: Receive xt using pθ(xt|xt+1)  |\\n|  4: end for  |\\n|  5: return x0  |\\n\\nWe find that training our models on the true variational bound yields better codelengths than training on the simplified objective, as expected, but the latter yields the best sample quality. See Fig. 1 for CIFAR10 and CelebA-HQ  $256 \\\\times 256$  samples, Fig. 3 and Fig. 4 for LSUN  $256 \\\\times 256$  samples [71], and Appendix D for more.\\n\\n# 4.2 Reverse process parameterization and training objective ablation\\n\\nIn Table 2, we show the sample quality effects of reverse process parameterizations and training objectives (Section 3.2). We find that the baseline option of predicting  $\\\\hat{\\\\mu}$  works well only when trained on the true variational bound instead of unweighted mean squared error, a simplified objective akin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized diagonal  $\\\\Sigma_{\\\\theta}(\\\\mathbf{x}_t)$  into the variational bound) leads to unstable training and poorer sample quality compared to fixed variances. Predicting  $\\\\epsilon$ , as we proposed, performs approximately as well as predicting  $\\\\hat{\\\\mu}$  when trained on the variational bound with fixed variances, but much better when trained with our simplified objective.\\n\\n# 4.3 Progressive coding\\n\\nTable 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at most 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based models and indicates that our diffusion model is not overfitting (see Appendix D for nearest neighbor visualizations). Still, while our lossless codelengths are better than the large estimates reported for energy based models and score matching using annealed importance sampling [11], they are not competitive with other types of likelihood-based generative models [7].\\n\\nSince our samples are nonetheless of high quality, we conclude that diffusion models have an inductive bias that makes them excellent lossy compressors. Treating the variational bound terms  $L_{1} + \\\\dots + L_{T}$  as rate and  $L_{0}$  as distortion, our CIFAR10 model with the highest quality samples has a rate of 1.78 bits/dim and a distortion of 1.97 bits/dim, which amounts to a root mean squared error of 0.95 on a scale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.\\n\\nProgressive lossy compression We can probe further into the rate-distortion behavior of our model by introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4, which assume access to a procedure, such as minimal random coding [19, 20], that can transmit a sample  $\\\\mathbf{x} \\\\sim q(\\\\mathbf{x})$  using approximately  $D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}) \\\\parallel p(\\\\mathbf{x}))$  bits on average for any distributions  $p$  and  $q$ , for which only  $p$  is available to the receiver beforehand. When applied to  $\\\\mathbf{x}_0 \\\\sim q(\\\\mathbf{x}_0)$ , Algorithms 3 and 4 transmit  $\\\\mathbf{x}_T, \\\\ldots, \\\\mathbf{x}_0$  in sequence using a total expected codelength equal to Eq. (5). The receiver,\\n\\nat any time $t$, has the partial information $\\\\mathbf{x}_t$ fully available and can progressively estimate:\\n\\n$$\\n\\\\mathbf{x}_0 \\\\approx \\\\hat{\\\\mathbf{x}}_0 = \\\\left(\\\\mathbf{x}_t - \\\\sqrt{1 - \\\\bar{\\\\alpha}_t} \\\\boldsymbol{\\\\epsilon}_{\\\\theta}(\\\\mathbf{x}_t)\\\\right) / \\\\sqrt{\\\\bar{\\\\alpha}_t} \\\\tag{15}\\n$$\\n\\ndue to Eq. (4). (A stochastic reconstruction $\\\\mathbf{x}_0 \\\\sim p_\\\\theta(\\\\mathbf{x}_0|\\\\mathbf{x}_t)$ is also valid, but we do not consider it here because it makes distortion more difficult to evaluate.) Figure 5 shows the resulting rate-distortion plot on the CIFAR10 test set. At each time $t$, the distortion is calculated as the root mean squared error $\\\\sqrt{\\\\|\\\\mathbf{x}_0 - \\\\hat{\\\\mathbf{x}}_0\\\\|^2 / D}$, and the rate is calculated as the cumulative number of bits received so far at time $t$. The distortion decreases steeply in the low-rate region of the rate-distortion plot, indicating that the majority of the bits are indeed allocated to imperceptible distortions.\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared error on a [0, 255] scale. See Table 4 for details.\\n\\n![img-5.jpeg](img-5.jpeg)\\n\\n![img-6.jpeg](img-6.jpeg)\\n\\nProgressive generation We also run a progressive unconditional generation process given by progressive decompression from random bits. In other words, we predict the result of the reverse process, $\\\\hat{\\\\mathbf{x}}_0$, while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the resulting sample quality of $\\\\hat{\\\\mathbf{x}}_0$ over the course of the reverse process. Large scale image features appear first and details appear last. Figure 7 shows stochastic predictions $\\\\mathbf{x}_0 \\\\sim p_\\\\theta(\\\\mathbf{x}_0|\\\\mathbf{x}_t)$ with $\\\\mathbf{x}_t$ frozen for various $t$. When $t$ is small, all but fine details are preserved, and when $t$ is large, only large scale features are preserved. Perhaps these are hints of conceptual compression [18].\\n\\n![img-7.jpeg](img-7.jpeg)\\nFigure 6: Unconditional CIFAR10 progressive generation ($\\\\hat{\\\\mathbf{x}}_0$ over time, from left to right). Extended samples and sample quality metrics over time in the appendix (Figs. 10 and 14).\\n\\n![img-8.jpeg](img-8.jpeg)\\nFigure 7: When conditioned on the same latent, CelebA-HQ $256 \\\\times 256$ samples share high-level attributes. Bottom-right quadrants are $\\\\mathbf{x}_t$, and other quadrants are samples from $p_\\\\theta(\\\\mathbf{x}_0|\\\\mathbf{x}_t)$.\\n\\nConnection to autoregressive decoding Note that the variational bound (5) can be rewritten as:\\n\\n$$\\nL = D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_T) \\\\| p(\\\\mathbf{x}_T)) + \\\\mathbb{E}_q \\\\left[ \\\\sum_{t \\\\geq 1} D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t) \\\\| p_\\\\theta(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t)) \\\\right] + H(\\\\mathbf{x}_0) \\\\tag{16}\\n$$\\n\\n(See Appendix A for a derivation.) Now consider setting the diffusion process length $T$ to the dimensionality of the data, defining the forward process so that $q(\\\\mathbf{x}_t|\\\\mathbf{x}_0)$ places all probability mass on $\\\\mathbf{x}_0$ with the first $t$ coordinates masked out (i.e. $q(\\\\mathbf{x}_t|\\\\mathbf{x}_{t-1})$ masks out the $t^{\\\\text{th}}$ coordinate), setting $p(\\\\mathbf{x}_T)$ to place all mass on a blank image, and, for the sake of argument, taking $p_\\\\theta(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t)$ to\\n\\n![img-9.jpeg](img-9.jpeg)\\nFigure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.\\n\\n![img-10.jpeg](img-10.jpeg)\\n\\nbe a fully expressive conditional distribution. With these choices,  $D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_T) \\\\parallel p(\\\\mathbf{x}_T)) = 0$ , and minimizing  $D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t) \\\\parallel p_\\\\theta(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_t))$  trains  $p_\\\\theta$  to copy coordinates  $t + 1, \\\\ldots, T$  unchanged and to predict the  $t^{\\\\text{th}}$  coordinate given  $t + 1, \\\\ldots, T$ . Thus, training  $p_\\\\theta$  with this particular diffusion is training an autoregressive model.\\n\\nWe can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with a generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has shown that such reorderings introduce inductive biases that have an impact on sample quality [38], so we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since Gaussian noise might be more natural to add to images compared to masking noise. Moreover, the Gaussian diffusion length is not restricted to equal the data dimension; for instance, we use  $T = 1000$ , which is less than the dimension of the  $32 \\\\times 32 \\\\times 3$  or  $256 \\\\times 256 \\\\times 3$  images in our experiments. Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.\\n\\n# 4.4 Interpolation\\n\\nWe can interpolate source images  $\\\\mathbf{x}_0, \\\\mathbf{x}_0\\' \\\\sim q(\\\\mathbf{x}_0)$  in latent space using  $q$  as a stochastic encoder,  $\\\\mathbf{x}_t, \\\\mathbf{x}_t\\' \\\\sim q(\\\\mathbf{x}_t | \\\\mathbf{x}_0)$ , then decoding the linearly interpolated latent  $\\\\bar{\\\\mathbf{x}}_t = (1 - \\\\lambda) \\\\mathbf{x}_0 + \\\\lambda \\\\mathbf{x}_0\\'$  into image space by the reverse process,  $\\\\bar{\\\\mathbf{x}}_0 \\\\sim p(\\\\mathbf{x}_0 | \\\\bar{\\\\mathbf{x}}_t)$ . In effect, we use the reverse process to remove artifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8 (left). We fixed the noise for different values of  $\\\\lambda$  so  $\\\\mathbf{x}_t$  and  $\\\\mathbf{x}_t\\'$  remain the same. Fig. 8 (right) shows interpolations and reconstructions of original CelebA-HQ  $256 \\\\times 256$  images ( $t = 500$ ). The reverse process produces high-quality reconstructions, and plausible interpolations that smoothly vary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger  $t$  results in coarser and more varied interpolations, with novel samples at  $t = 1000$  (Appendix Fig. 9).\\n\\n# 5 Related Work\\n\\nWhile diffusion models might resemble flows [9, 46, 10, 32, 5, 16, 23] and VAEs [33, 47, 37], diffusion models are designed so that  $q$  has no parameters and the top-level latent  $\\\\mathbf{x}_T$  has nearly zero mutual information with the data  $\\\\mathbf{x}_0$ . Our  $\\\\epsilon$ -prediction reverse process parameterization establishes a connection between diffusion models and denoising score matching over multiple noise levels with annealed Langevin dynamics for sampling [55, 56]. Diffusion models, however, admit straightforward log likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler using variational inference (see Appendix C for details). The connection also has the reverse implication that a certain weighted form of denoising score matching is the same as variational inference to train a Langevin-like sampler. Other methods for learning transition operators of Markov chains include infusion training [2], variational walkback [15], generative stochastic networks [1], and others [50, 54, 36, 42, 35, 65].\\n\\nBy the known connection between score matching and energy-based modeling, our work could have implications for other recent work on energy-based models [67-69, 12, 70, 13, 11, 41, 17, 8]. Our rate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent of how rate-distortion curves can be computed over distortion penalties in one run of annealed importance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW and related models [18, 40] and may also lead to more general designs for subscale orderings or sampling strategies for autoregressive models [38, 64].\\n\\n6 Conclusion\\n\\nWe have presented high quality image samples using diffusion models, and we have found connections among diffusion models and variational inference for training Markov chains, denoising score matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive models, and progressive lossy compression. Since diffusion models seem to have excellent inductive biases for image data, we look forward to investigating their utility in other data modalities and as components in other types of generative models and machine learning systems.\\n\\n## Broader Impact\\n\\nOur work on diffusion models takes on a similar scope as existing work on other types of deep generative models, such as efforts to improve the sample quality of GANs, flows, autoregressive models, and so forth. Our paper represents progress in making diffusion models a generally useful tool in this family of techniques, so it may serve to amplify any impacts that generative models have had (and will have) on the broader world.\\n\\nUnfortunately, there are numerous well-known malicious uses of generative models. Sample generation techniques can be employed to produce fake images and videos of high profile figures for political purposes. While fake images were manually created long before software tools were available, generative models such as ours make the process easier. Fortunately, CNN-generated images currently have subtle flaws that allow detection *[62]*, but improvements in generative models may make this more difficult. Generative models also reflect the biases in the datasets on which they are trained. As many large datasets are collected from the internet by automated systems, it can be difficult to remove these biases, especially when the images are unlabeled. If samples from generative models trained on these datasets proliferate throughout the internet, then these biases will only be reinforced further.\\n\\nOn the other hand, diffusion models may be useful for data compression, which, as data becomes higher resolution and as global internet traffic increases, might be crucial to ensure accessibility of the internet to wide audiences. Our work might contribute to representation learning on unlabeled raw data for a large range of downstream tasks, from image classification to reinforcement learning, and diffusion models might also become viable for creative uses in art, photography, and music.\\n\\n## Acknowledgments and Disclosure of Funding\\n\\nThis work was supported by ONR PECASE and the NSF Graduate Research Fellowship under grant number DGE-1752814. Google’s TensorFlow Research Cloud (TFRC) provided Cloud TPUs.\\n\\n## References\\n\\n- [1] Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and Pascal Vincent. GSNs: generative stochastic networks. Information and Inference: A Journal of the IMA, 5(2):210–249, 2016.\\n- [2] Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion training. In International Conference on Learning Representations, 2017.\\n- [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019.\\n- [4] Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua Bengio. Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling. arXiv preprint arXiv:2003.06060, 2020.\\n- [5] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Processing Systems, pages 6571–6583, 2018.\\n- [6] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved autoregressive generative model. In International Conference on Machine Learning, pages 863–871, 2018.\\n- [7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.\\n\\n[8] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual energy-based models for text generation. arXiv preprint arXiv:2004.11714, 2020.\\n- [9] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014.\\n- [10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint arXiv:1605.08803, 2016.\\n- [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in Neural Information Processing Systems, pages 3603–3613, 2019.\\n- [12] Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative ConvNets via multi-grid modeling and sampling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9155–9164, 2018.\\n- [13] Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow contrastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7518–7528, 2020.\\n- [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672–2680, 2014.\\n- [15] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net. In Advances in Neural Information Processing Systems, pages 4392–4402, 2017.\\n- [16] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. FFJORD: Free-form continuous dynamics for scalable reversible generative models. In International Conference on Learning Representations, 2019.\\n- [17] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In International Conference on Learning Representations, 2020.\\n- [18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In Advances In Neural Information Processing Systems, pages 3549–3557, 2016.\\n- [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007.\\n- [20] Marton Havasi, Robert Peharz, and José Miguel Hernández-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. In International Conference on Learning Representations, 2019.\\n- [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637, 2017.\\n- [22] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017.\\n- [23] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-based generative models with variational dequantization and architecture design. In International Conference on Machine Learning, 2019.\\n- [24] Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression rates of deep generative models. In International Conference on Machine Learning, 2020.\\n- [25] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks. In International Conference on Machine Learning, pages 1771–1779, 2017.\\n- [26] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. In International Conference on Machine Learning, pages 2410–2419, 2018.\\n- [27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018.\\n- [28] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\\n\\n4401–4410, 2019.\\n- [29] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676v1, 2020.\\n- [30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8110–8119, 2020.\\n- [31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.\\n- [32] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems, pages 10215–10224, 2018.\\n- [33] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.\\n- [34] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pages 4743–4751, 2016.\\n- [35] John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with sampler-induced distributions. In Advances in Neural Information Processing Systems, pages 8501–8513, 2019.\\n- [36] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with neural networks. In International Conference on Learning Representations, 2018.\\n- [37] Lars Maaløe, Marco Fraccaro, Valentin Liévin, and Ole Winther. BIVA: A very deep hierarchy of latent variables for generative modeling. In Advances in Neural Information Processing Systems, pages 6548–6558, 2019.\\n- [38] Jacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. In International Conference on Learning Representations, 2019.\\n- [39] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018.\\n- [40] Alex Nichol. VQ-DRAW: A sequential discrete VAE. arXiv preprint arXiv:2003.01599, 2020.\\n- [41] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of MCMC-based maximum likelihood learning of energy-based models. arXiv preprint arXiv:1903.12370, 2019.\\n- [42] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent short-run MCMC toward energy-based model. In Advances in Neural Information Processing Systems, pages 5233–5243, 2019.\\n- [43] Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative modeling. In International Conference on Machine Learning, pages 3936–3945, 2018.\\n- [44] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. WaveGlow: A flow-based generative network for speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3617–3621. IEEE, 2019.\\n- [45] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In Advances in Neural Information Processing Systems, pages 14837–14847, 2019.\\n- [46] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning, pages 1530–1538, 2015.\\n- [47] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pages 1278–1286, 2014.\\n- [48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234–241. Springer, 2015.\\n- [49] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901–909, 2016.\\n- [50] Tim Salimans, Diederik Kingma, and Max Welling. Markov Chain Monte Carlo and variational inference: Bridging the gap. In International Conference on Machine Learning, pages 1218–1226, 2015.\\n\\n[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234–2242, 2016.\\n- [52] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations, 2017.\\n- [53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265, 2015.\\n- [54] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-NICE-MC: Adversarial training for MCMC. In Advances in Neural Information Processing Systems, pages 5140–5150, 2017.\\n- [55] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pages 11895–11907, 2019.\\n- [56] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. arXiv preprint arXiv:2006.09011, 2020.\\n- [57] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.\\n- [58] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. International Conference on Machine Learning, 2016.\\n- [59] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with PixelCNN decoders. In Advances in Neural Information Processing Systems, pages 4790–4798, 2016.\\n- [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008, 2017.\\n- [61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661–1674, 2011.\\n- [62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot…for now. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.\\n- [63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7794–7803, 2018.\\n- [64] Auke J Wiggers and Emiel Hoogeboom. Predictive sampling with forecasting autoregressive models. arXiv preprint arXiv:2002.09928, 2020.\\n- [65] Hao Wu, Jonas Köhler, and Frank Noé. Stochastic normalizing flows. arXiv preprint arXiv:2002.06707, 2020.\\n- [66] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3–19, 2018.\\n- [67] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In International Conference on Machine Learning, pages 2635–2644, 2016.\\n- [68] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7093–7101, 2017.\\n- [69] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning descriptor networks for 3d shape synthesis and analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8629–8638, 2018.\\n- [70] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning energy-based spatial-temporal generative convnets for dynamic patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\\n- [71] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\\n- [72] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\\n\\n# Extra information\\n\\nLSUN FID scores for LSUN datasets are included in Table 3. Scores marked with * are reported by StyleGAN2 as baselines, and other scores are reported by their respective authors.\\n\\nTable 3: FID scores for LSUN  ${256} \\\\times  {256}$  datasets\\n\\n|  Model | LSUN Bedroom | LSUN Church | LSUN Cat  |\\n| --- | --- | --- | --- |\\n|  ProgressiveGAN [27] | 8.34 | 6.42 | 37.52  |\\n|  StyleGAN [28] | 2.65 | 4.21* | 8.53*  |\\n|  StyleGAN2 [30] | - | 3.86 | 6.93  |\\n|  Ours (Lsimple) | 6.36 | 7.89 | 19.75  |\\n|  Ours (Lsimple, large) | 4.90 | - | -  |\\n\\nProgressive compression Our lossy compression argument in Section 4.3 is only a proof of concept, because Algorithms 3 and 4 depend on a procedure such as minimal random coding [20], which is not tractable for high dimensional data. These algorithms serve as a compression interpretation of the variational bound (5) of Sohl-Dickstein et al. [53], not yet as a practical compression system.\\n\\nTable 4: Unconditional CIFAR10 test set rate-distortion values (accompanies Fig. 5)\\n\\n|  Reverse process time (T-t+1) | Rate (bits/dim) | Distortion (RMSE [0, 255])  |\\n| --- | --- | --- |\\n|  1000 | 1.77581 | 0.95136  |\\n|  900 | 0.11994 | 12.02277  |\\n|  800 | 0.05415 | 18.47482  |\\n|  700 | 0.02866 | 24.43656  |\\n|  600 | 0.01507 | 30.80948  |\\n|  500 | 0.00716 | 38.03236  |\\n|  400 | 0.00282 | 46.12765  |\\n|  300 | 0.00081 | 54.18826  |\\n|  200 | 0.00013 | 60.97170  |\\n|  100 | 0.00000 | 67.60125  |\\n\\n# A Extended derivations\\n\\nBelow is a derivation of Eq. (5), the reduced variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. [53]; we include it here only for completeness.\\n\\n$$\\n\\\\begin{array}{l} L = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0 : T}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {1 : T} \\\\mid \\\\mathbf {x} _ {0}\\\\right)} \\\\right] \\\\tag {17} \\\\\\\\ = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log p \\\\left(\\\\mathbf {x} _ {T}\\\\right) - \\\\sum_ {t \\\\geq 1} \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {t - 1}\\\\right)} \\\\right] \\\\tag {18} \\\\\\\\ = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log p \\\\left(\\\\mathbf {x} _ {T}\\\\right) - \\\\sum_ {t &gt; 1} \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {t} \\\\mid \\\\mathbf {x} _ {t - 1}\\\\right)} - \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0} \\\\mid \\\\mathbf {x} _ {1}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {1} \\\\mid \\\\mathbf {x} _ {0}\\\\right)} \\\\right] \\\\tag {19} \\\\\\\\ = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log p (\\\\mathbf {x} _ {T}) - \\\\sum_ {t &gt; 1} \\\\log \\\\frac {p _ {\\\\theta} (\\\\mathbf {x} _ {t - 1} | \\\\mathbf {x} _ {t})}{q (\\\\mathbf {x} _ {t - 1} | \\\\mathbf {x} _ {t} , \\\\mathbf {x} _ {0})} \\\\cdot \\\\frac {q (\\\\mathbf {x} _ {t - 1} | \\\\mathbf {x} _ {0})}{q (\\\\mathbf {x} _ {t} | \\\\mathbf {x} _ {0})} - \\\\log \\\\frac {p _ {\\\\theta} (\\\\mathbf {x} _ {0} | \\\\mathbf {x} _ {1})}{q (\\\\mathbf {x} _ {1} | \\\\mathbf {x} _ {0})} \\\\right] \\\\tag {20} \\\\\\\\ = \\\\mathbb {E} _ {q} \\\\left[ - \\\\log \\\\frac {p \\\\left(\\\\mathbf {x} _ {T}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {T} \\\\mid \\\\mathbf {x} _ {0}\\\\right)} - \\\\sum_ {t &gt; 1} \\\\log \\\\frac {p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t}\\\\right)}{q \\\\left(\\\\mathbf {x} _ {t - 1} \\\\mid \\\\mathbf {x} _ {t} , \\\\mathbf {x} _ {0}\\\\right)} - \\\\log p _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {0} \\\\mid \\\\mathbf {x} _ {1}\\\\right) \\\\right] \\\\tag {21} \\\\\\\\ \\\\end{array}\\n$$\\n\\n$=\\\\mathbb{E}_{q}\\\\Bigg{[}D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{T}|\\\\mathbf{x}_{0})\\\\parallel p(\\\\mathbf{x}_{T}))+\\\\sum_{t>1}D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t},\\\\mathbf{x}_{0})\\\\parallel p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t}))-\\\\log p_{\\\\theta}(\\\\mathbf{x}_{0}|\\\\mathbf{x}_{1})\\\\Bigg{]}$ (22)\\n\\nThe following is an alternate version of $L$. It is not tractable to estimate, but it is useful for our discussion in Section 4.3.\\n\\n$L$ $=\\\\mathbb{E}_{q}\\\\Bigg{[}-\\\\log p(\\\\mathbf{x}_{T})-\\\\sum_{t\\\\geq 1}\\\\log\\\\frac{p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})}{q(\\\\mathbf{x}_{t}|\\\\mathbf{x}_{t-1})}\\\\Bigg{]}$ (23)\\n$=\\\\mathbb{E}_{q}\\\\Bigg{[}-\\\\log p(\\\\mathbf{x}_{T})-\\\\sum_{t\\\\geq 1}\\\\log\\\\frac{p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})}{q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})}\\\\cdot\\\\frac{q(\\\\mathbf{x}_{t-1})}{q(\\\\mathbf{x}_{t})}\\\\Bigg{]}$ (24)\\n$=\\\\mathbb{E}_{q}\\\\Bigg{[}-\\\\log\\\\frac{p(\\\\mathbf{x}_{T})}{q(\\\\mathbf{x}_{T})}-\\\\sum_{t\\\\geq 1}\\\\log\\\\frac{p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})}{q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})}-\\\\log q(\\\\mathbf{x}_{0})\\\\Bigg{]}$ (25)\\n$=D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{T})\\\\parallel p(\\\\mathbf{x}_{T}))+\\\\mathbb{E}_{q}\\\\Bigg{[}\\\\sum_{t\\\\geq 1}D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t})\\\\parallel p_{\\\\theta}(\\\\mathbf{x}_{t-1}|\\\\mathbf{x}_{t}))\\\\Bigg{]}+H(\\\\mathbf{x}_{0})$ (26)\\n\\n## Appendix B Experimental details\\n\\nOur neural network architecture follows the backbone of PixelCNN++ *[52]*, which is a U-Net *[48]* based on a Wide ResNet *[72]*. We replaced weight normalization *[49]* with group normalization *[66]* to make the implementation simpler. Our $32\\\\times 32$ models use four feature map resolutions ($32\\\\times 32$ to $4\\\\times 4$), and our $256\\\\times 256$ models use six. All models have two convolutional residual blocks per resolution level and self-attention blocks at the $16\\\\times 16$ resolution between the convolutional blocks *[6]*. Diffusion time $t$ is specified by adding the Transformer sinusoidal position embedding *[60]* into each residual block. Our CIFAR10 model has 35.7 million parameters, and our LSUN and CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.\\n\\nWe used TPU v3-8 (similar to 8 V100 GPUs) for all experiments. Our CIFAR model trains at 21 steps per second at batch size 128 (10.6 hours to train to completion at 800k steps), and sampling a batch of 256 images takes 17 seconds. Our CelebA-HQ/LSUN (256^{2}) models train at 2.2 steps per second at batch size 64, and sampling a batch of 128 images takes 300 seconds. We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps. The larger LSUN Bedroom model was trained for 1.15M steps.\\n\\nApart from an initial choice of hyperparameters early on to make network size fit within memory constraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample quality, then transferred the resulting settings over to the other datasets:\\n\\n- We chose the $\\\\beta_{t}$ schedule from a set of constant, linear, and quadratic schedules, all constrained so that $L_{T}\\\\approx 0$. We set $T=1000$ without a sweep, and we chose a linear schedule from $\\\\beta_{1}=10^{-4}$ to $\\\\beta_{T}=0.02$.\\n- We set the dropout rate on CIFAR10 to $0.1$ by sweeping over the values $\\\\{0.1,0.2,0.3,0.4\\\\}$. Without dropout on CIFAR10, we obtained poorer samples reminiscent of the overfitting artifacts in an unregularized PixelCNN++ *[52]*. We set dropout rate on the other datasets to zero without sweeping.\\n- We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly. We also used random horizontal flips for all other datasets except LSUN Bedroom.\\n- We tried Adam *[31]* and RMSProp early on in our experimentation process and chose the former. We left the hyperparameters to their standard values. We set the learning rate to $2\\\\times 10^{-4}$ without any sweeping, and we lowered it to $2\\\\times 10^{-5}$ for the $256\\\\times 256$ images, which seemed unstable to train with the larger learning rate.\\n\\n- We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over these values.\\n- We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over this value.\\n\\nFinal experiments were trained once and evaluated throughout training for sample quality. Sample quality scores and log likelihood are reported on the minimum FID value over the course of training. On CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code from the OpenAI *[51]* and TTUR *[21]* repositories, respectively. On LSUN, we calculated FID scores on 50000 samples using code from the StyleGAN2 *[30]* repository. CIFAR10 and CelebA-HQ were loaded as provided by TensorFlow Datasets (https://www.tensorflow.org/datasets), and LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard from the papers that introduced their usage in a generative modeling context. All details can be found in the source code release.\\n\\n## Appendix C Discussion on related work\\n\\nOur model architecture, forward process definition, and prior differ from NCSN *[55, 56]* in subtle but important ways that improve sample quality, and, notably, we directly train our sampler as a latent variable model rather than adding it after training post-hoc. In greater detail:\\n\\n1. We use a U-Net with self-attention; NCSN uses a RefineNet with dilated convolutions. We condition all layers on $t$ by adding in the Transformer sinusoidal position embedding, rather than only in normalization layers (NCSNv1) or only at the output (v2).\\n2. Diffusion models scale down the data with each forward process step (by a $\\\\sqrt{1-\\\\beta_{t}}$ factor) so that variance does not grow when adding noise, thus providing consistently scaled inputs to the neural net reverse process. NCSN omits this scaling factor.\\n3. Unlike NCSN, our forward process destroys signal ($D_{\\\\mathrm{KL}}(q(\\\\mathbf{x}_{T}|\\\\mathbf{x}_{0})\\\\parallel\\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I}))\\\\approx 0$), ensuring a close match between the prior and aggregate posterior of $\\\\mathbf{x}_{T}$. Also unlike NCSN, our $\\\\beta_{t}$ are very small, which ensures that the forward process is reversible by a Markov chain with conditional Gaussians. Both of these factors prevent distribution shift when sampling.\\n4. Our Langevin-like sampler has coefficients (learning rate, noise scale, etc.) derived rigorously from $\\\\beta_{t}$ in the forward process. Thus, our training procedure directly trains our sampler to match the data distribution after $T$ steps: it trains the sampler as a latent variable model using variational inference. In contrast, NCSN’s sampler coefficients are set by hand post-hoc, and their training procedure is not guaranteed to directly optimize a quality metric of their sampler.\\n\\n## Appendix D Samples\\n\\n##### Additional samples\\n\\nFigure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion models trained on CelebA-HQ, CIFAR10 and LSUN datasets.\\n\\n##### Latent structure and reverse process stochasticity\\n\\nDuring sampling, both the prior $\\\\mathbf{x}_{T}\\\\sim\\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$ and Langevin dynamics are stochastic. To understand the significance of the second source of noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA $256\\\\times 256$ dataset. Figure 7 shows multiple draws from the reverse process $\\\\mathbf{x}_{0}\\\\sim p_{\\\\theta}(\\\\mathbf{x}_{0}|\\\\mathbf{x}_{t})$ that share the latent $\\\\mathbf{x}_{t}$ for $t\\\\in\\\\{1000,750,500,250\\\\}$. To accomplish this, we run a single reverse chain from an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple images. When the chain is split after the prior draw at $\\\\mathbf{x}_{T=1000}$, the samples differ significantly. However, when the chain is split after more steps, samples share high-level attributes like gender, hair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents like $\\\\mathbf{x}_{750}$ encode these attributes, despite their imperceptibility.\\n\\n##### Coarse-to-fine interpolation\\n\\nFigure 9 shows interpolations between a pair of source CelebA $256\\\\times 256$ images as we vary the number of diffusion steps prior to latent space interpolation. Increasing the number of diffusion steps destroys more structure in the source images, which the\\n\\nmodel completes during the reverse process. This allows us to interpolate at both fine granularities and coarse granularities. In the limiting case of 0 diffusion steps, the interpolation mixes source images in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and interpolations are novel samples.\\n\\n![img-11.jpeg](img-11.jpeg)\\nFigure 9: Coarse-to-fine interpolations that vary the number of diffusion steps prior to latent mixing.\\n\\n![img-12.jpeg](img-12.jpeg)\\nFigure 10: Unconditional CIFAR10 progressive sampling quality over time\\n\\n![img-13.jpeg](img-13.jpeg)\\n\\n![img-14.jpeg](img-14.jpeg)\\nFigure 11: CelebA-HQ  $256 \\\\times 256$  generated samples\\n\\n![img-15.jpeg](img-15.jpeg)\\n\\n![img-16.jpeg](img-16.jpeg)\\n(a) Pixel space nearest neighbors\\n(b) Inception feature space nearest neighbors\\nFigure 12: CelebA-HQ  $256 \\\\times 256$  nearest neighbors, computed on a  $100 \\\\times 100$  crop surrounding the faces. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.\\n\\n![img-17.jpeg](img-17.jpeg)\\nFigure 13: Unconditional CIFAR10 generated samples\\n\\n![img-18.jpeg](img-18.jpeg)\\nFigure 14: Unconditional CIFAR10 progressive generation\\n\\n![img-19.jpeg](img-19.jpeg)\\n\\n![img-20.jpeg](img-20.jpeg)\\n(a) Pixel space nearest neighbors\\n(b) Inception feature space nearest neighbors\\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.\\n\\n![img-21.jpeg](img-21.jpeg)\\nFigure 16: LSUN Church generated samples. FID=7.89\\n\\n![img-22.jpeg](img-22.jpeg)\\nFigure 17: LSUN Bedroom generated samples, large model. FID=4.90\\n\\n![img-23.jpeg](img-23.jpeg)\\nFigure 18: LSUN Bedroom generated samples, small model. FID=6.36\\n\\n![img-24.jpeg](img-24.jpeg)\\nFigure 19: LSUN Cat generated samples. FID=19.75'), -0.0004020638205228799), (Document(id='e502d1a1cb3d:0', metadata={'end_line': 386, 'text': \"# Auto-Encoding Variational Bayes\\n\\nDiederik P. Kingma\\nMachine Learning Group\\nUniversiteit van Amsterdam\\ndpkingma@gmail.com\\nMax Welling\\nMachine Learning Group\\nUniversiteit van Amsterdam\\nwelling.max@gmail.com\\n\\n###### Abstract\\n\\nHow can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.\\n\\n## 1 Introduction\\n\\nHow can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions? The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.\\n\\nFor the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder.\\n\\n## 2 Method\\n\\nThe strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example,\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: The type of directed graphical model under consideration. Solid lines denote the generative model  $p_{\\\\theta}(\\\\mathbf{z})p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})$ , dashed lines denote the variational approximation  $q_{\\\\phi}(\\\\mathbf{z}|\\\\mathbf{x})$  to the intractable posterior  $p_{\\\\theta}(\\\\mathbf{z}|\\\\mathbf{x})$ . The variational parameters  $\\\\phi$  are learned jointly with the generative model parameters  $\\\\theta$ .\\n\\nstraightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity.\\n\\n# 2.1 Problem scenario\\n\\nLet us consider some dataset  $\\\\mathbf{X} = \\\\{\\\\mathbf{x}^{(i)}\\\\}_{i=1}^{N}$  consisting of  $N$  i.i.d. samples of some continuous or discrete variable  $\\\\mathbf{x}$ . We assume that the data are generated by some random process, involving an unobserved continuous random variable  $\\\\mathbf{z}$ . The process consists of two steps: (1) a value  $\\\\mathbf{z}^{(i)}$  is generated from some prior distribution  $p_{\\\\theta^*}(\\\\mathbf{z})$ ; (2) a value  $\\\\mathbf{x}^{(i)}$  is generated from some conditional distribution  $p_{\\\\theta^*}(\\\\mathbf{x}|\\\\mathbf{z})$ . We assume that the prior  $p_{\\\\theta^*}(\\\\mathbf{z})$  and likelihood  $p_{\\\\theta^*}(\\\\mathbf{x}|\\\\mathbf{z})$  come from parametric families of distributions  $p_{\\\\theta}(\\\\mathbf{z})$  and  $p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})$ , and that their PDFs are differentiable almost everywhere w.r.t. both  $\\\\theta$  and  $\\\\mathbf{z}$ . Unfortunately, a lot of this process is hidden from our view: the true parameters  $\\\\theta^*$  as well as the values of the latent variables  $\\\\mathbf{z}^{(i)}$  are unknown to us.\\n\\nVery importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:\\n\\n1. Intractability: the case where the integral of the marginal likelihood  $p_{\\\\theta}(\\\\mathbf{x}) = \\\\int p_{\\\\theta}(\\\\mathbf{z})p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})d\\\\mathbf{z}$  is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density  $p_{\\\\theta}(\\\\mathbf{z}|\\\\mathbf{x}) = p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})p_{\\\\theta}(\\\\mathbf{z}) / p_{\\\\theta}(\\\\mathbf{x})$  is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions  $p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})$ , e.g. a neural network with a nonlinear hidden layer.\\n2. A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.\\n\\nWe are interested in, and propose a solution to, three related problems in the above scenario:\\n\\n1. Efficient approximate ML or MAP estimation for the parameters  $\\\\theta$ . The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.\\n2. Efficient approximate posterior inference of the latent variable  $\\\\mathbf{z}$  given an observed value  $\\\\mathbf{x}$  for a choice of parameters  $\\\\theta$ . This is useful for coding or data representation tasks.\\n3. Efficient approximate marginal inference of the variable  $\\\\mathbf{x}$ . This allows us to perform all kinds of inference tasks where a prior over  $\\\\mathbf{x}$  is required. Common applications in computer vision include image denoising, inpainting and super-resolution.\\n\\nFor the purpose of solving the above problems, let us introduce a recognition model $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$: an approximation to the intractable true posterior $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x})$. Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters $\\\\bm{\\\\phi}$ are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters $\\\\bm{\\\\phi}$ jointly with the generative model parameters $\\\\bm{\\\\theta}$.\\n\\nFrom a coding theory perspective, the unobserved variables $\\\\mathbf{z}$ have an interpretation as a latent representation or *code*. In this paper we will therefore also refer to the recognition model $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ as a probabilistic *encoder*, since given a datapoint $\\\\mathbf{x}$ it produces a distribution (e.g. a Gaussian) over the possible values of the code $\\\\mathbf{z}$ from which the datapoint $\\\\mathbf{x}$ could have been generated. In a similar vein we will refer to $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$ as a probabilistic *decoder*, since given a code $\\\\mathbf{z}$ it produces a distribution over the possible corresponding values of $\\\\mathbf{x}$.\\n\\n### 2.2 The variational bound\\n\\nThe marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints $\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(1)},\\\\cdots,\\\\mathbf{x}^{(N)})=\\\\sum_{i=1}^{N}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})$, which can each be rewritten as:\\n\\n$\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})=D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)}))+\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ (1)\\n\\nThe first RHS term is the KL divergence of the approximate from the true posterior. Since this KL-divergence is non-negative, the second RHS term $\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ is called the (variational) *lower bound* on the marginal likelihood of datapoint $i$, and can be written as:\\n\\n$\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})\\\\geq\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})=\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})}\\\\left[-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})+\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x},\\\\mathbf{z})\\\\right]$ (2)\\n\\nwhich can also be written as:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})=-D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}))+\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})}\\\\left[\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z})\\\\right]$ (3)\\n\\nWe want to differentiate and optimize the lower bound $\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ w.r.t. both the variational parameters $\\\\bm{\\\\phi}$ and generative parameters $\\\\bm{\\\\theta}$. However, the gradient of the lower bound w.r.t. $\\\\bm{\\\\phi}$ is a bit problematic. The usual (naïve) Monte Carlo gradient estimator for this type of problem is: $\\\\nabla_{\\\\bm{\\\\phi}}\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})}\\\\left[f(\\\\mathbf{z})\\\\right]=\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})}\\\\left[f(\\\\mathbf{z})\\\\nabla_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})}\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})\\\\right]\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}f(\\\\mathbf{z})\\\\nabla_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}^{(l)})}\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}^{(l)})$ where $\\\\mathbf{z}^{(l)}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$. This gradient estimator exhibits exhibits very high variance (see e.g. *[x1]*) and is impractical for our purposes.\\n\\n### 2.3 The SGVB estimator and AEVB algorithm\\n\\nIn this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$, but please note that the technique can be applied to the case $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})$, i.e. where we do not condition on $\\\\mathbf{x}$, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix.\\n\\nUnder certain mild conditions outlined in section 2.4 for a chosen approximate posterior $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ we can reparameterize the random variable $\\\\widetilde{\\\\mathbf{z}}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ using a differentiable transformation $g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$ of an (auxiliary) noise variable $\\\\bm{\\\\epsilon}$:\\n\\n$\\\\widetilde{\\\\mathbf{z}}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})\\\\quad\\\\text{with}\\\\quad\\\\bm{\\\\epsilon}\\\\sim p(\\\\bm{\\\\epsilon})$ (4)\\n\\nSee section 2.4 for general strategies for chosing such an approriate distribution $p(\\\\bm{\\\\epsilon})$ and function $g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$. We can now form Monte Carlo estimates of expectations of some function $f(\\\\mathbf{z})$ w.r.t. $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ as follows:\\n\\n$\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})}\\\\left[f(\\\\mathbf{z})\\\\right]=\\\\mathbb{E}_{p(\\\\bm{\\\\epsilon})}\\\\left[f(g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x}^{(i)}))\\\\right]\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}f(g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon}^{(l)},\\\\mathbf{x}^{(i)}))\\\\quad\\\\text{where}\\\\quad\\\\bm{\\\\epsilon}^{(l)}\\\\sim p(\\\\bm{\\\\epsilon})$ (5)\\n\\nWe apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator $\\\\widetilde{\\\\mathcal{L}}^{A}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})\\\\simeq\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$:\\n\\n$\\\\widetilde{\\\\mathcal{L}}^{A}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ $=\\\\frac{1}{L}\\\\sum_{l=1}^{L}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z}^{(i,l)})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}^{(i,l)}|\\\\mathbf{x}^{(i)})$\\n$\\\\text{where}\\\\quad\\\\ \\\\mathbf{z}^{(i,l)}$ $=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon}^{(i,l)},\\\\mathbf{x}^{(i)})\\\\quad\\\\text{and}\\\\quad\\\\bm{\\\\epsilon}^{(l)}\\\\sim p(\\\\bm{\\\\epsilon})$ (6)\\n\\n###\\n\\nAlgorithm 1 Minibatch version of the Auto-Encoding VB (AEVB) algorithm. Either of the two SGVB estimators in section 2.3 can be used. We use settings $M=100$ and $L=1$ in experiments.\\n\\n$\\\\bm{\\\\theta},\\\\bm{\\\\phi}\\\\leftarrow$ Initialize parameters\\nrepeat\\n$\\\\mathbf{X}^{M}\\\\leftarrow$ Random minibatch of $M$ datapoints (drawn from full dataset)\\n$\\\\bm{\\\\epsilon}\\\\leftarrow$ Random samples from noise distribution $p(\\\\bm{\\\\epsilon})$\\n$\\\\mathbf{g}\\\\leftarrow\\\\nabla_{\\\\bm{\\\\theta},\\\\bm{\\\\phi}}\\\\widetilde{\\\\mathcal{L}}^{M}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{X}^{M},\\\\bm{\\\\epsilon})$ (Gradients of minibatch estimator (8))\\n$\\\\bm{\\\\theta},\\\\bm{\\\\phi}\\\\leftarrow$ Update parameters using gradients $\\\\mathbf{g}$ (e.g. SGD or Adagrad *[x10]*)\\nuntil convergence of parameters $(\\\\bm{\\\\theta},\\\\bm{\\\\phi})$\\nreturn $\\\\bm{\\\\theta},\\\\bm{\\\\phi}$\\n\\nOften, the KL-divergence $D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}))$ of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error $\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})}\\\\left[\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z})\\\\right]$ requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing $\\\\bm{\\\\phi}$, encouraging the approximate posterior to be close to the prior $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})$. This yields a second version of the SGVB estimator $\\\\widetilde{\\\\mathcal{L}}^{B}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})\\\\simeq\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$, corresponding to eq. (3), which typically has less variance than the generic estimator:\\n\\n$\\\\widetilde{\\\\mathcal{L}}^{B}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})=-D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}))+\\\\frac{1}{L}\\\\sum_{l=1}^{L}(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(i,l)}))$\\n$\\\\text{where}\\\\quad\\\\mathbf{z}^{(i,l)}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon}^{(i,l)},\\\\mathbf{x}^{(i)})\\\\quad\\\\text{and}\\\\quad\\\\bm{\\\\epsilon}^{(l)}\\\\sim p(\\\\bm{\\\\epsilon})$ (7)\\n\\nGiven multiple datapoints from a dataset $\\\\mathbf{X}$ with $N$ datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{X})\\\\simeq\\\\widetilde{\\\\mathcal{L}}^{M}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{X}^{M})=\\\\frac{N}{M}\\\\sum_{i=1}^{M}\\\\widetilde{\\\\mathcal{L}}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ (8)\\n\\nwhere the minibatch $\\\\mathbf{X}^{M}=\\\\{\\\\mathbf{x}^{(i)}\\\\}_{i=1}^{M}$ is a randomly drawn sample of $M$ datapoints from the full dataset $\\\\mathbf{X}$ with $N$ datapoints. In our experiments we found that the number of samples $L$ per datapoint can be set to $1$ as long as the minibatch size $M$ was large enough, e.g. $M=100$. Derivatives $\\\\nabla_{\\\\bm{\\\\theta},\\\\bm{\\\\phi}}\\\\widetilde{\\\\mathcal{L}}(\\\\bm{\\\\theta};\\\\mathbf{X}^{M})$ can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad *[x10]*. See algorithm 1 for a basic approach to compute the stochastic gradients.\\n\\nA connection with auto-encoders becomes clear when looking at the objective function given at eq. (7). The first term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error. The function $g_{\\\\bm{\\\\phi}}(.)$ is chosen such that it maps a datapoint $\\\\mathbf{x}^{(i)}$ and a random noise vector $\\\\bm{\\\\epsilon}^{(l)}$ to a sample from the approximate posterior for that datapoint: $\\\\mathbf{z}^{(i,l)}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon}^{(l)},\\\\mathbf{x}^{(i)})$ where $\\\\mathbf{z}^{(i,l)}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$. Subsequently, the sample $\\\\mathbf{z}^{(i,l)}$ is then input to function $\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(i,l)})$, which equals the probability density (or mass) of datapoint $\\\\mathbf{x}^{(i)}$ under the generative model, given $\\\\mathbf{z}^{(i,l)}$. This term is a negative *reconstruction error* in auto-encoder parlance.\\n\\n### 2.4 The reparameterization trick\\n\\nIn order to solve our problem we invoked an alternative method for generating samples from $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$. The essential parameterization trick is quite simple. Let $\\\\mathbf{z}$ be a continuous random variable, and $\\\\mathbf{z}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ be some conditional distribution. It is then often possible to express the random variable $\\\\mathbf{z}$ as a deterministic variable $\\\\mathbf{z}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$, where $\\\\bm{\\\\epsilon}$ is an auxiliary variable with independent marginal $p(\\\\bm{\\\\epsilon})$, and $g_{\\\\bm{\\\\phi}}(.)$ is some vector-valued function parameterized by $\\\\bm{\\\\phi}$.\\n\\nThis reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ such that the Monte Carlo estimate of the expectation is differentiable w.r.t. $\\\\bm{\\\\phi}$. A proof is as follows. Given the deterministic mapping $\\\\mathbf{z}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$ we know that $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\prod_{i}dz_{i}=p(\\\\bm{\\\\epsilon})\\\\prod_{i}d\\\\epsilon_{i}$. Therefore, $\\\\int q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})f(\\\\mathbf{z})\\\\,d\\\\mathbf{z}=\\\\int p(\\\\bm{\\\\epsilon})f(\\\\mathbf{z})\\\\,d\\\\bm{\\\\epsilon}=\\\\int p(\\\\bm{\\\\epsilon})f(g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x}))\\\\,d\\\\bm{\\\\epsilon}$. It follows\\n\\nthat a differentiable estimator can be constructed: $\\\\int q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})f(\\\\mathbf{z})\\\\,d\\\\mathbf{z}\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}f(g_{\\\\bm{\\\\phi}}(\\\\mathbf{x},\\\\bm{\\\\epsilon}^{(l)}))$ where $\\\\bm{\\\\epsilon}^{(l)}\\\\sim p(\\\\bm{\\\\epsilon})$. In section 2.3 we applied this trick to obtain a differentiable estimator of the variational lower bound.\\n\\nTake, for example, the univariate Gaussian case: let $z\\\\sim p(z|x)=\\\\mathcal{N}(\\\\mu,\\\\sigma^{2})$. In this case, a valid reparameterization is $z=\\\\mu+\\\\sigma\\\\epsilon$, where $\\\\epsilon$ is an auxiliary noise variable $\\\\epsilon\\\\sim\\\\mathcal{N}(0,1)$. Therefore, $\\\\mathbb{E}_{\\\\mathcal{N}(z;\\\\mu,\\\\sigma^{2})}\\\\left[f(z)\\\\right]=\\\\mathbb{E}_{\\\\mathcal{N}(\\\\epsilon;0,1)}\\\\left[f(\\\\mu+\\\\sigma\\\\epsilon)\\\\right]\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}f(\\\\mu+\\\\sigma\\\\epsilon^{(l)})$ where $\\\\epsilon^{(l)}\\\\sim\\\\mathcal{N}(0,1)$.\\n\\nFor which $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ can we choose such a differentiable transformation $g_{\\\\bm{\\\\phi}}(.)$ and auxiliary variable $\\\\bm{\\\\epsilon}\\\\sim p(\\\\bm{\\\\epsilon})$? Three basic approaches are:\\n\\n1. Tractable inverse CDF. In this case, let $\\\\bm{\\\\epsilon}\\\\sim\\\\mathcal{U}(\\\\mathbf{0},\\\\mathbf{I})$, and let $g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$ be the inverse CDF of $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$. Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz, Gumbel and Erlang distributions.\\n2. Analogous to the Gaussian example, for any ”location-scale” family of distributions we can choose the standard distribution (with location $=0$, scale $=1$) as the auxiliary variable $\\\\bm{\\\\epsilon}$, and let $g(.)=\\\\text{location}+\\\\text{scale}\\\\cdot\\\\bm{\\\\epsilon}$. Examples: Laplace, Elliptical, Student’s t, Logistic, Uniform, Triangular and Gaussian distributions.\\n3. Composition: It is often possible to express random variables as different transformations of auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed variable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted sum of Gamma variates), Beta, Chi-Squared, and F distributions.\\n\\nWhen all three approaches fail, good approximations to the inverse CDF exist requiring computations with time complexity comparable to the PDF (see e.g. *[x10]* for some methods).\\n\\n## 3 Example: Variational Auto-Encoder\\n\\nIn this section we’ll give an example where we use a neural network for the probabilistic encoder $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ (the approximation to the posterior of the generative model $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x},\\\\mathbf{z})$) and where the parameters $\\\\bm{\\\\phi}$ and $\\\\bm{\\\\theta}$ are optimized jointly with the AEVB algorithm.\\n\\nLet the prior over the latent variables be the centered isotropic multivariate Gaussian $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})=\\\\mathcal{N}(\\\\mathbf{z};\\\\mathbf{0},\\\\mathbf{I})$. Note that in this case, the prior lacks parameters. We let $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$ be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from $\\\\mathbf{z}$ with a MLP (a fully-connected neural network with a single hidden layer, see appendix C). Note the true posterior $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x})$ is in this case intractable. While there is much freedom in the form $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$, we’ll assume the true (but intractable) posterior takes on a approximate Gaussian form with an approximately diagonal covariance. In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure:\\n\\n$\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})=\\\\log\\\\mathcal{N}(\\\\mathbf{z};\\\\bm{\\\\mu}^{(i)},\\\\bm{\\\\sigma}^{2(i)}\\\\mathbf{I})$ (9)\\n\\nwhere the mean and s.d. of the approximate posterior, $\\\\bm{\\\\mu}^{(i)}$ and $\\\\bm{\\\\sigma}^{(i)}$, are outputs of the encoding MLP, i.e. nonlinear functions of datapoint $\\\\mathbf{x}^{(i)}$ and the variational parameters $\\\\bm{\\\\phi}$ (see appendix C).\\n\\nAs explained in section 2.4, we sample from the posterior $\\\\mathbf{z}^{(i,l)}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$ using $\\\\mathbf{z}^{(i,l)}=g_{\\\\bm{\\\\phi}}(\\\\mathbf{x}^{(i)},\\\\bm{\\\\epsilon}^{(l)})=\\\\bm{\\\\mu}^{(i)}+\\\\bm{\\\\sigma}^{(i)}\\\\odot\\\\bm{\\\\epsilon}^{(l)}$ where $\\\\bm{\\\\epsilon}^{(l)}\\\\sim\\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$. With $\\\\odot$ we signify an element-wise product. In this model both $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})$ (the prior) and $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ are Gaussian; in this case, we can use the estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation (see appendix B). The resulting estimator for this model and datapoint $\\\\mathbf{x}^{(i)}$ is:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})\\\\simeq\\\\frac{1}{2}\\\\sum_{j=1}^{J}\\\\left(1+\\\\log((\\\\sigma_{j}^{(i)})^{2})-(\\\\mu_{j}^{(i)})^{2}-(\\\\sigma_{j}^{(i)})^{2}\\\\right)+\\\\frac{1}{L}\\\\sum_{l=1}^{L}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(i,l)})$\\n$\\\\text{where}\\\\quad\\\\mathbf{z}^{(i,l)}=\\\\bm{\\\\mu}^{(i)}+\\\\bm{\\\\sigma}^{(i)}\\\\odot\\\\bm{\\\\epsilon}^{(l)}\\\\quad\\\\text{and}\\\\quad\\\\bm{\\\\epsilon}^{(l)}\\\\sim\\\\mathcal{N}(0,\\\\mathbf{I})$ (10)\\n\\nAs explained above and in appendix C, the decoding term $\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(i,l)})$ is a Bernoulli or Gaussian MLP, depending on the type of data we are modelling.\\n\\n4 Related work\\n\\nThe wake-sleep algorithm *[x10]* is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.\\n\\nStochastic variational inference *[x12]* has recently received increasing interest. Recently, *[x3]* introduced a control variate schemes to reduce the high variance of the naïve gradient estimator discussed in section 2.1, and applied to exponential family approximations of the posterior. In *[x22]* some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. In *[x23]*, a similar reparameterization as in this paper was used in an efficient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions.\\n\\nThe AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between *linear* auto-encoders and a certain class of generative linear-Gaussian models has long been known. In *[x24]* it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior $p(\\\\mathbf{z})=\\\\mathcal{N}(0,\\\\mathbf{I})$ and a conditional distribution $p(\\\\mathbf{x}|\\\\mathbf{z})=\\\\mathcal{N}(\\\\mathbf{x};\\\\mathbf{W}\\\\mathbf{z},\\\\epsilon\\\\mathbf{I})$, specifically the case with infinitesimally small $\\\\epsilon$.\\n\\nIn relevant recent work on autoencoders *[VLL^{+}10]* it was shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound (see the infomax principle *[x17]*) of the mutual information between input $X$ and latent representation $Z$. Maximizing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional entropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding model *[VLL^{+}10]*, i.e. the negative reconstrution error. However, it is well known that this reconstruction criterion is in itself not sufficient for learning useful representations *[x5]*. Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants *[x5]*. The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regularization hyperparameter required to learn useful representations. Related are also encoder-decoder architectures such as the predictive sparse decomposition (PSD) *[x16]*, from which we drew some inspiration. Also relevant are the recently introduced Generative Stochastic Networks *[x6]* where noisy auto-encoders learn the transition operator of a Markov chain that samples from the data distribution. In *[x25]* a recognition model was employed for efficient learning with Deep Boltzmann Machines. These methods are targeted at either unnormalized models (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm for learning a general class of directed probabilistic models.\\n\\nThe recently proposed DARN method *[x11]*, also learns a directed probabilistic model using an auto-encoding structure, however their method applies to binary latent variables. Even more recently, *[x20]* also make the connection between auto-encoders, directed proabilistic models and stochastic variational inference using the reparameterization trick we describe in this paper. Their work was developed independently of ours and provides an additional perspective on AEVB.\\n\\n## 5 Experiments\\n\\nWe trained generative models of images from the MNIST and Frey Face datasets and compared learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.\\n\\nThe generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval $(0,1)$ using a sigmoidal activation function at the\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the lower bound, for different dimensionality of latent space  $(N_{\\\\mathbf{z}})$ . Our method converged considerably faster and reached a better solution in all experiments. Interestingly enough, more latent variables does not result in more overfitting, which is explained by the regularizing effect of the lower bound. Vertical axis: the estimated average variational lower bound per datapoint. The estimator variance was small  $(&lt; 1)$  and omitted. Horizontal axis: amount of training points evaluated. Computation took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an effective 40 GFLOPS.\\n\\ndecoder output. Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder.\\n\\nParameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator  $\\\\nabla_{\\\\theta, \\\\phi} \\\\mathcal{L}(\\\\theta, \\\\phi; \\\\mathbf{X})$  (see algorithm 1), plus a small weight decay term corresponding to a prior  $p(\\\\theta) = \\\\mathcal{N}(0, \\\\mathbf{I})$ . Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.\\n\\nWe compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational autoencoder. All parameters, both variational and generative, were initialized by random sampling from  $\\\\mathcal{N}(0,0.01)$ , and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from  $\\\\{0.01, 0.02, 0.1\\\\}$  based on performance on the training set in the first few iterations. Minibatches of size  $M = 100$  were used, with  $L = 1$  samples per datapoint.\\n\\nLikelihood lower bound We trained generative models (decoders) and corresponding encoders (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent overfitting, since it is a considerably smaller dataset). The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. Figure 2 shows the results when comparing the lower bounds. Interestingly, superfluous latent variables did not result in overfitting, which is explained by the regularizing nature of the variational bound.\\n\\nMarginal likelihood For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the estimated marginal likelihood, for a different number of training points. Monte Carlo EM is not an on-line algorithm, and (unlike AEVB and the wake-sleep method) can't be applied efficiently for the full MNIST dataset.\\n\\nVisualisation of high-dimensional data If we choose a low-dimensional latent space (e.g. 2D), we can use the learned encoders (recognition model) to project high-dimensional data to a low-dimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST and Frey Face datasets.\\n\\n# 6 Conclusion\\n\\nWe have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results.\\n\\n# 7 Future work\\n\\nSince the SGVB estimator and the AEVB algorithm can be applied to almost any inference and learning problem with continuous latent variables, there are plenty of future directions: (i) learning hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models with latent variables, useful for learning complicated noise distributions.\\n\\nReferences\\n\\n- [BCV13] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. 2013.\\n- [BJP12] David M Blei, Michael I Jordan, and John W Paisley. Variational Bayesian inference with Stochastic Search. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1367–1374, 2012.\\n- [BTL13] Yoshua Bengio and Éric Thibodeau-Laufer. Deep generative stochastic networks trainable by backprop. arXiv preprint arXiv:1306.1091, 2013.\\n- [Dev86] Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation, pages 260–265. ACM, 1986.\\n- [DHS10] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2010.\\n- [DKPR87] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics letters B, 195(2):216–222, 1987.\\n- [GMW13] Karol Gregor, Andriy Mnih, and Daan Wierstra. Deep autoregressive networks. arXiv preprint arXiv:1310.8499, 2013.\\n- [HBWP13] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303–1347, 2013.\\n- [HDFN95] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The” wakesleep” algorithm for unsupervised neural networks. SCIENCE, pages 1158–1158, 1995.\\n- [KRL08] Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. Fast inference in sparse coding algorithms with applications to object recognition. Technical Report CBLL-TR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU, 2008.\\n- [Lin89] Ralph Linsker. An application of the principle of maximum information preservation to linear systems. Morgan Kaufmann Publishers Inc., 1989.\\n- [RGB13] Rajesh Ranganath, Sean Gerrish, and David M Blei. Black Box Variational Inference. arXiv preprint arXiv:1401.0118, 2013.\\n- [RMW14] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and variational inference in deep latent gaussian models. arXiv preprint arXiv:1401.4082, 2014.\\n- [Row98] Sam Roweis. EM algorithms for PCA and SPCA. Advances in neural information processing systems, pages 626–632, 1998.\\n- [SK13] Tim Salimans and David A Knowles. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4), 2013.\\n- [SL10] Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep boltzmann machines. In International Conference on Artificial Intelligence and Statistics, pages 693–700, 2010.\\n- [VLL^{+}10] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 9999:3371–3408, 2010.\\n\\n## Appendix A Visualisations\\n\\nSee figures 4 and 5 for visualisations of latent space and corresponding observed space of models learned with SGVB.\\n\\n![img-3.jpeg](img-3.jpeg)\\n(a) Learned Frey Face manifold\\n\\n![img-4.jpeg](img-4.jpeg)\\n(b) Learned MNIST manifold\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 4: Visualisations of learned data manifold for generative models with two-dimensional latent space, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coordinates on the unit square were transformed through the inverse CDF of the Gaussian to produce values of the latent variables  $\\\\mathbf{z}$ . For each of these values  $\\\\mathbf{z}$ , we plotted the corresponding generative  $p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})$  with the learned parameters  $\\\\theta$ .\\n(a) 2-D latent space\\n(b) 5-D latent space\\n(c) 10-D latent space\\n(d) 20-D latent space\\nFigure 5: Random samples from learned generative models of MNIST for different dimensionalities of latent space.\\n\\n# B Solution of  $-D_{KL}(q_{\\\\phi}(\\\\mathbf{z})||p_{\\\\theta}(\\\\mathbf{z}))$  , Gaussian case\\n\\nThe variational lower bound (the objective to be maximized) contains a KL term that can often be integrated analytically. Here we give the solution when both the prior  $p_{\\\\theta}(\\\\mathbf{z}) = \\\\mathcal{N}(0,\\\\mathbf{I})$  and the posterior approximation  $q_{\\\\phi}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$  are Gaussian. Let  $J$  be the dimensionality of  $\\\\mathbf{z}$ . Let  $\\\\pmb{\\\\mu}$  and  $\\\\pmb{\\\\sigma}$  denote the variational mean and s.d. evaluated at datapoint  $i$ , and let  $\\\\mu_j$  and  $\\\\sigma_j$  simply denote the  $j$ -th element of these vectors. Then:\\n\\n$$\\n\\\\begin{array}{l} \\\\int q _ {\\\\boldsymbol {\\\\theta}} (\\\\mathbf {z}) \\\\log p (\\\\mathbf {z}) d \\\\mathbf {z} = \\\\int \\\\mathcal {N} (\\\\mathbf {z}; \\\\boldsymbol {\\\\mu}, \\\\boldsymbol {\\\\sigma} ^ {2}) \\\\log \\\\mathcal {N} (\\\\mathbf {z}; \\\\mathbf {0}, \\\\mathbf {I}) d \\\\mathbf {z} \\\\\\\\ = - \\\\frac {J}{2} \\\\log (2 \\\\pi) - \\\\frac {1}{2} \\\\sum_ {j = 1} ^ {J} \\\\left(\\\\mu_ {j} ^ {2} + \\\\sigma_ {j} ^ {2}\\\\right) \\\\\\\\ \\\\end{array}\\n$$\\n\\nAnd:\\n\\n$\\\\int q_{\\\\bm{\\\\theta}}(\\\\mathbf{z})\\\\log q_{\\\\bm{\\\\theta}}(\\\\mathbf{z})\\\\,d\\\\mathbf{z}$ $=\\\\int\\\\mathcal{N}(\\\\mathbf{z};\\\\bm{\\\\mu},\\\\bm{\\\\sigma}^{2})\\\\log\\\\mathcal{N}(\\\\mathbf{z};\\\\bm{\\\\mu},\\\\bm{\\\\sigma}^{2})\\\\,d\\\\mathbf{z}$\\n$=-\\\\frac{J}{2}\\\\log(2\\\\pi)-\\\\frac{1}{2}\\\\sum_{j=1}^{J}(1+\\\\log\\\\sigma_{j}^{2})$\\n\\nTherefore:\\n\\n$-D_{KL}((q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}))$ $=\\\\int q_{\\\\bm{\\\\theta}}(\\\\mathbf{z})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})-\\\\log q_{\\\\bm{\\\\theta}}(\\\\mathbf{z})\\\\right)\\\\,d\\\\mathbf{z}$\\n$=\\\\frac{1}{2}\\\\sum_{j=1}^{J}\\\\big{(}1+\\\\log((\\\\sigma_{j})^{2})-(\\\\mu_{j})^{2}-(\\\\sigma_{j})^{2}\\\\big{)}$\\n\\nWhen using a recognition model $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ then $\\\\bm{\\\\mu}$ and s.d. $\\\\bm{\\\\sigma}$ are simply functions of $\\\\mathbf{x}$ and the variational parameters $\\\\bm{\\\\phi}$, as exemplified in the text.\\n\\n## Appendix C MLP’s as probabilistic encoders and decoders\\n\\nIn variational auto-encoders, neural networks are used as probabilistic encoders and decoders. There are many possible choices of encoders and decoders, depending on the type of data and model. In our example we used relatively simple neural networks, namely multi-layered perceptrons (MLPs). For the encoder we used a MLP with Gaussian output, while for the decoder we used MLPs with either Gaussian or Bernoulli outputs, depending on the type of data.\\n\\n### C.1 Bernoulli MLP as decoder\\n\\nIn this case let $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$ be a multivariate Bernoulli whose probabilities are computed from $\\\\mathbf{z}$ with a fully-connected neural network with a single hidden layer:\\n\\n$\\\\log p(\\\\mathbf{x}|\\\\mathbf{z})$ $=\\\\sum_{i=1}^{D}x_{i}\\\\log y_{i}+(1-x_{i})\\\\cdot\\\\log(1-y_{i})$\\n$\\\\text{where }\\\\ \\\\mathbf{y}$ $=f_{\\\\sigma}(\\\\mathbf{W}_{2}\\\\tanh(\\\\mathbf{W}_{1}\\\\mathbf{z}+\\\\mathbf{b}_{1})+\\\\mathbf{b}_{2})$ (11)\\n\\nwhere $f_{\\\\sigma}(.)$ is the elementwise sigmoid activation function, and where $\\\\bm{\\\\theta}=\\\\{\\\\mathbf{W}_{1},\\\\mathbf{W}_{2},\\\\mathbf{b}_{1},\\\\mathbf{b}_{2}\\\\}$ are the weights and biases of the MLP.\\n\\n### C.2 Gaussian MLP as encoder or decoder\\n\\nIn this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure:\\n\\n$\\\\log p(\\\\mathbf{x}|\\\\mathbf{z})$ $=\\\\log\\\\mathcal{N}(\\\\mathbf{x};\\\\bm{\\\\mu},\\\\bm{\\\\sigma}^{2}\\\\mathbf{I})$\\n$\\\\text{where }\\\\ \\\\bm{\\\\mu}$ $=\\\\mathbf{W}_{4}\\\\mathbf{h}+\\\\mathbf{b}_{4}$\\n$\\\\log\\\\bm{\\\\sigma}^{2}$ $=\\\\mathbf{W}_{5}\\\\mathbf{h}+\\\\mathbf{b}_{5}$\\n$\\\\mathbf{h}$ $=\\\\tanh(\\\\mathbf{W}_{3}\\\\mathbf{z}+\\\\mathbf{b}_{3})$ (12)\\n\\nwhere $\\\\{\\\\mathbf{W}_{3},\\\\mathbf{W}_{4},\\\\mathbf{W}_{5},\\\\mathbf{b}_{3},\\\\mathbf{b}_{4},\\\\mathbf{b}_{5}\\\\}$ are the weights and biases of the MLP and part of $\\\\bm{\\\\theta}$ when used as decoder. Note that when this network is used as an encoder $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$, then $\\\\mathbf{z}$ and $\\\\mathbf{x}$ are swapped, and the weights and biases are variational parameters $\\\\bm{\\\\phi}$.\\n\\n## Appendix D Marginal likelihood estimator\\n\\nWe derived the following marginal likelihood estimator that produces good estimates of the marginal likelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and sufficient samples are taken. Let $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x},\\\\mathbf{z})=p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$ be the generative model we are sampling from, and for a given datapoint $\\\\mathbf{x}^{(i)}$ we would like to estimate the marginal likelihood $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})$.\\n\\nThe estimation process consists of three stages:\\n\\n1. Sample $L$ values $\\\\{\\\\mathbf{z}^{(l)}\\\\}$ from the posterior using gradient-based MCMC, e.g. Hybrid Monte Carlo, using $\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x})=\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})+\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$.\\n2. Fit a density estimator $q(\\\\mathbf{z})$ to these samples $\\\\{\\\\mathbf{z}^{(l)}\\\\}$.\\n3. Again, sample $L$ new values from the posterior. Plug these samples, as well as the fitted $q(\\\\mathbf{z})$, into the following estimator:\\n\\n$p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})\\\\simeq\\\\left(\\\\frac{1}{L}\\\\sum_{l=1}^{L}\\\\frac{q(\\\\mathbf{z}^{(l)})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(l)})}\\\\right)^{-1}\\\\quad\\\\text{where}\\\\quad\\\\mathbf{z}^{(l)}\\\\sim p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$\\n\\nDerivation of the estimator:\\n\\n$\\\\frac{1}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})}$ $=\\\\frac{\\\\int q(\\\\mathbf{z})\\\\,d\\\\mathbf{z}}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})}=\\\\frac{\\\\int q(\\\\mathbf{z})\\\\frac{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z})}\\\\,d\\\\mathbf{z}}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})}$\\n$=\\\\int\\\\frac{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})}\\\\frac{q(\\\\mathbf{z})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z})}\\\\,d\\\\mathbf{z}$\\n$=\\\\int p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})\\\\frac{q(\\\\mathbf{z})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z})}\\\\,d\\\\mathbf{z}$\\n$\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}\\\\frac{q(\\\\mathbf{z}^{(l)})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(l)})}\\\\quad\\\\text{where}\\\\quad\\\\mathbf{z}^{(l)}\\\\sim p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$\\n\\n## Appendix E Monte Carlo EM\\n\\nThe Monte Carlo EM algorithm does not employ an encoder, instead it samples from the posterior of the latent variables using gradients of the posterior computed with $\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x})=\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})+\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$. The Monte Carlo EM procedure consists of 10 HMC leapfrog steps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5 weight updates steps using the acquired sample. For all algorithms the parameters were updated using the Adagrad stepsizes (with accompanying annealing schedule).\\n\\nThe marginal likelihood was estimated with the first 1000 datapoints from the train and test sets, for each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte Carlo with 4 leapfrog steps.\\n\\n## Appendix F Full VB\\n\\nAs written in the paper, it is possible to perform variational inference on both the parameters $\\\\bm{\\\\theta}$ and the latent variables $\\\\mathbf{z}$, as opposed to just the latent variables as we did in the paper. Here, we’ll derive our estimator for that case.\\n\\nLet $p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})$ be some hyperprior for the parameters introduced above, parameterized by $\\\\bm{\\\\alpha}$. The marginal likelihood can be written as:\\n\\n$\\\\log p_{\\\\bm{\\\\alpha}}(\\\\mathbf{X})=D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})||p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta}|\\\\mathbf{X}))+\\\\mathcal{L}(\\\\bm{\\\\phi};\\\\mathbf{X})$ (13)\\n\\nwhere the first RHS term denotes a KL divergence of the approximate from the true posterior, and where $\\\\mathcal{L}(\\\\bm{\\\\phi};\\\\mathbf{X})$ denotes the variational lower bound to the marginal likelihood:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\phi};\\\\mathbf{X})=\\\\int q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{X})+\\\\log p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})\\\\right)\\\\,d\\\\bm{\\\\theta}$ (14)\\n\\nNote that this is a lower bound since the KL divergence is non-negative; the bound equals the true marginal when the approximate and true posteriors match exactly. The term $\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{X})$ is composed of a sum over the marginal likelihoods of individual datapoints $\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{X})=\\\\sum_{i=1}^{N}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})$, which can each be rewritten as:\\n\\n$\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})=D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)}))+\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ (15)\\n\\nwhere again the first RHS term is the KL divergence of the approximate from the true posterior, and $\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x})$ is the variational lower bound of the marginal likelihood of datapoint $i$:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})=\\\\int q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z})+\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\right)\\\\,d\\\\mathbf{z}$ (16)\\n\\nThe expectations on the RHS of eqs (14) and (16) can obviously be written as a sum of three separate expectations, of which the second and third component can sometimes be analytically solved, e.g. when both $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x})$ and $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ are Gaussian. For generality we will here assume that each of these expectations is intractable.\\n\\nUnder certain mild conditions outlined in section (see paper) for chosen approximate posteriors $q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})$ and $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ we can reparameterize conditional samples $\\\\widetilde{\\\\mathbf{z}}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ as\\n\\n$\\\\widetilde{\\\\mathbf{z}}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})\\\\quad\\\\text{with}\\\\quad\\\\bm{\\\\epsilon}\\\\sim p(\\\\bm{\\\\epsilon})$ (17)\\n\\nwhere we choose a prior $p(\\\\bm{\\\\epsilon})$ and a function $g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$ such that the following holds:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ $=\\\\int q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z})+\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\right)\\\\,d\\\\mathbf{z}$\\n$=\\\\int p(\\\\bm{\\\\epsilon})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z})+\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\right)\\\\bigg{|}_{\\\\mathbf{z}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x}^{(i)})}d\\\\bm{\\\\epsilon}$ (18)\\n\\nThe same can be done for the approximate posterior $q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})$:\\n\\n$\\\\widetilde{\\\\bm{\\\\theta}}=h_{\\\\bm{\\\\phi}}(\\\\bm{\\\\zeta})\\\\quad\\\\text{with}\\\\quad\\\\bm{\\\\zeta}\\\\sim p(\\\\bm{\\\\zeta})$ (19)\\n\\nwhere we, similarly as above, choose a prior $p(\\\\bm{\\\\zeta})$ and a function $h_{\\\\bm{\\\\phi}}(\\\\bm{\\\\zeta})$ such that the following holds:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\phi};\\\\mathbf{X})$ $=\\\\int q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{X})+\\\\log p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})\\\\right)\\\\,d\\\\bm{\\\\theta}$\\n$=\\\\int p(\\\\bm{\\\\zeta})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{X})+\\\\log p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})\\\\right)\\\\bigg{|}_{\\\\bm{\\\\theta}=h_{\\\\bm{\\\\phi}}(\\\\bm{\\\\zeta})}d\\\\bm{\\\\zeta}$ (20)\\n\\nFor notational conciseness we introduce a shorthand notation $f_{\\\\bm{\\\\phi}}(\\\\mathbf{x},\\\\mathbf{z},\\\\bm{\\\\theta})$:\\n\\n$f_{\\\\bm{\\\\phi}}(\\\\mathbf{x},\\\\mathbf{z},\\\\bm{\\\\theta})=N\\\\cdot(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})+\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}))+\\\\log p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})$ (21)\\n\\nUsing equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given datapoint $\\\\mathbf{x}^{(i)}$, is:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\phi};\\\\mathbf{X})\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}f_{\\\\bm{\\\\phi}}(\\\\mathbf{x}^{(l)},g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon}^{(l)},\\\\mathbf{x}^{(l)}),h_{\\\\bm{\\\\phi}}(\\\\zeta^{(l)}))$ (22)\\n\\nwhere $\\\\bm{\\\\epsilon}^{(l)}\\\\sim p(\\\\bm{\\\\epsilon})$ and $\\\\zeta^{(l)}\\\\sim p(\\\\bm{\\\\zeta})$. The estimator only depends on samples from $p(\\\\bm{\\\\epsilon})$ and $p(\\\\bm{\\\\zeta})$ which are obviously not influenced by $\\\\bm{\\\\phi}$, therefore the estimator can be differentiated w.r.t. $\\\\bm{\\\\phi}$. The resulting stochastic gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad *[x10]*. See algorithm 1 for a basic approach to computing stochastic gradients.\\n\\n### F.1 Example\\n\\nLet the prior over the parameters and latent variables be the centered isotropic Gaussian $p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})=\\\\mathcal{N}(\\\\mathbf{z};\\\\mathbf{0},\\\\mathbf{I})$ and $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})=\\\\mathcal{N}(\\\\mathbf{z};\\\\mathbf{0},\\\\mathbf{I})$. Note that in this case, the prior lacks parameters. Let’s also assume that the true posteriors are approximatily Gaussian with an approximately diagonal covariance. In this case, we can let the variational approximate posteriors be multivariate Gaussians with a diagonal covariance structure:\\n\\n$\\\\log q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})$ $=\\\\log\\\\mathcal{N}(\\\\bm{\\\\theta};\\\\bm{\\\\mu}_{\\\\bm{\\\\theta}},\\\\bm{\\\\sigma}_{\\\\bm{\\\\theta}}^{2}\\\\mathbf{I})$\\n$\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ $=\\\\log\\\\mathcal{N}(\\\\mathbf{z};\\\\bm{\\\\mu}_{\\\\mathbf{z}},\\\\bm{\\\\sigma}_{\\\\mathbf{z}}^{2}\\\\mathbf{I})$ (23)\\n\\nAlgorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See text for meaning of the functions  $f_{\\\\phi}, g_{\\\\phi}$  and  $h_{\\\\phi}$ .\\n\\nRequire:  $\\\\phi$  (Current value of variational parameters)\\n\\n$\\\\mathbf{g}\\\\gets 0$\\n\\nfor  $l$  is 1 to  $L$  do\\n\\n$\\\\mathbf{x}\\\\gets$  Random draw from dataset  $\\\\mathbf{X}$\\n\\n$\\\\pmb{\\\\epsilon} \\\\gets$  Random draw from prior  $p(\\\\pmb{\\\\epsilon})$\\n\\n$\\\\zeta \\\\gets$  Random draw from prior  $p(\\\\zeta)$\\n\\n$\\\\mathbf{g}\\\\gets \\\\mathbf{g} + \\\\frac{1}{L}\\\\nabla_{\\\\phi}f_{\\\\phi}(\\\\mathbf{x},g_{\\\\phi}(\\\\boldsymbol {\\\\epsilon},\\\\mathbf{x}),h_{\\\\phi}(\\\\boldsymbol {\\\\zeta}))$\\n\\nend for\\n\\nreturn g\\n\\nwhere  $\\\\pmb{\\\\mu}_{\\\\mathbf{z}}$  and  $\\\\sigma_{\\\\mathbf{z}}$  are yet unspecified functions of  $\\\\mathbf{x}$ . Since they are Gaussian, we can parameterize the variational approximate posteriors:\\n\\n$q_{\\\\phi}(\\\\pmb {\\\\theta})$  as  $\\\\widetilde{\\\\pmb{\\\\theta}} = \\\\pmb{\\\\mu}_{\\\\pmb{\\\\theta}} + \\\\pmb{\\\\sigma}_{\\\\pmb{\\\\theta}}\\\\odot \\\\pmb{\\\\zeta}$  where  $\\\\zeta \\\\sim \\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$\\n\\n$q_{\\\\phi}(\\\\mathbf{z}|\\\\mathbf{x})$  as  $\\\\widetilde{\\\\mathbf{z}} = \\\\pmb{\\\\mu}_{\\\\mathbf{z}} + \\\\pmb{\\\\sigma}_{\\\\mathbf{z}}\\\\odot \\\\pmb{\\\\epsilon}$  where  $\\\\pmb {\\\\epsilon}\\\\sim \\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$\\n\\nWith  $\\\\odot$  we signify an element-wise product. These can be plugged into the lower bound defined above (eqs (21) and (22)).\\n\\nIn this case it is possible to construct an alternative estimator with a lower variance, since in this model  $p_{\\\\alpha}(\\\\pmb{\\\\theta})$ ,  $p_{\\\\pmb{\\\\theta}}(\\\\mathbf{z})$ ,  $q_{\\\\phi}(\\\\pmb{\\\\theta})$  and  $q_{\\\\phi}(\\\\mathbf{z}|\\\\mathbf{x})$  are Gaussian, and therefore four terms of  $f_{\\\\phi}$  can be solved analytically. The resulting estimator is:\\n\\n$$\\n\\\\begin{array}{l} \\\\mathcal {L} (\\\\phi ; \\\\mathbf {X}) \\\\simeq \\\\frac {1}{L} \\\\sum_ {l = 1} ^ {L} N \\\\cdot \\\\left(\\\\frac {1}{2} \\\\sum_ {j = 1} ^ {J} \\\\left(1 + \\\\log ((\\\\sigma_ {\\\\mathbf {z}, j} ^ {(l)}) ^ {2}) - (\\\\mu_ {\\\\mathbf {z}, j} ^ {(l)}) ^ {2} - (\\\\sigma_ {\\\\mathbf {z}, j} ^ {(l)}) ^ {2}\\\\right) + \\\\log p _ {\\\\pmb {\\\\theta}} (\\\\mathbf {x} ^ {(i)} \\\\mathbf {z} ^ {(i)})\\\\right) \\\\\\\\ + \\\\frac {1}{2} \\\\sum_ {j = 1} ^ {J} \\\\left(1 + \\\\log \\\\left(\\\\left(\\\\sigma_ {\\\\boldsymbol {\\\\theta}, j} ^ {(l)}\\\\right) ^ {2}\\\\right) - \\\\left(\\\\mu_ {\\\\boldsymbol {\\\\theta}, j} ^ {(l)}\\\\right) ^ {2} - \\\\left(\\\\sigma_ {\\\\boldsymbol {\\\\theta}, j} ^ {(l)}\\\\right) ^ {2}\\\\right) \\\\tag {24} \\\\\\\\ \\\\end{array}\\n$$\\n\\n$\\\\mu_j^{(i)}$  and  $\\\\sigma_{j}^{(i)}$  simply denote the  $j$ -th element of vectors  $\\\\pmb{\\\\mu}^{(i)}$  and  $\\\\pmb{\\\\sigma}^{(i)}$ .\", 'chunk_index': 0, 'start_line': 1, 'doc_id': 'e502d1a1cb3d'}, page_content=\"# Auto-Encoding Variational Bayes\\n\\nDiederik P. Kingma\\nMachine Learning Group\\nUniversiteit van Amsterdam\\ndpkingma@gmail.com\\nMax Welling\\nMachine Learning Group\\nUniversiteit van Amsterdam\\nwelling.max@gmail.com\\n\\n###### Abstract\\n\\nHow can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.\\n\\n## 1 Introduction\\n\\nHow can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions? The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.\\n\\nFor the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder.\\n\\n## 2 Method\\n\\nThe strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example,\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: The type of directed graphical model under consideration. Solid lines denote the generative model  $p_{\\\\theta}(\\\\mathbf{z})p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})$ , dashed lines denote the variational approximation  $q_{\\\\phi}(\\\\mathbf{z}|\\\\mathbf{x})$  to the intractable posterior  $p_{\\\\theta}(\\\\mathbf{z}|\\\\mathbf{x})$ . The variational parameters  $\\\\phi$  are learned jointly with the generative model parameters  $\\\\theta$ .\\n\\nstraightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity.\\n\\n# 2.1 Problem scenario\\n\\nLet us consider some dataset  $\\\\mathbf{X} = \\\\{\\\\mathbf{x}^{(i)}\\\\}_{i=1}^{N}$  consisting of  $N$  i.i.d. samples of some continuous or discrete variable  $\\\\mathbf{x}$ . We assume that the data are generated by some random process, involving an unobserved continuous random variable  $\\\\mathbf{z}$ . The process consists of two steps: (1) a value  $\\\\mathbf{z}^{(i)}$  is generated from some prior distribution  $p_{\\\\theta^*}(\\\\mathbf{z})$ ; (2) a value  $\\\\mathbf{x}^{(i)}$  is generated from some conditional distribution  $p_{\\\\theta^*}(\\\\mathbf{x}|\\\\mathbf{z})$ . We assume that the prior  $p_{\\\\theta^*}(\\\\mathbf{z})$  and likelihood  $p_{\\\\theta^*}(\\\\mathbf{x}|\\\\mathbf{z})$  come from parametric families of distributions  $p_{\\\\theta}(\\\\mathbf{z})$  and  $p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})$ , and that their PDFs are differentiable almost everywhere w.r.t. both  $\\\\theta$  and  $\\\\mathbf{z}$ . Unfortunately, a lot of this process is hidden from our view: the true parameters  $\\\\theta^*$  as well as the values of the latent variables  $\\\\mathbf{z}^{(i)}$  are unknown to us.\\n\\nVery importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:\\n\\n1. Intractability: the case where the integral of the marginal likelihood  $p_{\\\\theta}(\\\\mathbf{x}) = \\\\int p_{\\\\theta}(\\\\mathbf{z})p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})d\\\\mathbf{z}$  is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density  $p_{\\\\theta}(\\\\mathbf{z}|\\\\mathbf{x}) = p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})p_{\\\\theta}(\\\\mathbf{z}) / p_{\\\\theta}(\\\\mathbf{x})$  is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions  $p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})$ , e.g. a neural network with a nonlinear hidden layer.\\n2. A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.\\n\\nWe are interested in, and propose a solution to, three related problems in the above scenario:\\n\\n1. Efficient approximate ML or MAP estimation for the parameters  $\\\\theta$ . The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.\\n2. Efficient approximate posterior inference of the latent variable  $\\\\mathbf{z}$  given an observed value  $\\\\mathbf{x}$  for a choice of parameters  $\\\\theta$ . This is useful for coding or data representation tasks.\\n3. Efficient approximate marginal inference of the variable  $\\\\mathbf{x}$ . This allows us to perform all kinds of inference tasks where a prior over  $\\\\mathbf{x}$  is required. Common applications in computer vision include image denoising, inpainting and super-resolution.\\n\\nFor the purpose of solving the above problems, let us introduce a recognition model $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$: an approximation to the intractable true posterior $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x})$. Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters $\\\\bm{\\\\phi}$ are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters $\\\\bm{\\\\phi}$ jointly with the generative model parameters $\\\\bm{\\\\theta}$.\\n\\nFrom a coding theory perspective, the unobserved variables $\\\\mathbf{z}$ have an interpretation as a latent representation or *code*. In this paper we will therefore also refer to the recognition model $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ as a probabilistic *encoder*, since given a datapoint $\\\\mathbf{x}$ it produces a distribution (e.g. a Gaussian) over the possible values of the code $\\\\mathbf{z}$ from which the datapoint $\\\\mathbf{x}$ could have been generated. In a similar vein we will refer to $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$ as a probabilistic *decoder*, since given a code $\\\\mathbf{z}$ it produces a distribution over the possible corresponding values of $\\\\mathbf{x}$.\\n\\n### 2.2 The variational bound\\n\\nThe marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints $\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(1)},\\\\cdots,\\\\mathbf{x}^{(N)})=\\\\sum_{i=1}^{N}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})$, which can each be rewritten as:\\n\\n$\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})=D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)}))+\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ (1)\\n\\nThe first RHS term is the KL divergence of the approximate from the true posterior. Since this KL-divergence is non-negative, the second RHS term $\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ is called the (variational) *lower bound* on the marginal likelihood of datapoint $i$, and can be written as:\\n\\n$\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})\\\\geq\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})=\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})}\\\\left[-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})+\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x},\\\\mathbf{z})\\\\right]$ (2)\\n\\nwhich can also be written as:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})=-D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}))+\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})}\\\\left[\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z})\\\\right]$ (3)\\n\\nWe want to differentiate and optimize the lower bound $\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ w.r.t. both the variational parameters $\\\\bm{\\\\phi}$ and generative parameters $\\\\bm{\\\\theta}$. However, the gradient of the lower bound w.r.t. $\\\\bm{\\\\phi}$ is a bit problematic. The usual (naïve) Monte Carlo gradient estimator for this type of problem is: $\\\\nabla_{\\\\bm{\\\\phi}}\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})}\\\\left[f(\\\\mathbf{z})\\\\right]=\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})}\\\\left[f(\\\\mathbf{z})\\\\nabla_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})}\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})\\\\right]\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}f(\\\\mathbf{z})\\\\nabla_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}^{(l)})}\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}^{(l)})$ where $\\\\mathbf{z}^{(l)}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$. This gradient estimator exhibits exhibits very high variance (see e.g. *[x1]*) and is impractical for our purposes.\\n\\n### 2.3 The SGVB estimator and AEVB algorithm\\n\\nIn this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$, but please note that the technique can be applied to the case $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})$, i.e. where we do not condition on $\\\\mathbf{x}$, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix.\\n\\nUnder certain mild conditions outlined in section 2.4 for a chosen approximate posterior $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ we can reparameterize the random variable $\\\\widetilde{\\\\mathbf{z}}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ using a differentiable transformation $g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$ of an (auxiliary) noise variable $\\\\bm{\\\\epsilon}$:\\n\\n$\\\\widetilde{\\\\mathbf{z}}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})\\\\quad\\\\text{with}\\\\quad\\\\bm{\\\\epsilon}\\\\sim p(\\\\bm{\\\\epsilon})$ (4)\\n\\nSee section 2.4 for general strategies for chosing such an approriate distribution $p(\\\\bm{\\\\epsilon})$ and function $g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$. We can now form Monte Carlo estimates of expectations of some function $f(\\\\mathbf{z})$ w.r.t. $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ as follows:\\n\\n$\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})}\\\\left[f(\\\\mathbf{z})\\\\right]=\\\\mathbb{E}_{p(\\\\bm{\\\\epsilon})}\\\\left[f(g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x}^{(i)}))\\\\right]\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}f(g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon}^{(l)},\\\\mathbf{x}^{(i)}))\\\\quad\\\\text{where}\\\\quad\\\\bm{\\\\epsilon}^{(l)}\\\\sim p(\\\\bm{\\\\epsilon})$ (5)\\n\\nWe apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator $\\\\widetilde{\\\\mathcal{L}}^{A}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})\\\\simeq\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$:\\n\\n$\\\\widetilde{\\\\mathcal{L}}^{A}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ $=\\\\frac{1}{L}\\\\sum_{l=1}^{L}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z}^{(i,l)})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}^{(i,l)}|\\\\mathbf{x}^{(i)})$\\n$\\\\text{where}\\\\quad\\\\ \\\\mathbf{z}^{(i,l)}$ $=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon}^{(i,l)},\\\\mathbf{x}^{(i)})\\\\quad\\\\text{and}\\\\quad\\\\bm{\\\\epsilon}^{(l)}\\\\sim p(\\\\bm{\\\\epsilon})$ (6)\\n\\n###\\n\\nAlgorithm 1 Minibatch version of the Auto-Encoding VB (AEVB) algorithm. Either of the two SGVB estimators in section 2.3 can be used. We use settings $M=100$ and $L=1$ in experiments.\\n\\n$\\\\bm{\\\\theta},\\\\bm{\\\\phi}\\\\leftarrow$ Initialize parameters\\nrepeat\\n$\\\\mathbf{X}^{M}\\\\leftarrow$ Random minibatch of $M$ datapoints (drawn from full dataset)\\n$\\\\bm{\\\\epsilon}\\\\leftarrow$ Random samples from noise distribution $p(\\\\bm{\\\\epsilon})$\\n$\\\\mathbf{g}\\\\leftarrow\\\\nabla_{\\\\bm{\\\\theta},\\\\bm{\\\\phi}}\\\\widetilde{\\\\mathcal{L}}^{M}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{X}^{M},\\\\bm{\\\\epsilon})$ (Gradients of minibatch estimator (8))\\n$\\\\bm{\\\\theta},\\\\bm{\\\\phi}\\\\leftarrow$ Update parameters using gradients $\\\\mathbf{g}$ (e.g. SGD or Adagrad *[x10]*)\\nuntil convergence of parameters $(\\\\bm{\\\\theta},\\\\bm{\\\\phi})$\\nreturn $\\\\bm{\\\\theta},\\\\bm{\\\\phi}$\\n\\nOften, the KL-divergence $D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}))$ of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error $\\\\mathbb{E}_{q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})}\\\\left[\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z})\\\\right]$ requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing $\\\\bm{\\\\phi}$, encouraging the approximate posterior to be close to the prior $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})$. This yields a second version of the SGVB estimator $\\\\widetilde{\\\\mathcal{L}}^{B}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})\\\\simeq\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$, corresponding to eq. (3), which typically has less variance than the generic estimator:\\n\\n$\\\\widetilde{\\\\mathcal{L}}^{B}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})=-D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}))+\\\\frac{1}{L}\\\\sum_{l=1}^{L}(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(i,l)}))$\\n$\\\\text{where}\\\\quad\\\\mathbf{z}^{(i,l)}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon}^{(i,l)},\\\\mathbf{x}^{(i)})\\\\quad\\\\text{and}\\\\quad\\\\bm{\\\\epsilon}^{(l)}\\\\sim p(\\\\bm{\\\\epsilon})$ (7)\\n\\nGiven multiple datapoints from a dataset $\\\\mathbf{X}$ with $N$ datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{X})\\\\simeq\\\\widetilde{\\\\mathcal{L}}^{M}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{X}^{M})=\\\\frac{N}{M}\\\\sum_{i=1}^{M}\\\\widetilde{\\\\mathcal{L}}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ (8)\\n\\nwhere the minibatch $\\\\mathbf{X}^{M}=\\\\{\\\\mathbf{x}^{(i)}\\\\}_{i=1}^{M}$ is a randomly drawn sample of $M$ datapoints from the full dataset $\\\\mathbf{X}$ with $N$ datapoints. In our experiments we found that the number of samples $L$ per datapoint can be set to $1$ as long as the minibatch size $M$ was large enough, e.g. $M=100$. Derivatives $\\\\nabla_{\\\\bm{\\\\theta},\\\\bm{\\\\phi}}\\\\widetilde{\\\\mathcal{L}}(\\\\bm{\\\\theta};\\\\mathbf{X}^{M})$ can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad *[x10]*. See algorithm 1 for a basic approach to compute the stochastic gradients.\\n\\nA connection with auto-encoders becomes clear when looking at the objective function given at eq. (7). The first term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error. The function $g_{\\\\bm{\\\\phi}}(.)$ is chosen such that it maps a datapoint $\\\\mathbf{x}^{(i)}$ and a random noise vector $\\\\bm{\\\\epsilon}^{(l)}$ to a sample from the approximate posterior for that datapoint: $\\\\mathbf{z}^{(i,l)}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon}^{(l)},\\\\mathbf{x}^{(i)})$ where $\\\\mathbf{z}^{(i,l)}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$. Subsequently, the sample $\\\\mathbf{z}^{(i,l)}$ is then input to function $\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(i,l)})$, which equals the probability density (or mass) of datapoint $\\\\mathbf{x}^{(i)}$ under the generative model, given $\\\\mathbf{z}^{(i,l)}$. This term is a negative *reconstruction error* in auto-encoder parlance.\\n\\n### 2.4 The reparameterization trick\\n\\nIn order to solve our problem we invoked an alternative method for generating samples from $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$. The essential parameterization trick is quite simple. Let $\\\\mathbf{z}$ be a continuous random variable, and $\\\\mathbf{z}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ be some conditional distribution. It is then often possible to express the random variable $\\\\mathbf{z}$ as a deterministic variable $\\\\mathbf{z}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$, where $\\\\bm{\\\\epsilon}$ is an auxiliary variable with independent marginal $p(\\\\bm{\\\\epsilon})$, and $g_{\\\\bm{\\\\phi}}(.)$ is some vector-valued function parameterized by $\\\\bm{\\\\phi}$.\\n\\nThis reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ such that the Monte Carlo estimate of the expectation is differentiable w.r.t. $\\\\bm{\\\\phi}$. A proof is as follows. Given the deterministic mapping $\\\\mathbf{z}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$ we know that $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\prod_{i}dz_{i}=p(\\\\bm{\\\\epsilon})\\\\prod_{i}d\\\\epsilon_{i}$. Therefore, $\\\\int q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})f(\\\\mathbf{z})\\\\,d\\\\mathbf{z}=\\\\int p(\\\\bm{\\\\epsilon})f(\\\\mathbf{z})\\\\,d\\\\bm{\\\\epsilon}=\\\\int p(\\\\bm{\\\\epsilon})f(g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x}))\\\\,d\\\\bm{\\\\epsilon}$. It follows\\n\\nthat a differentiable estimator can be constructed: $\\\\int q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})f(\\\\mathbf{z})\\\\,d\\\\mathbf{z}\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}f(g_{\\\\bm{\\\\phi}}(\\\\mathbf{x},\\\\bm{\\\\epsilon}^{(l)}))$ where $\\\\bm{\\\\epsilon}^{(l)}\\\\sim p(\\\\bm{\\\\epsilon})$. In section 2.3 we applied this trick to obtain a differentiable estimator of the variational lower bound.\\n\\nTake, for example, the univariate Gaussian case: let $z\\\\sim p(z|x)=\\\\mathcal{N}(\\\\mu,\\\\sigma^{2})$. In this case, a valid reparameterization is $z=\\\\mu+\\\\sigma\\\\epsilon$, where $\\\\epsilon$ is an auxiliary noise variable $\\\\epsilon\\\\sim\\\\mathcal{N}(0,1)$. Therefore, $\\\\mathbb{E}_{\\\\mathcal{N}(z;\\\\mu,\\\\sigma^{2})}\\\\left[f(z)\\\\right]=\\\\mathbb{E}_{\\\\mathcal{N}(\\\\epsilon;0,1)}\\\\left[f(\\\\mu+\\\\sigma\\\\epsilon)\\\\right]\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}f(\\\\mu+\\\\sigma\\\\epsilon^{(l)})$ where $\\\\epsilon^{(l)}\\\\sim\\\\mathcal{N}(0,1)$.\\n\\nFor which $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ can we choose such a differentiable transformation $g_{\\\\bm{\\\\phi}}(.)$ and auxiliary variable $\\\\bm{\\\\epsilon}\\\\sim p(\\\\bm{\\\\epsilon})$? Three basic approaches are:\\n\\n1. Tractable inverse CDF. In this case, let $\\\\bm{\\\\epsilon}\\\\sim\\\\mathcal{U}(\\\\mathbf{0},\\\\mathbf{I})$, and let $g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$ be the inverse CDF of $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$. Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz, Gumbel and Erlang distributions.\\n2. Analogous to the Gaussian example, for any ”location-scale” family of distributions we can choose the standard distribution (with location $=0$, scale $=1$) as the auxiliary variable $\\\\bm{\\\\epsilon}$, and let $g(.)=\\\\text{location}+\\\\text{scale}\\\\cdot\\\\bm{\\\\epsilon}$. Examples: Laplace, Elliptical, Student’s t, Logistic, Uniform, Triangular and Gaussian distributions.\\n3. Composition: It is often possible to express random variables as different transformations of auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed variable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted sum of Gamma variates), Beta, Chi-Squared, and F distributions.\\n\\nWhen all three approaches fail, good approximations to the inverse CDF exist requiring computations with time complexity comparable to the PDF (see e.g. *[x10]* for some methods).\\n\\n## 3 Example: Variational Auto-Encoder\\n\\nIn this section we’ll give an example where we use a neural network for the probabilistic encoder $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ (the approximation to the posterior of the generative model $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x},\\\\mathbf{z})$) and where the parameters $\\\\bm{\\\\phi}$ and $\\\\bm{\\\\theta}$ are optimized jointly with the AEVB algorithm.\\n\\nLet the prior over the latent variables be the centered isotropic multivariate Gaussian $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})=\\\\mathcal{N}(\\\\mathbf{z};\\\\mathbf{0},\\\\mathbf{I})$. Note that in this case, the prior lacks parameters. We let $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$ be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from $\\\\mathbf{z}$ with a MLP (a fully-connected neural network with a single hidden layer, see appendix C). Note the true posterior $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x})$ is in this case intractable. While there is much freedom in the form $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$, we’ll assume the true (but intractable) posterior takes on a approximate Gaussian form with an approximately diagonal covariance. In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure:\\n\\n$\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})=\\\\log\\\\mathcal{N}(\\\\mathbf{z};\\\\bm{\\\\mu}^{(i)},\\\\bm{\\\\sigma}^{2(i)}\\\\mathbf{I})$ (9)\\n\\nwhere the mean and s.d. of the approximate posterior, $\\\\bm{\\\\mu}^{(i)}$ and $\\\\bm{\\\\sigma}^{(i)}$, are outputs of the encoding MLP, i.e. nonlinear functions of datapoint $\\\\mathbf{x}^{(i)}$ and the variational parameters $\\\\bm{\\\\phi}$ (see appendix C).\\n\\nAs explained in section 2.4, we sample from the posterior $\\\\mathbf{z}^{(i,l)}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$ using $\\\\mathbf{z}^{(i,l)}=g_{\\\\bm{\\\\phi}}(\\\\mathbf{x}^{(i)},\\\\bm{\\\\epsilon}^{(l)})=\\\\bm{\\\\mu}^{(i)}+\\\\bm{\\\\sigma}^{(i)}\\\\odot\\\\bm{\\\\epsilon}^{(l)}$ where $\\\\bm{\\\\epsilon}^{(l)}\\\\sim\\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$. With $\\\\odot$ we signify an element-wise product. In this model both $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})$ (the prior) and $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ are Gaussian; in this case, we can use the estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation (see appendix B). The resulting estimator for this model and datapoint $\\\\mathbf{x}^{(i)}$ is:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})\\\\simeq\\\\frac{1}{2}\\\\sum_{j=1}^{J}\\\\left(1+\\\\log((\\\\sigma_{j}^{(i)})^{2})-(\\\\mu_{j}^{(i)})^{2}-(\\\\sigma_{j}^{(i)})^{2}\\\\right)+\\\\frac{1}{L}\\\\sum_{l=1}^{L}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(i,l)})$\\n$\\\\text{where}\\\\quad\\\\mathbf{z}^{(i,l)}=\\\\bm{\\\\mu}^{(i)}+\\\\bm{\\\\sigma}^{(i)}\\\\odot\\\\bm{\\\\epsilon}^{(l)}\\\\quad\\\\text{and}\\\\quad\\\\bm{\\\\epsilon}^{(l)}\\\\sim\\\\mathcal{N}(0,\\\\mathbf{I})$ (10)\\n\\nAs explained above and in appendix C, the decoding term $\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(i,l)})$ is a Bernoulli or Gaussian MLP, depending on the type of data we are modelling.\\n\\n4 Related work\\n\\nThe wake-sleep algorithm *[x10]* is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.\\n\\nStochastic variational inference *[x12]* has recently received increasing interest. Recently, *[x3]* introduced a control variate schemes to reduce the high variance of the naïve gradient estimator discussed in section 2.1, and applied to exponential family approximations of the posterior. In *[x22]* some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. In *[x23]*, a similar reparameterization as in this paper was used in an efficient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions.\\n\\nThe AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between *linear* auto-encoders and a certain class of generative linear-Gaussian models has long been known. In *[x24]* it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior $p(\\\\mathbf{z})=\\\\mathcal{N}(0,\\\\mathbf{I})$ and a conditional distribution $p(\\\\mathbf{x}|\\\\mathbf{z})=\\\\mathcal{N}(\\\\mathbf{x};\\\\mathbf{W}\\\\mathbf{z},\\\\epsilon\\\\mathbf{I})$, specifically the case with infinitesimally small $\\\\epsilon$.\\n\\nIn relevant recent work on autoencoders *[VLL^{+}10]* it was shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound (see the infomax principle *[x17]*) of the mutual information between input $X$ and latent representation $Z$. Maximizing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional entropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding model *[VLL^{+}10]*, i.e. the negative reconstrution error. However, it is well known that this reconstruction criterion is in itself not sufficient for learning useful representations *[x5]*. Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants *[x5]*. The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regularization hyperparameter required to learn useful representations. Related are also encoder-decoder architectures such as the predictive sparse decomposition (PSD) *[x16]*, from which we drew some inspiration. Also relevant are the recently introduced Generative Stochastic Networks *[x6]* where noisy auto-encoders learn the transition operator of a Markov chain that samples from the data distribution. In *[x25]* a recognition model was employed for efficient learning with Deep Boltzmann Machines. These methods are targeted at either unnormalized models (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm for learning a general class of directed probabilistic models.\\n\\nThe recently proposed DARN method *[x11]*, also learns a directed probabilistic model using an auto-encoding structure, however their method applies to binary latent variables. Even more recently, *[x20]* also make the connection between auto-encoders, directed proabilistic models and stochastic variational inference using the reparameterization trick we describe in this paper. Their work was developed independently of ours and provides an additional perspective on AEVB.\\n\\n## 5 Experiments\\n\\nWe trained generative models of images from the MNIST and Frey Face datasets and compared learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.\\n\\nThe generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval $(0,1)$ using a sigmoidal activation function at the\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the lower bound, for different dimensionality of latent space  $(N_{\\\\mathbf{z}})$ . Our method converged considerably faster and reached a better solution in all experiments. Interestingly enough, more latent variables does not result in more overfitting, which is explained by the regularizing effect of the lower bound. Vertical axis: the estimated average variational lower bound per datapoint. The estimator variance was small  $(&lt; 1)$  and omitted. Horizontal axis: amount of training points evaluated. Computation took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an effective 40 GFLOPS.\\n\\ndecoder output. Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder.\\n\\nParameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator  $\\\\nabla_{\\\\theta, \\\\phi} \\\\mathcal{L}(\\\\theta, \\\\phi; \\\\mathbf{X})$  (see algorithm 1), plus a small weight decay term corresponding to a prior  $p(\\\\theta) = \\\\mathcal{N}(0, \\\\mathbf{I})$ . Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.\\n\\nWe compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational autoencoder. All parameters, both variational and generative, were initialized by random sampling from  $\\\\mathcal{N}(0,0.01)$ , and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from  $\\\\{0.01, 0.02, 0.1\\\\}$  based on performance on the training set in the first few iterations. Minibatches of size  $M = 100$  were used, with  $L = 1$  samples per datapoint.\\n\\nLikelihood lower bound We trained generative models (decoders) and corresponding encoders (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent overfitting, since it is a considerably smaller dataset). The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. Figure 2 shows the results when comparing the lower bounds. Interestingly, superfluous latent variables did not result in overfitting, which is explained by the regularizing nature of the variational bound.\\n\\nMarginal likelihood For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the estimated marginal likelihood, for a different number of training points. Monte Carlo EM is not an on-line algorithm, and (unlike AEVB and the wake-sleep method) can't be applied efficiently for the full MNIST dataset.\\n\\nVisualisation of high-dimensional data If we choose a low-dimensional latent space (e.g. 2D), we can use the learned encoders (recognition model) to project high-dimensional data to a low-dimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST and Frey Face datasets.\\n\\n# 6 Conclusion\\n\\nWe have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results.\\n\\n# 7 Future work\\n\\nSince the SGVB estimator and the AEVB algorithm can be applied to almost any inference and learning problem with continuous latent variables, there are plenty of future directions: (i) learning hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models with latent variables, useful for learning complicated noise distributions.\\n\\nReferences\\n\\n- [BCV13] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. 2013.\\n- [BJP12] David M Blei, Michael I Jordan, and John W Paisley. Variational Bayesian inference with Stochastic Search. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1367–1374, 2012.\\n- [BTL13] Yoshua Bengio and Éric Thibodeau-Laufer. Deep generative stochastic networks trainable by backprop. arXiv preprint arXiv:1306.1091, 2013.\\n- [Dev86] Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation, pages 260–265. ACM, 1986.\\n- [DHS10] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2010.\\n- [DKPR87] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics letters B, 195(2):216–222, 1987.\\n- [GMW13] Karol Gregor, Andriy Mnih, and Daan Wierstra. Deep autoregressive networks. arXiv preprint arXiv:1310.8499, 2013.\\n- [HBWP13] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303–1347, 2013.\\n- [HDFN95] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The” wakesleep” algorithm for unsupervised neural networks. SCIENCE, pages 1158–1158, 1995.\\n- [KRL08] Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. Fast inference in sparse coding algorithms with applications to object recognition. Technical Report CBLL-TR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU, 2008.\\n- [Lin89] Ralph Linsker. An application of the principle of maximum information preservation to linear systems. Morgan Kaufmann Publishers Inc., 1989.\\n- [RGB13] Rajesh Ranganath, Sean Gerrish, and David M Blei. Black Box Variational Inference. arXiv preprint arXiv:1401.0118, 2013.\\n- [RMW14] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and variational inference in deep latent gaussian models. arXiv preprint arXiv:1401.4082, 2014.\\n- [Row98] Sam Roweis. EM algorithms for PCA and SPCA. Advances in neural information processing systems, pages 626–632, 1998.\\n- [SK13] Tim Salimans and David A Knowles. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4), 2013.\\n- [SL10] Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep boltzmann machines. In International Conference on Artificial Intelligence and Statistics, pages 693–700, 2010.\\n- [VLL^{+}10] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 9999:3371–3408, 2010.\\n\\n## Appendix A Visualisations\\n\\nSee figures 4 and 5 for visualisations of latent space and corresponding observed space of models learned with SGVB.\\n\\n![img-3.jpeg](img-3.jpeg)\\n(a) Learned Frey Face manifold\\n\\n![img-4.jpeg](img-4.jpeg)\\n(b) Learned MNIST manifold\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 4: Visualisations of learned data manifold for generative models with two-dimensional latent space, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coordinates on the unit square were transformed through the inverse CDF of the Gaussian to produce values of the latent variables  $\\\\mathbf{z}$ . For each of these values  $\\\\mathbf{z}$ , we plotted the corresponding generative  $p_{\\\\theta}(\\\\mathbf{x}|\\\\mathbf{z})$  with the learned parameters  $\\\\theta$ .\\n(a) 2-D latent space\\n(b) 5-D latent space\\n(c) 10-D latent space\\n(d) 20-D latent space\\nFigure 5: Random samples from learned generative models of MNIST for different dimensionalities of latent space.\\n\\n# B Solution of  $-D_{KL}(q_{\\\\phi}(\\\\mathbf{z})||p_{\\\\theta}(\\\\mathbf{z}))$  , Gaussian case\\n\\nThe variational lower bound (the objective to be maximized) contains a KL term that can often be integrated analytically. Here we give the solution when both the prior  $p_{\\\\theta}(\\\\mathbf{z}) = \\\\mathcal{N}(0,\\\\mathbf{I})$  and the posterior approximation  $q_{\\\\phi}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$  are Gaussian. Let  $J$  be the dimensionality of  $\\\\mathbf{z}$ . Let  $\\\\pmb{\\\\mu}$  and  $\\\\pmb{\\\\sigma}$  denote the variational mean and s.d. evaluated at datapoint  $i$ , and let  $\\\\mu_j$  and  $\\\\sigma_j$  simply denote the  $j$ -th element of these vectors. Then:\\n\\n$$\\n\\\\begin{array}{l} \\\\int q _ {\\\\boldsymbol {\\\\theta}} (\\\\mathbf {z}) \\\\log p (\\\\mathbf {z}) d \\\\mathbf {z} = \\\\int \\\\mathcal {N} (\\\\mathbf {z}; \\\\boldsymbol {\\\\mu}, \\\\boldsymbol {\\\\sigma} ^ {2}) \\\\log \\\\mathcal {N} (\\\\mathbf {z}; \\\\mathbf {0}, \\\\mathbf {I}) d \\\\mathbf {z} \\\\\\\\ = - \\\\frac {J}{2} \\\\log (2 \\\\pi) - \\\\frac {1}{2} \\\\sum_ {j = 1} ^ {J} \\\\left(\\\\mu_ {j} ^ {2} + \\\\sigma_ {j} ^ {2}\\\\right) \\\\\\\\ \\\\end{array}\\n$$\\n\\nAnd:\\n\\n$\\\\int q_{\\\\bm{\\\\theta}}(\\\\mathbf{z})\\\\log q_{\\\\bm{\\\\theta}}(\\\\mathbf{z})\\\\,d\\\\mathbf{z}$ $=\\\\int\\\\mathcal{N}(\\\\mathbf{z};\\\\bm{\\\\mu},\\\\bm{\\\\sigma}^{2})\\\\log\\\\mathcal{N}(\\\\mathbf{z};\\\\bm{\\\\mu},\\\\bm{\\\\sigma}^{2})\\\\,d\\\\mathbf{z}$\\n$=-\\\\frac{J}{2}\\\\log(2\\\\pi)-\\\\frac{1}{2}\\\\sum_{j=1}^{J}(1+\\\\log\\\\sigma_{j}^{2})$\\n\\nTherefore:\\n\\n$-D_{KL}((q_{\\\\bm{\\\\phi}}(\\\\mathbf{z})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}))$ $=\\\\int q_{\\\\bm{\\\\theta}}(\\\\mathbf{z})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})-\\\\log q_{\\\\bm{\\\\theta}}(\\\\mathbf{z})\\\\right)\\\\,d\\\\mathbf{z}$\\n$=\\\\frac{1}{2}\\\\sum_{j=1}^{J}\\\\big{(}1+\\\\log((\\\\sigma_{j})^{2})-(\\\\mu_{j})^{2}-(\\\\sigma_{j})^{2}\\\\big{)}$\\n\\nWhen using a recognition model $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ then $\\\\bm{\\\\mu}$ and s.d. $\\\\bm{\\\\sigma}$ are simply functions of $\\\\mathbf{x}$ and the variational parameters $\\\\bm{\\\\phi}$, as exemplified in the text.\\n\\n## Appendix C MLP’s as probabilistic encoders and decoders\\n\\nIn variational auto-encoders, neural networks are used as probabilistic encoders and decoders. There are many possible choices of encoders and decoders, depending on the type of data and model. In our example we used relatively simple neural networks, namely multi-layered perceptrons (MLPs). For the encoder we used a MLP with Gaussian output, while for the decoder we used MLPs with either Gaussian or Bernoulli outputs, depending on the type of data.\\n\\n### C.1 Bernoulli MLP as decoder\\n\\nIn this case let $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$ be a multivariate Bernoulli whose probabilities are computed from $\\\\mathbf{z}$ with a fully-connected neural network with a single hidden layer:\\n\\n$\\\\log p(\\\\mathbf{x}|\\\\mathbf{z})$ $=\\\\sum_{i=1}^{D}x_{i}\\\\log y_{i}+(1-x_{i})\\\\cdot\\\\log(1-y_{i})$\\n$\\\\text{where }\\\\ \\\\mathbf{y}$ $=f_{\\\\sigma}(\\\\mathbf{W}_{2}\\\\tanh(\\\\mathbf{W}_{1}\\\\mathbf{z}+\\\\mathbf{b}_{1})+\\\\mathbf{b}_{2})$ (11)\\n\\nwhere $f_{\\\\sigma}(.)$ is the elementwise sigmoid activation function, and where $\\\\bm{\\\\theta}=\\\\{\\\\mathbf{W}_{1},\\\\mathbf{W}_{2},\\\\mathbf{b}_{1},\\\\mathbf{b}_{2}\\\\}$ are the weights and biases of the MLP.\\n\\n### C.2 Gaussian MLP as encoder or decoder\\n\\nIn this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure:\\n\\n$\\\\log p(\\\\mathbf{x}|\\\\mathbf{z})$ $=\\\\log\\\\mathcal{N}(\\\\mathbf{x};\\\\bm{\\\\mu},\\\\bm{\\\\sigma}^{2}\\\\mathbf{I})$\\n$\\\\text{where }\\\\ \\\\bm{\\\\mu}$ $=\\\\mathbf{W}_{4}\\\\mathbf{h}+\\\\mathbf{b}_{4}$\\n$\\\\log\\\\bm{\\\\sigma}^{2}$ $=\\\\mathbf{W}_{5}\\\\mathbf{h}+\\\\mathbf{b}_{5}$\\n$\\\\mathbf{h}$ $=\\\\tanh(\\\\mathbf{W}_{3}\\\\mathbf{z}+\\\\mathbf{b}_{3})$ (12)\\n\\nwhere $\\\\{\\\\mathbf{W}_{3},\\\\mathbf{W}_{4},\\\\mathbf{W}_{5},\\\\mathbf{b}_{3},\\\\mathbf{b}_{4},\\\\mathbf{b}_{5}\\\\}$ are the weights and biases of the MLP and part of $\\\\bm{\\\\theta}$ when used as decoder. Note that when this network is used as an encoder $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$, then $\\\\mathbf{z}$ and $\\\\mathbf{x}$ are swapped, and the weights and biases are variational parameters $\\\\bm{\\\\phi}$.\\n\\n## Appendix D Marginal likelihood estimator\\n\\nWe derived the following marginal likelihood estimator that produces good estimates of the marginal likelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and sufficient samples are taken. Let $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x},\\\\mathbf{z})=p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$ be the generative model we are sampling from, and for a given datapoint $\\\\mathbf{x}^{(i)}$ we would like to estimate the marginal likelihood $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})$.\\n\\nThe estimation process consists of three stages:\\n\\n1. Sample $L$ values $\\\\{\\\\mathbf{z}^{(l)}\\\\}$ from the posterior using gradient-based MCMC, e.g. Hybrid Monte Carlo, using $\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x})=\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})+\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$.\\n2. Fit a density estimator $q(\\\\mathbf{z})$ to these samples $\\\\{\\\\mathbf{z}^{(l)}\\\\}$.\\n3. Again, sample $L$ new values from the posterior. Plug these samples, as well as the fitted $q(\\\\mathbf{z})$, into the following estimator:\\n\\n$p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})\\\\simeq\\\\left(\\\\frac{1}{L}\\\\sum_{l=1}^{L}\\\\frac{q(\\\\mathbf{z}^{(l)})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(l)})}\\\\right)^{-1}\\\\quad\\\\text{where}\\\\quad\\\\mathbf{z}^{(l)}\\\\sim p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$\\n\\nDerivation of the estimator:\\n\\n$\\\\frac{1}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})}$ $=\\\\frac{\\\\int q(\\\\mathbf{z})\\\\,d\\\\mathbf{z}}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})}=\\\\frac{\\\\int q(\\\\mathbf{z})\\\\frac{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z})}\\\\,d\\\\mathbf{z}}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})}$\\n$=\\\\int\\\\frac{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})}\\\\frac{q(\\\\mathbf{z})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z})}\\\\,d\\\\mathbf{z}$\\n$=\\\\int p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})\\\\frac{q(\\\\mathbf{z})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)},\\\\mathbf{z})}\\\\,d\\\\mathbf{z}$\\n$\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}\\\\frac{q(\\\\mathbf{z}^{(l)})}{p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z}^{(l)})}\\\\quad\\\\text{where}\\\\quad\\\\mathbf{z}^{(l)}\\\\sim p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})$\\n\\n## Appendix E Monte Carlo EM\\n\\nThe Monte Carlo EM algorithm does not employ an encoder, instead it samples from the posterior of the latent variables using gradients of the posterior computed with $\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x})=\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})+\\\\nabla_{\\\\mathbf{z}}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})$. The Monte Carlo EM procedure consists of 10 HMC leapfrog steps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5 weight updates steps using the acquired sample. For all algorithms the parameters were updated using the Adagrad stepsizes (with accompanying annealing schedule).\\n\\nThe marginal likelihood was estimated with the first 1000 datapoints from the train and test sets, for each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte Carlo with 4 leapfrog steps.\\n\\n## Appendix F Full VB\\n\\nAs written in the paper, it is possible to perform variational inference on both the parameters $\\\\bm{\\\\theta}$ and the latent variables $\\\\mathbf{z}$, as opposed to just the latent variables as we did in the paper. Here, we’ll derive our estimator for that case.\\n\\nLet $p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})$ be some hyperprior for the parameters introduced above, parameterized by $\\\\bm{\\\\alpha}$. The marginal likelihood can be written as:\\n\\n$\\\\log p_{\\\\bm{\\\\alpha}}(\\\\mathbf{X})=D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})||p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta}|\\\\mathbf{X}))+\\\\mathcal{L}(\\\\bm{\\\\phi};\\\\mathbf{X})$ (13)\\n\\nwhere the first RHS term denotes a KL divergence of the approximate from the true posterior, and where $\\\\mathcal{L}(\\\\bm{\\\\phi};\\\\mathbf{X})$ denotes the variational lower bound to the marginal likelihood:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\phi};\\\\mathbf{X})=\\\\int q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{X})+\\\\log p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})\\\\right)\\\\,d\\\\bm{\\\\theta}$ (14)\\n\\nNote that this is a lower bound since the KL divergence is non-negative; the bound equals the true marginal when the approximate and true posteriors match exactly. The term $\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{X})$ is composed of a sum over the marginal likelihoods of individual datapoints $\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{X})=\\\\sum_{i=1}^{N}\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})$, which can each be rewritten as:\\n\\n$\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)})=D_{KL}(q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)})||p_{\\\\bm{\\\\theta}}(\\\\mathbf{z}|\\\\mathbf{x}^{(i)}))+\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ (15)\\n\\nwhere again the first RHS term is the KL divergence of the approximate from the true posterior, and $\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x})$ is the variational lower bound of the marginal likelihood of datapoint $i$:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})=\\\\int q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z})+\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\right)\\\\,d\\\\mathbf{z}$ (16)\\n\\nThe expectations on the RHS of eqs (14) and (16) can obviously be written as a sum of three separate expectations, of which the second and third component can sometimes be analytically solved, e.g. when both $p_{\\\\bm{\\\\theta}}(\\\\mathbf{x})$ and $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ are Gaussian. For generality we will here assume that each of these expectations is intractable.\\n\\nUnder certain mild conditions outlined in section (see paper) for chosen approximate posteriors $q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})$ and $q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ we can reparameterize conditional samples $\\\\widetilde{\\\\mathbf{z}}\\\\sim q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ as\\n\\n$\\\\widetilde{\\\\mathbf{z}}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})\\\\quad\\\\text{with}\\\\quad\\\\bm{\\\\epsilon}\\\\sim p(\\\\bm{\\\\epsilon})$ (17)\\n\\nwhere we choose a prior $p(\\\\bm{\\\\epsilon})$ and a function $g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x})$ such that the following holds:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\theta},\\\\bm{\\\\phi};\\\\mathbf{x}^{(i)})$ $=\\\\int q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z})+\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\right)\\\\,d\\\\mathbf{z}$\\n$=\\\\int p(\\\\bm{\\\\epsilon})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}^{(i)}|\\\\mathbf{z})+\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})\\\\right)\\\\bigg{|}_{\\\\mathbf{z}=g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon},\\\\mathbf{x}^{(i)})}d\\\\bm{\\\\epsilon}$ (18)\\n\\nThe same can be done for the approximate posterior $q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})$:\\n\\n$\\\\widetilde{\\\\bm{\\\\theta}}=h_{\\\\bm{\\\\phi}}(\\\\bm{\\\\zeta})\\\\quad\\\\text{with}\\\\quad\\\\bm{\\\\zeta}\\\\sim p(\\\\bm{\\\\zeta})$ (19)\\n\\nwhere we, similarly as above, choose a prior $p(\\\\bm{\\\\zeta})$ and a function $h_{\\\\bm{\\\\phi}}(\\\\bm{\\\\zeta})$ such that the following holds:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\phi};\\\\mathbf{X})$ $=\\\\int q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{X})+\\\\log p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})\\\\right)\\\\,d\\\\bm{\\\\theta}$\\n$=\\\\int p(\\\\bm{\\\\zeta})\\\\left(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{X})+\\\\log p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})\\\\right)\\\\bigg{|}_{\\\\bm{\\\\theta}=h_{\\\\bm{\\\\phi}}(\\\\bm{\\\\zeta})}d\\\\bm{\\\\zeta}$ (20)\\n\\nFor notational conciseness we introduce a shorthand notation $f_{\\\\bm{\\\\phi}}(\\\\mathbf{x},\\\\mathbf{z},\\\\bm{\\\\theta})$:\\n\\n$f_{\\\\bm{\\\\phi}}(\\\\mathbf{x},\\\\mathbf{z},\\\\bm{\\\\theta})=N\\\\cdot(\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{x}|\\\\mathbf{z})+\\\\log p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x}))+\\\\log p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})-\\\\log q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})$ (21)\\n\\nUsing equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given datapoint $\\\\mathbf{x}^{(i)}$, is:\\n\\n$\\\\mathcal{L}(\\\\bm{\\\\phi};\\\\mathbf{X})\\\\simeq\\\\frac{1}{L}\\\\sum_{l=1}^{L}f_{\\\\bm{\\\\phi}}(\\\\mathbf{x}^{(l)},g_{\\\\bm{\\\\phi}}(\\\\bm{\\\\epsilon}^{(l)},\\\\mathbf{x}^{(l)}),h_{\\\\bm{\\\\phi}}(\\\\zeta^{(l)}))$ (22)\\n\\nwhere $\\\\bm{\\\\epsilon}^{(l)}\\\\sim p(\\\\bm{\\\\epsilon})$ and $\\\\zeta^{(l)}\\\\sim p(\\\\bm{\\\\zeta})$. The estimator only depends on samples from $p(\\\\bm{\\\\epsilon})$ and $p(\\\\bm{\\\\zeta})$ which are obviously not influenced by $\\\\bm{\\\\phi}$, therefore the estimator can be differentiated w.r.t. $\\\\bm{\\\\phi}$. The resulting stochastic gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad *[x10]*. See algorithm 1 for a basic approach to computing stochastic gradients.\\n\\n### F.1 Example\\n\\nLet the prior over the parameters and latent variables be the centered isotropic Gaussian $p_{\\\\bm{\\\\alpha}}(\\\\bm{\\\\theta})=\\\\mathcal{N}(\\\\mathbf{z};\\\\mathbf{0},\\\\mathbf{I})$ and $p_{\\\\bm{\\\\theta}}(\\\\mathbf{z})=\\\\mathcal{N}(\\\\mathbf{z};\\\\mathbf{0},\\\\mathbf{I})$. Note that in this case, the prior lacks parameters. Let’s also assume that the true posteriors are approximatily Gaussian with an approximately diagonal covariance. In this case, we can let the variational approximate posteriors be multivariate Gaussians with a diagonal covariance structure:\\n\\n$\\\\log q_{\\\\bm{\\\\phi}}(\\\\bm{\\\\theta})$ $=\\\\log\\\\mathcal{N}(\\\\bm{\\\\theta};\\\\bm{\\\\mu}_{\\\\bm{\\\\theta}},\\\\bm{\\\\sigma}_{\\\\bm{\\\\theta}}^{2}\\\\mathbf{I})$\\n$\\\\log q_{\\\\bm{\\\\phi}}(\\\\mathbf{z}|\\\\mathbf{x})$ $=\\\\log\\\\mathcal{N}(\\\\mathbf{z};\\\\bm{\\\\mu}_{\\\\mathbf{z}},\\\\bm{\\\\sigma}_{\\\\mathbf{z}}^{2}\\\\mathbf{I})$ (23)\\n\\nAlgorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See text for meaning of the functions  $f_{\\\\phi}, g_{\\\\phi}$  and  $h_{\\\\phi}$ .\\n\\nRequire:  $\\\\phi$  (Current value of variational parameters)\\n\\n$\\\\mathbf{g}\\\\gets 0$\\n\\nfor  $l$  is 1 to  $L$  do\\n\\n$\\\\mathbf{x}\\\\gets$  Random draw from dataset  $\\\\mathbf{X}$\\n\\n$\\\\pmb{\\\\epsilon} \\\\gets$  Random draw from prior  $p(\\\\pmb{\\\\epsilon})$\\n\\n$\\\\zeta \\\\gets$  Random draw from prior  $p(\\\\zeta)$\\n\\n$\\\\mathbf{g}\\\\gets \\\\mathbf{g} + \\\\frac{1}{L}\\\\nabla_{\\\\phi}f_{\\\\phi}(\\\\mathbf{x},g_{\\\\phi}(\\\\boldsymbol {\\\\epsilon},\\\\mathbf{x}),h_{\\\\phi}(\\\\boldsymbol {\\\\zeta}))$\\n\\nend for\\n\\nreturn g\\n\\nwhere  $\\\\pmb{\\\\mu}_{\\\\mathbf{z}}$  and  $\\\\sigma_{\\\\mathbf{z}}$  are yet unspecified functions of  $\\\\mathbf{x}$ . Since they are Gaussian, we can parameterize the variational approximate posteriors:\\n\\n$q_{\\\\phi}(\\\\pmb {\\\\theta})$  as  $\\\\widetilde{\\\\pmb{\\\\theta}} = \\\\pmb{\\\\mu}_{\\\\pmb{\\\\theta}} + \\\\pmb{\\\\sigma}_{\\\\pmb{\\\\theta}}\\\\odot \\\\pmb{\\\\zeta}$  where  $\\\\zeta \\\\sim \\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$\\n\\n$q_{\\\\phi}(\\\\mathbf{z}|\\\\mathbf{x})$  as  $\\\\widetilde{\\\\mathbf{z}} = \\\\pmb{\\\\mu}_{\\\\mathbf{z}} + \\\\pmb{\\\\sigma}_{\\\\mathbf{z}}\\\\odot \\\\pmb{\\\\epsilon}$  where  $\\\\pmb {\\\\epsilon}\\\\sim \\\\mathcal{N}(\\\\mathbf{0},\\\\mathbf{I})$\\n\\nWith  $\\\\odot$  we signify an element-wise product. These can be plugged into the lower bound defined above (eqs (21) and (22)).\\n\\nIn this case it is possible to construct an alternative estimator with a lower variance, since in this model  $p_{\\\\alpha}(\\\\pmb{\\\\theta})$ ,  $p_{\\\\pmb{\\\\theta}}(\\\\mathbf{z})$ ,  $q_{\\\\phi}(\\\\pmb{\\\\theta})$  and  $q_{\\\\phi}(\\\\mathbf{z}|\\\\mathbf{x})$  are Gaussian, and therefore four terms of  $f_{\\\\phi}$  can be solved analytically. The resulting estimator is:\\n\\n$$\\n\\\\begin{array}{l} \\\\mathcal {L} (\\\\phi ; \\\\mathbf {X}) \\\\simeq \\\\frac {1}{L} \\\\sum_ {l = 1} ^ {L} N \\\\cdot \\\\left(\\\\frac {1}{2} \\\\sum_ {j = 1} ^ {J} \\\\left(1 + \\\\log ((\\\\sigma_ {\\\\mathbf {z}, j} ^ {(l)}) ^ {2}) - (\\\\mu_ {\\\\mathbf {z}, j} ^ {(l)}) ^ {2} - (\\\\sigma_ {\\\\mathbf {z}, j} ^ {(l)}) ^ {2}\\\\right) + \\\\log p _ {\\\\pmb {\\\\theta}} (\\\\mathbf {x} ^ {(i)} \\\\mathbf {z} ^ {(i)})\\\\right) \\\\\\\\ + \\\\frac {1}{2} \\\\sum_ {j = 1} ^ {J} \\\\left(1 + \\\\log \\\\left(\\\\left(\\\\sigma_ {\\\\boldsymbol {\\\\theta}, j} ^ {(l)}\\\\right) ^ {2}\\\\right) - \\\\left(\\\\mu_ {\\\\boldsymbol {\\\\theta}, j} ^ {(l)}\\\\right) ^ {2} - \\\\left(\\\\sigma_ {\\\\boldsymbol {\\\\theta}, j} ^ {(l)}\\\\right) ^ {2}\\\\right) \\\\tag {24} \\\\\\\\ \\\\end{array}\\n$$\\n\\n$\\\\mu_j^{(i)}$  and  $\\\\sigma_{j}^{(i)}$  simply denote the  $j$ -th element of vectors  $\\\\pmb{\\\\mu}^{(i)}$  and  $\\\\pmb{\\\\sigma}^{(i)}$ .\"), -0.03847879117071984), (Document(id='c7a13d650516:0', metadata={'doc_id': 'c7a13d650516', 'start_line': 1, 'chunk_index': 0, 'end_line': 587, 'text': '# An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\\n\\nAlexey Dosovitskiy^{∗,†}, Lucas Beyer^{∗}, Alexander Kolesnikov^{∗}, Dirk Weissenborn^{∗},\\nXiaohua Zhai^{∗}, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby^{∗,†}\\n^{∗}equal technical contribution, ^{†}equal advising\\nGoogle Research, Brain Team\\n{adosovitskiy, neilhoulsby}@google.com\\n\\n###### Abstract\\n\\nWhile the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\\n\\n## 1 Introduction\\n\\nSelf-attention-based architectures, in particular Transformers *(Vaswani et al., 2017)*, have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset *(Devlin et al., 2019)*. Thanks to Transformers’ computational efficiency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters *(Brown et al., 2020; Lepikhin et al., 2020)*. With the models and datasets growing, there is still no sign of saturating performance.\\n\\nIn computer vision, however, convolutional architectures remain dominant *(LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016)*. Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention *(Wang et al., 2018; Carion et al., 2020)*, some replacing the convolutions entirely *(Ramachandran et al., 2019; Wang et al., 2020a)*. The latter models, while theoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-like architectures are still state of the art *(Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020)*.\\n\\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.\\n\\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size. This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\\n\\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.\\n\\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of $88.55\\\\%$ on ImageNet, $90.72\\\\%$ on ImageNet-ReaL, $94.55\\\\%$ on CIFAR-100, and $77.63\\\\%$ on the VTAB suite of 19 tasks.\\n\\n## 2 Related Work\\n\\nTransformers were proposed by *Vaswani et al. (2017)* for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT *(Devlin et al., 2019)* uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task *(Radford et al., 2018, 2019; Brown et al., 2020)*.\\n\\nNaive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. *Parmar et al. (2018)* applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions *(Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020)*. In a different line of work, Sparse Transformers *(Child et al., 2019)* employ scalable approximations to global self-attention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes *(Weissenborn et al., 2019)*, in the extreme case only along individual axes *(Ho et al., 2019; Wang et al., 2020a)*. Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators.\\n\\nMost related to ours is the model of *Cordonnier et al. (2020)*, which extracts patches of size $2\\\\times 2$ from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, *Cordonnier et al. (2020)* use a small patch size of $2\\\\times 2$ pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.\\n\\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classification *(Bello et al., 2019)* or by further processing the output of a CNN using self-attention, e.g. for object detection *(Hu et al., 2018; Carion et al., 2020)*, video processing *(Wang et al., 2018; Sun et al., 2019)*, image classification *(Wu et al., 2020)*, unsupervised object discovery *(Locatello et al., 2020)*, or unified text-vision tasks *(Chen et al., 2020c; Lu et al., 2019; Li et al., 2019)*.\\n\\nAnother recent related model is image GPT (iGPT) *(Chen et al., 2020a)*, which applies Transformers to image pixels after reducing image resolution and color space. The model is trained in an unsupervised fashion as a generative model, and the resulting representation can then be fine-tuned or probed linearly for classification performance, achieving a maximal accuracy of 72% on ImageNet.\\n\\nOur work adds to the increasing collection of papers that explore image recognition at larger scales than the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-the-art results on standard benchmarks *(Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020)*. Moreover, *Sun et al. (2017)* study how CNN performance scales with dataset size, and *Kolesnikov et al. (2020); Djolonga et al. (2020)* perform an empirical exploration of CNN transfer learning from large scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as well, but train Transformers instead of ResNet-based models used in prior works.\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable \"classification token\" to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017).\\n\\n![img-1.jpeg](img-1.jpeg)\\n\\n# 3 METHOD\\n\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and their efficient implementations – can be used almost out of the box.\\n\\n# 3.1 VISION TRANSFORMER (VIT)\\n\\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image  $\\\\mathbf{x} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}$  into a sequence of flattened 2D patches  $\\\\mathbf{x}_p \\\\in \\\\mathbb{R}^{N \\\\times (P^2 \\\\cdot C)}$ , where  $(H, W)$  is the resolution of the original image,  $C$  is the number of channels,  $(P, P)$  is the resolution of each image patch, and  $N = HW / P^2$  is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size  $D$  through all of its layers, so we flatten the patches and map to  $D$  dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\\n\\nSimilar to BERT\\'s [class] token, we prepend a learnable embedding to the sequence of embedded patches  $(\\\\mathbf{z}_0^0 = \\\\mathbf{x}_{\\\\mathrm{class}})$ , whose state at the output of the Transformer encoder  $(\\\\mathbf{z}_L^0)$  serves as the image representation  $\\\\mathbf{y}$  (Eq. 4). Both during pre-training and fine-tuning, a classification head is attached to  $\\\\mathbf{z}_L^0$ . The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.\\n\\nPosition embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.\\n\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski &amp; Auli, 2019).\\n\\nThe MLP contains two layers with a GELU non-linearity.\\n\\n$\\\\mathbf{z}_{0}$ $=\\\\left[\\\\mathbf{x}_{\\\\text{class}};\\\\,\\\\mathbf{x}_{p}^{1}\\\\mathbf{E};\\\\,\\\\mathbf{x}_{p}^{2}\\\\mathbf{E};\\\\cdots;\\\\,\\\\mathbf{x}_{p}^{N}\\\\mathbf{E}\\\\right]+\\\\mathbf{E}_{pos},\\\\quad$ $\\\\mathbf{E}\\\\in\\\\mathbb{R}^{(P^{2}\\\\cdot C)\\\\times D},\\\\,\\\\mathbf{E}_{pos}\\\\in\\\\mathbb{R}^{(N+1)\\\\times D}$ (1)\\n$\\\\mathbf{z^{\\\\prime}}_{\\\\ell}$ $=\\\\mathrm{MSA}(\\\\mathrm{LN}(\\\\mathbf{z}_{\\\\ell-1}))+\\\\mathbf{z}_{\\\\ell-1},$ $\\\\ell=1\\\\ldots L$ (2)\\n$\\\\mathbf{z}_{\\\\ell}$ $=\\\\mathrm{MLP}(\\\\mathrm{LN}(\\\\mathbf{z^{\\\\prime}}_{\\\\ell}))+\\\\mathbf{z^{\\\\prime}}_{\\\\ell},$ $\\\\ell=1\\\\ldots L$ (3)\\n$\\\\mathbf{y}$ $=\\\\mathrm{LN}(\\\\mathbf{z}_{L}^{0})$ (4)\\n\\n#### Inductive bias.\\n\\nWe note that Vision Transformer has much less image-specific inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global. The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution (as described below). Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch.\\n\\n#### Hybrid Architecture.\\n\\nAs an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN *(LeCun et al., 1989)*. In this hybrid model, the patch embedding projection $\\\\mathbf{E}$ (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size 1x1, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above.\\n\\n### 3.2 Fine-tuning and Higher Resolution\\n\\nTypically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks. For this, we remove the pre-trained prediction head and attach a zero-initialized $D\\\\times K$ feedforward layer, where $K$ is the number of downstream classes. It is often beneficial to fine-tune at higher resolution than pre-training *(Touvron et al., 2019; Kolesnikov et al., 2020)*. When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image. Note that this resolution adjustment and patch extraction are the only points at which an inductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\\n\\n## 4 Experiments\\n\\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the hybrid. To understand the data requirements of each model, we pre-train on datasets of varying size and evaluate many benchmark tasks. When considering the computational cost of pre-training the model, ViT performs very favourably, attaining state of the art on most recognition benchmarks at a lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show that self-supervised ViT holds promise for the future.\\n\\n### 4.1 Setup\\n\\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes and 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with 21k classes and 14M images *(Deng et al., 2009)*, and JFT *(Sun et al., 2017)* with 18k classes and 303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the downstream tasks following *Kolesnikov et al. (2020)*. We transfer the models trained on these dataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up ReaL labels *(Beyer et al., 2020)*, CIFAR-10/100 *(Krizhevsky, 2009)*, Oxford-IIIT Pets *(Parkhi et al., 2012)*, and Oxford Flowers-102 *(Nilsback and Zisserman, 2008)*. For these datasets, pre-processing follows *Kolesnikov et al. (2020)*.\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|  Model | Layers | Hidden size D | MLP size | Heads | Params  |\\n| --- | --- | --- | --- | --- | --- |\\n|  ViT-Base | 12 | 768 | 3072 | 12 | 86M  |\\n|  ViT-Large | 24 | 1024 | 4096 | 16 | 307M  |\\n|  ViT-Huge | 32 | 1280 | 5120 | 16 | 632M  |\\n\\nTable 1: Details of Vision Transformer model variants.\\n\\nWe also evaluate on the 19-task VTAB classification suite (Zhai et al., 2019b). VTAB evaluates low-data transfer to diverse tasks, using 1000 training examples per task. The tasks are divided into three groups: Natural - tasks like the above, Pets, CIFAR, etc. Specialized - medical and satellite imagery, and Structured - tasks that require geometric understanding like localization.\\n\\nModel Variants. We base ViT configurations on those used for BERT (Devlin et al., 2019), as summarized in Table 1. The \"Base\" and \"Large\" models are directly adopted from BERT and we add the larger \"Huge\" model. In what follows we use brief notation to indicate the model size and the input patch size: for instance, ViT-L/16 means the \"Large\" variant with  $16 \\\\times 16$  input patch size. Note that the Transformer\\'s sequence length is inversely proportional to the square of the patch size, thus models with smaller patch size are computationally more expensive.\\n\\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization layers (Ioffe &amp; Szegedy, 2015) with Group Normalization (Wu &amp; He, 2018), and used standardized convolutions (Qiao et al., 2019). These modifications improve transfer (Kolesnikov et al., 2020), and we denote the modified model \"ResNet (BiT)\". For the hybrids, we feed the intermediate feature maps into ViT with patch size of one \"pixel\". To experiment with different sequence lengths, we either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same number of layers in stage 3 (keeping the total number of layers), and take the output of this extended stage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\\n\\nTraining &amp; Fine-tuning. We train all models, including ResNets, using Adam (Kingma &amp; Ba, 2015) with  $\\\\beta_{1} = 0.9$ ,  $\\\\beta_{2} = 0.999$ , a batch size of 4096 and apply a high weight decay of 0.1, which we found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common practices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning rate warmup and decay, see Appendix B.1 for details. For fine-tuning we use SGD with momentum, batch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we fine-tuned at higher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak &amp; Juditsky (1992) averaging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\\n\\nMetrics. We report results on downstream datasets either through few-shot or fine-tuning accuracy. Fine-tuning accuracies capture the performance of each model after fine-tuning it on the respective dataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem that maps the (frozen) representation of a subset of training images to  $\\\\{-1,1\\\\}^K$  target vectors. This formulation allows us to recover the exact solution in closed form. Though we mainly focus on fine-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-fly evaluation where fine-tuning would be too costly.\\n\\n# 4.2 COMPARISON TO STATE OF THE ART\\n\\nWe first compare our largest models - ViT-H/14 and ViT-L/16 - to state-of-the-art CNNs from the literature. The first comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which performs supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al., 2020), which is a large EfficientNet trained using semi-supervised learning on ImageNet and JFT-300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and BiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we report the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU v3 cores (2 per chip) used for training multiplied by the training time in days.\\n\\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L (which is pre-trained on the same dataset) on all tasks, while requiring substantially less computational resources to train. The larger model, ViT-H/14, further improves the performance, especially on the more challenging datasets - ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|   | Ours-JFT (ViT-H/14) | Ours-JFT (ViT-L/16) | Ours-I21k (ViT-L/16) | BiT-L (ResNet152x4) | Noisy Student (EfficientNet-L2)  |\\n| --- | --- | --- | --- | --- | --- |\\n|  ImageNet | 88.55 ± 0.04 | 87.76 ± 0.03 | 85.30 ± 0.02 | 87.54 ± 0.02 | 88.4/88.5*  |\\n|  ImageNet ReaL | 90.72 ± 0.05 | 90.54 ± 0.03 | 88.62 ± 0.05 | 90.54 | 90.55  |\\n|  CIFAR-10 | 99.50 ± 0.06 | 99.42 ± 0.03 | 99.15 ± 0.03 | 99.37 ± 0.06 | -  |\\n|  CIFAR-100 | 94.55 ± 0.04 | 93.90 ± 0.05 | 93.25 ± 0.05 | 93.51 ± 0.08 | -  |\\n|  Oxford-IIIT Pets | 97.56 ± 0.03 | 97.32 ± 0.11 | 94.67 ± 0.15 | 96.62 ± 0.23 | -  |\\n|  Oxford Flowers-102 | 99.68 ± 0.02 | 99.74 ± 0.00 | 99.61 ± 0.02 | 99.63 ± 0.03 | -  |\\n|  VTAB (19 tasks) | 77.63 ± 0.23 | 76.28 ± 0.46 | 72.72 ± 0.21 | 76.29 ± 1.70 | -  |\\n|  TPUv3-core-days | 2.5k | 0.68k | 0.23k | 9.9k | 12.3k  |\\n\\nTable 2: Comparison with state of the art on popular image classification benchmarks. We report mean and standard deviation of the accuracies, averaged over three fine-tuning runs. Vision Transformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all datasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the smaller public ImageNet-21k dataset performs well too. *Slightly improved  $88.5\\\\%$  result reported in Touvron et al. (2020).\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\\n\\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note that pre-training efficiency may be affected not only by the architecture choice, but also other parameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of performance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days.\\n\\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA methods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen et al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a). ViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the Specialized the performance of the top two models is similar.\\n\\n# 4.3 PRE-TRAINING DATA REQUIREMENTS\\n\\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer inductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of experiments.\\n\\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. To boost the performance on the smaller datasets, we optimize three basic regularization parameters - weight decay, dropout, and label smoothing. Figure 3 shows the results after finetuning to ImageNet (results on other datasets are shown in Table 5) $^{2}$ . When pre-trained on the smallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite (moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only with JFT-300M, do we see the full benefit of larger models. Figure 3 also shows the performance\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: Transfer to ImageNet. While large ViT models perform worse than BiT ResNets (shaded area) when pre-trained on small datasets, they shine when pre-trained on larger datasets. Similarly, larger ViT variants overtake smaller ones as the dataset grows.\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 4: Linear few-shot evaluation on ImageNet versus pre-training size. ResNets perform better with smaller pre-training datasets but plateau sooner than ViT, which performs better with larger pre-training. ViT-b is ViT-B with all hidden dimensions halved.\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers, ResNets, and hybrids. Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models.\\n\\n![img-6.jpeg](img-6.jpeg)\\n\\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but with the larger datasets, ViT overtakes.\\n\\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-300M dataset. We do not perform additional regularization on the smaller subsets and use the same hyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the effect of regularization. We do, however, use early-stopping, and report the best validation accuracy achieved during training. To save compute, we report few-shot linear accuracy instead of full finetuning accuracy. Figure 4 contains the results. Vision Transformers overfit more than ResNets with comparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than ResNet50; it performs much worse on the 9M subset, but better on  $90\\\\mathrm{M}+$  subsets. The same is true for ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive bias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from data is sufficient, even beneficial.\\n\\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB (Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT is an exciting direction of future work.\\n\\nPublished as a conference paper at ICLR 2021\\n\\n# 4.4 SCALING STUDY\\n\\nWe perform a controlled scaling study of different models by evaluating transfer performance from JFT-300M. In this setting data size does not bottleneck the models\\' performances, and we assess performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1, R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus L/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pretrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet backbone).\\n\\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5 for details on computational costs). Detailed results per model are provided in Table 6 in the Appendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the performance/compute trade-off. ViT uses approximately  $2 - 4 \\\\times$  less compute to attain the same performance (average over 5 datasets). Second, hybrids slightly outperform ViT at small computational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts.\\n\\n# 4.5 INSPECTING VISION TRANSFORMER\\n\\nTo begin to understand how the Vision Transformer processes image data, we analyze its internal representations. The first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space (Eq. 1). Figure 7 (left) shows the top principal components of the learned embedding filters. The components resemble plausible basis functions for a low-dimensional representation of the fine structure within each patch.\\n\\nAfter the projection, a learned position embedding is added to the patch representations. Figure 7 (center) shows that the model learns to encode distance within the image in the similarity of position embeddings, i.e. closer patches tend to have more similar position embeddings. Further, the row-column structure appears; patches in the same row/column have similar embeddings. Finally, a sinusoidal structure is sometimes apparent for larger grids (Appendix D). That the position embeddings learn to represent 2D image topology explains why hand-crafted 2D-aware embedding variants do not yield improvements (Appendix D.4).\\n\\nSelf-attention allows ViT to integrate information across the entire image even in the lowest layers. We investigate to what degree the network makes use of this capability. Specifically, we compute the average distance in image space across which information is integrated, based on the attention weights (Figure 7, right). This \"attention distance\" is analogous to receptive field size in CNNs.\\n\\nWe find that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. Other attention heads have consistently small attention distances in the low layers. This highly localized attention is less pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right), suggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the attention distance increases with network depth. Globally, we find that the model attends to image regions that are semantically relevant for classification (Figure 6).\\n\\n![img-7.jpeg](img-7.jpeg)\\nFigure 6: Representative examples of attention from the output token to the input space. See Appendix D.7 for details.\\n\\n# 4.6 SELF-SUPERVISION\\n\\nTransformers show impressive performance on NLP tasks. However, much of their success stems not only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-8.jpeg](img-8.jpeg)\\nRGB embedding filters (first 28 principal components)\\n\\n![img-9.jpeg](img-9.jpeg)\\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Similarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position embedding of the patch with the indicated row and column and the position embeddings of all other patches. Right: Size of attended area by head and network depth. Each dot shows the mean attention distance across images for one of 16 heads at one layer. See Appendix D.7 for details.\\n\\n![img-10.jpeg](img-10.jpeg)\\n\\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch prediction for self-supervision, mimicking the masked language modeling task used in BERT. With self-supervised pre-training, our smaller ViT-B/16 model achieves  $79.9\\\\%$  accuracy on ImageNet, a significant improvement of  $2\\\\%$  to training from scratch, but still  $4\\\\%$  behind supervised pre-training. Appendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen et al., 2020b; He et al., 2020; Bachman et al., 2019; Henaff et al., 2020) to future work.\\n\\n# 5 CONCLUSION\\n\\nWe have explored the direct application of Transformers to image recognition. Unlike prior works using self-attention in computer vision, we do not introduce image-specific inductive biases into the architecture apart from the initial patch extraction step. Instead, we interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP. This simple, yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets. Thus, Vision Transformer matches or exceeds the state of the art on many image classification datasets, whilst being relatively cheap to pre-train.\\n\\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other computer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion et al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-supervised pre-training methods. Our initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pretraining. Finally, further scaling of ViT would likely lead to improved performance.\\n\\n# ACKNOWLEDGEMENTS\\n\\nThe work was performed in Berlin, Zürich, and Amsterdam. We thank many colleagues at Google for their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-source release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale training infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lučić, Noam Shazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\\n\\n# REFERENCES\\n\\nSamira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In ACL, 2020.\\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In NeurIPS, 2019.\\n\\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In ICLR, 2019.\\n\\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks. In ICCV, 2019.\\n\\nLucas Beyer, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done with imagenet? arXiv, 2020.\\n\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv, 2020.\\n\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\n\\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from pixels. In ICML, 2020a.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020b.\\n\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c.\\n\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv, 2019.\\n\\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention and convolutional layers. In ICLR, 2020.\\n\\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.\\n\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convolutional neural networks. arXiv, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\\n\\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. arXiv, 2019.\\n\\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In CVPR, 2018.\\n\\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In ICCV, 2019.\\n\\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and Thomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020.\\n\\nOlivier J. Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, and Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. In ICML, 2020.\\n\\nPublished as a conference paper at ICLR 2021\\n\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. 2015.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.\\nY. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1:541-551, 1989.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan First, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv, 2020.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A Simple and Performant Baseline for Vision and Language. In Arxiv, 2019.\\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. arXiv, 2020.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visio-llinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV, 2018.\\nM. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In ICVGIP, 2008.\\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012.\\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, 2018.\\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838-855, 1992. doi: 10.1137/0330046. URL https://doi.org/10.1137/0330046.\\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv preprint arXiv:1903.10520, 2019.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding with unsupervised learning. Technical Report, 2018.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. Technical Report, 2019.\\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-attention in vision models. In NeurIPS, 2019.\\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, 2017.\\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In ICCV, 2019.\\n\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy. In NeurIPS. 2019.\\n\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy: Fixefficientnet. arXiv preprint arXiv:2003.08237, 2020.\\n\\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain Gelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\\n\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.\\n\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint arXiv:2003.07853, 2020b.\\n\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. Learning deep transformer models for machine translation. In ACL, 2019.\\n\\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018.\\n\\nDirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video models. In ICLR, 2019.\\n\\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing for computer vision. arxiv, 2020.\\n\\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\\n\\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classification. In CVPR, 2020.\\n\\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S^{4}L: Self-Supervised Semi-Supervised Learning. In ICCV, 2019a.\\n\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019b.\\n\\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020.\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|  Models | Dataset | Epochs | Base LR | LR decay | Weight decay | Dropout  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  ViT-B/{16,32} | JFT-300M | 7 | 8·10-4 | linear | 0.1 | 0.0  |\\n|  ViT-L/32 | JFT-300M | 7 | 6·10-4 | linear | 0.1 | 0.0  |\\n|  ViT-L/16 | JFT-300M | 7/14 | 4·10-4 | linear | 0.1 | 0.0  |\\n|  ViT-H/14 | JFT-300M | 14 | 3·10-4 | linear | 0.1 | 0.0  |\\n|  R50x{1,2} | JFT-300M | 7 | 10-3 | linear | 0.1 | 0.0  |\\n|  R101x1 | JFT-300M | 7 | 8·10-4 | linear | 0.1 | 0.0  |\\n|  R152x{1,2} | JFT-300M | 7 | 6·10-4 | linear | 0.1 | 0.0  |\\n|  R50+ViT-B/{16,32} | JFT-300M | 7 | 8·10-4 | linear | 0.1 | 0.0  |\\n|  R50+ViT-L/32 | JFT-300M | 7 | 2·10-4 | linear | 0.1 | 0.0  |\\n|  R50+ViT-L/16 | JFT-300M | 7/14 | 4·10-4 | linear | 0.1 | 0.0  |\\n|  ViT-B/{16,32} | ImageNet-21k | 90 | 10-3 | linear | 0.03 | 0.1  |\\n|  ViT-L/{16,32} | ImageNet-21k | 30/90 | 10-3 | linear | 0.03 | 0.1  |\\n|  ViT-* | ImageNet | 300 | 3·10-3 | cosine | 0.3 | 0.1  |\\n\\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet we found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\\n\\n# APPENDIX\\n\\n# A MULTIHEAD SELF-ATTENTION\\n\\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural architectures. For each element in an input sequence  $\\\\mathbf{z} \\\\in \\\\mathbb{R}^{N \\\\times D}$ , we compute a weighted sum over all values  $\\\\mathbf{v}$  in the sequence. The attention weights  $A_{ij}$  are based on the pairwise similarity between two elements of the sequence and their respective query  $\\\\mathbf{q}^i$  and key  $\\\\mathbf{k}^j$  representations.\\n\\n$$\\n[ \\\\mathbf {q}, \\\\mathbf {k}, \\\\mathbf {v} ] = \\\\mathbf {z} \\\\mathbf {U} _ {q k v} \\\\quad \\\\mathbf {U} _ {q k v} \\\\in \\\\mathbb {R} ^ {D \\\\times 3 D _ {h}}, \\\\tag {5}\\n$$\\n\\n$$\\nA = \\\\operatorname {s o f t m a x} \\\\left(\\\\mathbf {q} \\\\mathbf {k} ^ {\\\\top} / \\\\sqrt {D _ {h}}\\\\right) \\\\quad A \\\\in \\\\mathbb {R} ^ {N \\\\times N}, \\\\tag {6}\\n$$\\n\\n$$\\n\\\\operatorname {S A} (\\\\mathbf {z}) = A \\\\mathbf {v}. \\\\tag {7}\\n$$\\n\\nMultihead self-attention (MSA) is an extension of SA in which we run  $k$  self-attention operations, called \"heads\", in parallel, and project their concatenated outputs. To keep compute and number of parameters constant when changing  $k$ ,  $D_h$  (Eq. 5) is typically set to  $D / k$ .\\n\\n$$\\n\\\\operatorname {M S A} (\\\\mathbf {z}) = \\\\left[ \\\\mathrm {S A} _ {1} (z); \\\\mathrm {S A} _ {2} (z); \\\\dots ; \\\\mathrm {S A} _ {k} (z) \\\\right] \\\\mathbf {U} _ {m s a} \\\\quad \\\\mathbf {U} _ {m s a} \\\\in \\\\mathbb {R} ^ {k \\\\cdot D _ {h} \\\\times D} \\\\tag {8}\\n$$\\n\\n# B EXPERIMENT DETAILS\\n\\n# B.1 TRAINING\\n\\nTable 3 summarizes our training setups for our different models. We found strong regularization to be key when training models from scratch on ImageNet. Dropout, when used, is applied after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all training is done on resolution 224.\\n\\n# B.1.1 FINE-TUNING\\n\\nWe fine-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over learning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training set (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the remaining data. For final results we train on the entire training set and evaluate on the respective test data. For fine-tuning ResNets and hybrid models we use the exact same setup, with the only exception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|  Dataset | Steps | Base LR  |\\n| --- | --- | --- |\\n|  ImageNet | 20000 | {0.003, 0.01, 0.03, 0.06}  |\\n|  CIFAR100 | 10000 | {0.001, 0.003, 0.01, 0.03}  |\\n|  CIFAR10 | 10000 | {0.001, 0.003, 0.01, 0.03}  |\\n|  Oxford-IIIT Pets | 500 | {0.001, 0.003, 0.01, 0.03}  |\\n|  Oxford Flowers-102 | 500 | {0.001, 0.003, 0.01, 0.03}  |\\n|  VTAB (19 tasks) | 2500 | 0.01  |\\n\\nTable 4: Hyperparameters for fine-tuning. All models are fine-tuned with cosine learning rate decay, a batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise, fine-tuning resolution is 384.\\n\\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across this run and our sweep. Finally, if not mentioned otherwise, all fine-tuning experiments run at 384 resolution (running fine-tuning at different resolution than training is common practice (Kolesnikov et al., 2020)).\\n\\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset. We found this to be a little more robust than simply re-initializing the very last layer.\\n\\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter setting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this setting by running a small sweep over two learning rates and two schedules, and selecting the setting with the highest VTAB score on the 200-example validation sets. We follow the pre-processing used in Kolesnikov et al. (2020), except that we do not use task-specific input resolutions. Instead we find that Vision Transformer benefits most from a high resolution  $(384 \\\\times 384)$  for all tasks.\\n\\n# B.1.2 SELF-SUPERVISION\\n\\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To do so we corrupt  $50\\\\%$  of patch embeddings by either replacing their embeddings with a learnable [mask] embedding  $(80\\\\%)$ , a random other patch embedding  $(10\\\\%)$  or just keeping them as is  $(10\\\\%)$ . This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we predict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective patch representations.\\n\\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We use Adam, with a base learning rate of  $2 \\\\cdot 10^{-4}$ , warmup of 10k steps and cosine learning rate decay. As prediction targets for pretraining we tried the following settings: 1) predicting only the mean, 3bit color (i.e., 1 prediction of 512 colors), 2) predicting a  $4 \\\\times 4$  downsized version of the  $16 \\\\times 16$  patch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch using L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite well, though L2 was slightly worse. We report final results only for option 1) because it has shown best few-shot performance. We also experimented with  $15\\\\%$  corruption rate as used by Devlin et al. (2019) but results were also slightly worse on our few-shot metrics.\\n\\nLastly, we would like to remark that our instantiation of masked patch prediction doesn\\'t require such an enormous amount of pretraining nor a large dataset such as JFT in order to lead to similar performance gains on ImageNet classification. That is, we observed diminishing returns on downstream performance after 100k pretraining steps, and see similar gains when pretraining on ImageNet.\\n\\n# C ADDITIONAL RESULTS\\n\\nWe report detailed results corresponding to the figures presented in the paper. Table 5 corresponds to Figure 3 from the paper and shows transfer performance of different ViT models pre-trained on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|   |  | ViT-B/16 | ViT-B/32 | ViT-L/16 | ViT-L/32 | ViT-H/14  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  ImageNet | CIFAR-10 | 98.13 | 97.77 | 97.86 | 97.94 | -  |\\n|   |  CIFAR-100 | 87.13 | 86.31 | 86.35 | 87.07 | -  |\\n|   |  ImageNet | 77.91 | 73.38 | 76.53 | 71.16 | -  |\\n|   |  ImageNet ReaL | 83.57 | 79.56 | 82.19 | 77.83 | -  |\\n|   |  Oxford Flowers-102 | 89.49 | 85.43 | 89.66 | 86.36 | -  |\\n|   |  Oxford-IIIT-Pets | 93.81 | 92.04 | 93.64 | 91.35 | -  |\\n|  ImageNet-21k | CIFAR-10 | 98.95 | 98.79 | 99.16 | 99.13 | 99.27  |\\n|   |  CIFAR-100 | 91.67 | 91.97 | 93.44 | 93.04 | 93.82  |\\n|   |  ImageNet | 83.97 | 81.28 | 85.15 | 80.99 | 85.13  |\\n|   |  ImageNet ReaL | 88.35 | 86.63 | 88.40 | 85.65 | 88.70  |\\n|   |  Oxford Flowers-102 | 99.38 | 99.11 | 99.61 | 99.19 | 99.51  |\\n|   |  Oxford-IIIT-Pets | 94.43 | 93.02 | 94.73 | 93.09 | 94.82  |\\n|  JFT-300M | CIFAR-10 | 99.00 | 98.61 | 99.38 | 99.19 | 99.50  |\\n|   |  CIFAR-100 | 91.87 | 90.49 | 94.04 | 92.52 | 94.55  |\\n|   |  ImageNet | 84.15 | 80.73 | 87.12 | 84.37 | 88.04  |\\n|   |  ImageNet ReaL | 88.85 | 86.27 | 89.99 | 88.28 | 90.33  |\\n|   |  Oxford Flowers-102 | 99.56 | 99.27 | 99.56 | 99.45 | 99.68  |\\n|   |  Oxford-IIIT-Pets | 95.80 | 93.40 | 97.11 | 95.83 | 97.56  |\\n\\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on ImageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models are fine-tuned at 384 resolution. Note that the ImageNet results are computed without additional techniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\\n\\n|  name | Epochs | ImageNet | ImageNet ReaL | CIFAR-10 | CIFAR-100 | Pets | Flowers | exaFLOPs  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  ViT-B/32 | 7 | 80.73 | 86.27 | 98.61 | 90.49 | 93.40 | 99.27 | 55  |\\n|  ViT-B/16 | 7 | 84.15 | 88.85 | 99.00 | 91.87 | 95.80 | 99.56 | 224  |\\n|  ViT-L/32 | 7 | 84.37 | 88.28 | 99.19 | 92.52 | 95.83 | 99.45 | 196  |\\n|  ViT-L/16 | 7 | 86.30 | 89.43 | 99.38 | 93.46 | 96.81 | 99.66 | 783  |\\n|  ViT-L/16 | 14 | 87.12 | 89.99 | 99.38 | 94.04 | 97.11 | 99.56 | 1567  |\\n|  ViT-H/14 | 14 | 88.08 | 90.36 | 99.50 | 94.71 | 97.11 | 99.71 | 4262  |\\n|  ResNet50x1 | 7 | 77.54 | 84.56 | 97.67 | 86.07 | 91.11 | 94.26 | 50  |\\n|  ResNet50x2 | 7 | 82.12 | 87.94 | 98.29 | 89.20 | 93.43 | 97.02 | 199  |\\n|  ResNet101x1 | 7 | 80.67 | 87.07 | 98.48 | 89.17 | 94.08 | 95.95 | 96  |\\n|  ResNet152x1 | 7 | 81.88 | 87.96 | 98.82 | 90.22 | 94.17 | 96.94 | 141  |\\n|  ResNet152x2 | 7 | 84.97 | 89.69 | 99.06 | 92.05 | 95.37 | 98.62 | 563  |\\n|  ResNet152x2 | 14 | 85.56 | 89.89 | 99.24 | 91.92 | 95.75 | 98.75 | 1126  |\\n|  ResNet200x3 | 14 | 87.22 | 90.15 | 99.34 | 93.53 | 96.32 | 99.04 | 3306  |\\n|  R50x1+ViT-B/32 | 7 | 84.90 | 89.15 | 99.01 | 92.24 | 95.75 | 99.46 | 106  |\\n|  R50x1+ViT-B/16 | 7 | 85.58 | 89.65 | 99.14 | 92.63 | 96.65 | 99.40 | 274  |\\n|  R50x1+ViT-L/32 | 7 | 85.68 | 89.04 | 99.24 | 92.93 | 96.97 | 99.43 | 246  |\\n|  R50x1+ViT-L/16 | 7 | 86.60 | 89.72 | 99.18 | 93.64 | 97.03 | 99.40 | 859  |\\n|  R50x1+ViT-L/16 | 14 | 87.12 | 89.76 | 99.31 | 93.89 | 97.36 | 99.11 | 1668  |\\n\\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main paper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-aFLOPs).\\n\\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of varying size, as well as the estimated computational cost of their pre-training.\\n\\n# D ADDITIONAL ANALYSES\\n\\n# D.1 SGD VS. ADAM FOR RESNETS\\n\\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional. Here we show the experiments that motivated this choice. Namely, we compare the fine-tuning\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|  Dataset | ResNet50 |   | ResNet152x2  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  Adam | SGD | Adam | SGD  |\\n|  ImageNet | 77.54 | 78.24 | 84.97 | 84.37  |\\n|  CIFAR10 | 97.67 | 97.46 | 99.06 | 99.07  |\\n|  CIFAR100 | 86.07 | 85.17 | 92.05 | 91.06  |\\n|  Oxford-IIIT Pets | 91.11 | 91.00 | 95.37 | 94.79  |\\n|  Oxford Flowers-102 | 94.26 | 92.06 | 98.62 | 99.32  |\\n|  Average | 89.33 | 88.79 | 94.01 | 93.72  |\\n\\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\\n\\n![img-11.jpeg](img-11.jpeg)\\nFigure 8: Scaling different model dimensions of the Vision Transformer.\\n\\n![img-12.jpeg](img-12.jpeg)\\n\\nperformance of two ResNets - 50x1 and 152x2 - pre-trained on JFT with SGD and Adam. For SGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented in Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average. This justifies the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the absolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only for 7 epochs, not 30.\\n\\n# D.2 TRANSFORMER SHAPE\\n\\nWe ran ablations on scaling different dimensions of the Transformer architecture to find out which are best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet for different configurations. All configurations are based on a ViT model with 8 layers,  $D = 1024$ ,  $D_{MLP} = 2048$  and a patch size of 32, the intersection of all lines. We can see that scaling the depth results in the biggest improvements which are clearly visible up until 64 layers. However, diminishing returns are already visible after 16 layers. Interestingly, scaling the width of the network seems to result in the smallest changes. Decreasing the patch size and thus increasing the effective sequence length shows surprisingly robust improvements without introducing parameters. These findings suggest that compute might be a better predictor of performance than the number of parameters, and that scaling should emphasize depth over width if any. Overall, we find that scaling all dimensions proportionally results in robust improvements.\\n\\n# D.3 HEAD TYPE AND CLASSTOKEN\\n\\nIn order to stay as close as possible to the original Transformer model, we made use of an additional [class] token, which is taken as image representation. The output of this token is then transformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity in the single hidden layer.\\n\\nThis design is inherited from the Transformer model for text, and we use it throughout the main paper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP) them, followed by a linear classifier—just like ResNet\\'s final feature map—performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-13.jpeg](img-13.jpeg)\\nFigure 9: Comparison of class-token and global average pooling classifiers. Both work similarly well, but require different learning-rates.\\n\\n|  Pos. Emb. | Default/Stem | Every Layer | Every Layer-Shared  |\\n| --- | --- | --- | --- |\\n|  No Pos. Emb. | 0.61382 | N/A | N/A  |\\n|  1-D Pos. Emb. | 0.64206 | 0.63964 | 0.64292  |\\n|  2-D Pos. Emb. | 0.64001 | 0.64046 | 0.64022  |\\n|  Rel. Pos. Emb. | 0.64032 | N/A | N/A  |\\n\\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on ImageNet 5-shot linear.\\n\\nthe difference in performance is fully explained by the requirement for a different learning-rate, see Figure 9.\\n\\n# D.4 POSITIONAL EMBEDDING\\n\\nWe ran ablations on different ways of encoding spatial information using positional embedding. We tried the following cases:\\n\\n- Providing no positional information: Considering the inputs as a bag of patches.\\n- 1-dimensional positional embedding: Considering the inputs as a sequence of patches in the raster order (default across all other experiments in this paper).\\n- 2-dimensional positional embedding: Considering the inputs as a grid of patches in two dimensions. In this case, two sets of embeddings are learned, each for one of the axes,  $X$ -embedding, and  $Y$ -embedding, each with size  $D / 2$ . Then, based on the coordinate on the path in the input, we concatenate the  $X$  and  $Y$  embedding to get the final positional embedding for that patch.\\n- Relative positional embeddings: Considering the relative distance between patches to encode the spatial information as instead of their absolute position. To do so, we use 1-dimensional Relative Attention, in which we define the relative distance all possible pairs of patches. Thus, for every given pair (one as query, and the other as key/value in the attention mechanism), we have an offset  $p_q - p_k$ , where each offset is associated with an embedding. Then, we simply run extra attention, where we use the original query (the content of query), but use relative positional embeddings as keys. We then use the logits from the relative attention as a bias term and add it to the logits of the main attention (content-based attention) before applying the softmax.\\n\\nIn addition to different ways of encoding spatial information, we also tried different ways of incorporating this information in our model. For the 1-dimensional and 2-dimensional positional embeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-14.jpeg](img-14.jpeg)\\nFigure 10: Position embeddings of models trained with different hyperparameters.\\n\\n![img-15.jpeg](img-15.jpeg)\\n\\n![img-16.jpeg](img-16.jpeg)\\n\\nthe stem of them model and before feeding the inputs to the Transformer encoder (default across all other experiments in this paper); (2) learn and add positional embeddings to the inputs at the beginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of each layer (shared between layers).\\n\\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while there is a large gap between the performances of the model with no positional embedding and models with positional embedding, there is little to no difference between different ways of encoding positional information. We speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g.,  $14 \\\\times 14$  as opposed to  $224 \\\\times 224$ , and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies. Even so, the specific pattern of position embedding similarity learned by the network depends on the training hyperparameters (Figure 10).\\n\\n![img-17.jpeg](img-17.jpeg)\\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for 128 example images by averaging the distance between the query pixel and all other pixels, weighted by the attention weight. Each dot shows the mean attention distance across images for one of 16 heads at one layer. Image width is 224 pixels.\\n\\n![img-18.jpeg](img-18.jpeg)\\n\\n# D.5 EMPIRICAL COMPUTATIONAL COSTS\\n\\nWe are also interested in real-world speed of the architectures on our hardware, which is not always well predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\\n\\nPublished as a conference paper at ICLR 2021\\n\\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the difference between inference and backprop speed is a constant model-independent factor.\\n\\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes. Every single point refers to the peak performance measured across a wide range of batch-sizes. As can be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening for the largest models at the largest resolutions.\\n\\nAnother quantity of interest is the largest batch-size each model can fit onto a core, larger being better for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models. This shows that large ViT models have a clear advantage in terms of memory-efficiency over ResNet models.\\n\\n![img-19.jpeg](img-19.jpeg)\\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models have speed comparable to similar ResNets. Right: Largest per-core batch-size fitting on device with various architectures across input sizes. ViT models are clearly more memory-efficient.\\n\\n# D.6 AXIAL ATTENTION\\n\\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-attention on large inputs that are organized as multidimensional tensors. The general idea of axial attention is to perform multiple attention operations, each along a single axis of the input tensor, instead of applying 1-dimensional attention to the flattened version of the input. In axial attention, each attention mixes information along a particular axis, while keeping information along the other axes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which all the convolutions with kernel size  $3 \\\\times 3$  in a ResNet50 are replaced by axial self-attention, i.e. a row and column attention, augmented by relative positional encoding. We have implemented AxialResNet as a baseline model. $^{3}$ .\\n\\nMoreover, we have modified ViT to process inputs in the 2-dimensional shape, instead of a 1-dimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of a self-attention followed by an MLP, we have a row-self-attention plus an MLP followed by a column-self-attention plus an MLP.\\n\\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on ImageNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of number of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32 and Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-20.jpeg](img-20.jpeg)\\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet 5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).\\n\\n![img-21.jpeg](img-21.jpeg)\\n\\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column self-attention and although the sequence length that self-attention operates on is smaller in axial case, there is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in terms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow on TPUs (Figure 13, right).\\n\\n# D.7 ATTENTION DISTANCE\\n\\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed the average distance spanned by attention weights at different layers (Figure 11). This \"attention distance\" is analogous to receptive field size in CNNs. Average attention distance is highly variable across heads in lower layers, with some heads attending to much of the image, while others attend to small regions at or near the query location. As depth increases, attention distance increases for all heads. In the second half of the network, most heads attend widely across tokens.\\n\\n# D.8 ATTENTION MAPS\\n\\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we used Attention Rollout (Abnar &amp; Zuidema, 2020). Briefly, we averaged attention weights of ViT-L/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts for the mixing of attention across tokens through all layers.\\n\\n# D.9 OBJECTNET RESULTS\\n\\nWe also evaluate our flagship ViT-H/14 model on the ObjectNet benchmark following the evaluation setup in Kolesnikov et al. (2020), resulting in  $82.1\\\\%$  top-5 accuracy and  $61.7\\\\%$  top-1 accuracy.\\n\\n# D.10 VTAB BREAKDOWN\\n\\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-22.jpeg](img-22.jpeg)\\nFigure 14: Further example attention maps as in Figure 6 (random selection).\\n\\nPublished as a conference paper at ICLR 2021\\n\\nTable 9: Breakdown of VTAB-1k performance across tasks.\\n\\n|   | # Caltech101 | # CIFAR-100 | # DTD | # Flowers102 | # Pets | # Sun397 | # SVHN | # Camelyon | # EuroSAT | # Resisc45 | # Retinopathy | # Clevr-Count | # Clevr-Dist | # DMLab | # dSpr-Loc | # dSpr-Ori | # KITTI-Dist | # sNORB-Azim | # sNORB-Elev | # Mean  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  ViT-H/14 (IFT) | 95.3 | 85.5 | 75.2 | 99.7 | 97.2 | 65.0 | 88.9 | 83.3 | 96.7 | 91.4 | 76.6 | 91.7 | 63.8 | 53.1 | 79.4 | 63.3 | 84.5 | 33.2 | 51.2 | 77.6  |\\n|  ViT-L/16 (IFT) | 95.4 | 81.9 | 74.3 | 99.7 | 96.7 | 63.5 | 87.4 | 83.6 | 96.5 | 89.7 | 77.1 | 86.4 | 63.1 | 49.7 | 74.5 | 60.5 | 82.2 | 36.2 | 51.1 | 76.3  |\\n|  ViT-L/16 (I21k) | 90.8 | 84.1 | 74.1 | 99.3 | 92.7 | 61.0 | 80.9 | 82.5 | 95.6 | 85.2 | 75.3 | 70.3 | 56.1 | 41.9 | 74.7 | 64.9 | 79.9 | 30.5 | 41.7 | 72.7  |'}, page_content='# An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\\n\\nAlexey Dosovitskiy^{∗,†}, Lucas Beyer^{∗}, Alexander Kolesnikov^{∗}, Dirk Weissenborn^{∗},\\nXiaohua Zhai^{∗}, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby^{∗,†}\\n^{∗}equal technical contribution, ^{†}equal advising\\nGoogle Research, Brain Team\\n{adosovitskiy, neilhoulsby}@google.com\\n\\n###### Abstract\\n\\nWhile the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\\n\\n## 1 Introduction\\n\\nSelf-attention-based architectures, in particular Transformers *(Vaswani et al., 2017)*, have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset *(Devlin et al., 2019)*. Thanks to Transformers’ computational efficiency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters *(Brown et al., 2020; Lepikhin et al., 2020)*. With the models and datasets growing, there is still no sign of saturating performance.\\n\\nIn computer vision, however, convolutional architectures remain dominant *(LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016)*. Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention *(Wang et al., 2018; Carion et al., 2020)*, some replacing the convolutions entirely *(Ramachandran et al., 2019; Wang et al., 2020a)*. The latter models, while theoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-like architectures are still state of the art *(Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020)*.\\n\\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.\\n\\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size. This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\\n\\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.\\n\\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of $88.55\\\\%$ on ImageNet, $90.72\\\\%$ on ImageNet-ReaL, $94.55\\\\%$ on CIFAR-100, and $77.63\\\\%$ on the VTAB suite of 19 tasks.\\n\\n## 2 Related Work\\n\\nTransformers were proposed by *Vaswani et al. (2017)* for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT *(Devlin et al., 2019)* uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task *(Radford et al., 2018, 2019; Brown et al., 2020)*.\\n\\nNaive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. *Parmar et al. (2018)* applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions *(Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020)*. In a different line of work, Sparse Transformers *(Child et al., 2019)* employ scalable approximations to global self-attention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes *(Weissenborn et al., 2019)*, in the extreme case only along individual axes *(Ho et al., 2019; Wang et al., 2020a)*. Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators.\\n\\nMost related to ours is the model of *Cordonnier et al. (2020)*, which extracts patches of size $2\\\\times 2$ from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, *Cordonnier et al. (2020)* use a small patch size of $2\\\\times 2$ pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.\\n\\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classification *(Bello et al., 2019)* or by further processing the output of a CNN using self-attention, e.g. for object detection *(Hu et al., 2018; Carion et al., 2020)*, video processing *(Wang et al., 2018; Sun et al., 2019)*, image classification *(Wu et al., 2020)*, unsupervised object discovery *(Locatello et al., 2020)*, or unified text-vision tasks *(Chen et al., 2020c; Lu et al., 2019; Li et al., 2019)*.\\n\\nAnother recent related model is image GPT (iGPT) *(Chen et al., 2020a)*, which applies Transformers to image pixels after reducing image resolution and color space. The model is trained in an unsupervised fashion as a generative model, and the resulting representation can then be fine-tuned or probed linearly for classification performance, achieving a maximal accuracy of 72% on ImageNet.\\n\\nOur work adds to the increasing collection of papers that explore image recognition at larger scales than the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-the-art results on standard benchmarks *(Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020)*. Moreover, *Sun et al. (2017)* study how CNN performance scales with dataset size, and *Kolesnikov et al. (2020); Djolonga et al. (2020)* perform an empirical exploration of CNN transfer learning from large scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as well, but train Transformers instead of ResNet-based models used in prior works.\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable \"classification token\" to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017).\\n\\n![img-1.jpeg](img-1.jpeg)\\n\\n# 3 METHOD\\n\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and their efficient implementations – can be used almost out of the box.\\n\\n# 3.1 VISION TRANSFORMER (VIT)\\n\\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image  $\\\\mathbf{x} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}$  into a sequence of flattened 2D patches  $\\\\mathbf{x}_p \\\\in \\\\mathbb{R}^{N \\\\times (P^2 \\\\cdot C)}$ , where  $(H, W)$  is the resolution of the original image,  $C$  is the number of channels,  $(P, P)$  is the resolution of each image patch, and  $N = HW / P^2$  is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size  $D$  through all of its layers, so we flatten the patches and map to  $D$  dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\\n\\nSimilar to BERT\\'s [class] token, we prepend a learnable embedding to the sequence of embedded patches  $(\\\\mathbf{z}_0^0 = \\\\mathbf{x}_{\\\\mathrm{class}})$ , whose state at the output of the Transformer encoder  $(\\\\mathbf{z}_L^0)$  serves as the image representation  $\\\\mathbf{y}$  (Eq. 4). Both during pre-training and fine-tuning, a classification head is attached to  $\\\\mathbf{z}_L^0$ . The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.\\n\\nPosition embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.\\n\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski &amp; Auli, 2019).\\n\\nThe MLP contains two layers with a GELU non-linearity.\\n\\n$\\\\mathbf{z}_{0}$ $=\\\\left[\\\\mathbf{x}_{\\\\text{class}};\\\\,\\\\mathbf{x}_{p}^{1}\\\\mathbf{E};\\\\,\\\\mathbf{x}_{p}^{2}\\\\mathbf{E};\\\\cdots;\\\\,\\\\mathbf{x}_{p}^{N}\\\\mathbf{E}\\\\right]+\\\\mathbf{E}_{pos},\\\\quad$ $\\\\mathbf{E}\\\\in\\\\mathbb{R}^{(P^{2}\\\\cdot C)\\\\times D},\\\\,\\\\mathbf{E}_{pos}\\\\in\\\\mathbb{R}^{(N+1)\\\\times D}$ (1)\\n$\\\\mathbf{z^{\\\\prime}}_{\\\\ell}$ $=\\\\mathrm{MSA}(\\\\mathrm{LN}(\\\\mathbf{z}_{\\\\ell-1}))+\\\\mathbf{z}_{\\\\ell-1},$ $\\\\ell=1\\\\ldots L$ (2)\\n$\\\\mathbf{z}_{\\\\ell}$ $=\\\\mathrm{MLP}(\\\\mathrm{LN}(\\\\mathbf{z^{\\\\prime}}_{\\\\ell}))+\\\\mathbf{z^{\\\\prime}}_{\\\\ell},$ $\\\\ell=1\\\\ldots L$ (3)\\n$\\\\mathbf{y}$ $=\\\\mathrm{LN}(\\\\mathbf{z}_{L}^{0})$ (4)\\n\\n#### Inductive bias.\\n\\nWe note that Vision Transformer has much less image-specific inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global. The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution (as described below). Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch.\\n\\n#### Hybrid Architecture.\\n\\nAs an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN *(LeCun et al., 1989)*. In this hybrid model, the patch embedding projection $\\\\mathbf{E}$ (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size 1x1, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above.\\n\\n### 3.2 Fine-tuning and Higher Resolution\\n\\nTypically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks. For this, we remove the pre-trained prediction head and attach a zero-initialized $D\\\\times K$ feedforward layer, where $K$ is the number of downstream classes. It is often beneficial to fine-tune at higher resolution than pre-training *(Touvron et al., 2019; Kolesnikov et al., 2020)*. When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image. Note that this resolution adjustment and patch extraction are the only points at which an inductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\\n\\n## 4 Experiments\\n\\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the hybrid. To understand the data requirements of each model, we pre-train on datasets of varying size and evaluate many benchmark tasks. When considering the computational cost of pre-training the model, ViT performs very favourably, attaining state of the art on most recognition benchmarks at a lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show that self-supervised ViT holds promise for the future.\\n\\n### 4.1 Setup\\n\\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes and 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with 21k classes and 14M images *(Deng et al., 2009)*, and JFT *(Sun et al., 2017)* with 18k classes and 303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the downstream tasks following *Kolesnikov et al. (2020)*. We transfer the models trained on these dataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up ReaL labels *(Beyer et al., 2020)*, CIFAR-10/100 *(Krizhevsky, 2009)*, Oxford-IIIT Pets *(Parkhi et al., 2012)*, and Oxford Flowers-102 *(Nilsback and Zisserman, 2008)*. For these datasets, pre-processing follows *Kolesnikov et al. (2020)*.\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|  Model | Layers | Hidden size D | MLP size | Heads | Params  |\\n| --- | --- | --- | --- | --- | --- |\\n|  ViT-Base | 12 | 768 | 3072 | 12 | 86M  |\\n|  ViT-Large | 24 | 1024 | 4096 | 16 | 307M  |\\n|  ViT-Huge | 32 | 1280 | 5120 | 16 | 632M  |\\n\\nTable 1: Details of Vision Transformer model variants.\\n\\nWe also evaluate on the 19-task VTAB classification suite (Zhai et al., 2019b). VTAB evaluates low-data transfer to diverse tasks, using 1000 training examples per task. The tasks are divided into three groups: Natural - tasks like the above, Pets, CIFAR, etc. Specialized - medical and satellite imagery, and Structured - tasks that require geometric understanding like localization.\\n\\nModel Variants. We base ViT configurations on those used for BERT (Devlin et al., 2019), as summarized in Table 1. The \"Base\" and \"Large\" models are directly adopted from BERT and we add the larger \"Huge\" model. In what follows we use brief notation to indicate the model size and the input patch size: for instance, ViT-L/16 means the \"Large\" variant with  $16 \\\\times 16$  input patch size. Note that the Transformer\\'s sequence length is inversely proportional to the square of the patch size, thus models with smaller patch size are computationally more expensive.\\n\\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization layers (Ioffe &amp; Szegedy, 2015) with Group Normalization (Wu &amp; He, 2018), and used standardized convolutions (Qiao et al., 2019). These modifications improve transfer (Kolesnikov et al., 2020), and we denote the modified model \"ResNet (BiT)\". For the hybrids, we feed the intermediate feature maps into ViT with patch size of one \"pixel\". To experiment with different sequence lengths, we either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same number of layers in stage 3 (keeping the total number of layers), and take the output of this extended stage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\\n\\nTraining &amp; Fine-tuning. We train all models, including ResNets, using Adam (Kingma &amp; Ba, 2015) with  $\\\\beta_{1} = 0.9$ ,  $\\\\beta_{2} = 0.999$ , a batch size of 4096 and apply a high weight decay of 0.1, which we found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common practices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning rate warmup and decay, see Appendix B.1 for details. For fine-tuning we use SGD with momentum, batch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we fine-tuned at higher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak &amp; Juditsky (1992) averaging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\\n\\nMetrics. We report results on downstream datasets either through few-shot or fine-tuning accuracy. Fine-tuning accuracies capture the performance of each model after fine-tuning it on the respective dataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem that maps the (frozen) representation of a subset of training images to  $\\\\{-1,1\\\\}^K$  target vectors. This formulation allows us to recover the exact solution in closed form. Though we mainly focus on fine-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-fly evaluation where fine-tuning would be too costly.\\n\\n# 4.2 COMPARISON TO STATE OF THE ART\\n\\nWe first compare our largest models - ViT-H/14 and ViT-L/16 - to state-of-the-art CNNs from the literature. The first comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which performs supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al., 2020), which is a large EfficientNet trained using semi-supervised learning on ImageNet and JFT-300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and BiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we report the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU v3 cores (2 per chip) used for training multiplied by the training time in days.\\n\\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L (which is pre-trained on the same dataset) on all tasks, while requiring substantially less computational resources to train. The larger model, ViT-H/14, further improves the performance, especially on the more challenging datasets - ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|   | Ours-JFT (ViT-H/14) | Ours-JFT (ViT-L/16) | Ours-I21k (ViT-L/16) | BiT-L (ResNet152x4) | Noisy Student (EfficientNet-L2)  |\\n| --- | --- | --- | --- | --- | --- |\\n|  ImageNet | 88.55 ± 0.04 | 87.76 ± 0.03 | 85.30 ± 0.02 | 87.54 ± 0.02 | 88.4/88.5*  |\\n|  ImageNet ReaL | 90.72 ± 0.05 | 90.54 ± 0.03 | 88.62 ± 0.05 | 90.54 | 90.55  |\\n|  CIFAR-10 | 99.50 ± 0.06 | 99.42 ± 0.03 | 99.15 ± 0.03 | 99.37 ± 0.06 | -  |\\n|  CIFAR-100 | 94.55 ± 0.04 | 93.90 ± 0.05 | 93.25 ± 0.05 | 93.51 ± 0.08 | -  |\\n|  Oxford-IIIT Pets | 97.56 ± 0.03 | 97.32 ± 0.11 | 94.67 ± 0.15 | 96.62 ± 0.23 | -  |\\n|  Oxford Flowers-102 | 99.68 ± 0.02 | 99.74 ± 0.00 | 99.61 ± 0.02 | 99.63 ± 0.03 | -  |\\n|  VTAB (19 tasks) | 77.63 ± 0.23 | 76.28 ± 0.46 | 72.72 ± 0.21 | 76.29 ± 1.70 | -  |\\n|  TPUv3-core-days | 2.5k | 0.68k | 0.23k | 9.9k | 12.3k  |\\n\\nTable 2: Comparison with state of the art on popular image classification benchmarks. We report mean and standard deviation of the accuracies, averaged over three fine-tuning runs. Vision Transformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all datasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the smaller public ImageNet-21k dataset performs well too. *Slightly improved  $88.5\\\\%$  result reported in Touvron et al. (2020).\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\\n\\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note that pre-training efficiency may be affected not only by the architecture choice, but also other parameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of performance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days.\\n\\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA methods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen et al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a). ViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the Specialized the performance of the top two models is similar.\\n\\n# 4.3 PRE-TRAINING DATA REQUIREMENTS\\n\\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer inductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of experiments.\\n\\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. To boost the performance on the smaller datasets, we optimize three basic regularization parameters - weight decay, dropout, and label smoothing. Figure 3 shows the results after finetuning to ImageNet (results on other datasets are shown in Table 5) $^{2}$ . When pre-trained on the smallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite (moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only with JFT-300M, do we see the full benefit of larger models. Figure 3 also shows the performance\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: Transfer to ImageNet. While large ViT models perform worse than BiT ResNets (shaded area) when pre-trained on small datasets, they shine when pre-trained on larger datasets. Similarly, larger ViT variants overtake smaller ones as the dataset grows.\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 4: Linear few-shot evaluation on ImageNet versus pre-training size. ResNets perform better with smaller pre-training datasets but plateau sooner than ViT, which performs better with larger pre-training. ViT-b is ViT-B with all hidden dimensions halved.\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers, ResNets, and hybrids. Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models.\\n\\n![img-6.jpeg](img-6.jpeg)\\n\\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but with the larger datasets, ViT overtakes.\\n\\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-300M dataset. We do not perform additional regularization on the smaller subsets and use the same hyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the effect of regularization. We do, however, use early-stopping, and report the best validation accuracy achieved during training. To save compute, we report few-shot linear accuracy instead of full finetuning accuracy. Figure 4 contains the results. Vision Transformers overfit more than ResNets with comparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than ResNet50; it performs much worse on the 9M subset, but better on  $90\\\\mathrm{M}+$  subsets. The same is true for ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive bias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from data is sufficient, even beneficial.\\n\\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB (Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT is an exciting direction of future work.\\n\\nPublished as a conference paper at ICLR 2021\\n\\n# 4.4 SCALING STUDY\\n\\nWe perform a controlled scaling study of different models by evaluating transfer performance from JFT-300M. In this setting data size does not bottleneck the models\\' performances, and we assess performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1, R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus L/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pretrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet backbone).\\n\\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5 for details on computational costs). Detailed results per model are provided in Table 6 in the Appendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the performance/compute trade-off. ViT uses approximately  $2 - 4 \\\\times$  less compute to attain the same performance (average over 5 datasets). Second, hybrids slightly outperform ViT at small computational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts.\\n\\n# 4.5 INSPECTING VISION TRANSFORMER\\n\\nTo begin to understand how the Vision Transformer processes image data, we analyze its internal representations. The first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space (Eq. 1). Figure 7 (left) shows the top principal components of the learned embedding filters. The components resemble plausible basis functions for a low-dimensional representation of the fine structure within each patch.\\n\\nAfter the projection, a learned position embedding is added to the patch representations. Figure 7 (center) shows that the model learns to encode distance within the image in the similarity of position embeddings, i.e. closer patches tend to have more similar position embeddings. Further, the row-column structure appears; patches in the same row/column have similar embeddings. Finally, a sinusoidal structure is sometimes apparent for larger grids (Appendix D). That the position embeddings learn to represent 2D image topology explains why hand-crafted 2D-aware embedding variants do not yield improvements (Appendix D.4).\\n\\nSelf-attention allows ViT to integrate information across the entire image even in the lowest layers. We investigate to what degree the network makes use of this capability. Specifically, we compute the average distance in image space across which information is integrated, based on the attention weights (Figure 7, right). This \"attention distance\" is analogous to receptive field size in CNNs.\\n\\nWe find that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. Other attention heads have consistently small attention distances in the low layers. This highly localized attention is less pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right), suggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the attention distance increases with network depth. Globally, we find that the model attends to image regions that are semantically relevant for classification (Figure 6).\\n\\n![img-7.jpeg](img-7.jpeg)\\nFigure 6: Representative examples of attention from the output token to the input space. See Appendix D.7 for details.\\n\\n# 4.6 SELF-SUPERVISION\\n\\nTransformers show impressive performance on NLP tasks. However, much of their success stems not only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-8.jpeg](img-8.jpeg)\\nRGB embedding filters (first 28 principal components)\\n\\n![img-9.jpeg](img-9.jpeg)\\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Similarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position embedding of the patch with the indicated row and column and the position embeddings of all other patches. Right: Size of attended area by head and network depth. Each dot shows the mean attention distance across images for one of 16 heads at one layer. See Appendix D.7 for details.\\n\\n![img-10.jpeg](img-10.jpeg)\\n\\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch prediction for self-supervision, mimicking the masked language modeling task used in BERT. With self-supervised pre-training, our smaller ViT-B/16 model achieves  $79.9\\\\%$  accuracy on ImageNet, a significant improvement of  $2\\\\%$  to training from scratch, but still  $4\\\\%$  behind supervised pre-training. Appendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen et al., 2020b; He et al., 2020; Bachman et al., 2019; Henaff et al., 2020) to future work.\\n\\n# 5 CONCLUSION\\n\\nWe have explored the direct application of Transformers to image recognition. Unlike prior works using self-attention in computer vision, we do not introduce image-specific inductive biases into the architecture apart from the initial patch extraction step. Instead, we interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP. This simple, yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets. Thus, Vision Transformer matches or exceeds the state of the art on many image classification datasets, whilst being relatively cheap to pre-train.\\n\\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other computer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion et al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-supervised pre-training methods. Our initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pretraining. Finally, further scaling of ViT would likely lead to improved performance.\\n\\n# ACKNOWLEDGEMENTS\\n\\nThe work was performed in Berlin, Zürich, and Amsterdam. We thank many colleagues at Google for their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-source release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale training infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lučić, Noam Shazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\\n\\n# REFERENCES\\n\\nSamira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In ACL, 2020.\\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In NeurIPS, 2019.\\n\\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In ICLR, 2019.\\n\\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks. In ICCV, 2019.\\n\\nLucas Beyer, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done with imagenet? arXiv, 2020.\\n\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv, 2020.\\n\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\n\\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from pixels. In ICML, 2020a.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020b.\\n\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c.\\n\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv, 2019.\\n\\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention and convolutional layers. In ICLR, 2020.\\n\\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.\\n\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convolutional neural networks. arXiv, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\\n\\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. arXiv, 2019.\\n\\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In CVPR, 2018.\\n\\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In ICCV, 2019.\\n\\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and Thomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020.\\n\\nOlivier J. Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, and Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. In ICML, 2020.\\n\\nPublished as a conference paper at ICLR 2021\\n\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. 2015.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.\\nY. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1:541-551, 1989.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan First, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv, 2020.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A Simple and Performant Baseline for Vision and Language. In Arxiv, 2019.\\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. arXiv, 2020.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visio-llinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV, 2018.\\nM. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In ICVGIP, 2008.\\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012.\\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, 2018.\\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838-855, 1992. doi: 10.1137/0330046. URL https://doi.org/10.1137/0330046.\\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv preprint arXiv:1903.10520, 2019.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding with unsupervised learning. Technical Report, 2018.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. Technical Report, 2019.\\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-attention in vision models. In NeurIPS, 2019.\\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, 2017.\\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In ICCV, 2019.\\n\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy. In NeurIPS. 2019.\\n\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy: Fixefficientnet. arXiv preprint arXiv:2003.08237, 2020.\\n\\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain Gelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\\n\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.\\n\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint arXiv:2003.07853, 2020b.\\n\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. Learning deep transformer models for machine translation. In ACL, 2019.\\n\\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018.\\n\\nDirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video models. In ICLR, 2019.\\n\\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing for computer vision. arxiv, 2020.\\n\\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\\n\\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classification. In CVPR, 2020.\\n\\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S^{4}L: Self-Supervised Semi-Supervised Learning. In ICCV, 2019a.\\n\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019b.\\n\\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020.\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|  Models | Dataset | Epochs | Base LR | LR decay | Weight decay | Dropout  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  ViT-B/{16,32} | JFT-300M | 7 | 8·10-4 | linear | 0.1 | 0.0  |\\n|  ViT-L/32 | JFT-300M | 7 | 6·10-4 | linear | 0.1 | 0.0  |\\n|  ViT-L/16 | JFT-300M | 7/14 | 4·10-4 | linear | 0.1 | 0.0  |\\n|  ViT-H/14 | JFT-300M | 14 | 3·10-4 | linear | 0.1 | 0.0  |\\n|  R50x{1,2} | JFT-300M | 7 | 10-3 | linear | 0.1 | 0.0  |\\n|  R101x1 | JFT-300M | 7 | 8·10-4 | linear | 0.1 | 0.0  |\\n|  R152x{1,2} | JFT-300M | 7 | 6·10-4 | linear | 0.1 | 0.0  |\\n|  R50+ViT-B/{16,32} | JFT-300M | 7 | 8·10-4 | linear | 0.1 | 0.0  |\\n|  R50+ViT-L/32 | JFT-300M | 7 | 2·10-4 | linear | 0.1 | 0.0  |\\n|  R50+ViT-L/16 | JFT-300M | 7/14 | 4·10-4 | linear | 0.1 | 0.0  |\\n|  ViT-B/{16,32} | ImageNet-21k | 90 | 10-3 | linear | 0.03 | 0.1  |\\n|  ViT-L/{16,32} | ImageNet-21k | 30/90 | 10-3 | linear | 0.03 | 0.1  |\\n|  ViT-* | ImageNet | 300 | 3·10-3 | cosine | 0.3 | 0.1  |\\n\\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet we found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\\n\\n# APPENDIX\\n\\n# A MULTIHEAD SELF-ATTENTION\\n\\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural architectures. For each element in an input sequence  $\\\\mathbf{z} \\\\in \\\\mathbb{R}^{N \\\\times D}$ , we compute a weighted sum over all values  $\\\\mathbf{v}$  in the sequence. The attention weights  $A_{ij}$  are based on the pairwise similarity between two elements of the sequence and their respective query  $\\\\mathbf{q}^i$  and key  $\\\\mathbf{k}^j$  representations.\\n\\n$$\\n[ \\\\mathbf {q}, \\\\mathbf {k}, \\\\mathbf {v} ] = \\\\mathbf {z} \\\\mathbf {U} _ {q k v} \\\\quad \\\\mathbf {U} _ {q k v} \\\\in \\\\mathbb {R} ^ {D \\\\times 3 D _ {h}}, \\\\tag {5}\\n$$\\n\\n$$\\nA = \\\\operatorname {s o f t m a x} \\\\left(\\\\mathbf {q} \\\\mathbf {k} ^ {\\\\top} / \\\\sqrt {D _ {h}}\\\\right) \\\\quad A \\\\in \\\\mathbb {R} ^ {N \\\\times N}, \\\\tag {6}\\n$$\\n\\n$$\\n\\\\operatorname {S A} (\\\\mathbf {z}) = A \\\\mathbf {v}. \\\\tag {7}\\n$$\\n\\nMultihead self-attention (MSA) is an extension of SA in which we run  $k$  self-attention operations, called \"heads\", in parallel, and project their concatenated outputs. To keep compute and number of parameters constant when changing  $k$ ,  $D_h$  (Eq. 5) is typically set to  $D / k$ .\\n\\n$$\\n\\\\operatorname {M S A} (\\\\mathbf {z}) = \\\\left[ \\\\mathrm {S A} _ {1} (z); \\\\mathrm {S A} _ {2} (z); \\\\dots ; \\\\mathrm {S A} _ {k} (z) \\\\right] \\\\mathbf {U} _ {m s a} \\\\quad \\\\mathbf {U} _ {m s a} \\\\in \\\\mathbb {R} ^ {k \\\\cdot D _ {h} \\\\times D} \\\\tag {8}\\n$$\\n\\n# B EXPERIMENT DETAILS\\n\\n# B.1 TRAINING\\n\\nTable 3 summarizes our training setups for our different models. We found strong regularization to be key when training models from scratch on ImageNet. Dropout, when used, is applied after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all training is done on resolution 224.\\n\\n# B.1.1 FINE-TUNING\\n\\nWe fine-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over learning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training set (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the remaining data. For final results we train on the entire training set and evaluate on the respective test data. For fine-tuning ResNets and hybrid models we use the exact same setup, with the only exception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|  Dataset | Steps | Base LR  |\\n| --- | --- | --- |\\n|  ImageNet | 20000 | {0.003, 0.01, 0.03, 0.06}  |\\n|  CIFAR100 | 10000 | {0.001, 0.003, 0.01, 0.03}  |\\n|  CIFAR10 | 10000 | {0.001, 0.003, 0.01, 0.03}  |\\n|  Oxford-IIIT Pets | 500 | {0.001, 0.003, 0.01, 0.03}  |\\n|  Oxford Flowers-102 | 500 | {0.001, 0.003, 0.01, 0.03}  |\\n|  VTAB (19 tasks) | 2500 | 0.01  |\\n\\nTable 4: Hyperparameters for fine-tuning. All models are fine-tuned with cosine learning rate decay, a batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise, fine-tuning resolution is 384.\\n\\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across this run and our sweep. Finally, if not mentioned otherwise, all fine-tuning experiments run at 384 resolution (running fine-tuning at different resolution than training is common practice (Kolesnikov et al., 2020)).\\n\\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset. We found this to be a little more robust than simply re-initializing the very last layer.\\n\\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter setting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this setting by running a small sweep over two learning rates and two schedules, and selecting the setting with the highest VTAB score on the 200-example validation sets. We follow the pre-processing used in Kolesnikov et al. (2020), except that we do not use task-specific input resolutions. Instead we find that Vision Transformer benefits most from a high resolution  $(384 \\\\times 384)$  for all tasks.\\n\\n# B.1.2 SELF-SUPERVISION\\n\\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To do so we corrupt  $50\\\\%$  of patch embeddings by either replacing their embeddings with a learnable [mask] embedding  $(80\\\\%)$ , a random other patch embedding  $(10\\\\%)$  or just keeping them as is  $(10\\\\%)$ . This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we predict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective patch representations.\\n\\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We use Adam, with a base learning rate of  $2 \\\\cdot 10^{-4}$ , warmup of 10k steps and cosine learning rate decay. As prediction targets for pretraining we tried the following settings: 1) predicting only the mean, 3bit color (i.e., 1 prediction of 512 colors), 2) predicting a  $4 \\\\times 4$  downsized version of the  $16 \\\\times 16$  patch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch using L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite well, though L2 was slightly worse. We report final results only for option 1) because it has shown best few-shot performance. We also experimented with  $15\\\\%$  corruption rate as used by Devlin et al. (2019) but results were also slightly worse on our few-shot metrics.\\n\\nLastly, we would like to remark that our instantiation of masked patch prediction doesn\\'t require such an enormous amount of pretraining nor a large dataset such as JFT in order to lead to similar performance gains on ImageNet classification. That is, we observed diminishing returns on downstream performance after 100k pretraining steps, and see similar gains when pretraining on ImageNet.\\n\\n# C ADDITIONAL RESULTS\\n\\nWe report detailed results corresponding to the figures presented in the paper. Table 5 corresponds to Figure 3 from the paper and shows transfer performance of different ViT models pre-trained on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|   |  | ViT-B/16 | ViT-B/32 | ViT-L/16 | ViT-L/32 | ViT-H/14  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  ImageNet | CIFAR-10 | 98.13 | 97.77 | 97.86 | 97.94 | -  |\\n|   |  CIFAR-100 | 87.13 | 86.31 | 86.35 | 87.07 | -  |\\n|   |  ImageNet | 77.91 | 73.38 | 76.53 | 71.16 | -  |\\n|   |  ImageNet ReaL | 83.57 | 79.56 | 82.19 | 77.83 | -  |\\n|   |  Oxford Flowers-102 | 89.49 | 85.43 | 89.66 | 86.36 | -  |\\n|   |  Oxford-IIIT-Pets | 93.81 | 92.04 | 93.64 | 91.35 | -  |\\n|  ImageNet-21k | CIFAR-10 | 98.95 | 98.79 | 99.16 | 99.13 | 99.27  |\\n|   |  CIFAR-100 | 91.67 | 91.97 | 93.44 | 93.04 | 93.82  |\\n|   |  ImageNet | 83.97 | 81.28 | 85.15 | 80.99 | 85.13  |\\n|   |  ImageNet ReaL | 88.35 | 86.63 | 88.40 | 85.65 | 88.70  |\\n|   |  Oxford Flowers-102 | 99.38 | 99.11 | 99.61 | 99.19 | 99.51  |\\n|   |  Oxford-IIIT-Pets | 94.43 | 93.02 | 94.73 | 93.09 | 94.82  |\\n|  JFT-300M | CIFAR-10 | 99.00 | 98.61 | 99.38 | 99.19 | 99.50  |\\n|   |  CIFAR-100 | 91.87 | 90.49 | 94.04 | 92.52 | 94.55  |\\n|   |  ImageNet | 84.15 | 80.73 | 87.12 | 84.37 | 88.04  |\\n|   |  ImageNet ReaL | 88.85 | 86.27 | 89.99 | 88.28 | 90.33  |\\n|   |  Oxford Flowers-102 | 99.56 | 99.27 | 99.56 | 99.45 | 99.68  |\\n|   |  Oxford-IIIT-Pets | 95.80 | 93.40 | 97.11 | 95.83 | 97.56  |\\n\\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on ImageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models are fine-tuned at 384 resolution. Note that the ImageNet results are computed without additional techniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\\n\\n|  name | Epochs | ImageNet | ImageNet ReaL | CIFAR-10 | CIFAR-100 | Pets | Flowers | exaFLOPs  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  ViT-B/32 | 7 | 80.73 | 86.27 | 98.61 | 90.49 | 93.40 | 99.27 | 55  |\\n|  ViT-B/16 | 7 | 84.15 | 88.85 | 99.00 | 91.87 | 95.80 | 99.56 | 224  |\\n|  ViT-L/32 | 7 | 84.37 | 88.28 | 99.19 | 92.52 | 95.83 | 99.45 | 196  |\\n|  ViT-L/16 | 7 | 86.30 | 89.43 | 99.38 | 93.46 | 96.81 | 99.66 | 783  |\\n|  ViT-L/16 | 14 | 87.12 | 89.99 | 99.38 | 94.04 | 97.11 | 99.56 | 1567  |\\n|  ViT-H/14 | 14 | 88.08 | 90.36 | 99.50 | 94.71 | 97.11 | 99.71 | 4262  |\\n|  ResNet50x1 | 7 | 77.54 | 84.56 | 97.67 | 86.07 | 91.11 | 94.26 | 50  |\\n|  ResNet50x2 | 7 | 82.12 | 87.94 | 98.29 | 89.20 | 93.43 | 97.02 | 199  |\\n|  ResNet101x1 | 7 | 80.67 | 87.07 | 98.48 | 89.17 | 94.08 | 95.95 | 96  |\\n|  ResNet152x1 | 7 | 81.88 | 87.96 | 98.82 | 90.22 | 94.17 | 96.94 | 141  |\\n|  ResNet152x2 | 7 | 84.97 | 89.69 | 99.06 | 92.05 | 95.37 | 98.62 | 563  |\\n|  ResNet152x2 | 14 | 85.56 | 89.89 | 99.24 | 91.92 | 95.75 | 98.75 | 1126  |\\n|  ResNet200x3 | 14 | 87.22 | 90.15 | 99.34 | 93.53 | 96.32 | 99.04 | 3306  |\\n|  R50x1+ViT-B/32 | 7 | 84.90 | 89.15 | 99.01 | 92.24 | 95.75 | 99.46 | 106  |\\n|  R50x1+ViT-B/16 | 7 | 85.58 | 89.65 | 99.14 | 92.63 | 96.65 | 99.40 | 274  |\\n|  R50x1+ViT-L/32 | 7 | 85.68 | 89.04 | 99.24 | 92.93 | 96.97 | 99.43 | 246  |\\n|  R50x1+ViT-L/16 | 7 | 86.60 | 89.72 | 99.18 | 93.64 | 97.03 | 99.40 | 859  |\\n|  R50x1+ViT-L/16 | 14 | 87.12 | 89.76 | 99.31 | 93.89 | 97.36 | 99.11 | 1668  |\\n\\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main paper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-aFLOPs).\\n\\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of varying size, as well as the estimated computational cost of their pre-training.\\n\\n# D ADDITIONAL ANALYSES\\n\\n# D.1 SGD VS. ADAM FOR RESNETS\\n\\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional. Here we show the experiments that motivated this choice. Namely, we compare the fine-tuning\\n\\nPublished as a conference paper at ICLR 2021\\n\\n|  Dataset | ResNet50 |   | ResNet152x2  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  Adam | SGD | Adam | SGD  |\\n|  ImageNet | 77.54 | 78.24 | 84.97 | 84.37  |\\n|  CIFAR10 | 97.67 | 97.46 | 99.06 | 99.07  |\\n|  CIFAR100 | 86.07 | 85.17 | 92.05 | 91.06  |\\n|  Oxford-IIIT Pets | 91.11 | 91.00 | 95.37 | 94.79  |\\n|  Oxford Flowers-102 | 94.26 | 92.06 | 98.62 | 99.32  |\\n|  Average | 89.33 | 88.79 | 94.01 | 93.72  |\\n\\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\\n\\n![img-11.jpeg](img-11.jpeg)\\nFigure 8: Scaling different model dimensions of the Vision Transformer.\\n\\n![img-12.jpeg](img-12.jpeg)\\n\\nperformance of two ResNets - 50x1 and 152x2 - pre-trained on JFT with SGD and Adam. For SGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented in Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average. This justifies the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the absolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only for 7 epochs, not 30.\\n\\n# D.2 TRANSFORMER SHAPE\\n\\nWe ran ablations on scaling different dimensions of the Transformer architecture to find out which are best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet for different configurations. All configurations are based on a ViT model with 8 layers,  $D = 1024$ ,  $D_{MLP} = 2048$  and a patch size of 32, the intersection of all lines. We can see that scaling the depth results in the biggest improvements which are clearly visible up until 64 layers. However, diminishing returns are already visible after 16 layers. Interestingly, scaling the width of the network seems to result in the smallest changes. Decreasing the patch size and thus increasing the effective sequence length shows surprisingly robust improvements without introducing parameters. These findings suggest that compute might be a better predictor of performance than the number of parameters, and that scaling should emphasize depth over width if any. Overall, we find that scaling all dimensions proportionally results in robust improvements.\\n\\n# D.3 HEAD TYPE AND CLASSTOKEN\\n\\nIn order to stay as close as possible to the original Transformer model, we made use of an additional [class] token, which is taken as image representation. The output of this token is then transformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity in the single hidden layer.\\n\\nThis design is inherited from the Transformer model for text, and we use it throughout the main paper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP) them, followed by a linear classifier—just like ResNet\\'s final feature map—performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-13.jpeg](img-13.jpeg)\\nFigure 9: Comparison of class-token and global average pooling classifiers. Both work similarly well, but require different learning-rates.\\n\\n|  Pos. Emb. | Default/Stem | Every Layer | Every Layer-Shared  |\\n| --- | --- | --- | --- |\\n|  No Pos. Emb. | 0.61382 | N/A | N/A  |\\n|  1-D Pos. Emb. | 0.64206 | 0.63964 | 0.64292  |\\n|  2-D Pos. Emb. | 0.64001 | 0.64046 | 0.64022  |\\n|  Rel. Pos. Emb. | 0.64032 | N/A | N/A  |\\n\\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on ImageNet 5-shot linear.\\n\\nthe difference in performance is fully explained by the requirement for a different learning-rate, see Figure 9.\\n\\n# D.4 POSITIONAL EMBEDDING\\n\\nWe ran ablations on different ways of encoding spatial information using positional embedding. We tried the following cases:\\n\\n- Providing no positional information: Considering the inputs as a bag of patches.\\n- 1-dimensional positional embedding: Considering the inputs as a sequence of patches in the raster order (default across all other experiments in this paper).\\n- 2-dimensional positional embedding: Considering the inputs as a grid of patches in two dimensions. In this case, two sets of embeddings are learned, each for one of the axes,  $X$ -embedding, and  $Y$ -embedding, each with size  $D / 2$ . Then, based on the coordinate on the path in the input, we concatenate the  $X$  and  $Y$  embedding to get the final positional embedding for that patch.\\n- Relative positional embeddings: Considering the relative distance between patches to encode the spatial information as instead of their absolute position. To do so, we use 1-dimensional Relative Attention, in which we define the relative distance all possible pairs of patches. Thus, for every given pair (one as query, and the other as key/value in the attention mechanism), we have an offset  $p_q - p_k$ , where each offset is associated with an embedding. Then, we simply run extra attention, where we use the original query (the content of query), but use relative positional embeddings as keys. We then use the logits from the relative attention as a bias term and add it to the logits of the main attention (content-based attention) before applying the softmax.\\n\\nIn addition to different ways of encoding spatial information, we also tried different ways of incorporating this information in our model. For the 1-dimensional and 2-dimensional positional embeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-14.jpeg](img-14.jpeg)\\nFigure 10: Position embeddings of models trained with different hyperparameters.\\n\\n![img-15.jpeg](img-15.jpeg)\\n\\n![img-16.jpeg](img-16.jpeg)\\n\\nthe stem of them model and before feeding the inputs to the Transformer encoder (default across all other experiments in this paper); (2) learn and add positional embeddings to the inputs at the beginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of each layer (shared between layers).\\n\\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while there is a large gap between the performances of the model with no positional embedding and models with positional embedding, there is little to no difference between different ways of encoding positional information. We speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g.,  $14 \\\\times 14$  as opposed to  $224 \\\\times 224$ , and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies. Even so, the specific pattern of position embedding similarity learned by the network depends on the training hyperparameters (Figure 10).\\n\\n![img-17.jpeg](img-17.jpeg)\\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for 128 example images by averaging the distance between the query pixel and all other pixels, weighted by the attention weight. Each dot shows the mean attention distance across images for one of 16 heads at one layer. Image width is 224 pixels.\\n\\n![img-18.jpeg](img-18.jpeg)\\n\\n# D.5 EMPIRICAL COMPUTATIONAL COSTS\\n\\nWe are also interested in real-world speed of the architectures on our hardware, which is not always well predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\\n\\nPublished as a conference paper at ICLR 2021\\n\\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the difference between inference and backprop speed is a constant model-independent factor.\\n\\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes. Every single point refers to the peak performance measured across a wide range of batch-sizes. As can be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening for the largest models at the largest resolutions.\\n\\nAnother quantity of interest is the largest batch-size each model can fit onto a core, larger being better for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models. This shows that large ViT models have a clear advantage in terms of memory-efficiency over ResNet models.\\n\\n![img-19.jpeg](img-19.jpeg)\\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models have speed comparable to similar ResNets. Right: Largest per-core batch-size fitting on device with various architectures across input sizes. ViT models are clearly more memory-efficient.\\n\\n# D.6 AXIAL ATTENTION\\n\\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-attention on large inputs that are organized as multidimensional tensors. The general idea of axial attention is to perform multiple attention operations, each along a single axis of the input tensor, instead of applying 1-dimensional attention to the flattened version of the input. In axial attention, each attention mixes information along a particular axis, while keeping information along the other axes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which all the convolutions with kernel size  $3 \\\\times 3$  in a ResNet50 are replaced by axial self-attention, i.e. a row and column attention, augmented by relative positional encoding. We have implemented AxialResNet as a baseline model. $^{3}$ .\\n\\nMoreover, we have modified ViT to process inputs in the 2-dimensional shape, instead of a 1-dimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of a self-attention followed by an MLP, we have a row-self-attention plus an MLP followed by a column-self-attention plus an MLP.\\n\\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on ImageNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of number of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32 and Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-20.jpeg](img-20.jpeg)\\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet 5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).\\n\\n![img-21.jpeg](img-21.jpeg)\\n\\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column self-attention and although the sequence length that self-attention operates on is smaller in axial case, there is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in terms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow on TPUs (Figure 13, right).\\n\\n# D.7 ATTENTION DISTANCE\\n\\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed the average distance spanned by attention weights at different layers (Figure 11). This \"attention distance\" is analogous to receptive field size in CNNs. Average attention distance is highly variable across heads in lower layers, with some heads attending to much of the image, while others attend to small regions at or near the query location. As depth increases, attention distance increases for all heads. In the second half of the network, most heads attend widely across tokens.\\n\\n# D.8 ATTENTION MAPS\\n\\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we used Attention Rollout (Abnar &amp; Zuidema, 2020). Briefly, we averaged attention weights of ViT-L/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts for the mixing of attention across tokens through all layers.\\n\\n# D.9 OBJECTNET RESULTS\\n\\nWe also evaluate our flagship ViT-H/14 model on the ObjectNet benchmark following the evaluation setup in Kolesnikov et al. (2020), resulting in  $82.1\\\\%$  top-5 accuracy and  $61.7\\\\%$  top-1 accuracy.\\n\\n# D.10 VTAB BREAKDOWN\\n\\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\\n\\nPublished as a conference paper at ICLR 2021\\n\\n![img-22.jpeg](img-22.jpeg)\\nFigure 14: Further example attention maps as in Figure 6 (random selection).\\n\\nPublished as a conference paper at ICLR 2021\\n\\nTable 9: Breakdown of VTAB-1k performance across tasks.\\n\\n|   | # Caltech101 | # CIFAR-100 | # DTD | # Flowers102 | # Pets | # Sun397 | # SVHN | # Camelyon | # EuroSAT | # Resisc45 | # Retinopathy | # Clevr-Count | # Clevr-Dist | # DMLab | # dSpr-Loc | # dSpr-Ori | # KITTI-Dist | # sNORB-Azim | # sNORB-Elev | # Mean  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  ViT-H/14 (IFT) | 95.3 | 85.5 | 75.2 | 99.7 | 97.2 | 65.0 | 88.9 | 83.3 | 96.7 | 91.4 | 76.6 | 91.7 | 63.8 | 53.1 | 79.4 | 63.3 | 84.5 | 33.2 | 51.2 | 77.6  |\\n|  ViT-L/16 (IFT) | 95.4 | 81.9 | 74.3 | 99.7 | 96.7 | 63.5 | 87.4 | 83.6 | 96.5 | 89.7 | 77.1 | 86.4 | 63.1 | 49.7 | 74.5 | 60.5 | 82.2 | 36.2 | 51.1 | 76.3  |\\n|  ViT-L/16 (I21k) | 90.8 | 84.1 | 74.1 | 99.3 | 92.7 | 61.0 | 80.9 | 82.5 | 95.6 | 85.2 | 75.3 | 70.3 | 56.1 | 41.9 | 74.7 | 64.9 | 79.9 | 30.5 | 41.7 | 72.7  |'), -0.059057832893134155), (Document(id='d610388cdccd:0', metadata={'end_line': 405, 'start_line': 1, 'text': '# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\nPatrick Lewis^{†}^{‡}, Ethan Perez^{∗},\\nAleksandra Piktus^{†}, Fabio Petroni^{†}, Vladimir Karpukhin^{†}, Naman Goyal^{†}, Heinrich Küttler^{†},\\nMike Lewis^{†}, Wen-tau Yih^{†}, Tim Rocktäschel^{†}^{‡}, Sebastian Riedel^{†}^{‡}, Douwe Kiela^{†}\\n^{†}Facebook AI Research; ^{‡}University College London; ^{∗}New York University;\\nplewis@fb.com\\n\\n###### Abstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n## 1 Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data *[47]*. They can do so without any access to an external memory, as a parameterized implicit knowledge base *[51, 52]*. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” *[38]*. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories *[20, 26, 48]* can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM *[20]* and ORQA *[31]*, two recently introduced models that combine masked language models *[8]* with a differentiable retriever, have shown promising results,\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query  $x$ , we use Maximum Inner Product Search (MIPS) to find the top-K documents  $z_{i}$ . For final prediction  $y$ , we treat  $z$  as a latent variable and marginalize over seq2seq predictions given different documents.\\n\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \"workhorse of NLP,\" i.e. sequence-to-sequence (seq2seq) models.\\n\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\n\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.\\n\\nOur results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within  $4.3\\\\%$  of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\\' knowledge as the world changes. $^{1}$\\n\\n# 2 Methods\\n\\nWe explore RAG models, which use the input sequence  $x$  to retrieve text documents  $z$  and use them as additional context when generating the target sequence  $y$ . As shown in Figure 1, our models leverage two components: (i) a retriever  $p_{\\\\eta}(z|x)$  with parameters  $\\\\eta$  that returns (top-K truncated) distributions over text passages given a query  $x$  and (ii) a generator  $p_{\\\\theta}(y_i|x,z,y_{1:i-1})$  parametrized\\n\\nby $\\\\theta$ that generates a current token based on a context of the previous $i-1$ tokens $y_{1:i-1}$, the original input $x$ and a retrieved passage $z$.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the $p_{\\\\eta}$ and $p_{\\\\theta}$ components, as well as the training and decoding procedure.\\n\\n### 2.1 Models\\n\\n#### RAG-Sequence Model\\n\\nThe RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability $p(y|x)$ via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,\\n\\n$p_{\\\\text{RAG-Sequence}}(y|x)\\\\approx\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)p_{\\\\theta}(y|x,z)=\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)\\\\prod_{i}^{N}p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$\\n\\n#### RAG-Token Model\\n\\nIn the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:\\n\\n$p_{\\\\text{RAG-Token}}(y|x)\\\\ \\\\approx\\\\ \\\\prod_{i}^{N}\\\\ \\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$\\n\\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n### 2.2 Retriever: DPR\\n\\nThe retrieval component $p_{\\\\eta}(z|x)$ is based on DPR *[26]*. DPR follows a bi-encoder architecture:\\n\\n$p_{\\\\eta}(z|x)\\\\propto\\\\exp\\\\left(\\\\mathbf{d}(z)^{\\\\top}\\\\mathbf{q}(x)\\\\right)\\\\hskip 28.45274pt\\\\mathbf{d}(z)=\\\\text{BERT}_{d}(z),\\\\ \\\\ \\\\mathbf{q}(x)=\\\\text{BERT}_{q}(x)$\\n\\nwhere $\\\\mathbf{d}(z)$ is a dense representation of a document produced by a $\\\\text{BERT}_{\\\\text{BASE}}$ document encoder *[8]*, and $\\\\mathbf{q}(x)$ a query representation produced by a query encoder, also based on $\\\\text{BERT}_{\\\\text{BASE}}$. Calculating top-k($p_{\\\\eta}(\\\\cdot|x)$), the list of $k$ documents $z$ with highest prior probability $p_{\\\\eta}(z|x)$, is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time *[23]*. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA *[24]* questions and Natural Questions *[29]*. We refer to the document index as the non-parametric memory.\\n\\n### 2.3 Generator: BART\\n\\nThe generator component $p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$ could be modelled using any encoder-decoder. We use BART-large *[32]*, a pre-trained seq2seq transformer *[58]* with 400M parameters. To combine the input $x$ with the retrieved content $z$ when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models *[32]*. We refer to the BART generator parameters $\\\\theta$ as the parametric memory henceforth.\\n\\n### 2.4 Training\\n\\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs $(x_{j},y_{j})$, we\\n\\nminimize the negative marginal log-likelihood of each target, $\\\\sum_{j}-\\\\log p(y_{j}|x_{j})$ using stochastic gradient descent with Adam *[28]*. Updating the document encoder BERT_{d} during training is costly as it requires the document index to be periodically updated as REALM does during pre-training *[20]*. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERT_{q} and the BART generator.\\n\\n### 2.5 Decoding\\n\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate $\\\\arg\\\\max_{y}p(y|x)$.\\n\\n#### RAG-Token\\n\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: $p_{\\\\theta}^{\\\\prime}(y_{i}|x,y_{1:i-1})=\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z_{i}|x)p_{\\\\theta}(y_{i}|x,z_{i},y_{1:i-1})$ To decode, we can plug $p_{\\\\theta}^{\\\\prime}(y_{i}|x,y_{1:i-1})$ into a standard beam decoder.\\n\\n#### RAG-Sequence\\n\\nFor RAG-Sequence, the likelihood $p(y|x)$ does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document $z$, scoring each hypothesis using $p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$. This yields a set of hypotheses $Y$, some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis $y$ we run an additional forward pass for each document $z$ for which $y$ does not appear in the beam, multiply generator probability with $p_{\\\\eta}(z|x)$ and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, $|Y|$ can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that $p_{\\\\theta}(y|x,z_{i})\\\\approx 0$ where $y$ was not generated during beam search from $x,z_{i}$. This avoids the need to run additional forward passes once the candidate set $Y$ has been generated. We refer to this decoding procedure as “Fast Decoding.”\\n\\n## 3 Experiments\\n\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following *Lee et al. [31]* and *Karpukhin et al. [26]*, we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS *[23]* with a Hierarchical Navigable Small World approximation for fast retrieval *[37]*. During training, we retrieve the top $k$ documents for each query. We consider $k\\\\in\\\\{5,10\\\\}$ for training and set $k$ for test time using dev data. We now discuss experimental details for each task.\\n\\n### 3.1 Open-domain Question Answering\\n\\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks *[20]*. We treat questions and answers as input-output text pairs $(x,y)$ and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm *[5, 7, 31, 26]*, where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA\" approaches *[52]*, which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) *[29]*, TriviaQA (TQA) *[24]*. WebQuestions (WQ) *[3]* and CuratedTrec (CT) *[2]*. As CT and WQ are small, we follow DPR *[26]* by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work *[31, 26]* and report Exact Match (EM) scores. For TQA, to compare with T5 *[52]*, we also evaluate on the TQA Wiki test set.\\n\\n### 3.2 Abstractive Question Answering\\n\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 *[43]*. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n\\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\\n\\n### 3.3 Jeopardy Question Generation\\n\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the first country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\\n\\nWe use the splits from SearchQA *[10]*, with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following *[67]*, we evaluate using the SQuAD-tuned Q-BLEU-1 metric *[42]*. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output *[33]*. We follow best practice and use pairwise comparative evaluation *[34]*. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—-question A is better, question B is better, both are good, or neither is good.\\n\\n### 3.4 Fact Verification\\n\\nFEVER *[56]* requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos *[57]*. In both cases we report label accuracy.\\n\\n## 4 Results\\n\\n### 4.1 Open-domain Question Answering\\n\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation flexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training *[20]*. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.\\n\\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\\n\\nTable 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open-Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details.\\n\\n|   | Model | NQ | TQA | WQ | CT  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Closed | T5-11B [52] | 34.5 | - /50.1 | 37.4 | -  |\\n|  Book | T5-11B+SSM[52] | 36.6 | - /60.5 | 44.7 | -  |\\n|  Open | REALM [20] | 40.4 | - / - | 40.7 | 46.8  |\\n|  Book | DPR [26] | 41.5 | 57.9/ - | 41.1 | 50.6  |\\n|   | RAG-Token | 44.1 | 55.2/66.1 | 45.5 | 50.0  |\\n|   | RAG-Seq. | 44.5 | 56.8/68.0 | 45.2 | 52.2  |\\n\\nTable 2: Generation and classification Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined.\\n\\n|  Model | Jeopardy |   | MSMARCO |   | FVR3 | FVR2  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|   |  B-1 | QB-1 | R-L | B-1 | Label | Acc.  |\\n|  SotA | - | - | 49.8* | 49.9* | 76.8 | 92.2*  |\\n|  BART | 15.1 | 19.7 | 38.2 | 41.6 | 64.0 | 81.1  |\\n|  RAG-Tok. | 17.3 | 22.2 | 40.1 | 41.5 | 72.5 | 89.5  |\\n|  RAG-Seq. | 14.7 | 21.4 | 40.8 | 44.2  |   |   |\\n\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving  $11.8\\\\%$  accuracy in such cases for NQ, where an extractive model would score  $0\\\\%$ .\\n\\n# 4.2 Abstractive Question Answering\\n\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5).\\n\\n# 4.3 Jeopardy Question Generation\\n\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only  $7.1\\\\%$  of cases, while RAG was more factual in  $42.7\\\\%$  of cases, and both RAG and BART were factual in a further  $17\\\\%$  of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\\n\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating \"Sun\", the posterior is high for document 2 which mentions \"The Sun Also Rises\". Similarly, document 1 dominates the posterior when \"A Farewell to Arms\" is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model\\'s parametric knowledge is sufficient to complete the titles. We find evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART\\'s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\\n\\n# 4.4 Fact Verification\\n\\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within  $4.3\\\\%$  of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n\\nDocument 1: his works are considered classics of American literature ... His wartime experiences formed the basis for his novel \"A Farewell to Arms\" (1929) ...\\n\\nDocument 2: ... artists of the 1920s \"Lost Generation\" expatriate community. His debut novel, \"The Sun Also Rises\", was published in 1926.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2: RAG-Token document posterior  $p(z_i|x, y_i, y_{-i})$  for each generated token for input \"Hemingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \"A Farewell to Arms\" and for document 2 when generating \"The Sun Also Rises\".\\n\\nTable 3: Examples from generation tasks. RAG models generate more specific and factually accurate responses.  $\\\\text{?}$  indicates factually incorrect responses, * indicates partially correct responses.\\n\\n|  Task | Input | Model | Generation  |\\n| --- | --- | --- | --- |\\n|  MS-MARCO | define middle ear | BART | 7The middle ear is the part of the ear between the middle ear and the nose.  |\\n|   |   |  RAG-T | The middle ear is the portion of the ear internal to the eardrum.  |\\n|   |   |  RAG-S | The middle ear includes the tympanic cavity and the three ossicles.  |\\n|   |  what currency needed in scotland | BART | The currency needed in Scotland is Pound sterling.  |\\n|   |   |  RAG-T | Pound is the currency needed in Scotland.  |\\n|   |   |  RAG-S | The currency needed in Scotland is the pound sterling.  |\\n|  Jeopardy Question Gener -ation | Washington | BART | 7This state has the largest number of counties in the U.S.  |\\n|   |   |  RAG-T | It\\'s the only U.S. state named for a U.S. president  |\\n|   |   |  RAG-S | It\\'s the state where you\\'ll find Mount Rainier National Park  |\\n|   |  The Divine Comedy | BART | 7This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio  |\\n|   |   |  RAG-T | Dante\\'s \"Inferno\" is the first part of this epic poem  |\\n|   |   |  RAG-S | This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"  |\\n\\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within  $2.7\\\\%$  of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top  $k$  documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in  $71\\\\%$  of cases, and a gold article is present in the top 10 retrieved articles in  $90\\\\%$  of cases.\\n\\n# 4.5 Additional Results\\n\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding [33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence\\'s generations are more diverse than RAG-Token\\'s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\\n\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\\n\\nWe compare RAG\\'s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG\\'s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating  $p(z|x)$ . Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\n\\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n\\nTable 4: Human assessments for the Jeopardy Question Generation Task.\\n\\n|   | Factuality | Specificity  |\\n| --- | --- | --- |\\n|  BART better | 7.1% | 16.8%  |\\n|  RAG better | 42.7% | 37.4%  |\\n|  Both good | 11.7% | 11.8%  |\\n|  Both poor | 17.7% | 6.9%  |\\n|  No majority | 20.8% | 20.1%  |\\n\\nTable 5: Ratio of distinct to total tri-grams for generation tasks.\\n\\n|   | MSMARCO | Jeopardy QGen  |\\n| --- | --- | --- |\\n|  Gold | 89.6% | 90.0%  |\\n|  BART | 70.7% | 32.4%  |\\n|  RAG-Token | 77.8% | 46.8%  |\\n|  RAG-Seq. | 83.5% | 53.8%  |\\n\\nTable 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.\\n\\n|  Model | NQ | TQA Exact | WQ Match | CT | Jeopardy-QGen |   | MSMarco |   | FVR-3 Label | FVR-2 Accuracy  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   |   |   |   |   |  B-1 | QB-1 | R-L | B-1  |   |   |\\n|  RAG-Token-BM25 | 29.7 | 41.5 | 32.1 | 33.1 | 17.5 | 22.3 | 55.5 | 48.4 | 75.1 | 91.6  |\\n|  RAG-Sequence-BM25 | 31.8 | 44.1 | 36.6 | 33.8 | 11.1 | 19.5 | 56.5 | 46.9  |   |   |\\n|  RAG-Token-Frozen | 37.8 | 50.1 | 37.1 | 51.1 | 16.7 | 21.7 | 55.9 | 49.4 | 72.9 | 89.4  |\\n|  RAG-Sequence-Frozen | 41.2 | 52.1 | 41.8 | 52.6 | 11.8 | 19.6 | 56.7 | 47.3  |   |   |\\n|  RAG-Token | 43.5 | 54.8 | 46.5 | 51.9 | 17.9 | 22.6 | 56.2 | 49.4 | 74.5 | 90.6  |\\n|  RAG-Sequence | 44.0 | 55.8 | 44.9 | 53.4 | 15.3 | 21.5 | 57.2 | 47.5  |   |   |\\n\\nbetween these dates and use a template \"Who is {position}?\" (e.g. \"Who is the President of Peru?\") to query our NQ RAG model with each index. RAG answers  $70\\\\%$  correctly using the 2016 index for 2016 world leaders and  $68\\\\%$  using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low ( $12\\\\%$  with the 2018 index and 2016 leaders,  $4\\\\%$  with the 2016 index and 2018 leaders). This shows we can update RAG\\'s world knowledge by simply replacing its non-parametric memory.\\n\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n\\n![img-3.jpeg](img-3.jpeg)\\n\\n![img-4.jpeg](img-4.jpeg)\\n\\n# 5 Related Work\\n\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\\n\\n#### General-Purpose Architectures for NLP\\n\\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks *[60, 61]* after fine-tuning *[49, 8]*. GPT-2 *[50]* later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART *[32]* and T5 *[51, 52]* propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\\n\\n#### Learned Retrieval\\n\\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models *[44, 26]* similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search *[46]*, reinforcement learning *[6, 63, 62]*, or a latent variable approach *[31, 20]* as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\\n\\n#### Memory-based Architectures\\n\\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks *[64, 55]*. Concurrent work *[14]* learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings *[15, 13]*. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval *[9]*.\\n\\n#### Retrieve-and-Edit approaches\\n\\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation *[18, 22]* and Semantic Parsing *[21]*. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\\n\\n## 6 Discussion\\n\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\\n\\nBroader Impact\\n\\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\\n\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 *[50]* are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content *[54]*. Advanced language models may also lead to the automation of various jobs in the coming decades *[16]*. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\\n\\n## Acknowledgments\\n\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\\n\\n## References\\n\\n- [1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n- [2] Petr Baudiš and Jan Šedivỳ. Modeling of the question answering task in the yodaqa system. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\\n- [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/D13-1160.\\n- [4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\\n- [5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anthology/P17-1171.\\n- [6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-fine question answering for long documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n-\\n\\n[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\\n- [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\\n- [9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\n- [10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\\n- [11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/P18-1082.\\n- [12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anthology/P19-1346.\\n- [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\\n- [14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\\n- [15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\\n- [16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\\n- [17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n- [18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\\n- [19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\\n\\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.\\n- [21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052–10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.\\n- [22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-erank text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anthology/2020.acl-main.228.\\n- [23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n- [24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\\n- [25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.\\n- [26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n- [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n- [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\\n- [29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.\\n- [30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.\\n- [31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n\\nfor Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/anthology/P19-1612.\\n- [32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n- [33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/N16-1014.\\n- [34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\\n- [35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291.\\n- [36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\\n- [37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\\n- [38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n- [39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\\n- [40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n- [41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n- [42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anthology/D18-1429.\\n- [43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n\\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n- [44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n- [45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\\n- [46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n- [47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n- [48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\\n- [49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\\n- [50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n- [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n- [52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\\n- [53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\\n- [54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\\n- [55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n\\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anthology/N18-1074.\\n- [57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification with elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\\n- [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n- [59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\\n- [60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446.\\n- [61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\\\text{textquotesingle} Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\\n- [62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R^{3}: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\\n- [63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\\n- [64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\\n- [65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anthology/W18-5713.\\n\\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n- [67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anthology/D19-1253.\\n- [68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\\n\\n# Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n# A Implementation Details\\n\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\n\\n# B Human Evaluation\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\".\\n\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\\n\\n# C Training setup Details\\n\\nWe train all RAG models and BART baselines using Fairseq [45]. We train with mixed precision floating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring  $\\\\sim 100$  GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers [66], which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\\'s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\\n\\nD Further Details on Open-Domain QA\\n\\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each $(q,a)$ pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\\n\\n#### CuratedTrec preprocessing\\n\\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models *[20]*. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\n\\n#### TriviaQA Evaluation setups\\n\\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR *[26]*, which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. *[52]* used the TriviaQA official Wikipedia test set instead. Févry et al. *[14]* follow this convention in order to compare with Roberts et al. *[52]* (See appendix of *[14]*). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\\n\\n## Appendix E Further Details on FEVER\\n\\nFor FEVER classification, we follow the practice from *[32]*, and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\\n\\n## Appendix F Null Document Probabilities\\n\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM *[20]* in order to model cases where no useful information could be retrieved for a given input. Here, if $k$ documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over $k+1$ predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\\n\\n## Appendix G Parameters\\n\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n\\nTable 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\n\\n|  Task | Train | Development | Test  |\\n| --- | --- | --- | --- |\\n|  Natural Questions | 79169 | 8758 | 3611  |\\n|  TriviaQA | 78786 | 8838 | 11314  |\\n|  WebQuestions | 3418 | 362 | 2033  |\\n|  CuratedTrec | 635 | 134 | 635  |\\n|  Jeopardy Question Generation | 97392 | 13714 | 26849  |\\n|  MS-MARCO | 153726 | 12468 | 101093*  |\\n|  FEVER-3-way | 145450 | 10000 | 10000  |\\n|  FEVER-2-way | 96966 | 6666 | 6666  |\\n\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consist of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating point precision to manage memory and disk footprints.\\n\\n# H Retrieval Collapse\\n\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would \"collapse\" and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.\\n\\n# I Number of instances per dataset\\n\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.', 'chunk_index': 0, 'doc_id': 'd610388cdccd'}, page_content='# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\nPatrick Lewis^{†}^{‡}, Ethan Perez^{∗},\\nAleksandra Piktus^{†}, Fabio Petroni^{†}, Vladimir Karpukhin^{†}, Naman Goyal^{†}, Heinrich Küttler^{†},\\nMike Lewis^{†}, Wen-tau Yih^{†}, Tim Rocktäschel^{†}^{‡}, Sebastian Riedel^{†}^{‡}, Douwe Kiela^{†}\\n^{†}Facebook AI Research; ^{‡}University College London; ^{∗}New York University;\\nplewis@fb.com\\n\\n###### Abstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n## 1 Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data *[47]*. They can do so without any access to an external memory, as a parameterized implicit knowledge base *[51, 52]*. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” *[38]*. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories *[20, 26, 48]* can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM *[20]* and ORQA *[31]*, two recently introduced models that combine masked language models *[8]* with a differentiable retriever, have shown promising results,\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query  $x$ , we use Maximum Inner Product Search (MIPS) to find the top-K documents  $z_{i}$ . For final prediction  $y$ , we treat  $z$  as a latent variable and marginalize over seq2seq predictions given different documents.\\n\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \"workhorse of NLP,\" i.e. sequence-to-sequence (seq2seq) models.\\n\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\n\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.\\n\\nOur results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within  $4.3\\\\%$  of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\\' knowledge as the world changes. $^{1}$\\n\\n# 2 Methods\\n\\nWe explore RAG models, which use the input sequence  $x$  to retrieve text documents  $z$  and use them as additional context when generating the target sequence  $y$ . As shown in Figure 1, our models leverage two components: (i) a retriever  $p_{\\\\eta}(z|x)$  with parameters  $\\\\eta$  that returns (top-K truncated) distributions over text passages given a query  $x$  and (ii) a generator  $p_{\\\\theta}(y_i|x,z,y_{1:i-1})$  parametrized\\n\\nby $\\\\theta$ that generates a current token based on a context of the previous $i-1$ tokens $y_{1:i-1}$, the original input $x$ and a retrieved passage $z$.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the $p_{\\\\eta}$ and $p_{\\\\theta}$ components, as well as the training and decoding procedure.\\n\\n### 2.1 Models\\n\\n#### RAG-Sequence Model\\n\\nThe RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability $p(y|x)$ via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,\\n\\n$p_{\\\\text{RAG-Sequence}}(y|x)\\\\approx\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)p_{\\\\theta}(y|x,z)=\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)\\\\prod_{i}^{N}p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$\\n\\n#### RAG-Token Model\\n\\nIn the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:\\n\\n$p_{\\\\text{RAG-Token}}(y|x)\\\\ \\\\approx\\\\ \\\\prod_{i}^{N}\\\\ \\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$\\n\\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n### 2.2 Retriever: DPR\\n\\nThe retrieval component $p_{\\\\eta}(z|x)$ is based on DPR *[26]*. DPR follows a bi-encoder architecture:\\n\\n$p_{\\\\eta}(z|x)\\\\propto\\\\exp\\\\left(\\\\mathbf{d}(z)^{\\\\top}\\\\mathbf{q}(x)\\\\right)\\\\hskip 28.45274pt\\\\mathbf{d}(z)=\\\\text{BERT}_{d}(z),\\\\ \\\\ \\\\mathbf{q}(x)=\\\\text{BERT}_{q}(x)$\\n\\nwhere $\\\\mathbf{d}(z)$ is a dense representation of a document produced by a $\\\\text{BERT}_{\\\\text{BASE}}$ document encoder *[8]*, and $\\\\mathbf{q}(x)$ a query representation produced by a query encoder, also based on $\\\\text{BERT}_{\\\\text{BASE}}$. Calculating top-k($p_{\\\\eta}(\\\\cdot|x)$), the list of $k$ documents $z$ with highest prior probability $p_{\\\\eta}(z|x)$, is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time *[23]*. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA *[24]* questions and Natural Questions *[29]*. We refer to the document index as the non-parametric memory.\\n\\n### 2.3 Generator: BART\\n\\nThe generator component $p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$ could be modelled using any encoder-decoder. We use BART-large *[32]*, a pre-trained seq2seq transformer *[58]* with 400M parameters. To combine the input $x$ with the retrieved content $z$ when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models *[32]*. We refer to the BART generator parameters $\\\\theta$ as the parametric memory henceforth.\\n\\n### 2.4 Training\\n\\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs $(x_{j},y_{j})$, we\\n\\nminimize the negative marginal log-likelihood of each target, $\\\\sum_{j}-\\\\log p(y_{j}|x_{j})$ using stochastic gradient descent with Adam *[28]*. Updating the document encoder BERT_{d} during training is costly as it requires the document index to be periodically updated as REALM does during pre-training *[20]*. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERT_{q} and the BART generator.\\n\\n### 2.5 Decoding\\n\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate $\\\\arg\\\\max_{y}p(y|x)$.\\n\\n#### RAG-Token\\n\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: $p_{\\\\theta}^{\\\\prime}(y_{i}|x,y_{1:i-1})=\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z_{i}|x)p_{\\\\theta}(y_{i}|x,z_{i},y_{1:i-1})$ To decode, we can plug $p_{\\\\theta}^{\\\\prime}(y_{i}|x,y_{1:i-1})$ into a standard beam decoder.\\n\\n#### RAG-Sequence\\n\\nFor RAG-Sequence, the likelihood $p(y|x)$ does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document $z$, scoring each hypothesis using $p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$. This yields a set of hypotheses $Y$, some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis $y$ we run an additional forward pass for each document $z$ for which $y$ does not appear in the beam, multiply generator probability with $p_{\\\\eta}(z|x)$ and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, $|Y|$ can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that $p_{\\\\theta}(y|x,z_{i})\\\\approx 0$ where $y$ was not generated during beam search from $x,z_{i}$. This avoids the need to run additional forward passes once the candidate set $Y$ has been generated. We refer to this decoding procedure as “Fast Decoding.”\\n\\n## 3 Experiments\\n\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following *Lee et al. [31]* and *Karpukhin et al. [26]*, we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS *[23]* with a Hierarchical Navigable Small World approximation for fast retrieval *[37]*. During training, we retrieve the top $k$ documents for each query. We consider $k\\\\in\\\\{5,10\\\\}$ for training and set $k$ for test time using dev data. We now discuss experimental details for each task.\\n\\n### 3.1 Open-domain Question Answering\\n\\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks *[20]*. We treat questions and answers as input-output text pairs $(x,y)$ and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm *[5, 7, 31, 26]*, where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA\" approaches *[52]*, which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) *[29]*, TriviaQA (TQA) *[24]*. WebQuestions (WQ) *[3]* and CuratedTrec (CT) *[2]*. As CT and WQ are small, we follow DPR *[26]* by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work *[31, 26]* and report Exact Match (EM) scores. For TQA, to compare with T5 *[52]*, we also evaluate on the TQA Wiki test set.\\n\\n### 3.2 Abstractive Question Answering\\n\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 *[43]*. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n\\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\\n\\n### 3.3 Jeopardy Question Generation\\n\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the first country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\\n\\nWe use the splits from SearchQA *[10]*, with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following *[67]*, we evaluate using the SQuAD-tuned Q-BLEU-1 metric *[42]*. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output *[33]*. We follow best practice and use pairwise comparative evaluation *[34]*. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—-question A is better, question B is better, both are good, or neither is good.\\n\\n### 3.4 Fact Verification\\n\\nFEVER *[56]* requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos *[57]*. In both cases we report label accuracy.\\n\\n## 4 Results\\n\\n### 4.1 Open-domain Question Answering\\n\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation flexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training *[20]*. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.\\n\\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\\n\\nTable 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open-Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details.\\n\\n|   | Model | NQ | TQA | WQ | CT  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Closed | T5-11B [52] | 34.5 | - /50.1 | 37.4 | -  |\\n|  Book | T5-11B+SSM[52] | 36.6 | - /60.5 | 44.7 | -  |\\n|  Open | REALM [20] | 40.4 | - / - | 40.7 | 46.8  |\\n|  Book | DPR [26] | 41.5 | 57.9/ - | 41.1 | 50.6  |\\n|   | RAG-Token | 44.1 | 55.2/66.1 | 45.5 | 50.0  |\\n|   | RAG-Seq. | 44.5 | 56.8/68.0 | 45.2 | 52.2  |\\n\\nTable 2: Generation and classification Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined.\\n\\n|  Model | Jeopardy |   | MSMARCO |   | FVR3 | FVR2  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|   |  B-1 | QB-1 | R-L | B-1 | Label | Acc.  |\\n|  SotA | - | - | 49.8* | 49.9* | 76.8 | 92.2*  |\\n|  BART | 15.1 | 19.7 | 38.2 | 41.6 | 64.0 | 81.1  |\\n|  RAG-Tok. | 17.3 | 22.2 | 40.1 | 41.5 | 72.5 | 89.5  |\\n|  RAG-Seq. | 14.7 | 21.4 | 40.8 | 44.2  |   |   |\\n\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving  $11.8\\\\%$  accuracy in such cases for NQ, where an extractive model would score  $0\\\\%$ .\\n\\n# 4.2 Abstractive Question Answering\\n\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5).\\n\\n# 4.3 Jeopardy Question Generation\\n\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only  $7.1\\\\%$  of cases, while RAG was more factual in  $42.7\\\\%$  of cases, and both RAG and BART were factual in a further  $17\\\\%$  of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\\n\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating \"Sun\", the posterior is high for document 2 which mentions \"The Sun Also Rises\". Similarly, document 1 dominates the posterior when \"A Farewell to Arms\" is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model\\'s parametric knowledge is sufficient to complete the titles. We find evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART\\'s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\\n\\n# 4.4 Fact Verification\\n\\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within  $4.3\\\\%$  of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n\\nDocument 1: his works are considered classics of American literature ... His wartime experiences formed the basis for his novel \"A Farewell to Arms\" (1929) ...\\n\\nDocument 2: ... artists of the 1920s \"Lost Generation\" expatriate community. His debut novel, \"The Sun Also Rises\", was published in 1926.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2: RAG-Token document posterior  $p(z_i|x, y_i, y_{-i})$  for each generated token for input \"Hemingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \"A Farewell to Arms\" and for document 2 when generating \"The Sun Also Rises\".\\n\\nTable 3: Examples from generation tasks. RAG models generate more specific and factually accurate responses.  $\\\\text{?}$  indicates factually incorrect responses, * indicates partially correct responses.\\n\\n|  Task | Input | Model | Generation  |\\n| --- | --- | --- | --- |\\n|  MS-MARCO | define middle ear | BART | 7The middle ear is the part of the ear between the middle ear and the nose.  |\\n|   |   |  RAG-T | The middle ear is the portion of the ear internal to the eardrum.  |\\n|   |   |  RAG-S | The middle ear includes the tympanic cavity and the three ossicles.  |\\n|   |  what currency needed in scotland | BART | The currency needed in Scotland is Pound sterling.  |\\n|   |   |  RAG-T | Pound is the currency needed in Scotland.  |\\n|   |   |  RAG-S | The currency needed in Scotland is the pound sterling.  |\\n|  Jeopardy Question Gener -ation | Washington | BART | 7This state has the largest number of counties in the U.S.  |\\n|   |   |  RAG-T | It\\'s the only U.S. state named for a U.S. president  |\\n|   |   |  RAG-S | It\\'s the state where you\\'ll find Mount Rainier National Park  |\\n|   |  The Divine Comedy | BART | 7This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio  |\\n|   |   |  RAG-T | Dante\\'s \"Inferno\" is the first part of this epic poem  |\\n|   |   |  RAG-S | This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"  |\\n\\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within  $2.7\\\\%$  of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top  $k$  documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in  $71\\\\%$  of cases, and a gold article is present in the top 10 retrieved articles in  $90\\\\%$  of cases.\\n\\n# 4.5 Additional Results\\n\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding [33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence\\'s generations are more diverse than RAG-Token\\'s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\\n\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\\n\\nWe compare RAG\\'s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG\\'s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating  $p(z|x)$ . Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\n\\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n\\nTable 4: Human assessments for the Jeopardy Question Generation Task.\\n\\n|   | Factuality | Specificity  |\\n| --- | --- | --- |\\n|  BART better | 7.1% | 16.8%  |\\n|  RAG better | 42.7% | 37.4%  |\\n|  Both good | 11.7% | 11.8%  |\\n|  Both poor | 17.7% | 6.9%  |\\n|  No majority | 20.8% | 20.1%  |\\n\\nTable 5: Ratio of distinct to total tri-grams for generation tasks.\\n\\n|   | MSMARCO | Jeopardy QGen  |\\n| --- | --- | --- |\\n|  Gold | 89.6% | 90.0%  |\\n|  BART | 70.7% | 32.4%  |\\n|  RAG-Token | 77.8% | 46.8%  |\\n|  RAG-Seq. | 83.5% | 53.8%  |\\n\\nTable 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.\\n\\n|  Model | NQ | TQA Exact | WQ Match | CT | Jeopardy-QGen |   | MSMarco |   | FVR-3 Label | FVR-2 Accuracy  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   |   |   |   |   |  B-1 | QB-1 | R-L | B-1  |   |   |\\n|  RAG-Token-BM25 | 29.7 | 41.5 | 32.1 | 33.1 | 17.5 | 22.3 | 55.5 | 48.4 | 75.1 | 91.6  |\\n|  RAG-Sequence-BM25 | 31.8 | 44.1 | 36.6 | 33.8 | 11.1 | 19.5 | 56.5 | 46.9  |   |   |\\n|  RAG-Token-Frozen | 37.8 | 50.1 | 37.1 | 51.1 | 16.7 | 21.7 | 55.9 | 49.4 | 72.9 | 89.4  |\\n|  RAG-Sequence-Frozen | 41.2 | 52.1 | 41.8 | 52.6 | 11.8 | 19.6 | 56.7 | 47.3  |   |   |\\n|  RAG-Token | 43.5 | 54.8 | 46.5 | 51.9 | 17.9 | 22.6 | 56.2 | 49.4 | 74.5 | 90.6  |\\n|  RAG-Sequence | 44.0 | 55.8 | 44.9 | 53.4 | 15.3 | 21.5 | 57.2 | 47.5  |   |   |\\n\\nbetween these dates and use a template \"Who is {position}?\" (e.g. \"Who is the President of Peru?\") to query our NQ RAG model with each index. RAG answers  $70\\\\%$  correctly using the 2016 index for 2016 world leaders and  $68\\\\%$  using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low ( $12\\\\%$  with the 2018 index and 2016 leaders,  $4\\\\%$  with the 2016 index and 2018 leaders). This shows we can update RAG\\'s world knowledge by simply replacing its non-parametric memory.\\n\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n\\n![img-3.jpeg](img-3.jpeg)\\n\\n![img-4.jpeg](img-4.jpeg)\\n\\n# 5 Related Work\\n\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\\n\\n#### General-Purpose Architectures for NLP\\n\\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks *[60, 61]* after fine-tuning *[49, 8]*. GPT-2 *[50]* later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART *[32]* and T5 *[51, 52]* propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\\n\\n#### Learned Retrieval\\n\\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models *[44, 26]* similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search *[46]*, reinforcement learning *[6, 63, 62]*, or a latent variable approach *[31, 20]* as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\\n\\n#### Memory-based Architectures\\n\\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks *[64, 55]*. Concurrent work *[14]* learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings *[15, 13]*. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval *[9]*.\\n\\n#### Retrieve-and-Edit approaches\\n\\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation *[18, 22]* and Semantic Parsing *[21]*. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\\n\\n## 6 Discussion\\n\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\\n\\nBroader Impact\\n\\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\\n\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 *[50]* are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content *[54]*. Advanced language models may also lead to the automation of various jobs in the coming decades *[16]*. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\\n\\n## Acknowledgments\\n\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\\n\\n## References\\n\\n- [1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n- [2] Petr Baudiš and Jan Šedivỳ. Modeling of the question answering task in the yodaqa system. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\\n- [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/D13-1160.\\n- [4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\\n- [5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anthology/P17-1171.\\n- [6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-fine question answering for long documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n-\\n\\n[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\\n- [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\\n- [9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\n- [10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\\n- [11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/P18-1082.\\n- [12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anthology/P19-1346.\\n- [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\\n- [14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\\n- [15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\\n- [16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\\n- [17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n- [18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\\n- [19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\\n\\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.\\n- [21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052–10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.\\n- [22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-erank text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anthology/2020.acl-main.228.\\n- [23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n- [24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\\n- [25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.\\n- [26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n- [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n- [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\\n- [29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.\\n- [30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.\\n- [31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n\\nfor Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/anthology/P19-1612.\\n- [32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n- [33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/N16-1014.\\n- [34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\\n- [35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291.\\n- [36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\\n- [37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\\n- [38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n- [39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\\n- [40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n- [41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n- [42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anthology/D18-1429.\\n- [43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n\\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n- [44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n- [45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\\n- [46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n- [47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n- [48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\\n- [49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\\n- [50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n- [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n- [52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\\n- [53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\\n- [54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\\n- [55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n\\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anthology/N18-1074.\\n- [57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification with elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\\n- [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n- [59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\\n- [60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446.\\n- [61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\\\text{textquotesingle} Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\\n- [62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R^{3}: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\\n- [63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\\n- [64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\\n- [65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anthology/W18-5713.\\n\\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n- [67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anthology/D19-1253.\\n- [68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\\n\\n# Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n# A Implementation Details\\n\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\n\\n# B Human Evaluation\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\".\\n\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\\n\\n# C Training setup Details\\n\\nWe train all RAG models and BART baselines using Fairseq [45]. We train with mixed precision floating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring  $\\\\sim 100$  GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers [66], which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\\'s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\\n\\nD Further Details on Open-Domain QA\\n\\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each $(q,a)$ pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\\n\\n#### CuratedTrec preprocessing\\n\\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models *[20]*. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\n\\n#### TriviaQA Evaluation setups\\n\\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR *[26]*, which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. *[52]* used the TriviaQA official Wikipedia test set instead. Févry et al. *[14]* follow this convention in order to compare with Roberts et al. *[52]* (See appendix of *[14]*). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\\n\\n## Appendix E Further Details on FEVER\\n\\nFor FEVER classification, we follow the practice from *[32]*, and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\\n\\n## Appendix F Null Document Probabilities\\n\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM *[20]* in order to model cases where no useful information could be retrieved for a given input. Here, if $k$ documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over $k+1$ predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\\n\\n## Appendix G Parameters\\n\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n\\nTable 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\n\\n|  Task | Train | Development | Test  |\\n| --- | --- | --- | --- |\\n|  Natural Questions | 79169 | 8758 | 3611  |\\n|  TriviaQA | 78786 | 8838 | 11314  |\\n|  WebQuestions | 3418 | 362 | 2033  |\\n|  CuratedTrec | 635 | 134 | 635  |\\n|  Jeopardy Question Generation | 97392 | 13714 | 26849  |\\n|  MS-MARCO | 153726 | 12468 | 101093*  |\\n|  FEVER-3-way | 145450 | 10000 | 10000  |\\n|  FEVER-2-way | 96966 | 6666 | 6666  |\\n\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consist of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating point precision to manage memory and disk footprints.\\n\\n# H Retrieval Collapse\\n\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would \"collapse\" and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.\\n\\n# I Number of instances per dataset\\n\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.'), -0.0934964442798707), (Document(id='6525a2245e91:0', metadata={'start_line': 1, 'end_line': 750, 'chunk_index': 0, 'text': '# Language Models are Few-Shot Learners\\n\\n|  Tom B. Brown* |   | Benjamin Mann* |   | Nick Ryder* |   | Melanie Subbiah*  |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Jared Kaplan† | Prafulla Dhariwal | Arvind Neelakantan | Pranav Shyam | Girish Sastry |  |  |   |\\n|  Amanda Askell | Sandhini Agarwal | Ariel Herbert-Voss | Gretchen Krueger | Tom Henighan |  |  |   |\\n|  Rewon Child | Aditya Ramesh | Daniel M. Ziegler | Jeffrey Wu | Clemens Winter |  |  |   |\\n|  Christopher Hesse | Mark Chen | Eric Sigler | Mateusz Litwin | Scott Gray |  |  |   |\\n|  Benjamin Chess |   | Jack Clark |   | Christopher Berner |   |  |   |\\n|  Sam McCandlish |   | Alec Radford | Ilya Sutskever | Dario Amodei |   |  |   |\\n\\nOpenAI\\n\\n# Abstract\\n\\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters,  $10\\\\mathrm{x}$  more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\\'s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\\n\\nAuthor contributions listed at end of paper.\\n\\n2\\n\\n# Contents\\n\\n1  Introduction  3\\n2  Approach  6\\n2.1  Model and Architectures  8\\n2.2  Training Dataset  8\\n2.3  Training Process  9\\n2.4  Evaluation  10\\n3  Results  10\\n3.1  Language Modeling, Cloze, and Completion Tasks  11\\n3.2  Closed Book Question Answering  13\\n3.3  Translation  14\\n3.4  Winograd-Style Tasks  16\\n3.5  Common Sense Reasoning  17\\n3.6  Reading Comprehension  18\\n3.7  SuperGLUE  18\\n3.8  NLI  20\\n3.9  Synthetic and Qualitative Tasks  21\\n4  Measuring and Preventing Memorization Of Benchmarks  29\\n5  Limitations  33\\n6  Broader Impacts  34\\n6.1  Misuse of Language Models  35\\n6.2  Fairness, Bias, and Representation  36\\n6.3  Energy Usage  39\\n7  Related Work  39\\n8  Conclusion  40\\nA  Details of Common Crawl Filtering  43\\nB  Details of Model Training  43\\nC  Details of Test Set Contamination Studies  43\\nD  Total Compute Used to Train Language Models  46\\nE  Human Quality Assessment of Synthetic News Articles  46\\nF  Additional Samples from GPT-3  48\\nG  Details of Task Phrasing and Specifications  50\\nH  Results on All Tasks for All Model Sizes  63\\n\\n# 1 Introduction\\n\\nRecent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models  $\\\\left[\\\\mathrm{VSP}^{+}17\\\\right]$  have been directly fine-tuned, entirely removing the need for task-specific architectures [RNSS18, DCLT18, HR18].\\n\\nThis last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR $^{+}$ 19, LOG $^{+}$ 19, YDY $^{+}$ 19, LCG $^{+}$ 19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Removing this limitation would be desirable, for several reasons.\\n\\nFirst, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.\\n\\nSecond, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance  $\\\\left[\\\\mathrm{HLW}^{+}20\\\\right]$  observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it  $\\\\left[\\\\mathrm{YdC}^{+}19,\\\\mathrm{MPL}19\\\\right]$ . Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task  $\\\\left[\\\\mathrm{GSL}^{+}18,\\\\mathrm{NK}19\\\\right]$ .\\n\\nThird, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term \"in-context learning\" to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 1.2: Larger models make increasingly efficient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper \"in-context learning curves\" for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks.\\n\\nsufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.\\n\\nOne potential route towards addressing these issues is meta-learning $^{1}$  – which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.\\n\\nWhile it has shown some initial promise, this approach still achieves results far inferior to fine-tuning – for example  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  achieves only  $4\\\\%$  on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.\\n\\nAnother recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters [DCLT18], to 1.5 billion parameters  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , to 8 billion parameters  $\\\\left[\\\\mathrm{SPP}^{+}19\\\\right]$ , 11 billion parameters  $\\\\left[\\\\mathrm{RSR}^{+}19\\\\right]$ , and finally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$ . Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.\\n\\nIn this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) \"few-shot learning\", or in-context learning where we allow as many demonstrations as will fit into the model\\'s context window (typically 10 to 100), (b) \"one-shot learning\", where we allow only one demonstration, and (c) \"zero-shot\" learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.\\n\\nFigure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model\\'s context,  $K$ . Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these \"learning\" curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.\\n\\nBroadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves  $64.3\\\\%$  accuracy on TriviaQA in the zero-shot setting,  $68.0\\\\%$  in the one-shot setting, and  $71.2\\\\%$  in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.\\n\\nGPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.\\n\\nAt the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3\\'s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.\\n\\nA heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).\\n\\nWe also undertake a systematic study of “data contamination” – a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we find that data contamination has a minimal effect on GPT-3’s performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.\\n\\nIn addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.\\n\\nFinally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard.\\n\\nThe remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes.\\n\\n## 2 Approach\\n\\nOur basic pre-training approach, including model, data, and training, is similar to the process described in *[RWC+19]*, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to *[RWC+19]*, but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on. Specifically, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration):\\n\\n- Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution *[x13]*, and the potential to exploit spurious features of the training data *[GSL+18, x15]*, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work.\\n- Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning *[RWC+19]*, but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion. We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model’s context window ($n_{\\\\mathrm{ctx}}=2048$). The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models. Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML *[x10, VBL+16]* – both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task.\\n- One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans. For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task. By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given.\\n\\nThe three settings we explore for in-context learning\\n\\n# Zero-shot\\n\\nThe model predicts the answer given only a natural language description of the task. No gradient updates are performed.\\n\\n![img-3.jpeg](img-3.jpeg)\\n\\n# One-shot\\n\\nIn addition to the task description, the model sees a single example of the task. No gradient updates are performed.\\n\\n![img-4.jpeg](img-4.jpeg)\\n\\n# Few-shot\\n\\nIn addition to the task description, the model sees a few examples of the task. No gradient updates are performed.\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning. The panels above show four methods for performing a task with a language model – fine-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task descriptions, examples and prompts can be found in Appendix G.\\n\\n# Traditional fine-tuning (not used for GPT-3)\\n\\n# Fine-tuning\\n\\nThe model is trained via repeated gradient updates using a large corpus of example tasks.\\n\\n![img-6.jpeg](img-6.jpeg)\\n\\n- Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task. This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of pre-training data), but is also the most challenging setting. In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases \"unfairly hard\". For example, if someone is asked to \"make a table of world records for the 200m dash\", this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult). Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks – for example, in the translation example in Figure 2.1, a human would likely know what to do from just the text instruction.\\n\\nFigure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.\\n\\nSections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.\\n\\n|  Model Name | nparams | nlayers | dmodel | nheads | dhead | Batch Size | Learning Rate  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Small | 125M | 12 | 768 | 12 | 64 | 0.5M | 6.0 × 10-4  |\\n|  GPT-3 Medium | 350M | 24 | 1024 | 16 | 64 | 0.5M | 3.0 × 10-4  |\\n|  GPT-3 Large | 760M | 24 | 1536 | 16 | 96 | 0.5M | 2.5 × 10-4  |\\n|  GPT-3 XL | 1.3B | 24 | 2048 | 24 | 128 | 1M | 2.0 × 10-4  |\\n|  GPT-3 2.7B | 2.7B | 32 | 2560 | 32 | 80 | 1M | 1.6 × 10-4  |\\n|  GPT-3 6.7B | 6.7B | 32 | 4096 | 32 | 128 | 2M | 1.2 × 10-4  |\\n|  GPT-3 13B | 13.0B | 40 | 5140 | 40 | 128 | 2M | 1.0 × 10-4  |\\n|  GPT-3 175B or “GPT-3” | 175.0B | 96 | 12288 | 96 | 128 | 3.2M | 0.6 × 10-4  |\\n\\nTable 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens.\\n\\n# 2.1 Model and Architectures\\n\\nWe use the same model and architecture as GPT-2  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.\\n\\nTable 2.1 shows the sizes and architectures of our 8 models. Here  $n_{\\\\mathrm{params}}$  is the total number of trainable parameters,  $n_{\\\\mathrm{layers}}$  is the total number of layers,  $d_{\\\\mathrm{model}}$  is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer,  $d_{\\\\mathrm{ff}} = 4 * d_{\\\\mathrm{model}}$ ), and  $d_{\\\\mathrm{head}}$  is the dimension of each attention head. All models use a context window of  $n_{\\\\mathrm{ctx}} = 2048$  tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\\'s. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.\\n\\n# 2.2 Training Dataset\\n\\nDatasets for language models have rapidly expanded, culminating in the Common Crawl dataset $^2$  [RSR $^{+}$ 19] constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy dedduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.\\n\\nDetails of the first two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , collected by scraping links over a longer period of time, and first described in  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$ , two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.\\n\\nTable 2.2 shows the final mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.\\n\\n![img-7.jpeg](img-7.jpeg)\\nTotal Compute Used During Training\\nFigure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B is almost  $10\\\\mathrm{x}$  larger than RoBERTa-Large (355M params), both models took roughly 50 petaflop/s-days of compute during pre-training. Methodology for these calculations can be found in Appendix D.\\n\\n|  Dataset | Quantity (tokens) | Weight in training mix | Epochs elapsed when training for 300B tokens  |\\n| --- | --- | --- | --- |\\n|  Common Crawl (filtered) | 410 billion | 60% | 0.44  |\\n|  WebText2 | 19 billion | 22% | 2.9  |\\n|  Books1 | 12 billion | 8% | 1.9  |\\n|  Books2 | 55 billion | 8% | 0.43  |\\n|  Wikipedia | 3 billion | 3% | 3.4  |\\n\\nTable 2.2: Datasets used to train GPT-3. \"Weight in training mix\" refers to the fraction of examples during training that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once.\\n\\nA major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.\\n\\n# 2.3 Training Process\\n\\nAs found in  $\\\\left[\\\\mathrm{KMH}^{+}20, \\\\mathrm{MKAT18}\\\\right]$ , larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU\\'s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B.\\n\\n2.4 Evaluation\\n\\nFor few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.\\n\\n$K$ can be any value from 0 to the maximum amount allowed by the model’s context window, which is $n_{\\\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples. Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.\\n\\nOn tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\\\frac{P(\\\\mathrm{completion}|\\\\mathrm{context})}{P(\\\\mathrm{completion}|\\\\mathrm{answer\\\\_context})}$, where $\\\\mathrm{answer\\\\_context}$ is the string \"Answer: \" or \"A: \" and is used to prompt that the completion should be an answer but is otherwise generic.\\n\\nOn tasks that involve binary classification, we give the options more semantically meaningful names (e.g. “True” or “False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by *[RSR^{+}19]* (see Appendix G) for details.\\n\\nOn tasks with free-form completion, we use beam search with the same parameters as *[RSR^{+}19]*: a beam width of 4 and a length penalty of $\\\\alpha=0.6$. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.\\n\\nFinal results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.\\n\\n## 3 Results\\n\\nIn Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in *[KMH^{+}20]*, language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.\\n\\nBelow, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks.\\n\\nIn Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on “closed book” question answering tasks: tasks which require using the information stored in the model’s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model’s ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model’s performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we briefly explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities – these tasks focus on on-the-fly reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings.\\n\\n![img-8.jpeg](img-8.jpeg)\\nFigure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts.\\n\\n|  Setting | PTB  |\\n| --- | --- |\\n|  SOTA (Zero-Shot) | 35.8a  |\\n|  GPT-3 Zero-Shot | 20.5  |\\n\\nTable 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3\\'s training data.  ${}^{a}\\\\left\\\\lbrack  {\\\\mathrm{{RWC}}}^{ + }{19}\\\\right\\\\rbrack$\\n\\n# 3.1 Language Modeling, Cloze, and Completion Tasks\\n\\nIn this section we test GPT-3\\'s performance on the traditional task of language modeling, as well as related tasks that involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible completions of a piece of text.\\n\\n# 3.1.1 Language Modeling\\n\\nWe calculate zero-shot perplexity on the Penn Tree Bank (PTB)  $\\\\left[\\\\mathrm{MKM}^{+}94\\\\right]$  dataset measured in  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . We omit the 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the one-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these issues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15 points, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to define one-shot or few-shot evaluation around, so we measure only zero-shot.\\n\\n# 3.1.2 LAMBADA\\n\\nThe LAMBADA dataset  $\\\\left[\\\\mathrm{PKL}^{+}16\\\\right]$  tests the modeling of long-range dependencies in text - the model is asked to predict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the continued scaling of language models is yielding diminishing returns on this difficult benchmark.  $\\\\left[\\\\mathrm{BHT}^{+}20\\\\right]$  reflect on the small  $1.5\\\\%$  improvement achieved by a doubling of model size between two recent state of the art results  $\\\\left(\\\\left[\\\\mathrm{SPP}^{+}19\\\\right]\\\\right.$\\n\\n|  Setting | LAMBADA (acc) | LAMBADA (ppl) | StoryCloze (acc) | HellaSwag (acc)  |\\n| --- | --- | --- | --- | --- |\\n|  SOTA | 68.0a | 8.63b | 91.8c | 85.6d  |\\n|  GPT-3 Zero-Shot | 76.2 | 3.00 | 83.2 | 78.9  |\\n|  GPT-3 One-Shot | 72.5 | 3.35 | 84.7 | 78.1  |\\n|  GPT-3 Few-Shot | 86.4 | 1.92 | 87.7 | 79.3  |\\n\\nTable 3.2: Performance on cloze and completion tasks. GPT-3 significantly improves SOTA on LAMBADA while achieving respectable performance on two difficult completion prediction datasets.  ${}^{a}$  [Tur20]  ${}^{b}$  [RWC+19]  ${}^{c}$  [LDL19]  ${}^{d}$  [LCH+20]\\n\\n![img-9.jpeg](img-9.jpeg)\\nFigure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of the art by  $18\\\\%$ . Note zero-shot uses a different format from one-shot and few-shot as described in the text.\\n\\nand [Tur20]) and argue that \"continuing to expand hardware and data sizes by orders of magnitude is not the path forward\". We find that path is still promising and in a zero-shot setting GPT-3 achieves  $76\\\\%$  on LAMBADA, a gain of  $8\\\\%$  over the previous state of the art.\\n\\nLAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  (which ban \"continuation\" words). The few-shot setting instead allows us to \"frame\" the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following fill-in-the-blank format:\\n\\nAlice was friends with Bob. Alice went to visit her friend  $\\\\underline{\\\\hspace{1cm}}$ .  $\\\\rightarrow$  Bob\\n\\nGeorge bought some baseball equipment, a ball, a glove, and a  $\\\\underline{\\\\hspace{1cm}}$ .  $\\\\rightarrow$\\n\\nWhen presented with examples formatted this way, GPT-3 achieves  $86.4\\\\%$  accuracy in the few-shot setting, an increase of over  $18\\\\%$  from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost  $20\\\\%$ , for GPT-3 it improves accuracy by  $10\\\\%$ . Finally, the fill-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern.\\n\\n|  Setting | NaturalQS | WebQS | TriviaQA  |\\n| --- | --- | --- | --- |\\n|  RAG (Fine-tuned, Open-Domain) [LPP+20] | 44.5 | 45.5 | 68.0  |\\n|  T5-11B+SSM (Fine-tuned, Closed-Book) [RRS20] | 36.6 | 44.7 | 60.5  |\\n|  T5-11B (Fine-tuned, Closed-Book) | 34.5 | 37.4 | 50.1  |\\n|  GPT-3 Zero-Shot | 14.6 | 14.4 | 64.3  |\\n|  GPT-3 One-Shot | 23.0 | 25.3 | 68.0  |\\n|  GPT-3 Few-Shot | 29.9 | 41.5 | 71.2  |\\n\\nTable 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the wiki split test server.\\n\\nOne note of caution is that an analysis of test set contamination identified that a significant minority of the LAMBADA dataset appears to be present in our training data – however analysis performed in Section 4 suggests negligible impact on performance.\\n\\n# 3.1.3 HellaSwag\\n\\nThe HellaSwag dataset  $\\\\left[\\\\mathrm{ZHB}^{+}19\\\\right]$  involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be difficult for language models while remaining easy for humans (who achieve  $95.6\\\\%$  accuracy). GPT-3 achieves  $78.1\\\\%$  accuracy in the one-shot setting and  $79.3\\\\%$  accuracy in the few-shot setting, outperforming the  $75.4\\\\%$  accuracy of a fine-tuned 1.5B parameter language model  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$  but still a fair amount lower than the overall SOTA of  $85.6\\\\%$  achieved by the fine-tuned multi-task model ALUM.\\n\\n# 3.1.4 StoryCloze\\n\\nWe next evaluate GPT-3 on the StoryCloze 2016 dataset  $\\\\left[\\\\mathrm{MCH}^{+}16\\\\right]$ , which involves selecting the correct ending sentence for five-sentence long stories. Here GPT-3 achieves  $83.2\\\\%$  in the zero-shot setting and  $87.7\\\\%$  in the few-shot setting (with  $K = 70$ ). This is still  $4.1\\\\%$  lower than the fine-tuned SOTA using a BERT based model [LDL19] but improves over previous zero-shot results by roughly  $10\\\\%$ .\\n\\n# 3.2 Closed Book Question Answering\\n\\nIn this section we measure GPT-3\\'s ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to find relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted \"open-book\". [RRS20] recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxiliary information. They denote this more restrictive evaluation setting as \"closed-book\". Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions  $\\\\left[\\\\mathrm{KPR}^{+}19\\\\right]$ , WebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, fine-tuning on the Q&amp;A dataset itself is also not permitted.\\n\\nThe results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve  $64.3\\\\%$  in the zero-shot setting,  $68.0\\\\%$  in the one-shot setting, and  $71.2\\\\%$  in the few-shot setting. The zero-shot result already outperforms the fine-tuned T5-11B by  $14.2\\\\%$ , and also outperforms a version with Q&amp;A tailored span prediction during pre-training by  $3.8\\\\%$ . The one-shot result improves by  $3.7\\\\%$  and matches the SOTA for an open-domain QA system which not only fine-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents  $\\\\left[\\\\mathrm{LPP}^{+}20\\\\right]$ . GPT-3\\'s few-shot result further improves performance another  $3.2\\\\%$  beyond this.\\n\\nOn WebQuestions (WebQs), GPT-3 achieves  $14.4\\\\%$  in the zero-shot setting,  $25.3\\\\%$  in the one-shot setting, and  $41.5\\\\%$  in the few-shot setting. This compares to  $37.4\\\\%$  for fine-tuned T5-11B, and  $44.7\\\\%$  for fine-tuned T5-11B+SSM, which uses a Q&amp;A-specific pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art fine-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to few-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions\\n\\n![img-10.jpeg](img-10.jpeg)\\nFigure 3.3: On TriviaQA GPT3\\'s performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG  $\\\\left[\\\\mathrm{LPP}^{+}20\\\\right]$\\n\\nand/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance in the few-shot setting.\\n\\nOn Natural Questions (NQs) GPT-3 achieves  $14.6\\\\%$  in the zero-shot setting,  $23.0\\\\%$  in the one-shot setting, and  $29.9\\\\%$  in the few-shot setting, compared to  $36.6\\\\%$  for fine-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to TriviaQA and WebQS. In particular, the questions in NQs tend towards very fine-grained knowledge on Wikipedia specifically which could be testing the limits of GPT-3\\'s capacity and broad pretraining distribution.\\n\\nOverall, on one of the three datasets GPT-3\\'s one-shot matches the open-domain fine-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using fine-tuning. On all 3 datasets, we find that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reflecting the idea that model capacity translates directly to more \\'knowledge\\' absorbed in the parameters of the model.\\n\\n# 3.3 Translation\\n\\nFor GPT-2 a filter was used on a multilingual collection of documents to produce an English only dataset due to capacity concerns. Even with this filtering GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training dataset to include more representation of other languages, though this remains an area for further improvement. As discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based filtering. Although GPT-3\\'s training data is still primarily English (93% by word count), it also includes 7% of text in other languages. These languages are documented in the supplemental material. In order to better understand translation capability, we also expand our analysis to include two additional commonly studied languages, German and Romanian.\\n\\nExisting unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation [SHB15] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages together in a natural way, combining them on a word, sentence, and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in particular. However, our one / few-shot settings aren\\'t strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data.\\n\\nResults are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task, still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for\\n\\n|  Setting | En→Fr | Fr→En | En→De | De→En | En→Ro | Ro→En  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  SOTA (Supervised) | 45.6a | 35.0b | 41.2c | 40.2d | 38.5e | 39.9e  |\\n|  XLM [LC19] | 33.4 | 33.3 | 26.4 | 34.3 | 33.3 | 31.8  |\\n|  MASS [STQ+19] | 37.5 | 34.9 | 28.3 | 35.2 | 35.2 | 33.1  |\\n|  mBART [LGG+20] | - | - | 29.8 | 34.0 | 35.0 | 30.5  |\\n|  GPT-3 Zero-Shot | 25.2 | 21.2 | 24.6 | 27.2 | 14.1 | 19.9  |\\n|  GPT-3 One-Shot | 28.3 | 33.7 | 26.2 | 30.4 | 20.6 | 38.6  |\\n|  GPT-3 Few-Shot | 32.6 | 39.2 | 29.7 | 40.6 | 21.0 | 39.5  |\\n\\nTable 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating into English reflecting its strength as an English LM. We report BLEU scores on the WMT\\'14 Fr  $\\\\leftrightarrow$  En, WMT\\'16 De  $\\\\leftrightarrow$  En, and WMT\\'16 Ro  $\\\\leftrightarrow$  En datasets as measured by multi-bleu.perl with XLM\\'s tokenization in order to compare most closely with prior unsupervised NMT work. SacreBLEU\\' [Pos18] results reported in Appendix H. Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA with relative confidence.  $^a$  [EOAG18]  $^b$  [DHKH14]  $^c$  [WXH+18]  $^d$  [oR16]  $^e$  [LGG+20]  $^f$  [SacreBLEU signature: BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]\\n\\n![img-11.jpeg](img-11.jpeg)\\nFigure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent trend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be stronger than translation from English.\\n\\n|  Setting | Winograd | Winogrande (XL)  |\\n| --- | --- | --- |\\n|  Fine-tuned SOTA | 90.1a | 84.6b  |\\n|  GPT-3 Zero-Shot | 88.3* | 70.2  |\\n|  GPT-3 One-Shot | 89.7* | 73.2  |\\n|  GPT-3 Few-Shot | 88.6* | 77.7  |\\n\\nTable 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set.  ${}^{a}$  [SBBC19]  ${}^{b}$  [LYN+20]\\n\\n![img-12.jpeg](img-12.jpeg)\\nFigure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a fine-tuned RoBERTA-large.\\n\\neach translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could find but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised finetuning on 608K labeled examples, and backtranslation [LHCG19b].\\n\\nFinally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three settings is shown in Appendix H.\\n\\n# 3.4 Winograd-Style Tasks\\n\\nThe Winograd Schemas Challenge [LDM12] is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently fine-tuned language models have achieved near-human performance on the original Winograd dataset, but more difficult versions\\n\\n|  Setting | PIQA | ARC (Easy) | ARC (Challenge) | OpenBookQA  |\\n| --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 79.4 | 92.0[KKS+20] | 78.5[KKS+20] | 87.2[KKS+20]  |\\n|  GPT-3 Zero-Shot | 80.5* | 68.8 | 51.4 | 57.6  |\\n|  GPT-3 One-Shot | 80.5* | 71.2 | 53.2 | 58.8  |\\n|  GPT-3 Few-Shot | 82.8* | 70.1 | 51.5 | 65.4  |\\n\\nTable 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set.\\n\\n![img-13.jpeg](img-13.jpeg)\\nFigure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a score on the development set in all three conditions that exceeds the best recorded score on the task.\\n\\nsuch as the adversarially-mined Winogrande dataset [SBBC19] still significantly lag human performance. We test GPT-3\\'s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting.\\n\\nOn Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same \"partial evaluation\" method described in  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classification and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves  $88.3\\\\%$ ,  $89.7\\\\%$ , and  $88.6\\\\%$  in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4).\\n\\nOn the more difficult Winogrande dataset, we do find gains to in-context learning: GPT-3 achieves  $70.2\\\\%$  in the zero-shot setting,  $73.2\\\\%$  in the one-shot setting, and  $77.7\\\\%$  in the few-shot setting. For comparison a fine-tuned RoBERTA model achieves  $79\\\\%$ , state-of-the-art is  $84.6\\\\%$  achieved with a fine-tuned high capacity model (T5), and human performance on the task as reported by [SBBC19] is  $94.0\\\\%$ .\\n\\n# 3.5 Common Sense Reasoning\\n\\nNext we consider three datasets which attempt to capture physical or scientific reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering. The first, PhysicalQA (PIQA)  $\\\\left[\\\\mathrm{BZB}^{+}19\\\\right]$ , asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world. GPT-3 achieves  $81.0\\\\%$  accuracy zero-shot,  $80.5\\\\%$  accuracy one-shot, and  $82.8\\\\%$  accuracy few-shot (the last measured on PIQA\\'s test server). This compares favorably to the  $79.4\\\\%$  accuracy prior state-of-the-art of a\\n\\n|  Setting | CoQA | DROP | QuAC | SQuADv2 | RACE-h | RACE-m  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 90.7a | 89.1b | 74.4c | 93.0d | 90.0e | 93.1e  |\\n|  GPT-3 Zero-Shot | 81.5 | 23.6 | 41.5 | 59.5 | 45.5 | 58.4  |\\n|  GPT-3 One-Shot | 84.0 | 34.3 | 43.3 | 65.4 | 45.9 | 57.4  |\\n|  GPT-3 Few-Shot | 85.0 | 36.5 | 44.3 | 69.8 | 46.8 | 58.1  |\\n\\nTable 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy.  ${}^{a}\\\\left\\\\lbrack  {\\\\mathrm{{JZC}}}^{ + }{19}\\\\right\\\\rbrack  {}^{b}\\\\left\\\\lbrack  {\\\\mathrm{{JN20}}}\\\\right\\\\rbrack  {}^{c}\\\\left\\\\lbrack  {\\\\mathrm{{AI19}}}\\\\right\\\\rbrack  {}^{d}\\\\left\\\\lbrack  {\\\\mathrm{{QIA20}}}\\\\right\\\\rbrack  {}^{e}\\\\left\\\\lbrack  {\\\\mathrm{{SPP}}}^{ + }{19}\\\\right\\\\rbrack$\\n\\nfine-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over  $10\\\\%$  worse than human performance, but GPT-3\\'s few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis flagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk. See Section 4 for details.\\n\\nARC  $\\\\left[\\\\mathrm{CCE}^{+}18\\\\right]$  is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the \"Challenge\" version of the dataset which has been filtered to questions which simple statistical or information retrieval methods are unable to correctly answer, GPT-3 achieves  $51.4\\\\%$  accuracy in the zero-shot setting,  $53.2\\\\%$  in the one-shot setting, and  $51.5\\\\%$  in the few-shot setting. This is approaching the performance of a fine-tuned RoBERTa baseline  $(55.9\\\\%)$  from UnifiedQA  $\\\\left[\\\\mathrm{KKS}^{+}20\\\\right]$ . On the \"Easy\" version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves  $68.8\\\\%$ ,  $71.2\\\\%$ , and  $70.1\\\\%$  which slightly exceeds a fine-tuned RoBERTa baseline from  $\\\\left[\\\\mathrm{KKS}^{+}20\\\\right]$ . However, both of these results are still much worse than the overall SOTAs achieved by the UnifiedQA which exceeds GPT-3\\'s few-shot results by  $27\\\\%$  on the challenge set and  $22\\\\%$  on the easy set.\\n\\nOn OpenBookQA [MCKS18], GPT-3 improves significantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3\\'s few-shot performance is similar to a fine-tuned BERT Large baseline on the leaderboard.\\n\\nOverall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a significant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings.\\n\\n# 3.6 Reading Comprehension\\n\\nNext we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3\\'s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.\\n\\nGPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC  $\\\\left[\\\\mathrm{CHI}^{+}18\\\\right]$  a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP  $\\\\left[\\\\mathrm{DWD}^{+}19\\\\right]$ , a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems  $\\\\left[\\\\mathrm{RLL}^{+}19\\\\right]$ . On SQuAD 2.0 [RJL18], GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best fine-tuned result in the original paper. On RACE  $\\\\left[\\\\mathrm{LXL}^{+}17\\\\right]$ , a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still  $45\\\\%$  behind SOTA.\\n\\n# 3.7 SuperGLUE\\n\\nIn order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark  $\\\\left[\\\\mathrm{WPN}^{+}19\\\\right]$ $\\\\left[\\\\mathrm{WPN}^{+}19\\\\right]$ $\\\\left[\\\\mathrm{CLC}^{+}19\\\\right]$  [DMST19] [RBG11]  $\\\\left[\\\\mathrm{KCR}^{+}18\\\\right]$ $\\\\left[\\\\mathrm{ZLL}^{+}18\\\\right]$  [DGM06]  $\\\\left[\\\\mathrm{BHDD}^{+}06\\\\right]$  [GMDD07]  $\\\\left[\\\\mathrm{BDD}^{+}09\\\\right]$  [PCC18]  $\\\\left[\\\\mathrm{PHR}^{+}18\\\\right]$ . GPT-3\\'s test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC\\n\\n![img-14.jpeg](img-14.jpeg)\\nFigure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting, only a few points behind measured human performance and state-of-the-art fine-tuned models. Zero-shot and one-shot performance is a few points behind, with the gains to few-shot being largest for bigger models.\\n\\n|   | SuperGLUE Average | BoolQ Accuracy | CB Accuracy | CB F1 | COPA Accuracy | RTE Accuracy  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 89.0 | 91.0 | 96.9 | 93.9 | 94.8 | 92.5  |\\n|  Fine-tuned BERT-Large | 69.0 | 77.4 | 83.6 | 75.7 | 70.6 | 71.7  |\\n|  GPT-3 Few-Shot | 71.8 | 76.4 | 75.6 | 52.0 | 92.0 | 69.0  |\\n|   | WiC Accuracy | WSC Accuracy | MultiRC Accuracy | MultiRC F1a | ReCoRD Accuracy | ReCoRD F1  |\\n|  Fine-tuned SOTA | 76.1 | 93.8 | 62.3 | 88.2 | 92.5 | 93.3  |\\n|  Fine-tuned BERT-Large | 69.6 | 64.6 | 24.1 | 70.0 | 71.3 | 72.0  |\\n|  GPT-3 Few-Shot | 49.4 | 80.1 | 30.5 | 75.4 | 90.2 | 91.1  |\\n\\nTable 3.8: Performance of GPT-3 on SuperGLUE compared to fine-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates.\\n\\n![img-15.jpeg](img-15.jpeg)\\nFigure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value of  $K = 32$  means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table 3.8). The BERT-Large reference model was fine-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was first fine-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further fine-tuning on the SuperGLUE training set (for a total of 630K fine-tuning examples). We find the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context.\\n\\n![img-16.jpeg](img-16.jpeg)\\n\\nand MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we used the same set of randomly drawn examples from the training set as context for all of the problems we evaluated.\\n\\nWe observe a wide range in GPT-3\\'s performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving second place on the leaderboard, where first place is held by a fine-tuned 11 billion parameter model (T5). On WSC, performance is still relatively strong, achieving  $80.1\\\\%$  in the few-shot setting (note that GPT-3 achieves  $88.6\\\\%$  on the original Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable, roughly matching that of a fine-tuned BERT-Large. On CB, we see signs of life at  $75.6\\\\%$  in the few-shot setting.\\n\\nWiC is a notable weak spot with few-shot performance at  $49.4\\\\%$  (at random chance). We tried a number of different phrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two sentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer in the next section (which discusses the ANLI benchmark) – GPT-3 appears to be weak in the few-shot or one-shot setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same way in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another. This could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses, GPT-3 still outperforms a fine-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to the state-of-the-art held by a fine-tuned 11 billion parameter model.\\n\\nFinally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of examples in the context showing increasing benefits from in-context learning (Figure 3.8). We scale  $K$  up to 32 examples per task, after which point additional examples will not reliably fit into our context. When sweeping over values of  $K$ , we find that GPT-3 requires less than eight total examples per task to outperform a fine-tuned BERT-Large on overall SuperGLUE score.\\n\\n# 3.8 NLI\\n\\nNatural Language Inference (NLI) [Fyo00] concerns the ability to understand the relationship between two sentences. In practice, this task is usually structured as a two or three class classification problem where the model classifies\\n\\n![img-17.jpeg](img-17.jpeg)\\nFigure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of  $1.2\\\\%$ ). We find that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix.\\n\\nwhether the second sentence logically follows from the first, contradicts the first sentence, or is possibly true (neutral). SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task fine-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset  $\\\\left[\\\\mathrm{NWD}^{+}19\\\\right]$ . ANLI is a difficult dataset employing a series of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our models smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting ( $\\\\sim 33\\\\%$ ), whereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results for all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very difficult task for language models and they are only just beginning to show signs of progress.\\n\\n# 3.9 Synthetic and Qualitative Tasks\\n\\nOne way to probe GPT-3\\'s range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which require it to perform simple on-the-fly computational reasoning, recognize a novel pattern that is unlikely to have occurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we test GPT-3\\'s ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3\\'s ability to solve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models.\\n\\n# 3.9.1 Arithmetic\\n\\nTo test GPT-3\\'s ability to perform simple arithmetic operations without task-specific training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:\\n\\n- 2 digit addition  $(2\\\\mathbf{D}+)$  - The model is asked to add two integers sampled uniformly from  $[0,100)$ , phrased in the form of a question, e.g. \"Q: What is 48 plus 76? A: 124.\"\\n- 2 digit subtraction (2D-) - The model is asked to subtract two integers sampled uniformly from  $[0,100)$ ; the answer may be negative. Example: \"Q: What is 34 minus 53? A: -19\".\\n- 3 digit addition  $(3\\\\mathbf{D}+)$  - Same as 2 digit addition, except numbers are uniformly sampled from [0, 1000).\\n\\n![img-18.jpeg](img-18.jpeg)\\nFigure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a significant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being able to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a significant fraction of the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot are shown in the appendix.\\n\\n- 3 digit subtraction (3D-) - Same as 2 digit subtraction, except numbers are uniformly sampled from [0, 1000).\\n- 4 digit addition  $(4\\\\mathbf{D} + )$  - Same as 3 digit addition, except uniformly sampled from [0, 10000).\\n- 4 digit subtraction (4D-) - Same as 3 digit subtraction, except uniformly sampled from [0, 10000).\\n- 5 digit addition  $(5\\\\mathbf{D} + )$  - Same as 3 digit addition, except uniformly sampled from [0, 100000).\\n- 5 digit subtraction (5D-) - Same as 3 digit subtraction, except uniformly sampled from [0, 100000).\\n- 2 digit multiplication (2Dx) - The model is asked to multiply two integers sampled uniformly from [0, 100), e.g. \"Q: What is 24 times 42? A: 1008\".\\n- One-digit composite (1DC) - The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the last two. For example, \"Q: What is  $6 + (4^{*}8)$ ? A: 38\". The three 1 digit numbers are selected uniformly on [0, 10) and the operations are selected uniformly from  $\\\\{+, -, *\\\\}$ .\\n\\nIn all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances.\\n\\nFirst we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction, GPT-3 displays strong proficiency when the number of digits is small, achieving  $100\\\\%$  accuracy on 2 digit addition,  $98.9\\\\%$  at 2 digit subtraction,  $80.2\\\\%$  at 3 digit addition, and  $94.2\\\\%$  at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves  $25 - 26\\\\%$  accuracy on four digit operations and  $9 - 10\\\\%$  accuracy on five digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves  $29.2\\\\%$  accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves  $21.3\\\\%$  accuracy at single digit combined operations (for example,  $9^{*}(7 + 5)$ ), suggesting that it has some robustness beyond just single operations.\\n\\nAs Figure 3.10 makes clear, small models do poorly on all of these tasks – even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than  $10\\\\%$  of the time.\\n\\nOne-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly. Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 significantly\\n\\n|  Setting | 2D+ | 2D- | 3D+ | 3D- | 4D+ | 4D- | 5D+ | 5D- | 2Dx | 1DC  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Zero-shot | 76.9 | 58.0 | 34.2 | 48.3 | 4.0 | 7.5 | 0.7 | 0.8 | 19.8 | 9.8  |\\n|  GPT-3 One-shot | 99.6 | 86.4 | 65.5 | 78.7 | 14.0 | 14.0 | 3.5 | 3.8 | 27.4 | 14.3  |\\n|  GPT-3 Few-shot | 100.0 | 98.9 | 80.4 | 94.2 | 25.5 | 26.8 | 9.3 | 9.9 | 29.2 | 21.3  |\\n\\nTable 3.9: Results on basic arithmetic tasks for GPT-3 175B.  $\\\\{2,3,4,5\\\\} \\\\mathrm{D}\\\\{+, - \\\\}$  is 2, 3, 4, and 5 digit addition or subtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger moving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows significant arithmetic abilities.\\n\\n|  Setting | CL | A1 | A2 | RI | RW  |\\n| --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Zero-shot | 3.66 | 2.28 | 8.91 | 8.26 | 0.09  |\\n|  GPT-3 One-shot | 21.7 | 8.62 | 25.9 | 45.4 | 0.48  |\\n|  GPT-3 Few-shot | 37.9 | 15.1 | 39.7 | 67.2 | 0.44  |\\n\\nTable 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and few-shot settings. CL is \"cycle letters in word\", A1 is anagrams of but the first and last letters, A2 is anagrams of all but the first and last two letters, RI is \"Random insertion in word\", RW is \"reversed words\".\\n\\noutperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table 3.9, and model capacity scaling for all three settings is shown in Appendix H.\\n\\nTo spot-check whether the model is simply memorizing specific arithmetic problems, we took the 3-digit arithmetic problems in our test set and searched for them in our training data in both the forms \"<num1> + <num2> = \" and \"<num1> plus <num2>\". Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000 subtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers could have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes such as not carrying a \"1\", suggesting it is actually attempting to perform the relevant computation rather than memorizing a table.\\n\\nOverall, GPT-3 displays reasonable proficiency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings.\\n\\n# 3.9.2 Word Scrambling and Manipulation Tasks\\n\\nTo test GPT-3\\'s ability to learn novel symbolic manipulations from a few examples, we designed a small battery of 5 \"character manipulation\" tasks. Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are:\\n\\n- Cycle letters in word (CL) - The model is given a word with its letters cycled, then the “=” symbol, and is expected to generate the original word. For example, it might be given “lyinevitab” and should output “inevitably”.\\n- Anagrams of all but first and last characters (A1) - The model is given a word where every letter except the first and last have been scrambled randomly, and must output the original word. Example: criroptuon = corruption.\\n- Anagrams of all but first and last 2 characters (A2) - The model is given a word where every letter except the first 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt  $\\\\rightarrow$  opponent.\\n- Random insertion in word (RI) - A random punctuation or space character is inserted between each letter of a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession.\\n- Reversed words (RW) - The model is given a word spelled backwards, and must output the original word. Example: stcejbo  $\\\\rightarrow$  objects.\\n\\nFor each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by [Nor09] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11. Task performance tends to grow smoothly with model size, with the full GPT-3 model achieving  $66.9\\\\%$  on removing</num2></num1></num2></num1>\\n\\n![img-19.jpeg](img-19.jpeg)\\nFigure 3.11: Few-shot performance on the five word scrambling tasks for different sizes of model. There is generally smooth improvement with model size although the random insertion task shows an upward slope of improvement with the 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in the appendix. All tasks are done with  $K = 100$ .\\n\\nrandom insertions,  $38.6\\\\%$  on cycling letters,  $40.2\\\\%$  on the easier anagram task, and  $15.1\\\\%$  on the more difficult anagram task (where only the first and last letters are held fixed). None of the models can reverse the letters in a word.\\n\\nIn the one-shot setting, performance is significantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artificial nature makes them unlikely to appear in the pre-training data (although we cannot confirm this with certainty).\\n\\nWe can further quantify performance by plotting \"in-context learning curves\", which show task performance as a function of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task in Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions.\\n\\nFinally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on significant fractions of a word (on average  $\\\\sim 0.7$  words per token), so from the LM\\'s perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also, CL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word), requiring the model to perform some search to find the correct unscrambling. Thus, the skills involved appear to require non-trivial pattern-matching and computation.\\n\\n# 3.9.3 SAT Analogies\\n\\nTo test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 \"SAT analogy\" problems [TLBS03]. Analogies are a style of multiple choice question that constituted a section of the SAT college entrance exam before 2005. A typical example is \"audacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation\". The student is expected to choose which of the five word pairs has the same relationship as the original word pair; in this example the answer is \"sanctimonious is to hypocrisy\". On this task GPT-3 achieves  $65.2\\\\%$  in the few-shot setting,  $59.1\\\\%$  in the one-shot setting, and  $53.7\\\\%$  in the zero-shot setting, whereas the average score among college applicants was  $57\\\\%$  [TL05] (random guessing yields  $20\\\\%$ ). As shown in Figure 3.12, the results improve with scale, with the full 175 billion model improving by over  $10\\\\%$  compared to the 13 billion parameter model.\\n\\n![img-20.jpeg](img-20.jpeg)\\nFigure 3.12: Zero-, one-, and few-shot performance on SAT analogy tasks, for different sizes of model. The largest model achieves  $65\\\\%$  accuracy in the few-shot setting, and also demonstrates significant gains to in-context learning which are not present in smaller models.\\n\\n# 3.9.4 News Article Generation\\n\\nPrevious work on generative language models qualitatively tested their ability to generate synthetic \"news articles\" by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . Relative to  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , the dataset used to train GPT-3 is much less weighted towards news articles, so trying to generate news articles via raw unconditional samples is less effective – for example GPT-3 often interprets the proposed first sentence of a \"news article\" as a tweet and then posts synthetic responses or follow-up tweets. To solve this problem we employed GPT-3\\'s few-shot learning abilities by providing three previous news articles in the model\\'s context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the \"news\" genre.\\n\\nTo gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles from real ones. Similar work has been carried out by Kreps et al. [KMB20] and Zellers et al.  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$ . Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality.\\n\\nIn order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model. Participants were asked to select whether the article was \"very likely written by a human\", \"more likely written by a human\", \"I don\\'t know\", \"more likely written by a machine\", or \"very likely written by a machine\".\\n\\nThe articles we selected were not in the models\\' training data and the model outputs were formatted and selected programmatically to prevent human cherry-picking. All models used the same context to condition outputs on and were pre-trained with the same context size and the same article titles and subtitles were used as prompts for each model. However, we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a \"control model\": a 160M parameter model with no context and increased output randomness.\\n\\n|   | Mean accuracy | 95% Confidence Interval (low, hi) | t compared to control (p-value) | “I don’t know” assignments  |\\n| --- | --- | --- | --- | --- |\\n|  Control (deliberately bad model) | 86% | 83%-90% | - | 3.6 %  |\\n|  GPT-3 Small | 76% | 72%-80% | 3.9 (2e-4) | 4.9%  |\\n|  GPT-3 Medium | 61% | 58%-65% | 10.3 (7e-21) | 6.0%  |\\n|  GPT-3 Large | 68% | 64%-72% | 7.3 (3e-11) | 8.7%  |\\n|  GPT-3 XL | 62% | 59%-65% | 10.7 (1e-19) | 7.5%  |\\n|  GPT-3 2.7B | 62% | 58%-65% | 10.4 (5e-19) | 7.1%  |\\n|  GPT-3 6.7B | 60% | 56%-63% | 11.2 (3e-21) | 6.2%  |\\n|  GPT-3 13B | 55% | 52%-58% | 15.3 (1e-32) | 7.1%  |\\n|  GPT-3 175B | 52% | 49%-54% | 16.9 (1e-34) | 7.8%  |\\n\\nTable 3.11: Human accuracy in identifying whether short (~200 word) news articles are model generated. We find that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from  $86\\\\%$  on the control model to  $52\\\\%$  on GPT-3 175B. This table compares mean accuracy between five different models, and shows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model (an unconditional GPT-3 Small model with increased output randomness).\\n\\nMean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that the intentionally bad articles were model generated was  $\\\\sim 86\\\\%$  where  $50\\\\%$  is chance level performance. By contrast, mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance at  $\\\\sim 52\\\\%$  (see Table 3.11). Human abilities to detect model generated text appear to decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance. This is true despite the fact that participants spend more time on each output as model size increases (see Appendix E).\\n\\nExamples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15. Much of the text is—as indicated by the evaluations—difficult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors, the models have no access to the specific facts that the article titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are often subtle enough that they are not noticed.\\n\\nRelated work on language model detection by Ippolito et al. [IDCBE19] indicates that automatic discriminators like GROVER  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$  and GLTR [GSR19] may have greater success at detecting model generated text than human evaluators. Automatic detection of these models may be a promising area of future research.\\n\\nIppolito et al. [IDCBE19] also note that human accuracy at detecting model generated text increases as humans observe more tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial experiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to compare human abilities to detect the articles generated by GPT-3 and a control model.\\n\\nWe found that mean human accuracy at detecting the intentionally bad longer articles from the control model was  $\\\\sim 88\\\\%$ , while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at  $\\\\sim 52\\\\%$  (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles.\\n\\n# 3.9.5 Learning and Using Novel Words\\n\\nA task studied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a word in a sentence after seeing it defined only once, or conversely inferring a word\\'s meaning from only one usage. Here we qualitatively test GPT-3\\'s ability to do the former. Specifically, we give GPT-3 the definition of a nonexistent word, such as \"Gigamuru\", and then ask it to use it in a sentence. We provide one to five previous examples of a (separate)\\n\\n![img-21.jpeg](img-21.jpeg)\\nFigure 3.13: People\\'s ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately-bad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance  $(50\\\\%)$  is indicated with the dashed line at the bottom. Line of best fit is a power law with  $95\\\\%$  confidence intervals.\\n\\n|   | Mean accuracy | 95% Confidence Interval (low, hi) | t compared to control (p-value) | “I don’t know” assignments  |\\n| --- | --- | --- | --- | --- |\\n|  Control | 88% | 84%-91% | - | 2.7%  |\\n|  GPT-3 175B | 52% | 48%-57% | 12.7 (3.2e-23) | 10.6%  |\\n\\nTable 3.12: People\\'s ability to identify whether  $\\\\sim 500$  word articles are model generated (as measured by the ratio of correct assignments to non-neutral assignments) was  $88\\\\%$  on the control model and  $52\\\\%$  on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with increased output randomness).\\n\\nTitle: United Methodists Agree to Historic Split\\nSubtitle: Those who oppose gay marriage will form their own denomination\\nArticle: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church\\'s annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination.\\nThe Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the \"largest Protestant denomination in the U.S.,\" but that it has been shrinking in recent decades. The new split will be the second in the church\\'s history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church. The Post notes that the proposed split \"comes at a critical time for the church, which has been losing members for years,\" which has been \"pushed toward the brink of a schism over the role of LGBTQ people in the church.\" Gay marriage is not the only issue that has divided the church. In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban them from serving as clergy, and the South Pacific regional conference voting to allow them.\\n\\nFigure 3.14: The GPT-3 generated news article that humans had the greatest difficulty distinguishing from a human written article (accuracy:  $12\\\\%$ ).\\n\\nTitle: Star\\'s Tux Promise Draws Megyn Kelly\\'s Sarcasm\\nSubtitle: Joaquin Phoenix pledged to not change for each awards event\\nArticle: A year ago, Joaquin Phoenix made headlines when he appeared on the red carpet at the Golden Globes wearing a tuxedo with a paper bag over his head that read, \"I am a shape-shifter. I can\\'t change the world. I can only change myself.\" It was a promise to not change to fit into the Hollywood mold: \"I think that\\'s a really special thing, to not change yourself. I think it\\'s a really special thing to say, \\'This is what\\'s inside of me, I\\'m proud of it, and I\\'m not going to be ashamed because of the way that someone else thinks I should be.\\'\" Now, it\\'s the Oscars, and Phoenix is at it again. But this time, his publicist is saying he\\'ll be wearing a tux no matter what.\\nMegyn Kelly was not impressed, and she let him have it on The Tonight Show. \"You know, I feel like, I feel like you could have worn the tux,\" she says. \"But you\\'re saying you\\'re a shape-shifter. I don\\'t know if you can change your tux, but you can change your mind. You can change your mind. You can change your mind.\" Phoenix says he did, but it didn\\'t stick. \"I was like, \\'Okay, I\\'m going to wear a tuxedo to this thing.\\' And then I thought, \\'I don\\'t want to wear a tuxedo to this thing.\\'\" Kelly goes on to encourage him to change his mind again, but Phoenix says it\\'s too late: \"I\\'m committed to wearing this.\"\\n\\nFigure 3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written article (accuracy:  $61\\\\%$ ).\\n\\n![img-22.jpeg](img-22.jpeg)\\nFigure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is GPT-3\\'s completions, plain text is human prompts. In the first example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 other than the conditioning shown here.\\n\\nnonexistent word being defined and used in a sentence, so the task is few-shot in terms of previous examples of the broad task and one-shot in terms of the specific word. Table 3.16 shows the 6 examples we generated; all definitions were human-generated, and the first answer was human-generated as conditioning while the subsequent answers were generated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the final sentence the model generates a plausible conjugation for the word \"screeg\" (namely \"screeghed\"), although the use of the word is slightly awkward (\"screeghed at each other\") despite being plausible in the sense that it could describe a toy sword fight. Overall, GPT-3 appears to be at least proficient at the task of using novel words in a sentence.\\n\\n# 3.9.6 Correcting English Grammar\\n\\nAnother task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the few-shot setting by giving prompts of the form \"Poor English Input: <sentence>\\\\n Good English Output: <sentence>\". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats). Results are shown in Figure 3.17.\\n\\n# 4 Measuring and Preventing Memorization Of Benchmarks\\n\\nSince our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to.\\n\\nThis concern is not just hypothetical. One of the first papers to train a language model on Common Crawl data [TL18] detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  also conducted post-hoc overlap analysis. Their study was relatively encouraging, finding that</sentence></sentence>\\n\\n![img-23.jpeg](img-23.jpeg)\\nFigure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface is GPT-3\\'s completions, plain text is human prompts. In the first few examples example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 aside from the first few examples as conditioning and the \"Poor English input/Good English output\" framing. We note that the distinction between \"poor\" and \"good\" English (and the terms themselves) is complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what \"good\" is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word \"cheap\" in a way that alters meaning).\\n\\n![img-24.jpeg](img-24.jpeg)\\nFigure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difficulty rather than overfitting.\\n\\nalthough models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).\\n\\nGPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared.\\n\\nWe initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn\\'t feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results.\\n\\nFor each benchmark, we produce a \\'clean\\' version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix C.\\n\\nWe then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over  $50\\\\%$ ), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance.\\n\\nBelow, we review in more detail the few specific cases where either (1) the model performs significantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difficult.\\n\\nOur analysis flagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English\\n\\n![img-25.jpeg](img-25.jpeg)\\nFigure 4.2: Benchmark contamination analysis We constructed cleaned versions of each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high confidence to be clean, and the y-axis shows the difference in performance when evaluating only on the verified clean subset. Performance on most benchmarks changed negligibly, but some were flagged for further review. On inspection we find some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section 3 with an asterisk. We find no evidence that other benchmarks are affected.\\n\\ntranslation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false positives. We summarize the results for each group of tasks below:\\n\\n- Reading Comprehension: Our initial analysis flagged  $&gt;90\\\\%$  of task examples from QuAC, SQuAD2, and DROP as potentially contaminated, so large that even measuring the differential on a clean subset was difficult. Upon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source text was present in our training data but the question/answer pairs were not, meaning the model gains only background information and cannot memorize the answer to a specific question.\\n- German translation: We found  $25\\\\%$  of the examples in the WMT16 German-English test set were marked as potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the flagged examples contain paired sentences resembling NMT training data and collisions were monolingual matches mostly of snippets of events discussed in the news.\\n- Reversed Words and Anagrams: Recall that these tasks are of the form \"alaok = koala\". Due to the short length of these tasks, we used 2-grams for filtering (ignoring punctuation). After inspecting the flagged overlaps, we found that they were not typically instances of real reversals or unscramblings in the training set, but rather palindromes or trivial unscramblings, e.g. \"kayak = kayak\". The amount of overlap was small, but removing the trivial tasks lead to an increase in difficulty and thus a spurious signal. Related to this, the symbol insertion task shows high overlap but no effect on performance – this is because that task involves removing non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to many spurious matches.\\n- PIQA: The overlap analysis flagged  $29\\\\%$  of examples as contaminated, and observed a 3 percentage point absolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was released after our training set was created and its labels are hidden, some of the web pages used by the crowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller model with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias rather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot rigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential contamination.\\n- Winograd: The overlap analysis flagged  $45\\\\%$  of examples, and found a  $2.6\\\\%$  decrease in performance on the clean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in fact present in our training set, though presented in a different format than we present the task to the model. Although the decrease in performance is small, we mark our Winograd results in the main paper with an asterisk.\\n\\n- Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the Children’s Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark.\\n\\nWe also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our fill-in-the-blank format precludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section.\\n\\nAn important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset. It remains possible that memorization inflates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing.\\n\\nOverall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the field in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C.\\n\\n## 5 Limitations\\n\\nGPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work.\\n\\nFirst, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3’s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with “common sense physics”, despite doing well on some datasets (such as PIQA *[BZB^{+}19]*) that test this domain. Specifically GPT-3 has difficulty with questions of the type “If I put cheese into the fridge, will it melt?”. Quantitatively, GPT-3’s in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some “comparison” tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3’s strong few-shot performance on many other tasks.\\n\\nGPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models *[RSR^{+}19]*. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3’s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the “best of both worlds”.\\n\\nA more fundamental limitation of the general approach described in this paper – scaling up any LM-like model, whether autoregressive or bidirectional – is that it may eventually run into (or could already be running into) the limits of the\\n\\npretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. *[x20]* demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world *[BHT^{+}20]*. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans *[ZSW^{+}19a]*, fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world *[CLY^{+}19]*.\\n\\nAnother limitation broadly shared by language models is poor sample efficiency during pre-training. While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime *[x14]*. Improving pre-training sample efficiency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements.\\n\\nA limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identifies tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a specific style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or defining nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research.\\n\\nA limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation *[x13]* of large models down to a manageable size for specific tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a specific task, suggesting that in principle aggressive distillation may be possible. Distillation is well-explored in general *[x15]* but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size.\\n\\nFinally, GPT-3 shares some limitations common to most deep learning systems – its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This last issue – biases in the data that may lead the model to generate stereotyped or prejudiced content – is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section 6).\\n\\n## 6 Broader Impacts\\n\\nLanguage models have a wide range of beneficial applications for society, including code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. But they also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difficulty of distinguishing synthetic text from human-written text. It therefore has the potential to advance both the beneficial and harmful applications of language models.\\n\\nHere we focus on the potential harms of improved language models, not because we believe the harms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also briefly discuss issues of energy efficiency (Section 6.3).\\n\\n6.1 Misuse of Language Models\\n\\nMalicious uses of language models can be somewhat difficult to anticipate because they often involve repurposing language models in a very different environment or for a different purpose than researchers intended. To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact *[x20]*. We discuss three factors: potential misuse applications, threat actors, and external incentive structures.\\n\\n#### 6.1.1 Potential Misuse Applications\\n\\nAny socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufficiently high quality text. Language models that produce high quality text generation could lower existing barriers to carrying out these activities and increase their efficacy.\\n\\nThe misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to generate several paragraphs of synthetic content that people find difficult to distinguish from human-written text in 3.9.4 represents a concerning milestone in this regard.\\n\\n#### 6.1.2 Threat Actor Analysis\\n\\nThreat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors who may be able to build a malicious product to ‘advanced persistent threats’ (APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas *[SBC+19]*.\\n\\nTo understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did find significant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate, but significant improvements in reliability could change this.\\n\\nBecause APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about possible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible difference in operations that may see potential gains by using language models. The assessment was that language models may not be worth investing significant resources in because there has been no convincing demonstration that current language models are significantly better than current methods for generating text, and because methods for “targeting” or “controlling” the content of language models are still at a very early stage.\\n\\n#### 6.1.3 External Incentive Structures\\n\\nEach threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their agenda. TTPs are influenced by economic factors like scalability and ease of deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login credentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment.\\n\\nEase of use is another significant incentive. Having stable infrastructure has a large impact on the adoption of TTPs. The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot produces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor required in operating this bot. But a human is still needed to filter the outputs, which restricts how scalable the operation can be.\\n\\nBased on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will eventually develop language models that are sufficiently consistent and steerable that they will be of greater interest to malicious actors. We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers.\\n\\n####\\n\\n6.2 Fairness, Bias, and Representation\\n\\nBiases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms *[x10]*. We have conducted an analysis of biases in the model in order to better understand GPT-3’s limitations when it comes to fairness, bias, and representation.\\n\\nOur goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reflect all of the model’s biases even within the studied categories.\\n\\nBroadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data. Below we discuss our preliminary findings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension.\\n\\n#### 6.2.1 Gender\\n\\nIn our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3. We measured this by feeding the model a context such as \"The detective was a\" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identifiers include midwife, nurse, receptionist, housekeeper etc.\\n\\nWe also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation} was a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent {occupation} was a,\" the majority of occupations had an even higher probability of being followed by a male identifier than a female one than was the case with our original neutral prompt, \"The {occupation} was a\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as $\\\\frac{1}{n_{\\\\text{jobs}}}\\\\sum_{\\\\text{jobs}}\\\\log(\\\\frac{P(\\\\text{female}|\\\\text{Context})}{P(\\\\text{male}|\\\\text{Context})})$ - was $-1.11$ for the Neutral Variant, $-2.14$ for the Competent Variant and $-1.15$ for the Incompetent Variant.\\n\\nWe also carried out pronoun resolution on the Winogender dataset *[x23]* using two methods which further corroborated the model’s tendency to associate most occupations with males. One method measured the models ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \"The advisor met with the advisee because she wanted to get advice about job applications. ’She’ refers to the\" and found the option with the lowest probability between the two possible options (Choices between Occupation Option: advisor; Participant Option: advisee).\\n\\nOccupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models.\\n\\nWe also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-selected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature\\n\\nTable 6.1: Most Biased Descriptive Words in 175B Model\\n\\n|  Top 10 Most Biased Male Descriptive Words with Raw Co-Occurrence Counts | Top 10 Most Biased Female Descriptive Words with Raw Co-Occurrence Counts  |\\n| --- | --- |\\n|  Average Number of Co-Occurrences Across All Words: 17.5 | Average Number of Co-Occurrences Across All Words: 23.9  |\\n|  Large (16) | Optimistic (12)  |\\n|  Mostly (15) | Bubbly (12)  |\\n|  Lazy (14) | Naughty (12)  |\\n|  Fantastic (13) | Easy-going (12)  |\\n|  Eccentric (13) | Petite (10)  |\\n|  Protect (10) | Tight (10)  |\\n|  Jolly (10) | Pregnant (10)  |\\n|  Stable (9) | Gorgeous (28)  |\\n|  Personable (22) | Sucked (8)  |\\n|  Survive (7) | Beautiful (158)  |\\n\\nof 1 and top_p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\", \"She was very\", \"He would be described as\", \"She would be described as\". We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more often described using appearance oriented words such as \"beautiful\" and \"gorgeous\" as compared to men who were more often described using adjectives that span a greater spectrum.\\n\\nTable 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. \"Most Favored\" here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender.\\n\\n# 6.2.2 Race\\n\\nTo investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\", \"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation  $\\\\left[\\\\mathrm{HZJ}^{+}19\\\\right]$ , we explored how race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5, horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).\\n\\nIt should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology.\\n\\nAcross the models we analyzed, \\'Asian\\' had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, \\'Black\\' had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data.\\n\\n![img-26.jpeg](img-26.jpeg)\\nFigure 6.1: Racial Sentiment Across Models\\n\\n|  Religion | Most Favored Descriptive Words  |\\n| --- | --- |\\n|  Atheism | ‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’, ‘Characterized’  |\\n|  Buddhism | ‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘En- lightenment’, ‘Non-Violent’  |\\n|  Christianity | ‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Com- ments’, ‘Officially’  |\\n|  Hinduism | ‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated’, ‘Africa’  |\\n|  Islam | ‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’, ‘Levant’, ‘Allah’, ‘Prophet’  |\\n|  Judaism | ‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’, ‘Game’, ‘Russian’  |\\n\\nTable 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model.\\n\\n# 6.2.3 Religion\\n\\nWe studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length  $\\\\approx 50$  with a temperature of 1 and a top  $p$  of 0.9 for every prompt. Our prompts were of the nature \" {Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words.\\n\\nThe following is an example output from the model:\\n\\n\"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic.\"', 'doc_id': '6525a2245e91'}, page_content='# Language Models are Few-Shot Learners\\n\\n|  Tom B. Brown* |   | Benjamin Mann* |   | Nick Ryder* |   | Melanie Subbiah*  |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Jared Kaplan† | Prafulla Dhariwal | Arvind Neelakantan | Pranav Shyam | Girish Sastry |  |  |   |\\n|  Amanda Askell | Sandhini Agarwal | Ariel Herbert-Voss | Gretchen Krueger | Tom Henighan |  |  |   |\\n|  Rewon Child | Aditya Ramesh | Daniel M. Ziegler | Jeffrey Wu | Clemens Winter |  |  |   |\\n|  Christopher Hesse | Mark Chen | Eric Sigler | Mateusz Litwin | Scott Gray |  |  |   |\\n|  Benjamin Chess |   | Jack Clark |   | Christopher Berner |   |  |   |\\n|  Sam McCandlish |   | Alec Radford | Ilya Sutskever | Dario Amodei |   |  |   |\\n\\nOpenAI\\n\\n# Abstract\\n\\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters,  $10\\\\mathrm{x}$  more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\\'s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\\n\\nAuthor contributions listed at end of paper.\\n\\n2\\n\\n# Contents\\n\\n1  Introduction  3\\n2  Approach  6\\n2.1  Model and Architectures  8\\n2.2  Training Dataset  8\\n2.3  Training Process  9\\n2.4  Evaluation  10\\n3  Results  10\\n3.1  Language Modeling, Cloze, and Completion Tasks  11\\n3.2  Closed Book Question Answering  13\\n3.3  Translation  14\\n3.4  Winograd-Style Tasks  16\\n3.5  Common Sense Reasoning  17\\n3.6  Reading Comprehension  18\\n3.7  SuperGLUE  18\\n3.8  NLI  20\\n3.9  Synthetic and Qualitative Tasks  21\\n4  Measuring and Preventing Memorization Of Benchmarks  29\\n5  Limitations  33\\n6  Broader Impacts  34\\n6.1  Misuse of Language Models  35\\n6.2  Fairness, Bias, and Representation  36\\n6.3  Energy Usage  39\\n7  Related Work  39\\n8  Conclusion  40\\nA  Details of Common Crawl Filtering  43\\nB  Details of Model Training  43\\nC  Details of Test Set Contamination Studies  43\\nD  Total Compute Used to Train Language Models  46\\nE  Human Quality Assessment of Synthetic News Articles  46\\nF  Additional Samples from GPT-3  48\\nG  Details of Task Phrasing and Specifications  50\\nH  Results on All Tasks for All Model Sizes  63\\n\\n# 1 Introduction\\n\\nRecent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models  $\\\\left[\\\\mathrm{VSP}^{+}17\\\\right]$  have been directly fine-tuned, entirely removing the need for task-specific architectures [RNSS18, DCLT18, HR18].\\n\\nThis last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR $^{+}$ 19, LOG $^{+}$ 19, YDY $^{+}$ 19, LCG $^{+}$ 19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Removing this limitation would be desirable, for several reasons.\\n\\nFirst, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.\\n\\nSecond, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance  $\\\\left[\\\\mathrm{HLW}^{+}20\\\\right]$  observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it  $\\\\left[\\\\mathrm{YdC}^{+}19,\\\\mathrm{MPL}19\\\\right]$ . Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task  $\\\\left[\\\\mathrm{GSL}^{+}18,\\\\mathrm{NK}19\\\\right]$ .\\n\\nThird, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term \"in-context learning\" to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 1.2: Larger models make increasingly efficient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper \"in-context learning curves\" for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks.\\n\\nsufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.\\n\\nOne potential route towards addressing these issues is meta-learning $^{1}$  – which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.\\n\\nWhile it has shown some initial promise, this approach still achieves results far inferior to fine-tuning – for example  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  achieves only  $4\\\\%$  on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.\\n\\nAnother recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters [DCLT18], to 1.5 billion parameters  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , to 8 billion parameters  $\\\\left[\\\\mathrm{SPP}^{+}19\\\\right]$ , 11 billion parameters  $\\\\left[\\\\mathrm{RSR}^{+}19\\\\right]$ , and finally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$ . Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.\\n\\nIn this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) \"few-shot learning\", or in-context learning where we allow as many demonstrations as will fit into the model\\'s context window (typically 10 to 100), (b) \"one-shot learning\", where we allow only one demonstration, and (c) \"zero-shot\" learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.\\n\\nFigure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model\\'s context,  $K$ . Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these \"learning\" curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.\\n\\nBroadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves  $64.3\\\\%$  accuracy on TriviaQA in the zero-shot setting,  $68.0\\\\%$  in the one-shot setting, and  $71.2\\\\%$  in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.\\n\\nGPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.\\n\\nAt the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3\\'s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.\\n\\nA heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).\\n\\nWe also undertake a systematic study of “data contamination” – a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we find that data contamination has a minimal effect on GPT-3’s performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.\\n\\nIn addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.\\n\\nFinally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard.\\n\\nThe remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes.\\n\\n## 2 Approach\\n\\nOur basic pre-training approach, including model, data, and training, is similar to the process described in *[RWC+19]*, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to *[RWC+19]*, but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on. Specifically, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration):\\n\\n- Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution *[x13]*, and the potential to exploit spurious features of the training data *[GSL+18, x15]*, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work.\\n- Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning *[RWC+19]*, but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion. We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model’s context window ($n_{\\\\mathrm{ctx}}=2048$). The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models. Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML *[x10, VBL+16]* – both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task.\\n- One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans. For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task. By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given.\\n\\nThe three settings we explore for in-context learning\\n\\n# Zero-shot\\n\\nThe model predicts the answer given only a natural language description of the task. No gradient updates are performed.\\n\\n![img-3.jpeg](img-3.jpeg)\\n\\n# One-shot\\n\\nIn addition to the task description, the model sees a single example of the task. No gradient updates are performed.\\n\\n![img-4.jpeg](img-4.jpeg)\\n\\n# Few-shot\\n\\nIn addition to the task description, the model sees a few examples of the task. No gradient updates are performed.\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning. The panels above show four methods for performing a task with a language model – fine-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task descriptions, examples and prompts can be found in Appendix G.\\n\\n# Traditional fine-tuning (not used for GPT-3)\\n\\n# Fine-tuning\\n\\nThe model is trained via repeated gradient updates using a large corpus of example tasks.\\n\\n![img-6.jpeg](img-6.jpeg)\\n\\n- Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task. This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of pre-training data), but is also the most challenging setting. In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases \"unfairly hard\". For example, if someone is asked to \"make a table of world records for the 200m dash\", this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult). Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks – for example, in the translation example in Figure 2.1, a human would likely know what to do from just the text instruction.\\n\\nFigure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.\\n\\nSections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.\\n\\n|  Model Name | nparams | nlayers | dmodel | nheads | dhead | Batch Size | Learning Rate  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Small | 125M | 12 | 768 | 12 | 64 | 0.5M | 6.0 × 10-4  |\\n|  GPT-3 Medium | 350M | 24 | 1024 | 16 | 64 | 0.5M | 3.0 × 10-4  |\\n|  GPT-3 Large | 760M | 24 | 1536 | 16 | 96 | 0.5M | 2.5 × 10-4  |\\n|  GPT-3 XL | 1.3B | 24 | 2048 | 24 | 128 | 1M | 2.0 × 10-4  |\\n|  GPT-3 2.7B | 2.7B | 32 | 2560 | 32 | 80 | 1M | 1.6 × 10-4  |\\n|  GPT-3 6.7B | 6.7B | 32 | 4096 | 32 | 128 | 2M | 1.2 × 10-4  |\\n|  GPT-3 13B | 13.0B | 40 | 5140 | 40 | 128 | 2M | 1.0 × 10-4  |\\n|  GPT-3 175B or “GPT-3” | 175.0B | 96 | 12288 | 96 | 128 | 3.2M | 0.6 × 10-4  |\\n\\nTable 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens.\\n\\n# 2.1 Model and Architectures\\n\\nWe use the same model and architecture as GPT-2  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.\\n\\nTable 2.1 shows the sizes and architectures of our 8 models. Here  $n_{\\\\mathrm{params}}$  is the total number of trainable parameters,  $n_{\\\\mathrm{layers}}$  is the total number of layers,  $d_{\\\\mathrm{model}}$  is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer,  $d_{\\\\mathrm{ff}} = 4 * d_{\\\\mathrm{model}}$ ), and  $d_{\\\\mathrm{head}}$  is the dimension of each attention head. All models use a context window of  $n_{\\\\mathrm{ctx}} = 2048$  tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\\'s. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.\\n\\n# 2.2 Training Dataset\\n\\nDatasets for language models have rapidly expanded, culminating in the Common Crawl dataset $^2$  [RSR $^{+}$ 19] constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy dedduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.\\n\\nDetails of the first two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , collected by scraping links over a longer period of time, and first described in  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$ , two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.\\n\\nTable 2.2 shows the final mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.\\n\\n![img-7.jpeg](img-7.jpeg)\\nTotal Compute Used During Training\\nFigure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B is almost  $10\\\\mathrm{x}$  larger than RoBERTa-Large (355M params), both models took roughly 50 petaflop/s-days of compute during pre-training. Methodology for these calculations can be found in Appendix D.\\n\\n|  Dataset | Quantity (tokens) | Weight in training mix | Epochs elapsed when training for 300B tokens  |\\n| --- | --- | --- | --- |\\n|  Common Crawl (filtered) | 410 billion | 60% | 0.44  |\\n|  WebText2 | 19 billion | 22% | 2.9  |\\n|  Books1 | 12 billion | 8% | 1.9  |\\n|  Books2 | 55 billion | 8% | 0.43  |\\n|  Wikipedia | 3 billion | 3% | 3.4  |\\n\\nTable 2.2: Datasets used to train GPT-3. \"Weight in training mix\" refers to the fraction of examples during training that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once.\\n\\nA major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.\\n\\n# 2.3 Training Process\\n\\nAs found in  $\\\\left[\\\\mathrm{KMH}^{+}20, \\\\mathrm{MKAT18}\\\\right]$ , larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU\\'s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B.\\n\\n2.4 Evaluation\\n\\nFor few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.\\n\\n$K$ can be any value from 0 to the maximum amount allowed by the model’s context window, which is $n_{\\\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples. Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.\\n\\nOn tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\\\frac{P(\\\\mathrm{completion}|\\\\mathrm{context})}{P(\\\\mathrm{completion}|\\\\mathrm{answer\\\\_context})}$, where $\\\\mathrm{answer\\\\_context}$ is the string \"Answer: \" or \"A: \" and is used to prompt that the completion should be an answer but is otherwise generic.\\n\\nOn tasks that involve binary classification, we give the options more semantically meaningful names (e.g. “True” or “False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by *[RSR^{+}19]* (see Appendix G) for details.\\n\\nOn tasks with free-form completion, we use beam search with the same parameters as *[RSR^{+}19]*: a beam width of 4 and a length penalty of $\\\\alpha=0.6$. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.\\n\\nFinal results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.\\n\\n## 3 Results\\n\\nIn Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in *[KMH^{+}20]*, language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.\\n\\nBelow, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks.\\n\\nIn Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on “closed book” question answering tasks: tasks which require using the information stored in the model’s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model’s ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model’s performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we briefly explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities – these tasks focus on on-the-fly reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings.\\n\\n![img-8.jpeg](img-8.jpeg)\\nFigure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts.\\n\\n|  Setting | PTB  |\\n| --- | --- |\\n|  SOTA (Zero-Shot) | 35.8a  |\\n|  GPT-3 Zero-Shot | 20.5  |\\n\\nTable 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3\\'s training data.  ${}^{a}\\\\left\\\\lbrack  {\\\\mathrm{{RWC}}}^{ + }{19}\\\\right\\\\rbrack$\\n\\n# 3.1 Language Modeling, Cloze, and Completion Tasks\\n\\nIn this section we test GPT-3\\'s performance on the traditional task of language modeling, as well as related tasks that involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible completions of a piece of text.\\n\\n# 3.1.1 Language Modeling\\n\\nWe calculate zero-shot perplexity on the Penn Tree Bank (PTB)  $\\\\left[\\\\mathrm{MKM}^{+}94\\\\right]$  dataset measured in  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . We omit the 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the one-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these issues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15 points, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to define one-shot or few-shot evaluation around, so we measure only zero-shot.\\n\\n# 3.1.2 LAMBADA\\n\\nThe LAMBADA dataset  $\\\\left[\\\\mathrm{PKL}^{+}16\\\\right]$  tests the modeling of long-range dependencies in text - the model is asked to predict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the continued scaling of language models is yielding diminishing returns on this difficult benchmark.  $\\\\left[\\\\mathrm{BHT}^{+}20\\\\right]$  reflect on the small  $1.5\\\\%$  improvement achieved by a doubling of model size between two recent state of the art results  $\\\\left(\\\\left[\\\\mathrm{SPP}^{+}19\\\\right]\\\\right.$\\n\\n|  Setting | LAMBADA (acc) | LAMBADA (ppl) | StoryCloze (acc) | HellaSwag (acc)  |\\n| --- | --- | --- | --- | --- |\\n|  SOTA | 68.0a | 8.63b | 91.8c | 85.6d  |\\n|  GPT-3 Zero-Shot | 76.2 | 3.00 | 83.2 | 78.9  |\\n|  GPT-3 One-Shot | 72.5 | 3.35 | 84.7 | 78.1  |\\n|  GPT-3 Few-Shot | 86.4 | 1.92 | 87.7 | 79.3  |\\n\\nTable 3.2: Performance on cloze and completion tasks. GPT-3 significantly improves SOTA on LAMBADA while achieving respectable performance on two difficult completion prediction datasets.  ${}^{a}$  [Tur20]  ${}^{b}$  [RWC+19]  ${}^{c}$  [LDL19]  ${}^{d}$  [LCH+20]\\n\\n![img-9.jpeg](img-9.jpeg)\\nFigure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of the art by  $18\\\\%$ . Note zero-shot uses a different format from one-shot and few-shot as described in the text.\\n\\nand [Tur20]) and argue that \"continuing to expand hardware and data sizes by orders of magnitude is not the path forward\". We find that path is still promising and in a zero-shot setting GPT-3 achieves  $76\\\\%$  on LAMBADA, a gain of  $8\\\\%$  over the previous state of the art.\\n\\nLAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  (which ban \"continuation\" words). The few-shot setting instead allows us to \"frame\" the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following fill-in-the-blank format:\\n\\nAlice was friends with Bob. Alice went to visit her friend  $\\\\underline{\\\\hspace{1cm}}$ .  $\\\\rightarrow$  Bob\\n\\nGeorge bought some baseball equipment, a ball, a glove, and a  $\\\\underline{\\\\hspace{1cm}}$ .  $\\\\rightarrow$\\n\\nWhen presented with examples formatted this way, GPT-3 achieves  $86.4\\\\%$  accuracy in the few-shot setting, an increase of over  $18\\\\%$  from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost  $20\\\\%$ , for GPT-3 it improves accuracy by  $10\\\\%$ . Finally, the fill-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern.\\n\\n|  Setting | NaturalQS | WebQS | TriviaQA  |\\n| --- | --- | --- | --- |\\n|  RAG (Fine-tuned, Open-Domain) [LPP+20] | 44.5 | 45.5 | 68.0  |\\n|  T5-11B+SSM (Fine-tuned, Closed-Book) [RRS20] | 36.6 | 44.7 | 60.5  |\\n|  T5-11B (Fine-tuned, Closed-Book) | 34.5 | 37.4 | 50.1  |\\n|  GPT-3 Zero-Shot | 14.6 | 14.4 | 64.3  |\\n|  GPT-3 One-Shot | 23.0 | 25.3 | 68.0  |\\n|  GPT-3 Few-Shot | 29.9 | 41.5 | 71.2  |\\n\\nTable 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the wiki split test server.\\n\\nOne note of caution is that an analysis of test set contamination identified that a significant minority of the LAMBADA dataset appears to be present in our training data – however analysis performed in Section 4 suggests negligible impact on performance.\\n\\n# 3.1.3 HellaSwag\\n\\nThe HellaSwag dataset  $\\\\left[\\\\mathrm{ZHB}^{+}19\\\\right]$  involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be difficult for language models while remaining easy for humans (who achieve  $95.6\\\\%$  accuracy). GPT-3 achieves  $78.1\\\\%$  accuracy in the one-shot setting and  $79.3\\\\%$  accuracy in the few-shot setting, outperforming the  $75.4\\\\%$  accuracy of a fine-tuned 1.5B parameter language model  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$  but still a fair amount lower than the overall SOTA of  $85.6\\\\%$  achieved by the fine-tuned multi-task model ALUM.\\n\\n# 3.1.4 StoryCloze\\n\\nWe next evaluate GPT-3 on the StoryCloze 2016 dataset  $\\\\left[\\\\mathrm{MCH}^{+}16\\\\right]$ , which involves selecting the correct ending sentence for five-sentence long stories. Here GPT-3 achieves  $83.2\\\\%$  in the zero-shot setting and  $87.7\\\\%$  in the few-shot setting (with  $K = 70$ ). This is still  $4.1\\\\%$  lower than the fine-tuned SOTA using a BERT based model [LDL19] but improves over previous zero-shot results by roughly  $10\\\\%$ .\\n\\n# 3.2 Closed Book Question Answering\\n\\nIn this section we measure GPT-3\\'s ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to find relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted \"open-book\". [RRS20] recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxiliary information. They denote this more restrictive evaluation setting as \"closed-book\". Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions  $\\\\left[\\\\mathrm{KPR}^{+}19\\\\right]$ , WebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, fine-tuning on the Q&amp;A dataset itself is also not permitted.\\n\\nThe results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve  $64.3\\\\%$  in the zero-shot setting,  $68.0\\\\%$  in the one-shot setting, and  $71.2\\\\%$  in the few-shot setting. The zero-shot result already outperforms the fine-tuned T5-11B by  $14.2\\\\%$ , and also outperforms a version with Q&amp;A tailored span prediction during pre-training by  $3.8\\\\%$ . The one-shot result improves by  $3.7\\\\%$  and matches the SOTA for an open-domain QA system which not only fine-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents  $\\\\left[\\\\mathrm{LPP}^{+}20\\\\right]$ . GPT-3\\'s few-shot result further improves performance another  $3.2\\\\%$  beyond this.\\n\\nOn WebQuestions (WebQs), GPT-3 achieves  $14.4\\\\%$  in the zero-shot setting,  $25.3\\\\%$  in the one-shot setting, and  $41.5\\\\%$  in the few-shot setting. This compares to  $37.4\\\\%$  for fine-tuned T5-11B, and  $44.7\\\\%$  for fine-tuned T5-11B+SSM, which uses a Q&amp;A-specific pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art fine-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to few-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions\\n\\n![img-10.jpeg](img-10.jpeg)\\nFigure 3.3: On TriviaQA GPT3\\'s performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG  $\\\\left[\\\\mathrm{LPP}^{+}20\\\\right]$\\n\\nand/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance in the few-shot setting.\\n\\nOn Natural Questions (NQs) GPT-3 achieves  $14.6\\\\%$  in the zero-shot setting,  $23.0\\\\%$  in the one-shot setting, and  $29.9\\\\%$  in the few-shot setting, compared to  $36.6\\\\%$  for fine-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to TriviaQA and WebQS. In particular, the questions in NQs tend towards very fine-grained knowledge on Wikipedia specifically which could be testing the limits of GPT-3\\'s capacity and broad pretraining distribution.\\n\\nOverall, on one of the three datasets GPT-3\\'s one-shot matches the open-domain fine-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using fine-tuning. On all 3 datasets, we find that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reflecting the idea that model capacity translates directly to more \\'knowledge\\' absorbed in the parameters of the model.\\n\\n# 3.3 Translation\\n\\nFor GPT-2 a filter was used on a multilingual collection of documents to produce an English only dataset due to capacity concerns. Even with this filtering GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training dataset to include more representation of other languages, though this remains an area for further improvement. As discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based filtering. Although GPT-3\\'s training data is still primarily English (93% by word count), it also includes 7% of text in other languages. These languages are documented in the supplemental material. In order to better understand translation capability, we also expand our analysis to include two additional commonly studied languages, German and Romanian.\\n\\nExisting unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation [SHB15] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages together in a natural way, combining them on a word, sentence, and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in particular. However, our one / few-shot settings aren\\'t strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data.\\n\\nResults are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task, still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for\\n\\n|  Setting | En→Fr | Fr→En | En→De | De→En | En→Ro | Ro→En  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  SOTA (Supervised) | 45.6a | 35.0b | 41.2c | 40.2d | 38.5e | 39.9e  |\\n|  XLM [LC19] | 33.4 | 33.3 | 26.4 | 34.3 | 33.3 | 31.8  |\\n|  MASS [STQ+19] | 37.5 | 34.9 | 28.3 | 35.2 | 35.2 | 33.1  |\\n|  mBART [LGG+20] | - | - | 29.8 | 34.0 | 35.0 | 30.5  |\\n|  GPT-3 Zero-Shot | 25.2 | 21.2 | 24.6 | 27.2 | 14.1 | 19.9  |\\n|  GPT-3 One-Shot | 28.3 | 33.7 | 26.2 | 30.4 | 20.6 | 38.6  |\\n|  GPT-3 Few-Shot | 32.6 | 39.2 | 29.7 | 40.6 | 21.0 | 39.5  |\\n\\nTable 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating into English reflecting its strength as an English LM. We report BLEU scores on the WMT\\'14 Fr  $\\\\leftrightarrow$  En, WMT\\'16 De  $\\\\leftrightarrow$  En, and WMT\\'16 Ro  $\\\\leftrightarrow$  En datasets as measured by multi-bleu.perl with XLM\\'s tokenization in order to compare most closely with prior unsupervised NMT work. SacreBLEU\\' [Pos18] results reported in Appendix H. Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA with relative confidence.  $^a$  [EOAG18]  $^b$  [DHKH14]  $^c$  [WXH+18]  $^d$  [oR16]  $^e$  [LGG+20]  $^f$  [SacreBLEU signature: BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]\\n\\n![img-11.jpeg](img-11.jpeg)\\nFigure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent trend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be stronger than translation from English.\\n\\n|  Setting | Winograd | Winogrande (XL)  |\\n| --- | --- | --- |\\n|  Fine-tuned SOTA | 90.1a | 84.6b  |\\n|  GPT-3 Zero-Shot | 88.3* | 70.2  |\\n|  GPT-3 One-Shot | 89.7* | 73.2  |\\n|  GPT-3 Few-Shot | 88.6* | 77.7  |\\n\\nTable 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set.  ${}^{a}$  [SBBC19]  ${}^{b}$  [LYN+20]\\n\\n![img-12.jpeg](img-12.jpeg)\\nFigure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a fine-tuned RoBERTA-large.\\n\\neach translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could find but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised finetuning on 608K labeled examples, and backtranslation [LHCG19b].\\n\\nFinally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three settings is shown in Appendix H.\\n\\n# 3.4 Winograd-Style Tasks\\n\\nThe Winograd Schemas Challenge [LDM12] is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently fine-tuned language models have achieved near-human performance on the original Winograd dataset, but more difficult versions\\n\\n|  Setting | PIQA | ARC (Easy) | ARC (Challenge) | OpenBookQA  |\\n| --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 79.4 | 92.0[KKS+20] | 78.5[KKS+20] | 87.2[KKS+20]  |\\n|  GPT-3 Zero-Shot | 80.5* | 68.8 | 51.4 | 57.6  |\\n|  GPT-3 One-Shot | 80.5* | 71.2 | 53.2 | 58.8  |\\n|  GPT-3 Few-Shot | 82.8* | 70.1 | 51.5 | 65.4  |\\n\\nTable 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set.\\n\\n![img-13.jpeg](img-13.jpeg)\\nFigure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a score on the development set in all three conditions that exceeds the best recorded score on the task.\\n\\nsuch as the adversarially-mined Winogrande dataset [SBBC19] still significantly lag human performance. We test GPT-3\\'s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting.\\n\\nOn Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same \"partial evaluation\" method described in  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classification and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves  $88.3\\\\%$ ,  $89.7\\\\%$ , and  $88.6\\\\%$  in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4).\\n\\nOn the more difficult Winogrande dataset, we do find gains to in-context learning: GPT-3 achieves  $70.2\\\\%$  in the zero-shot setting,  $73.2\\\\%$  in the one-shot setting, and  $77.7\\\\%$  in the few-shot setting. For comparison a fine-tuned RoBERTA model achieves  $79\\\\%$ , state-of-the-art is  $84.6\\\\%$  achieved with a fine-tuned high capacity model (T5), and human performance on the task as reported by [SBBC19] is  $94.0\\\\%$ .\\n\\n# 3.5 Common Sense Reasoning\\n\\nNext we consider three datasets which attempt to capture physical or scientific reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering. The first, PhysicalQA (PIQA)  $\\\\left[\\\\mathrm{BZB}^{+}19\\\\right]$ , asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world. GPT-3 achieves  $81.0\\\\%$  accuracy zero-shot,  $80.5\\\\%$  accuracy one-shot, and  $82.8\\\\%$  accuracy few-shot (the last measured on PIQA\\'s test server). This compares favorably to the  $79.4\\\\%$  accuracy prior state-of-the-art of a\\n\\n|  Setting | CoQA | DROP | QuAC | SQuADv2 | RACE-h | RACE-m  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 90.7a | 89.1b | 74.4c | 93.0d | 90.0e | 93.1e  |\\n|  GPT-3 Zero-Shot | 81.5 | 23.6 | 41.5 | 59.5 | 45.5 | 58.4  |\\n|  GPT-3 One-Shot | 84.0 | 34.3 | 43.3 | 65.4 | 45.9 | 57.4  |\\n|  GPT-3 Few-Shot | 85.0 | 36.5 | 44.3 | 69.8 | 46.8 | 58.1  |\\n\\nTable 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy.  ${}^{a}\\\\left\\\\lbrack  {\\\\mathrm{{JZC}}}^{ + }{19}\\\\right\\\\rbrack  {}^{b}\\\\left\\\\lbrack  {\\\\mathrm{{JN20}}}\\\\right\\\\rbrack  {}^{c}\\\\left\\\\lbrack  {\\\\mathrm{{AI19}}}\\\\right\\\\rbrack  {}^{d}\\\\left\\\\lbrack  {\\\\mathrm{{QIA20}}}\\\\right\\\\rbrack  {}^{e}\\\\left\\\\lbrack  {\\\\mathrm{{SPP}}}^{ + }{19}\\\\right\\\\rbrack$\\n\\nfine-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over  $10\\\\%$  worse than human performance, but GPT-3\\'s few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis flagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk. See Section 4 for details.\\n\\nARC  $\\\\left[\\\\mathrm{CCE}^{+}18\\\\right]$  is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the \"Challenge\" version of the dataset which has been filtered to questions which simple statistical or information retrieval methods are unable to correctly answer, GPT-3 achieves  $51.4\\\\%$  accuracy in the zero-shot setting,  $53.2\\\\%$  in the one-shot setting, and  $51.5\\\\%$  in the few-shot setting. This is approaching the performance of a fine-tuned RoBERTa baseline  $(55.9\\\\%)$  from UnifiedQA  $\\\\left[\\\\mathrm{KKS}^{+}20\\\\right]$ . On the \"Easy\" version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves  $68.8\\\\%$ ,  $71.2\\\\%$ , and  $70.1\\\\%$  which slightly exceeds a fine-tuned RoBERTa baseline from  $\\\\left[\\\\mathrm{KKS}^{+}20\\\\right]$ . However, both of these results are still much worse than the overall SOTAs achieved by the UnifiedQA which exceeds GPT-3\\'s few-shot results by  $27\\\\%$  on the challenge set and  $22\\\\%$  on the easy set.\\n\\nOn OpenBookQA [MCKS18], GPT-3 improves significantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3\\'s few-shot performance is similar to a fine-tuned BERT Large baseline on the leaderboard.\\n\\nOverall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a significant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings.\\n\\n# 3.6 Reading Comprehension\\n\\nNext we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3\\'s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.\\n\\nGPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC  $\\\\left[\\\\mathrm{CHI}^{+}18\\\\right]$  a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP  $\\\\left[\\\\mathrm{DWD}^{+}19\\\\right]$ , a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems  $\\\\left[\\\\mathrm{RLL}^{+}19\\\\right]$ . On SQuAD 2.0 [RJL18], GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best fine-tuned result in the original paper. On RACE  $\\\\left[\\\\mathrm{LXL}^{+}17\\\\right]$ , a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still  $45\\\\%$  behind SOTA.\\n\\n# 3.7 SuperGLUE\\n\\nIn order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark  $\\\\left[\\\\mathrm{WPN}^{+}19\\\\right]$ $\\\\left[\\\\mathrm{WPN}^{+}19\\\\right]$ $\\\\left[\\\\mathrm{CLC}^{+}19\\\\right]$  [DMST19] [RBG11]  $\\\\left[\\\\mathrm{KCR}^{+}18\\\\right]$ $\\\\left[\\\\mathrm{ZLL}^{+}18\\\\right]$  [DGM06]  $\\\\left[\\\\mathrm{BHDD}^{+}06\\\\right]$  [GMDD07]  $\\\\left[\\\\mathrm{BDD}^{+}09\\\\right]$  [PCC18]  $\\\\left[\\\\mathrm{PHR}^{+}18\\\\right]$ . GPT-3\\'s test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC\\n\\n![img-14.jpeg](img-14.jpeg)\\nFigure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting, only a few points behind measured human performance and state-of-the-art fine-tuned models. Zero-shot and one-shot performance is a few points behind, with the gains to few-shot being largest for bigger models.\\n\\n|   | SuperGLUE Average | BoolQ Accuracy | CB Accuracy | CB F1 | COPA Accuracy | RTE Accuracy  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 89.0 | 91.0 | 96.9 | 93.9 | 94.8 | 92.5  |\\n|  Fine-tuned BERT-Large | 69.0 | 77.4 | 83.6 | 75.7 | 70.6 | 71.7  |\\n|  GPT-3 Few-Shot | 71.8 | 76.4 | 75.6 | 52.0 | 92.0 | 69.0  |\\n|   | WiC Accuracy | WSC Accuracy | MultiRC Accuracy | MultiRC F1a | ReCoRD Accuracy | ReCoRD F1  |\\n|  Fine-tuned SOTA | 76.1 | 93.8 | 62.3 | 88.2 | 92.5 | 93.3  |\\n|  Fine-tuned BERT-Large | 69.6 | 64.6 | 24.1 | 70.0 | 71.3 | 72.0  |\\n|  GPT-3 Few-Shot | 49.4 | 80.1 | 30.5 | 75.4 | 90.2 | 91.1  |\\n\\nTable 3.8: Performance of GPT-3 on SuperGLUE compared to fine-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates.\\n\\n![img-15.jpeg](img-15.jpeg)\\nFigure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value of  $K = 32$  means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table 3.8). The BERT-Large reference model was fine-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was first fine-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further fine-tuning on the SuperGLUE training set (for a total of 630K fine-tuning examples). We find the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context.\\n\\n![img-16.jpeg](img-16.jpeg)\\n\\nand MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we used the same set of randomly drawn examples from the training set as context for all of the problems we evaluated.\\n\\nWe observe a wide range in GPT-3\\'s performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving second place on the leaderboard, where first place is held by a fine-tuned 11 billion parameter model (T5). On WSC, performance is still relatively strong, achieving  $80.1\\\\%$  in the few-shot setting (note that GPT-3 achieves  $88.6\\\\%$  on the original Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable, roughly matching that of a fine-tuned BERT-Large. On CB, we see signs of life at  $75.6\\\\%$  in the few-shot setting.\\n\\nWiC is a notable weak spot with few-shot performance at  $49.4\\\\%$  (at random chance). We tried a number of different phrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two sentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer in the next section (which discusses the ANLI benchmark) – GPT-3 appears to be weak in the few-shot or one-shot setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same way in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another. This could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses, GPT-3 still outperforms a fine-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to the state-of-the-art held by a fine-tuned 11 billion parameter model.\\n\\nFinally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of examples in the context showing increasing benefits from in-context learning (Figure 3.8). We scale  $K$  up to 32 examples per task, after which point additional examples will not reliably fit into our context. When sweeping over values of  $K$ , we find that GPT-3 requires less than eight total examples per task to outperform a fine-tuned BERT-Large on overall SuperGLUE score.\\n\\n# 3.8 NLI\\n\\nNatural Language Inference (NLI) [Fyo00] concerns the ability to understand the relationship between two sentences. In practice, this task is usually structured as a two or three class classification problem where the model classifies\\n\\n![img-17.jpeg](img-17.jpeg)\\nFigure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of  $1.2\\\\%$ ). We find that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix.\\n\\nwhether the second sentence logically follows from the first, contradicts the first sentence, or is possibly true (neutral). SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task fine-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset  $\\\\left[\\\\mathrm{NWD}^{+}19\\\\right]$ . ANLI is a difficult dataset employing a series of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our models smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting ( $\\\\sim 33\\\\%$ ), whereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results for all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very difficult task for language models and they are only just beginning to show signs of progress.\\n\\n# 3.9 Synthetic and Qualitative Tasks\\n\\nOne way to probe GPT-3\\'s range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which require it to perform simple on-the-fly computational reasoning, recognize a novel pattern that is unlikely to have occurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we test GPT-3\\'s ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3\\'s ability to solve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models.\\n\\n# 3.9.1 Arithmetic\\n\\nTo test GPT-3\\'s ability to perform simple arithmetic operations without task-specific training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:\\n\\n- 2 digit addition  $(2\\\\mathbf{D}+)$  - The model is asked to add two integers sampled uniformly from  $[0,100)$ , phrased in the form of a question, e.g. \"Q: What is 48 plus 76? A: 124.\"\\n- 2 digit subtraction (2D-) - The model is asked to subtract two integers sampled uniformly from  $[0,100)$ ; the answer may be negative. Example: \"Q: What is 34 minus 53? A: -19\".\\n- 3 digit addition  $(3\\\\mathbf{D}+)$  - Same as 2 digit addition, except numbers are uniformly sampled from [0, 1000).\\n\\n![img-18.jpeg](img-18.jpeg)\\nFigure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a significant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being able to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a significant fraction of the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot are shown in the appendix.\\n\\n- 3 digit subtraction (3D-) - Same as 2 digit subtraction, except numbers are uniformly sampled from [0, 1000).\\n- 4 digit addition  $(4\\\\mathbf{D} + )$  - Same as 3 digit addition, except uniformly sampled from [0, 10000).\\n- 4 digit subtraction (4D-) - Same as 3 digit subtraction, except uniformly sampled from [0, 10000).\\n- 5 digit addition  $(5\\\\mathbf{D} + )$  - Same as 3 digit addition, except uniformly sampled from [0, 100000).\\n- 5 digit subtraction (5D-) - Same as 3 digit subtraction, except uniformly sampled from [0, 100000).\\n- 2 digit multiplication (2Dx) - The model is asked to multiply two integers sampled uniformly from [0, 100), e.g. \"Q: What is 24 times 42? A: 1008\".\\n- One-digit composite (1DC) - The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the last two. For example, \"Q: What is  $6 + (4^{*}8)$ ? A: 38\". The three 1 digit numbers are selected uniformly on [0, 10) and the operations are selected uniformly from  $\\\\{+, -, *\\\\}$ .\\n\\nIn all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances.\\n\\nFirst we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction, GPT-3 displays strong proficiency when the number of digits is small, achieving  $100\\\\%$  accuracy on 2 digit addition,  $98.9\\\\%$  at 2 digit subtraction,  $80.2\\\\%$  at 3 digit addition, and  $94.2\\\\%$  at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves  $25 - 26\\\\%$  accuracy on four digit operations and  $9 - 10\\\\%$  accuracy on five digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves  $29.2\\\\%$  accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves  $21.3\\\\%$  accuracy at single digit combined operations (for example,  $9^{*}(7 + 5)$ ), suggesting that it has some robustness beyond just single operations.\\n\\nAs Figure 3.10 makes clear, small models do poorly on all of these tasks – even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than  $10\\\\%$  of the time.\\n\\nOne-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly. Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 significantly\\n\\n|  Setting | 2D+ | 2D- | 3D+ | 3D- | 4D+ | 4D- | 5D+ | 5D- | 2Dx | 1DC  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Zero-shot | 76.9 | 58.0 | 34.2 | 48.3 | 4.0 | 7.5 | 0.7 | 0.8 | 19.8 | 9.8  |\\n|  GPT-3 One-shot | 99.6 | 86.4 | 65.5 | 78.7 | 14.0 | 14.0 | 3.5 | 3.8 | 27.4 | 14.3  |\\n|  GPT-3 Few-shot | 100.0 | 98.9 | 80.4 | 94.2 | 25.5 | 26.8 | 9.3 | 9.9 | 29.2 | 21.3  |\\n\\nTable 3.9: Results on basic arithmetic tasks for GPT-3 175B.  $\\\\{2,3,4,5\\\\} \\\\mathrm{D}\\\\{+, - \\\\}$  is 2, 3, 4, and 5 digit addition or subtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger moving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows significant arithmetic abilities.\\n\\n|  Setting | CL | A1 | A2 | RI | RW  |\\n| --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Zero-shot | 3.66 | 2.28 | 8.91 | 8.26 | 0.09  |\\n|  GPT-3 One-shot | 21.7 | 8.62 | 25.9 | 45.4 | 0.48  |\\n|  GPT-3 Few-shot | 37.9 | 15.1 | 39.7 | 67.2 | 0.44  |\\n\\nTable 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and few-shot settings. CL is \"cycle letters in word\", A1 is anagrams of but the first and last letters, A2 is anagrams of all but the first and last two letters, RI is \"Random insertion in word\", RW is \"reversed words\".\\n\\noutperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table 3.9, and model capacity scaling for all three settings is shown in Appendix H.\\n\\nTo spot-check whether the model is simply memorizing specific arithmetic problems, we took the 3-digit arithmetic problems in our test set and searched for them in our training data in both the forms \"<num1> + <num2> = \" and \"<num1> plus <num2>\". Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000 subtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers could have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes such as not carrying a \"1\", suggesting it is actually attempting to perform the relevant computation rather than memorizing a table.\\n\\nOverall, GPT-3 displays reasonable proficiency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings.\\n\\n# 3.9.2 Word Scrambling and Manipulation Tasks\\n\\nTo test GPT-3\\'s ability to learn novel symbolic manipulations from a few examples, we designed a small battery of 5 \"character manipulation\" tasks. Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are:\\n\\n- Cycle letters in word (CL) - The model is given a word with its letters cycled, then the “=” symbol, and is expected to generate the original word. For example, it might be given “lyinevitab” and should output “inevitably”.\\n- Anagrams of all but first and last characters (A1) - The model is given a word where every letter except the first and last have been scrambled randomly, and must output the original word. Example: criroptuon = corruption.\\n- Anagrams of all but first and last 2 characters (A2) - The model is given a word where every letter except the first 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt  $\\\\rightarrow$  opponent.\\n- Random insertion in word (RI) - A random punctuation or space character is inserted between each letter of a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession.\\n- Reversed words (RW) - The model is given a word spelled backwards, and must output the original word. Example: stcejbo  $\\\\rightarrow$  objects.\\n\\nFor each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by [Nor09] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11. Task performance tends to grow smoothly with model size, with the full GPT-3 model achieving  $66.9\\\\%$  on removing</num2></num1></num2></num1>\\n\\n![img-19.jpeg](img-19.jpeg)\\nFigure 3.11: Few-shot performance on the five word scrambling tasks for different sizes of model. There is generally smooth improvement with model size although the random insertion task shows an upward slope of improvement with the 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in the appendix. All tasks are done with  $K = 100$ .\\n\\nrandom insertions,  $38.6\\\\%$  on cycling letters,  $40.2\\\\%$  on the easier anagram task, and  $15.1\\\\%$  on the more difficult anagram task (where only the first and last letters are held fixed). None of the models can reverse the letters in a word.\\n\\nIn the one-shot setting, performance is significantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artificial nature makes them unlikely to appear in the pre-training data (although we cannot confirm this with certainty).\\n\\nWe can further quantify performance by plotting \"in-context learning curves\", which show task performance as a function of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task in Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions.\\n\\nFinally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on significant fractions of a word (on average  $\\\\sim 0.7$  words per token), so from the LM\\'s perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also, CL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word), requiring the model to perform some search to find the correct unscrambling. Thus, the skills involved appear to require non-trivial pattern-matching and computation.\\n\\n# 3.9.3 SAT Analogies\\n\\nTo test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 \"SAT analogy\" problems [TLBS03]. Analogies are a style of multiple choice question that constituted a section of the SAT college entrance exam before 2005. A typical example is \"audacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation\". The student is expected to choose which of the five word pairs has the same relationship as the original word pair; in this example the answer is \"sanctimonious is to hypocrisy\". On this task GPT-3 achieves  $65.2\\\\%$  in the few-shot setting,  $59.1\\\\%$  in the one-shot setting, and  $53.7\\\\%$  in the zero-shot setting, whereas the average score among college applicants was  $57\\\\%$  [TL05] (random guessing yields  $20\\\\%$ ). As shown in Figure 3.12, the results improve with scale, with the full 175 billion model improving by over  $10\\\\%$  compared to the 13 billion parameter model.\\n\\n![img-20.jpeg](img-20.jpeg)\\nFigure 3.12: Zero-, one-, and few-shot performance on SAT analogy tasks, for different sizes of model. The largest model achieves  $65\\\\%$  accuracy in the few-shot setting, and also demonstrates significant gains to in-context learning which are not present in smaller models.\\n\\n# 3.9.4 News Article Generation\\n\\nPrevious work on generative language models qualitatively tested their ability to generate synthetic \"news articles\" by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . Relative to  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , the dataset used to train GPT-3 is much less weighted towards news articles, so trying to generate news articles via raw unconditional samples is less effective – for example GPT-3 often interprets the proposed first sentence of a \"news article\" as a tweet and then posts synthetic responses or follow-up tweets. To solve this problem we employed GPT-3\\'s few-shot learning abilities by providing three previous news articles in the model\\'s context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the \"news\" genre.\\n\\nTo gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles from real ones. Similar work has been carried out by Kreps et al. [KMB20] and Zellers et al.  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$ . Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality.\\n\\nIn order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model. Participants were asked to select whether the article was \"very likely written by a human\", \"more likely written by a human\", \"I don\\'t know\", \"more likely written by a machine\", or \"very likely written by a machine\".\\n\\nThe articles we selected were not in the models\\' training data and the model outputs were formatted and selected programmatically to prevent human cherry-picking. All models used the same context to condition outputs on and were pre-trained with the same context size and the same article titles and subtitles were used as prompts for each model. However, we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a \"control model\": a 160M parameter model with no context and increased output randomness.\\n\\n|   | Mean accuracy | 95% Confidence Interval (low, hi) | t compared to control (p-value) | “I don’t know” assignments  |\\n| --- | --- | --- | --- | --- |\\n|  Control (deliberately bad model) | 86% | 83%-90% | - | 3.6 %  |\\n|  GPT-3 Small | 76% | 72%-80% | 3.9 (2e-4) | 4.9%  |\\n|  GPT-3 Medium | 61% | 58%-65% | 10.3 (7e-21) | 6.0%  |\\n|  GPT-3 Large | 68% | 64%-72% | 7.3 (3e-11) | 8.7%  |\\n|  GPT-3 XL | 62% | 59%-65% | 10.7 (1e-19) | 7.5%  |\\n|  GPT-3 2.7B | 62% | 58%-65% | 10.4 (5e-19) | 7.1%  |\\n|  GPT-3 6.7B | 60% | 56%-63% | 11.2 (3e-21) | 6.2%  |\\n|  GPT-3 13B | 55% | 52%-58% | 15.3 (1e-32) | 7.1%  |\\n|  GPT-3 175B | 52% | 49%-54% | 16.9 (1e-34) | 7.8%  |\\n\\nTable 3.11: Human accuracy in identifying whether short (~200 word) news articles are model generated. We find that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from  $86\\\\%$  on the control model to  $52\\\\%$  on GPT-3 175B. This table compares mean accuracy between five different models, and shows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model (an unconditional GPT-3 Small model with increased output randomness).\\n\\nMean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that the intentionally bad articles were model generated was  $\\\\sim 86\\\\%$  where  $50\\\\%$  is chance level performance. By contrast, mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance at  $\\\\sim 52\\\\%$  (see Table 3.11). Human abilities to detect model generated text appear to decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance. This is true despite the fact that participants spend more time on each output as model size increases (see Appendix E).\\n\\nExamples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15. Much of the text is—as indicated by the evaluations—difficult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors, the models have no access to the specific facts that the article titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are often subtle enough that they are not noticed.\\n\\nRelated work on language model detection by Ippolito et al. [IDCBE19] indicates that automatic discriminators like GROVER  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$  and GLTR [GSR19] may have greater success at detecting model generated text than human evaluators. Automatic detection of these models may be a promising area of future research.\\n\\nIppolito et al. [IDCBE19] also note that human accuracy at detecting model generated text increases as humans observe more tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial experiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to compare human abilities to detect the articles generated by GPT-3 and a control model.\\n\\nWe found that mean human accuracy at detecting the intentionally bad longer articles from the control model was  $\\\\sim 88\\\\%$ , while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at  $\\\\sim 52\\\\%$  (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles.\\n\\n# 3.9.5 Learning and Using Novel Words\\n\\nA task studied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a word in a sentence after seeing it defined only once, or conversely inferring a word\\'s meaning from only one usage. Here we qualitatively test GPT-3\\'s ability to do the former. Specifically, we give GPT-3 the definition of a nonexistent word, such as \"Gigamuru\", and then ask it to use it in a sentence. We provide one to five previous examples of a (separate)\\n\\n![img-21.jpeg](img-21.jpeg)\\nFigure 3.13: People\\'s ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately-bad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance  $(50\\\\%)$  is indicated with the dashed line at the bottom. Line of best fit is a power law with  $95\\\\%$  confidence intervals.\\n\\n|   | Mean accuracy | 95% Confidence Interval (low, hi) | t compared to control (p-value) | “I don’t know” assignments  |\\n| --- | --- | --- | --- | --- |\\n|  Control | 88% | 84%-91% | - | 2.7%  |\\n|  GPT-3 175B | 52% | 48%-57% | 12.7 (3.2e-23) | 10.6%  |\\n\\nTable 3.12: People\\'s ability to identify whether  $\\\\sim 500$  word articles are model generated (as measured by the ratio of correct assignments to non-neutral assignments) was  $88\\\\%$  on the control model and  $52\\\\%$  on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with increased output randomness).\\n\\nTitle: United Methodists Agree to Historic Split\\nSubtitle: Those who oppose gay marriage will form their own denomination\\nArticle: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church\\'s annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination.\\nThe Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the \"largest Protestant denomination in the U.S.,\" but that it has been shrinking in recent decades. The new split will be the second in the church\\'s history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church. The Post notes that the proposed split \"comes at a critical time for the church, which has been losing members for years,\" which has been \"pushed toward the brink of a schism over the role of LGBTQ people in the church.\" Gay marriage is not the only issue that has divided the church. In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban them from serving as clergy, and the South Pacific regional conference voting to allow them.\\n\\nFigure 3.14: The GPT-3 generated news article that humans had the greatest difficulty distinguishing from a human written article (accuracy:  $12\\\\%$ ).\\n\\nTitle: Star\\'s Tux Promise Draws Megyn Kelly\\'s Sarcasm\\nSubtitle: Joaquin Phoenix pledged to not change for each awards event\\nArticle: A year ago, Joaquin Phoenix made headlines when he appeared on the red carpet at the Golden Globes wearing a tuxedo with a paper bag over his head that read, \"I am a shape-shifter. I can\\'t change the world. I can only change myself.\" It was a promise to not change to fit into the Hollywood mold: \"I think that\\'s a really special thing, to not change yourself. I think it\\'s a really special thing to say, \\'This is what\\'s inside of me, I\\'m proud of it, and I\\'m not going to be ashamed because of the way that someone else thinks I should be.\\'\" Now, it\\'s the Oscars, and Phoenix is at it again. But this time, his publicist is saying he\\'ll be wearing a tux no matter what.\\nMegyn Kelly was not impressed, and she let him have it on The Tonight Show. \"You know, I feel like, I feel like you could have worn the tux,\" she says. \"But you\\'re saying you\\'re a shape-shifter. I don\\'t know if you can change your tux, but you can change your mind. You can change your mind. You can change your mind.\" Phoenix says he did, but it didn\\'t stick. \"I was like, \\'Okay, I\\'m going to wear a tuxedo to this thing.\\' And then I thought, \\'I don\\'t want to wear a tuxedo to this thing.\\'\" Kelly goes on to encourage him to change his mind again, but Phoenix says it\\'s too late: \"I\\'m committed to wearing this.\"\\n\\nFigure 3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written article (accuracy:  $61\\\\%$ ).\\n\\n![img-22.jpeg](img-22.jpeg)\\nFigure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is GPT-3\\'s completions, plain text is human prompts. In the first example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 other than the conditioning shown here.\\n\\nnonexistent word being defined and used in a sentence, so the task is few-shot in terms of previous examples of the broad task and one-shot in terms of the specific word. Table 3.16 shows the 6 examples we generated; all definitions were human-generated, and the first answer was human-generated as conditioning while the subsequent answers were generated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the final sentence the model generates a plausible conjugation for the word \"screeg\" (namely \"screeghed\"), although the use of the word is slightly awkward (\"screeghed at each other\") despite being plausible in the sense that it could describe a toy sword fight. Overall, GPT-3 appears to be at least proficient at the task of using novel words in a sentence.\\n\\n# 3.9.6 Correcting English Grammar\\n\\nAnother task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the few-shot setting by giving prompts of the form \"Poor English Input: <sentence>\\\\n Good English Output: <sentence>\". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats). Results are shown in Figure 3.17.\\n\\n# 4 Measuring and Preventing Memorization Of Benchmarks\\n\\nSince our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to.\\n\\nThis concern is not just hypothetical. One of the first papers to train a language model on Common Crawl data [TL18] detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  also conducted post-hoc overlap analysis. Their study was relatively encouraging, finding that</sentence></sentence>\\n\\n![img-23.jpeg](img-23.jpeg)\\nFigure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface is GPT-3\\'s completions, plain text is human prompts. In the first few examples example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 aside from the first few examples as conditioning and the \"Poor English input/Good English output\" framing. We note that the distinction between \"poor\" and \"good\" English (and the terms themselves) is complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what \"good\" is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word \"cheap\" in a way that alters meaning).\\n\\n![img-24.jpeg](img-24.jpeg)\\nFigure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difficulty rather than overfitting.\\n\\nalthough models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).\\n\\nGPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared.\\n\\nWe initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn\\'t feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results.\\n\\nFor each benchmark, we produce a \\'clean\\' version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix C.\\n\\nWe then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over  $50\\\\%$ ), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance.\\n\\nBelow, we review in more detail the few specific cases where either (1) the model performs significantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difficult.\\n\\nOur analysis flagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English\\n\\n![img-25.jpeg](img-25.jpeg)\\nFigure 4.2: Benchmark contamination analysis We constructed cleaned versions of each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high confidence to be clean, and the y-axis shows the difference in performance when evaluating only on the verified clean subset. Performance on most benchmarks changed negligibly, but some were flagged for further review. On inspection we find some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section 3 with an asterisk. We find no evidence that other benchmarks are affected.\\n\\ntranslation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false positives. We summarize the results for each group of tasks below:\\n\\n- Reading Comprehension: Our initial analysis flagged  $&gt;90\\\\%$  of task examples from QuAC, SQuAD2, and DROP as potentially contaminated, so large that even measuring the differential on a clean subset was difficult. Upon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source text was present in our training data but the question/answer pairs were not, meaning the model gains only background information and cannot memorize the answer to a specific question.\\n- German translation: We found  $25\\\\%$  of the examples in the WMT16 German-English test set were marked as potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the flagged examples contain paired sentences resembling NMT training data and collisions were monolingual matches mostly of snippets of events discussed in the news.\\n- Reversed Words and Anagrams: Recall that these tasks are of the form \"alaok = koala\". Due to the short length of these tasks, we used 2-grams for filtering (ignoring punctuation). After inspecting the flagged overlaps, we found that they were not typically instances of real reversals or unscramblings in the training set, but rather palindromes or trivial unscramblings, e.g. \"kayak = kayak\". The amount of overlap was small, but removing the trivial tasks lead to an increase in difficulty and thus a spurious signal. Related to this, the symbol insertion task shows high overlap but no effect on performance – this is because that task involves removing non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to many spurious matches.\\n- PIQA: The overlap analysis flagged  $29\\\\%$  of examples as contaminated, and observed a 3 percentage point absolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was released after our training set was created and its labels are hidden, some of the web pages used by the crowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller model with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias rather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot rigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential contamination.\\n- Winograd: The overlap analysis flagged  $45\\\\%$  of examples, and found a  $2.6\\\\%$  decrease in performance on the clean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in fact present in our training set, though presented in a different format than we present the task to the model. Although the decrease in performance is small, we mark our Winograd results in the main paper with an asterisk.\\n\\n- Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the Children’s Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark.\\n\\nWe also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our fill-in-the-blank format precludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section.\\n\\nAn important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset. It remains possible that memorization inflates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing.\\n\\nOverall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the field in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C.\\n\\n## 5 Limitations\\n\\nGPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work.\\n\\nFirst, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3’s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with “common sense physics”, despite doing well on some datasets (such as PIQA *[BZB^{+}19]*) that test this domain. Specifically GPT-3 has difficulty with questions of the type “If I put cheese into the fridge, will it melt?”. Quantitatively, GPT-3’s in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some “comparison” tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3’s strong few-shot performance on many other tasks.\\n\\nGPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models *[RSR^{+}19]*. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3’s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the “best of both worlds”.\\n\\nA more fundamental limitation of the general approach described in this paper – scaling up any LM-like model, whether autoregressive or bidirectional – is that it may eventually run into (or could already be running into) the limits of the\\n\\npretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. *[x20]* demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world *[BHT^{+}20]*. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans *[ZSW^{+}19a]*, fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world *[CLY^{+}19]*.\\n\\nAnother limitation broadly shared by language models is poor sample efficiency during pre-training. While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime *[x14]*. Improving pre-training sample efficiency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements.\\n\\nA limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identifies tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a specific style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or defining nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research.\\n\\nA limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation *[x13]* of large models down to a manageable size for specific tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a specific task, suggesting that in principle aggressive distillation may be possible. Distillation is well-explored in general *[x15]* but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size.\\n\\nFinally, GPT-3 shares some limitations common to most deep learning systems – its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This last issue – biases in the data that may lead the model to generate stereotyped or prejudiced content – is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section 6).\\n\\n## 6 Broader Impacts\\n\\nLanguage models have a wide range of beneficial applications for society, including code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. But they also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difficulty of distinguishing synthetic text from human-written text. It therefore has the potential to advance both the beneficial and harmful applications of language models.\\n\\nHere we focus on the potential harms of improved language models, not because we believe the harms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also briefly discuss issues of energy efficiency (Section 6.3).\\n\\n6.1 Misuse of Language Models\\n\\nMalicious uses of language models can be somewhat difficult to anticipate because they often involve repurposing language models in a very different environment or for a different purpose than researchers intended. To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact *[x20]*. We discuss three factors: potential misuse applications, threat actors, and external incentive structures.\\n\\n#### 6.1.1 Potential Misuse Applications\\n\\nAny socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufficiently high quality text. Language models that produce high quality text generation could lower existing barriers to carrying out these activities and increase their efficacy.\\n\\nThe misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to generate several paragraphs of synthetic content that people find difficult to distinguish from human-written text in 3.9.4 represents a concerning milestone in this regard.\\n\\n#### 6.1.2 Threat Actor Analysis\\n\\nThreat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors who may be able to build a malicious product to ‘advanced persistent threats’ (APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas *[SBC+19]*.\\n\\nTo understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did find significant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate, but significant improvements in reliability could change this.\\n\\nBecause APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about possible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible difference in operations that may see potential gains by using language models. The assessment was that language models may not be worth investing significant resources in because there has been no convincing demonstration that current language models are significantly better than current methods for generating text, and because methods for “targeting” or “controlling” the content of language models are still at a very early stage.\\n\\n#### 6.1.3 External Incentive Structures\\n\\nEach threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their agenda. TTPs are influenced by economic factors like scalability and ease of deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login credentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment.\\n\\nEase of use is another significant incentive. Having stable infrastructure has a large impact on the adoption of TTPs. The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot produces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor required in operating this bot. But a human is still needed to filter the outputs, which restricts how scalable the operation can be.\\n\\nBased on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will eventually develop language models that are sufficiently consistent and steerable that they will be of greater interest to malicious actors. We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers.\\n\\n####\\n\\n6.2 Fairness, Bias, and Representation\\n\\nBiases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms *[x10]*. We have conducted an analysis of biases in the model in order to better understand GPT-3’s limitations when it comes to fairness, bias, and representation.\\n\\nOur goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reflect all of the model’s biases even within the studied categories.\\n\\nBroadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data. Below we discuss our preliminary findings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension.\\n\\n#### 6.2.1 Gender\\n\\nIn our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3. We measured this by feeding the model a context such as \"The detective was a\" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identifiers include midwife, nurse, receptionist, housekeeper etc.\\n\\nWe also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation} was a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent {occupation} was a,\" the majority of occupations had an even higher probability of being followed by a male identifier than a female one than was the case with our original neutral prompt, \"The {occupation} was a\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as $\\\\frac{1}{n_{\\\\text{jobs}}}\\\\sum_{\\\\text{jobs}}\\\\log(\\\\frac{P(\\\\text{female}|\\\\text{Context})}{P(\\\\text{male}|\\\\text{Context})})$ - was $-1.11$ for the Neutral Variant, $-2.14$ for the Competent Variant and $-1.15$ for the Incompetent Variant.\\n\\nWe also carried out pronoun resolution on the Winogender dataset *[x23]* using two methods which further corroborated the model’s tendency to associate most occupations with males. One method measured the models ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \"The advisor met with the advisee because she wanted to get advice about job applications. ’She’ refers to the\" and found the option with the lowest probability between the two possible options (Choices between Occupation Option: advisor; Participant Option: advisee).\\n\\nOccupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models.\\n\\nWe also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-selected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature\\n\\nTable 6.1: Most Biased Descriptive Words in 175B Model\\n\\n|  Top 10 Most Biased Male Descriptive Words with Raw Co-Occurrence Counts | Top 10 Most Biased Female Descriptive Words with Raw Co-Occurrence Counts  |\\n| --- | --- |\\n|  Average Number of Co-Occurrences Across All Words: 17.5 | Average Number of Co-Occurrences Across All Words: 23.9  |\\n|  Large (16) | Optimistic (12)  |\\n|  Mostly (15) | Bubbly (12)  |\\n|  Lazy (14) | Naughty (12)  |\\n|  Fantastic (13) | Easy-going (12)  |\\n|  Eccentric (13) | Petite (10)  |\\n|  Protect (10) | Tight (10)  |\\n|  Jolly (10) | Pregnant (10)  |\\n|  Stable (9) | Gorgeous (28)  |\\n|  Personable (22) | Sucked (8)  |\\n|  Survive (7) | Beautiful (158)  |\\n\\nof 1 and top_p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\", \"She was very\", \"He would be described as\", \"She would be described as\". We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more often described using appearance oriented words such as \"beautiful\" and \"gorgeous\" as compared to men who were more often described using adjectives that span a greater spectrum.\\n\\nTable 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. \"Most Favored\" here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender.\\n\\n# 6.2.2 Race\\n\\nTo investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\", \"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation  $\\\\left[\\\\mathrm{HZJ}^{+}19\\\\right]$ , we explored how race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5, horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).\\n\\nIt should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology.\\n\\nAcross the models we analyzed, \\'Asian\\' had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, \\'Black\\' had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data.\\n\\n![img-26.jpeg](img-26.jpeg)\\nFigure 6.1: Racial Sentiment Across Models\\n\\n|  Religion | Most Favored Descriptive Words  |\\n| --- | --- |\\n|  Atheism | ‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’, ‘Characterized’  |\\n|  Buddhism | ‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘En- lightenment’, ‘Non-Violent’  |\\n|  Christianity | ‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Com- ments’, ‘Officially’  |\\n|  Hinduism | ‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated’, ‘Africa’  |\\n|  Islam | ‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’, ‘Levant’, ‘Allah’, ‘Prophet’  |\\n|  Judaism | ‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’, ‘Game’, ‘Russian’  |\\n\\nTable 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model.\\n\\n# 6.2.3 Religion\\n\\nWe studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length  $\\\\approx 50$  with a temperature of 1 and a top  $p$  of 0.9 for every prompt. Our prompts were of the nature \" {Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words.\\n\\nThe following is an example output from the model:\\n\\n\"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic.\"'), -0.14001501679940564), (Document(id='ac66f9901c89:0', metadata={'end_line': 385, 'start_line': 1, 'chunk_index': 0, 'text': '# Attention Is All You Need\\n\\nAshish Vaswani*\\n\\nGoogle Brain\\n\\navaswani@google.com\\n\\nNoam Shazeer*\\n\\nGoogle Brain\\n\\nnoam@google.com\\n\\nNiki Parmar*\\n\\nGoogle Research\\n\\nnikip@google.com\\n\\nJakob Uszkoreit*\\n\\nGoogle Research\\n\\nusz@google.com\\n\\nLlion Jones*\\n\\nGoogle Research\\n\\nllion@google.com\\n\\nAidan N. Gomez*†\\n\\nUniversity of Toronto\\n\\naidan@cs.toronto.edu\\n\\nŁukasz Kaiser*\\n\\nGoogle Brain\\n\\nlukaszkaiser@google.com\\n\\nIllia Polosukhin*‡\\n\\nillia.polosukhin@gmail.com\\n\\n# Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n1 Introduction\\n\\nRecurrent neural networks, long short-term memory *[13]* and gated recurrent *[7]* neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation *[35, 2, 5]*. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures *[38, 24, 15]*.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_{t}$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks *[21]* and conditional computation *[32]*, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences *[2, 19]*. In all but a few cases *[27]*, however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n## 2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU *[16]*, ByteNet *[18]* and ConvS2S *[9]*, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions *[12]*. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations *[4, 27, 28, 22]*.\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks *[34]*.\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as *[17, 18]* and *[9]*.\\n\\n## 3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure *[5, 2, 35]*. Here, the encoder maps an input sequence of symbol representations $(x_{1},...,x_{n})$ to a sequence of continuous representations $\\\\mathbf{z}=(z_{1},...,z_{n})$. Given $\\\\mathbf{z}$, the decoder then generates an output sequence $(y_{1},...,y_{m})$ of symbols one element at a time. At each step the model is auto-regressive *[10]*, consuming the previously generated symbols as additional input when generating the next.\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n# 3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of  $N = 6$  identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm  $(x + \\\\text{Sublayer}(x))$ , where Sublayer  $(x)$  is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension  $d_{\\\\text{model}} = 512$ .\\n\\nDecoder: The decoder is also composed of a stack of  $N = 6$  identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position  $i$  can depend only on the known outputs at positions less than  $i$ .\\n\\n# 3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n![img-1.jpeg](img-1.jpeg)\\nScaled Dot-Product Attention\\n\\n![img-2.jpeg](img-2.jpeg)\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n# 3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension  $d_{k}$ , and values of dimension  $d_{v}$ . We compute the dot products of the query with all keys, divide each by  $\\\\sqrt{d_k}$ , and apply a softmax function to obtain the weights on the values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix  $Q$ . The keys and values are also packed together into matrices  $K$  and  $V$ . We compute the matrix of outputs as:\\n\\n$$\\n\\\\operatorname {A t t e n t i o n} (Q, K, V) = \\\\operatorname {s o f t m a x} \\\\left(\\\\frac {Q K ^ {T}}{\\\\sqrt {d _ {k}}}\\\\right) V \\\\tag {1}\\n$$\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of  $\\\\frac{1}{\\\\sqrt{d_k}}$ . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of  $d_{k}$  the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of  $d_{k}$  [3]. We suspect that for large values of  $d_{k}$ , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by  $\\\\frac{1}{\\\\sqrt{d_k}}$ .\\n\\n# 3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with  $d_{\\\\mathrm{model}}$ -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values  $h$  times with different, learned linear projections to  $d_k$ ,  $d_k$  and  $d_v$  dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding  $d_v$ -dimensional\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\n$\\\\mathrm{MultiHead}(Q,K,V)$ $=\\\\mathrm{Concat}(\\\\mathrm{head}_{1},...,\\\\mathrm{head}_{\\\\mathrm{h}})W^{O}$\\n$\\\\mathrm{where\\\\ head_{i}}$ $=\\\\mathrm{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$\\n\\nWhere the projections are parameter matrices $W_{i}^{Q}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{K}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{V}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{v}}$ and $W^{O}\\\\in\\\\mathbb{R}^{hd_{v}\\\\times d_{\\\\mathrm{model}}}$.\\n\\nIn this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_{k}=d_{v}=d_{\\\\mathrm{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n#### 3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as *[38, 2, 9]*.\\n- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $-\\\\infty$) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n### 3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\n$\\\\mathrm{FFN}(x)=\\\\max(0,xW_{1}+b_{1})W_{2}+b_{2}$ (2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{\\\\mathrm{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$.\\n\\n### 3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\\\mathrm{model}}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to *[30]*. In the embedding layers, we multiply those weights by $\\\\sqrt{d_{\\\\mathrm{model}}}$.\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.  $n$  is the sequence length,  $d$  is the representation dimension,  $k$  is the kernel size of convolutions and  $r$  the size of the neighborhood in restricted self-attention.\\n\\n|  Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length  |\\n| --- | --- | --- | --- |\\n|  Self-Attention | O(n2·d) | O(1) | O(1)  |\\n|  Recurrent | O(n·d2) | O(n) | O(n)  |\\n|  Convolutional | O(k·n·d2) | O(1) | O(logk(n))  |\\n|  Self-Attention (restricted) | O(r·n·d) | O(1) | O(n/r)  |\\n\\n# 3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension  $d_{\\\\mathrm{model}}$  as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\n$$\\nP E _ {(p o s, 2 i)} = \\\\sin (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\n$$\\nP E _ {(p o s, 2 i + 1)} = \\\\cos (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\nwhere  $pos$  is the position and  $i$  is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from  $2\\\\pi$  to  $10000 \\\\cdot 2\\\\pi$ . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset  $k$ ,  $PE_{pos+k}$  can be represented as a linear function of  $PE_{pos}$ .\\n\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n# 4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations  $(x_{1},\\\\dots,x_{n})$  to another sequence of equal length  $(z_{1},\\\\dots,z_{n})$ , with  $x_{i},z_{i}\\\\in \\\\mathbb{R}^{d}$ , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires  $O(n)$  sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\\n\\nlength $n$ is smaller than the representation dimensionality $d$, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece *[38]* and byte-pair *[31]* representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $r$ in the input sequence centered around the respective output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width $k<n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_{k}(n))$ in the case of dilated convolutions *[18]*, increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of $k$. Separable convolutions *[6]*, however, decrease the complexity considerably, to $O(k\\\\cdot n\\\\cdot d+n\\\\cdot d^{2})$. Even with $k=n$, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n## 5 Training\\n\\nThis section describes the training regime for our models.\\n\\n### 5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding *[3]*, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary *[38]*. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n### 5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n### 5.3 Optimizer\\n\\nWe used the Adam optimizer *[20]* with $\\\\beta_{1}=0.9$, $\\\\beta_{2}=0.98$ and $\\\\epsilon=10^{-9}$. We varied the learning rate over the course of training, according to the formula:\\n\\n$lrate=d_{\\\\text{model}}^{-0.5}\\\\cdot\\\\min(step\\\\_num^{-0.5},step\\\\_num\\\\cdot warmup\\\\_steps^{-1.5})$ (3)\\n\\nThis corresponds to increasing the learning rate linearly for the first $warmup\\\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used $warmup\\\\_steps=4000$.\\n\\n### 5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\n|  Model | BLEU |   | Training Cost (FLOPs)  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  EN-DE | EN-FR | EN-DE | EN-FR  |\\n|  ByteNet [18] | 23.75 |  |  |   |\\n|  Deep-Att + PosUnk [39] |  | 39.2 |  | 1.0 · 1020  |\\n|  GNMT + RL [38] | 24.6 | 39.92 | 2.3 · 1019 | 1.4 · 1020  |\\n|  ConvS2S [9] | 25.16 | 40.46 | 9.6 · 1018 | 1.5 · 1020  |\\n|  MoE [32] | 26.03 | 40.56 | 2.0 · 1019 | 1.2 · 1020  |\\n|  Deep-Att + PosUnk Ensemble [39] |  | 40.4 |  | 8.0 · 1020  |\\n|  GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 · 1020 | 1.1 · 1021  |\\n|  ConvS2S Ensemble [9] | 26.36 | 41.29 | 7.7 · 1019 | 1.2 · 1021  |\\n|  Transformer (base model) | 27.3 | 38.1 | 3.3 · 1018  |   |\\n|  Transformer (big) | 28.4 | 41.8 | 2.3 · 1019  |   |\\n\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of  $P_{drop} = 0.1$ .\\n\\nLabel Smoothing During training, we employed label smoothing of value  $\\\\epsilon_{ls} = 0.1$  [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n# 6 Results\\n\\n# 6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than  $1/4$  the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate  $P_{drop} = 0.1$ , instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty  $\\\\alpha = 0.6$  [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length  $+50$ , but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each  $\\\\mathrm{GPU}^5$ .\\n\\n# 6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\n|   | N | dmodel | dff | h | dk | dv | Pdrop | εls | train steps | PPL (dev) | BLEU (dev) | params ×106  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65  |\\n|  (A) |  |  |  | 1 | 512 | 512 |  |  |  | 5.29 | 24.9 |   |\\n|   |   |  |  | 4 | 128 | 128 |  |  |  | 5.00 | 25.5 |   |\\n|   |   |  |  | 16 | 32 | 32 |  |  |  | 4.91 | 25.8 |   |\\n|   |   |  |  | 32 | 16 | 16 |  |  |  | 5.01 | 25.4 |   |\\n|  (B) |  |  |  |  | 16 |  |  |  |  | 5.16 | 25.1 | 58  |\\n|   |   |  |  |  | 32 |  |  |  |  | 5.01 | 25.4 | 60  |\\n|  (C) | 2 |  |  |  |  |  |  |  |  | 6.11 | 23.7 | 36  |\\n|   |  4 |  |  |  |  |  |  |  |  | 5.19 | 25.3 | 50  |\\n|   |  8 |  |  |  |  |  |  |  |  | 4.88 | 25.5 | 80  |\\n|   |   | 256 |  |  | 32 | 32 |  |  |  | 5.75 | 24.5 | 28  |\\n|   |   | 1024 |  |  | 128 | 128 |  |  |  | 4.66 | 26.0 | 168  |\\n|   |   |  | 1024 |  |  |  |  |  |  | 5.12 | 25.4 | 53  |\\n|   |   |  | 4096 |  |  |  |  |  |  | 4.75 | 26.2 | 90  |\\n|  (D) |  |  |  |  |  |  | 0.0 |  |  | 5.77 | 24.6 |   |\\n|   |   |  |  |  |  |  | 0.2 |  |  | 4.95 | 25.5 |   |\\n|   |   |  |  |  |  |  |  | 0.0 |  | 4.67 | 25.3 |   |\\n|   |   |  |  |  |  |  |  | 0.2 |  | 5.47 | 25.7 |   |\\n|  (E) | positional embedding instead of sinusoids |   |   |   |   |   |   |   |   | 4.92 | 25.7 |   |\\n|  big | 6 | 1024 | 4096 | 16 |  |  | 0.3 |  | 300K | 4.33 | 26.4 | 213  |\\n\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\\n\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size  $d_{k}$  hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\\n\\n# 6.3 English Constituency Parsing\\n\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\\n\\nWe trained a 4-layer transformer with  $d_{model} = 1024$  on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\\n\\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\\n\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\\n\\n|  Parser | Training | WSJ 23 F1  |\\n| --- | --- | --- |\\n|  Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3  |\\n|  Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4  |\\n|  Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4  |\\n|  Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7  |\\n|  Transformer (4 layers) | WSJ only, discriminative | 91.3  |\\n|  Zhu et al. (2013) [40] | semi-supervised | 91.3  |\\n|  Huang & Harper (2009) [14] | semi-supervised | 91.3  |\\n|  McClosky et al. (2006) [26] | semi-supervised | 92.1  |\\n|  Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1  |\\n|  Transformer (4 layers) | semi-supervised | 92.7  |\\n|  Luong et al. (2015) [23] | multi-task | 93.0  |\\n|  Dyer et al. (2016) [8] | generative | 93.3  |\\n\\nincreased the maximum output length to input length  $+300$ . We used a beam size of 21 and  $\\\\alpha = 0.3$  for both WSJ only and the semi-supervised setting.\\n\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\\n\\n# 7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\n# References\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\\n- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\\n- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n- [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\n- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\\n- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\\n- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\\n- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\\n- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n- [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n- [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\\n- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n- [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\\n- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\\n- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\\n- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\\n- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n- [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n\\n# Attention Visualizations\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \\'making\\', completing the phrase \\'making...more difficult\\'. Attentions here shown only for the word \\'making\\'. Different colors represent different heads. Best viewed in color.\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \\'its\\' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n![img-5.jpeg](img-5.jpeg)\\n\\n![img-6.jpeg](img-6.jpeg)\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.', 'doc_id': 'ac66f9901c89'}, page_content='# Attention Is All You Need\\n\\nAshish Vaswani*\\n\\nGoogle Brain\\n\\navaswani@google.com\\n\\nNoam Shazeer*\\n\\nGoogle Brain\\n\\nnoam@google.com\\n\\nNiki Parmar*\\n\\nGoogle Research\\n\\nnikip@google.com\\n\\nJakob Uszkoreit*\\n\\nGoogle Research\\n\\nusz@google.com\\n\\nLlion Jones*\\n\\nGoogle Research\\n\\nllion@google.com\\n\\nAidan N. Gomez*†\\n\\nUniversity of Toronto\\n\\naidan@cs.toronto.edu\\n\\nŁukasz Kaiser*\\n\\nGoogle Brain\\n\\nlukaszkaiser@google.com\\n\\nIllia Polosukhin*‡\\n\\nillia.polosukhin@gmail.com\\n\\n# Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n1 Introduction\\n\\nRecurrent neural networks, long short-term memory *[13]* and gated recurrent *[7]* neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation *[35, 2, 5]*. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures *[38, 24, 15]*.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_{t}$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks *[21]* and conditional computation *[32]*, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences *[2, 19]*. In all but a few cases *[27]*, however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n## 2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU *[16]*, ByteNet *[18]* and ConvS2S *[9]*, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions *[12]*. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations *[4, 27, 28, 22]*.\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks *[34]*.\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as *[17, 18]* and *[9]*.\\n\\n## 3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure *[5, 2, 35]*. Here, the encoder maps an input sequence of symbol representations $(x_{1},...,x_{n})$ to a sequence of continuous representations $\\\\mathbf{z}=(z_{1},...,z_{n})$. Given $\\\\mathbf{z}$, the decoder then generates an output sequence $(y_{1},...,y_{m})$ of symbols one element at a time. At each step the model is auto-regressive *[10]*, consuming the previously generated symbols as additional input when generating the next.\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n# 3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of  $N = 6$  identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm  $(x + \\\\text{Sublayer}(x))$ , where Sublayer  $(x)$  is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension  $d_{\\\\text{model}} = 512$ .\\n\\nDecoder: The decoder is also composed of a stack of  $N = 6$  identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position  $i$  can depend only on the known outputs at positions less than  $i$ .\\n\\n# 3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n![img-1.jpeg](img-1.jpeg)\\nScaled Dot-Product Attention\\n\\n![img-2.jpeg](img-2.jpeg)\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n# 3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension  $d_{k}$ , and values of dimension  $d_{v}$ . We compute the dot products of the query with all keys, divide each by  $\\\\sqrt{d_k}$ , and apply a softmax function to obtain the weights on the values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix  $Q$ . The keys and values are also packed together into matrices  $K$  and  $V$ . We compute the matrix of outputs as:\\n\\n$$\\n\\\\operatorname {A t t e n t i o n} (Q, K, V) = \\\\operatorname {s o f t m a x} \\\\left(\\\\frac {Q K ^ {T}}{\\\\sqrt {d _ {k}}}\\\\right) V \\\\tag {1}\\n$$\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of  $\\\\frac{1}{\\\\sqrt{d_k}}$ . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of  $d_{k}$  the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of  $d_{k}$  [3]. We suspect that for large values of  $d_{k}$ , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by  $\\\\frac{1}{\\\\sqrt{d_k}}$ .\\n\\n# 3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with  $d_{\\\\mathrm{model}}$ -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values  $h$  times with different, learned linear projections to  $d_k$ ,  $d_k$  and  $d_v$  dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding  $d_v$ -dimensional\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\n$\\\\mathrm{MultiHead}(Q,K,V)$ $=\\\\mathrm{Concat}(\\\\mathrm{head}_{1},...,\\\\mathrm{head}_{\\\\mathrm{h}})W^{O}$\\n$\\\\mathrm{where\\\\ head_{i}}$ $=\\\\mathrm{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$\\n\\nWhere the projections are parameter matrices $W_{i}^{Q}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{K}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{V}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{v}}$ and $W^{O}\\\\in\\\\mathbb{R}^{hd_{v}\\\\times d_{\\\\mathrm{model}}}$.\\n\\nIn this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_{k}=d_{v}=d_{\\\\mathrm{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n#### 3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as *[38, 2, 9]*.\\n- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $-\\\\infty$) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n### 3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\n$\\\\mathrm{FFN}(x)=\\\\max(0,xW_{1}+b_{1})W_{2}+b_{2}$ (2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{\\\\mathrm{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$.\\n\\n### 3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\\\mathrm{model}}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to *[30]*. In the embedding layers, we multiply those weights by $\\\\sqrt{d_{\\\\mathrm{model}}}$.\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.  $n$  is the sequence length,  $d$  is the representation dimension,  $k$  is the kernel size of convolutions and  $r$  the size of the neighborhood in restricted self-attention.\\n\\n|  Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length  |\\n| --- | --- | --- | --- |\\n|  Self-Attention | O(n2·d) | O(1) | O(1)  |\\n|  Recurrent | O(n·d2) | O(n) | O(n)  |\\n|  Convolutional | O(k·n·d2) | O(1) | O(logk(n))  |\\n|  Self-Attention (restricted) | O(r·n·d) | O(1) | O(n/r)  |\\n\\n# 3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension  $d_{\\\\mathrm{model}}$  as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\n$$\\nP E _ {(p o s, 2 i)} = \\\\sin (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\n$$\\nP E _ {(p o s, 2 i + 1)} = \\\\cos (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\nwhere  $pos$  is the position and  $i$  is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from  $2\\\\pi$  to  $10000 \\\\cdot 2\\\\pi$ . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset  $k$ ,  $PE_{pos+k}$  can be represented as a linear function of  $PE_{pos}$ .\\n\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n# 4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations  $(x_{1},\\\\dots,x_{n})$  to another sequence of equal length  $(z_{1},\\\\dots,z_{n})$ , with  $x_{i},z_{i}\\\\in \\\\mathbb{R}^{d}$ , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires  $O(n)$  sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\\n\\nlength $n$ is smaller than the representation dimensionality $d$, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece *[38]* and byte-pair *[31]* representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $r$ in the input sequence centered around the respective output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width $k<n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_{k}(n))$ in the case of dilated convolutions *[18]*, increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of $k$. Separable convolutions *[6]*, however, decrease the complexity considerably, to $O(k\\\\cdot n\\\\cdot d+n\\\\cdot d^{2})$. Even with $k=n$, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n## 5 Training\\n\\nThis section describes the training regime for our models.\\n\\n### 5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding *[3]*, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary *[38]*. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n### 5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n### 5.3 Optimizer\\n\\nWe used the Adam optimizer *[20]* with $\\\\beta_{1}=0.9$, $\\\\beta_{2}=0.98$ and $\\\\epsilon=10^{-9}$. We varied the learning rate over the course of training, according to the formula:\\n\\n$lrate=d_{\\\\text{model}}^{-0.5}\\\\cdot\\\\min(step\\\\_num^{-0.5},step\\\\_num\\\\cdot warmup\\\\_steps^{-1.5})$ (3)\\n\\nThis corresponds to increasing the learning rate linearly for the first $warmup\\\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used $warmup\\\\_steps=4000$.\\n\\n### 5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\n|  Model | BLEU |   | Training Cost (FLOPs)  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  EN-DE | EN-FR | EN-DE | EN-FR  |\\n|  ByteNet [18] | 23.75 |  |  |   |\\n|  Deep-Att + PosUnk [39] |  | 39.2 |  | 1.0 · 1020  |\\n|  GNMT + RL [38] | 24.6 | 39.92 | 2.3 · 1019 | 1.4 · 1020  |\\n|  ConvS2S [9] | 25.16 | 40.46 | 9.6 · 1018 | 1.5 · 1020  |\\n|  MoE [32] | 26.03 | 40.56 | 2.0 · 1019 | 1.2 · 1020  |\\n|  Deep-Att + PosUnk Ensemble [39] |  | 40.4 |  | 8.0 · 1020  |\\n|  GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 · 1020 | 1.1 · 1021  |\\n|  ConvS2S Ensemble [9] | 26.36 | 41.29 | 7.7 · 1019 | 1.2 · 1021  |\\n|  Transformer (base model) | 27.3 | 38.1 | 3.3 · 1018  |   |\\n|  Transformer (big) | 28.4 | 41.8 | 2.3 · 1019  |   |\\n\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of  $P_{drop} = 0.1$ .\\n\\nLabel Smoothing During training, we employed label smoothing of value  $\\\\epsilon_{ls} = 0.1$  [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n# 6 Results\\n\\n# 6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than  $1/4$  the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate  $P_{drop} = 0.1$ , instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty  $\\\\alpha = 0.6$  [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length  $+50$ , but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each  $\\\\mathrm{GPU}^5$ .\\n\\n# 6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\n|   | N | dmodel | dff | h | dk | dv | Pdrop | εls | train steps | PPL (dev) | BLEU (dev) | params ×106  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65  |\\n|  (A) |  |  |  | 1 | 512 | 512 |  |  |  | 5.29 | 24.9 |   |\\n|   |   |  |  | 4 | 128 | 128 |  |  |  | 5.00 | 25.5 |   |\\n|   |   |  |  | 16 | 32 | 32 |  |  |  | 4.91 | 25.8 |   |\\n|   |   |  |  | 32 | 16 | 16 |  |  |  | 5.01 | 25.4 |   |\\n|  (B) |  |  |  |  | 16 |  |  |  |  | 5.16 | 25.1 | 58  |\\n|   |   |  |  |  | 32 |  |  |  |  | 5.01 | 25.4 | 60  |\\n|  (C) | 2 |  |  |  |  |  |  |  |  | 6.11 | 23.7 | 36  |\\n|   |  4 |  |  |  |  |  |  |  |  | 5.19 | 25.3 | 50  |\\n|   |  8 |  |  |  |  |  |  |  |  | 4.88 | 25.5 | 80  |\\n|   |   | 256 |  |  | 32 | 32 |  |  |  | 5.75 | 24.5 | 28  |\\n|   |   | 1024 |  |  | 128 | 128 |  |  |  | 4.66 | 26.0 | 168  |\\n|   |   |  | 1024 |  |  |  |  |  |  | 5.12 | 25.4 | 53  |\\n|   |   |  | 4096 |  |  |  |  |  |  | 4.75 | 26.2 | 90  |\\n|  (D) |  |  |  |  |  |  | 0.0 |  |  | 5.77 | 24.6 |   |\\n|   |   |  |  |  |  |  | 0.2 |  |  | 4.95 | 25.5 |   |\\n|   |   |  |  |  |  |  |  | 0.0 |  | 4.67 | 25.3 |   |\\n|   |   |  |  |  |  |  |  | 0.2 |  | 5.47 | 25.7 |   |\\n|  (E) | positional embedding instead of sinusoids |   |   |   |   |   |   |   |   | 4.92 | 25.7 |   |\\n|  big | 6 | 1024 | 4096 | 16 |  |  | 0.3 |  | 300K | 4.33 | 26.4 | 213  |\\n\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\\n\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size  $d_{k}$  hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\\n\\n# 6.3 English Constituency Parsing\\n\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\\n\\nWe trained a 4-layer transformer with  $d_{model} = 1024$  on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\\n\\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\\n\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\\n\\n|  Parser | Training | WSJ 23 F1  |\\n| --- | --- | --- |\\n|  Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3  |\\n|  Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4  |\\n|  Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4  |\\n|  Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7  |\\n|  Transformer (4 layers) | WSJ only, discriminative | 91.3  |\\n|  Zhu et al. (2013) [40] | semi-supervised | 91.3  |\\n|  Huang & Harper (2009) [14] | semi-supervised | 91.3  |\\n|  McClosky et al. (2006) [26] | semi-supervised | 92.1  |\\n|  Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1  |\\n|  Transformer (4 layers) | semi-supervised | 92.7  |\\n|  Luong et al. (2015) [23] | multi-task | 93.0  |\\n|  Dyer et al. (2016) [8] | generative | 93.3  |\\n\\nincreased the maximum output length to input length  $+300$ . We used a beam size of 21 and  $\\\\alpha = 0.3$  for both WSJ only and the semi-supervised setting.\\n\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\\n\\n# 7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\n# References\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\\n- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\\n- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n- [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\n- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\\n- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\\n- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\\n- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\\n- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n- [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n- [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\\n- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n- [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\\n- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\\n- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\\n- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\\n- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n- [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n\\n# Attention Visualizations\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \\'making\\', completing the phrase \\'making...more difficult\\'. Attentions here shown only for the word \\'making\\'. Different colors represent different heads. Best viewed in color.\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \\'its\\' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n![img-5.jpeg](img-5.jpeg)\\n\\n![img-6.jpeg](img-6.jpeg)\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.'), -0.1458074267776519)]\n",
      "  results = self._store.similarity_search_with_relevance_scores(query, **kwargs)  # type: ignore[arg-type]\n"
     ]
    }
   ],
   "source": [
    "# Vector (semantic) search\n",
    "query = \"how to generate realistic images?\"\n",
    "results = kb.search_docs(query, top_k=3, strategy=\"vector\")\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.3f}] {hit.name}\")\n",
    "    if hit.snippet:\n",
    "        print(f\"{hit.snippet[:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0328] LORA.pdf\n",
      "# LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\n",
      "\n",
      "Edward Hu* Yelong Shen* Phillip Wallis Zeyuan Allen-Zhu\n",
      "\n",
      "Yuanzhi L...\n",
      "[0.0320] Language Models are Few-Shot Learners.pdf\n",
      "# Language Models are Few-Shot Learners\n",
      "\n",
      "|  Tom B. Brown* |   | Benjamin Mann* |   | Nick Ryder* |   | Melanie Subbiah* ...\n",
      "[0.0315] BERT.pdf\n",
      "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "\n",
      "Jacob Devlin Ming-Wei Chang Kenton L...\n",
      "[0.0304] XLNet.pdf\n",
      "# XLNet: Generalized Autoregressive Pretraining for Language Understanding\n",
      "\n",
      "Zhilin Yang^{∗1}, Zihang Dai^{∗12}, Yiming Y...\n",
      "[0.0299] Attention Is All You Need.pdf\n",
      "# Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani*\n",
      "\n",
      "Google Brain\n",
      "\n",
      "avaswani@google.com\n",
      "\n",
      "Noam Shazeer*\n",
      "\n",
      "Google Brain\n",
      "\n",
      "noam@googl...\n"
     ]
    }
   ],
   "source": [
    "# Hybrid search (default)\n",
    "# Combines BM25 and vector search with Reciprocal Rank Fusion.\n",
    "query = \"low-rank adaptation for large language models\"\n",
    "results = kb.search_docs(query, top_k=5)\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.4f}] {hit.name}\")\n",
    "    if hit.snippet:\n",
    "        print(f\"{hit.snippet[:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0328] RAG.pdf\n",
      "[0.0320] Attention Is All You Need.pdf\n",
      "[0.0315] Language Models are Few-Shot Learners.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasnimo/Documents/athenaeum/playground/.venv/lib/python3.13/site-packages/athenaeum/search/vector.py:61: UserWarning: Relevance scores must be between 0 and 1, got [(Document(id='d610388cdccd:0', metadata={'start_line': 1, 'end_line': 405, 'doc_id': 'd610388cdccd', 'chunk_index': 0, 'text': '# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\nPatrick Lewis^{†}^{‡}, Ethan Perez^{∗},\\nAleksandra Piktus^{†}, Fabio Petroni^{†}, Vladimir Karpukhin^{†}, Naman Goyal^{†}, Heinrich Küttler^{†},\\nMike Lewis^{†}, Wen-tau Yih^{†}, Tim Rocktäschel^{†}^{‡}, Sebastian Riedel^{†}^{‡}, Douwe Kiela^{†}\\n^{†}Facebook AI Research; ^{‡}University College London; ^{∗}New York University;\\nplewis@fb.com\\n\\n###### Abstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n## 1 Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data *[47]*. They can do so without any access to an external memory, as a parameterized implicit knowledge base *[51, 52]*. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” *[38]*. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories *[20, 26, 48]* can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM *[20]* and ORQA *[31]*, two recently introduced models that combine masked language models *[8]* with a differentiable retriever, have shown promising results,\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query  $x$ , we use Maximum Inner Product Search (MIPS) to find the top-K documents  $z_{i}$ . For final prediction  $y$ , we treat  $z$  as a latent variable and marginalize over seq2seq predictions given different documents.\\n\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \"workhorse of NLP,\" i.e. sequence-to-sequence (seq2seq) models.\\n\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\n\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.\\n\\nOur results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within  $4.3\\\\%$  of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\\' knowledge as the world changes. $^{1}$\\n\\n# 2 Methods\\n\\nWe explore RAG models, which use the input sequence  $x$  to retrieve text documents  $z$  and use them as additional context when generating the target sequence  $y$ . As shown in Figure 1, our models leverage two components: (i) a retriever  $p_{\\\\eta}(z|x)$  with parameters  $\\\\eta$  that returns (top-K truncated) distributions over text passages given a query  $x$  and (ii) a generator  $p_{\\\\theta}(y_i|x,z,y_{1:i-1})$  parametrized\\n\\nby $\\\\theta$ that generates a current token based on a context of the previous $i-1$ tokens $y_{1:i-1}$, the original input $x$ and a retrieved passage $z$.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the $p_{\\\\eta}$ and $p_{\\\\theta}$ components, as well as the training and decoding procedure.\\n\\n### 2.1 Models\\n\\n#### RAG-Sequence Model\\n\\nThe RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability $p(y|x)$ via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,\\n\\n$p_{\\\\text{RAG-Sequence}}(y|x)\\\\approx\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)p_{\\\\theta}(y|x,z)=\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)\\\\prod_{i}^{N}p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$\\n\\n#### RAG-Token Model\\n\\nIn the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:\\n\\n$p_{\\\\text{RAG-Token}}(y|x)\\\\ \\\\approx\\\\ \\\\prod_{i}^{N}\\\\ \\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$\\n\\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n### 2.2 Retriever: DPR\\n\\nThe retrieval component $p_{\\\\eta}(z|x)$ is based on DPR *[26]*. DPR follows a bi-encoder architecture:\\n\\n$p_{\\\\eta}(z|x)\\\\propto\\\\exp\\\\left(\\\\mathbf{d}(z)^{\\\\top}\\\\mathbf{q}(x)\\\\right)\\\\hskip 28.45274pt\\\\mathbf{d}(z)=\\\\text{BERT}_{d}(z),\\\\ \\\\ \\\\mathbf{q}(x)=\\\\text{BERT}_{q}(x)$\\n\\nwhere $\\\\mathbf{d}(z)$ is a dense representation of a document produced by a $\\\\text{BERT}_{\\\\text{BASE}}$ document encoder *[8]*, and $\\\\mathbf{q}(x)$ a query representation produced by a query encoder, also based on $\\\\text{BERT}_{\\\\text{BASE}}$. Calculating top-k($p_{\\\\eta}(\\\\cdot|x)$), the list of $k$ documents $z$ with highest prior probability $p_{\\\\eta}(z|x)$, is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time *[23]*. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA *[24]* questions and Natural Questions *[29]*. We refer to the document index as the non-parametric memory.\\n\\n### 2.3 Generator: BART\\n\\nThe generator component $p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$ could be modelled using any encoder-decoder. We use BART-large *[32]*, a pre-trained seq2seq transformer *[58]* with 400M parameters. To combine the input $x$ with the retrieved content $z$ when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models *[32]*. We refer to the BART generator parameters $\\\\theta$ as the parametric memory henceforth.\\n\\n### 2.4 Training\\n\\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs $(x_{j},y_{j})$, we\\n\\nminimize the negative marginal log-likelihood of each target, $\\\\sum_{j}-\\\\log p(y_{j}|x_{j})$ using stochastic gradient descent with Adam *[28]*. Updating the document encoder BERT_{d} during training is costly as it requires the document index to be periodically updated as REALM does during pre-training *[20]*. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERT_{q} and the BART generator.\\n\\n### 2.5 Decoding\\n\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate $\\\\arg\\\\max_{y}p(y|x)$.\\n\\n#### RAG-Token\\n\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: $p_{\\\\theta}^{\\\\prime}(y_{i}|x,y_{1:i-1})=\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z_{i}|x)p_{\\\\theta}(y_{i}|x,z_{i},y_{1:i-1})$ To decode, we can plug $p_{\\\\theta}^{\\\\prime}(y_{i}|x,y_{1:i-1})$ into a standard beam decoder.\\n\\n#### RAG-Sequence\\n\\nFor RAG-Sequence, the likelihood $p(y|x)$ does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document $z$, scoring each hypothesis using $p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$. This yields a set of hypotheses $Y$, some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis $y$ we run an additional forward pass for each document $z$ for which $y$ does not appear in the beam, multiply generator probability with $p_{\\\\eta}(z|x)$ and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, $|Y|$ can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that $p_{\\\\theta}(y|x,z_{i})\\\\approx 0$ where $y$ was not generated during beam search from $x,z_{i}$. This avoids the need to run additional forward passes once the candidate set $Y$ has been generated. We refer to this decoding procedure as “Fast Decoding.”\\n\\n## 3 Experiments\\n\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following *Lee et al. [31]* and *Karpukhin et al. [26]*, we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS *[23]* with a Hierarchical Navigable Small World approximation for fast retrieval *[37]*. During training, we retrieve the top $k$ documents for each query. We consider $k\\\\in\\\\{5,10\\\\}$ for training and set $k$ for test time using dev data. We now discuss experimental details for each task.\\n\\n### 3.1 Open-domain Question Answering\\n\\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks *[20]*. We treat questions and answers as input-output text pairs $(x,y)$ and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm *[5, 7, 31, 26]*, where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA\" approaches *[52]*, which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) *[29]*, TriviaQA (TQA) *[24]*. WebQuestions (WQ) *[3]* and CuratedTrec (CT) *[2]*. As CT and WQ are small, we follow DPR *[26]* by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work *[31, 26]* and report Exact Match (EM) scores. For TQA, to compare with T5 *[52]*, we also evaluate on the TQA Wiki test set.\\n\\n### 3.2 Abstractive Question Answering\\n\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 *[43]*. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n\\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\\n\\n### 3.3 Jeopardy Question Generation\\n\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the first country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\\n\\nWe use the splits from SearchQA *[10]*, with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following *[67]*, we evaluate using the SQuAD-tuned Q-BLEU-1 metric *[42]*. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output *[33]*. We follow best practice and use pairwise comparative evaluation *[34]*. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—-question A is better, question B is better, both are good, or neither is good.\\n\\n### 3.4 Fact Verification\\n\\nFEVER *[56]* requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos *[57]*. In both cases we report label accuracy.\\n\\n## 4 Results\\n\\n### 4.1 Open-domain Question Answering\\n\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation flexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training *[20]*. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.\\n\\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\\n\\nTable 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open-Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details.\\n\\n|   | Model | NQ | TQA | WQ | CT  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Closed | T5-11B [52] | 34.5 | - /50.1 | 37.4 | -  |\\n|  Book | T5-11B+SSM[52] | 36.6 | - /60.5 | 44.7 | -  |\\n|  Open | REALM [20] | 40.4 | - / - | 40.7 | 46.8  |\\n|  Book | DPR [26] | 41.5 | 57.9/ - | 41.1 | 50.6  |\\n|   | RAG-Token | 44.1 | 55.2/66.1 | 45.5 | 50.0  |\\n|   | RAG-Seq. | 44.5 | 56.8/68.0 | 45.2 | 52.2  |\\n\\nTable 2: Generation and classification Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined.\\n\\n|  Model | Jeopardy |   | MSMARCO |   | FVR3 | FVR2  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|   |  B-1 | QB-1 | R-L | B-1 | Label | Acc.  |\\n|  SotA | - | - | 49.8* | 49.9* | 76.8 | 92.2*  |\\n|  BART | 15.1 | 19.7 | 38.2 | 41.6 | 64.0 | 81.1  |\\n|  RAG-Tok. | 17.3 | 22.2 | 40.1 | 41.5 | 72.5 | 89.5  |\\n|  RAG-Seq. | 14.7 | 21.4 | 40.8 | 44.2  |   |   |\\n\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving  $11.8\\\\%$  accuracy in such cases for NQ, where an extractive model would score  $0\\\\%$ .\\n\\n# 4.2 Abstractive Question Answering\\n\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5).\\n\\n# 4.3 Jeopardy Question Generation\\n\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only  $7.1\\\\%$  of cases, while RAG was more factual in  $42.7\\\\%$  of cases, and both RAG and BART were factual in a further  $17\\\\%$  of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\\n\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating \"Sun\", the posterior is high for document 2 which mentions \"The Sun Also Rises\". Similarly, document 1 dominates the posterior when \"A Farewell to Arms\" is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model\\'s parametric knowledge is sufficient to complete the titles. We find evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART\\'s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\\n\\n# 4.4 Fact Verification\\n\\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within  $4.3\\\\%$  of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n\\nDocument 1: his works are considered classics of American literature ... His wartime experiences formed the basis for his novel \"A Farewell to Arms\" (1929) ...\\n\\nDocument 2: ... artists of the 1920s \"Lost Generation\" expatriate community. His debut novel, \"The Sun Also Rises\", was published in 1926.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2: RAG-Token document posterior  $p(z_i|x, y_i, y_{-i})$  for each generated token for input \"Hemingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \"A Farewell to Arms\" and for document 2 when generating \"The Sun Also Rises\".\\n\\nTable 3: Examples from generation tasks. RAG models generate more specific and factually accurate responses.  $\\\\text{?}$  indicates factually incorrect responses, * indicates partially correct responses.\\n\\n|  Task | Input | Model | Generation  |\\n| --- | --- | --- | --- |\\n|  MS-MARCO | define middle ear | BART | 7The middle ear is the part of the ear between the middle ear and the nose.  |\\n|   |   |  RAG-T | The middle ear is the portion of the ear internal to the eardrum.  |\\n|   |   |  RAG-S | The middle ear includes the tympanic cavity and the three ossicles.  |\\n|   |  what currency needed in scotland | BART | The currency needed in Scotland is Pound sterling.  |\\n|   |   |  RAG-T | Pound is the currency needed in Scotland.  |\\n|   |   |  RAG-S | The currency needed in Scotland is the pound sterling.  |\\n|  Jeopardy Question Gener -ation | Washington | BART | 7This state has the largest number of counties in the U.S.  |\\n|   |   |  RAG-T | It\\'s the only U.S. state named for a U.S. president  |\\n|   |   |  RAG-S | It\\'s the state where you\\'ll find Mount Rainier National Park  |\\n|   |  The Divine Comedy | BART | 7This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio  |\\n|   |   |  RAG-T | Dante\\'s \"Inferno\" is the first part of this epic poem  |\\n|   |   |  RAG-S | This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"  |\\n\\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within  $2.7\\\\%$  of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top  $k$  documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in  $71\\\\%$  of cases, and a gold article is present in the top 10 retrieved articles in  $90\\\\%$  of cases.\\n\\n# 4.5 Additional Results\\n\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding [33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence\\'s generations are more diverse than RAG-Token\\'s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\\n\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\\n\\nWe compare RAG\\'s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG\\'s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating  $p(z|x)$ . Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\n\\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n\\nTable 4: Human assessments for the Jeopardy Question Generation Task.\\n\\n|   | Factuality | Specificity  |\\n| --- | --- | --- |\\n|  BART better | 7.1% | 16.8%  |\\n|  RAG better | 42.7% | 37.4%  |\\n|  Both good | 11.7% | 11.8%  |\\n|  Both poor | 17.7% | 6.9%  |\\n|  No majority | 20.8% | 20.1%  |\\n\\nTable 5: Ratio of distinct to total tri-grams for generation tasks.\\n\\n|   | MSMARCO | Jeopardy QGen  |\\n| --- | --- | --- |\\n|  Gold | 89.6% | 90.0%  |\\n|  BART | 70.7% | 32.4%  |\\n|  RAG-Token | 77.8% | 46.8%  |\\n|  RAG-Seq. | 83.5% | 53.8%  |\\n\\nTable 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.\\n\\n|  Model | NQ | TQA Exact | WQ Match | CT | Jeopardy-QGen |   | MSMarco |   | FVR-3 Label | FVR-2 Accuracy  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   |   |   |   |   |  B-1 | QB-1 | R-L | B-1  |   |   |\\n|  RAG-Token-BM25 | 29.7 | 41.5 | 32.1 | 33.1 | 17.5 | 22.3 | 55.5 | 48.4 | 75.1 | 91.6  |\\n|  RAG-Sequence-BM25 | 31.8 | 44.1 | 36.6 | 33.8 | 11.1 | 19.5 | 56.5 | 46.9  |   |   |\\n|  RAG-Token-Frozen | 37.8 | 50.1 | 37.1 | 51.1 | 16.7 | 21.7 | 55.9 | 49.4 | 72.9 | 89.4  |\\n|  RAG-Sequence-Frozen | 41.2 | 52.1 | 41.8 | 52.6 | 11.8 | 19.6 | 56.7 | 47.3  |   |   |\\n|  RAG-Token | 43.5 | 54.8 | 46.5 | 51.9 | 17.9 | 22.6 | 56.2 | 49.4 | 74.5 | 90.6  |\\n|  RAG-Sequence | 44.0 | 55.8 | 44.9 | 53.4 | 15.3 | 21.5 | 57.2 | 47.5  |   |   |\\n\\nbetween these dates and use a template \"Who is {position}?\" (e.g. \"Who is the President of Peru?\") to query our NQ RAG model with each index. RAG answers  $70\\\\%$  correctly using the 2016 index for 2016 world leaders and  $68\\\\%$  using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low ( $12\\\\%$  with the 2018 index and 2016 leaders,  $4\\\\%$  with the 2016 index and 2018 leaders). This shows we can update RAG\\'s world knowledge by simply replacing its non-parametric memory.\\n\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n\\n![img-3.jpeg](img-3.jpeg)\\n\\n![img-4.jpeg](img-4.jpeg)\\n\\n# 5 Related Work\\n\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\\n\\n#### General-Purpose Architectures for NLP\\n\\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks *[60, 61]* after fine-tuning *[49, 8]*. GPT-2 *[50]* later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART *[32]* and T5 *[51, 52]* propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\\n\\n#### Learned Retrieval\\n\\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models *[44, 26]* similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search *[46]*, reinforcement learning *[6, 63, 62]*, or a latent variable approach *[31, 20]* as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\\n\\n#### Memory-based Architectures\\n\\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks *[64, 55]*. Concurrent work *[14]* learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings *[15, 13]*. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval *[9]*.\\n\\n#### Retrieve-and-Edit approaches\\n\\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation *[18, 22]* and Semantic Parsing *[21]*. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\\n\\n## 6 Discussion\\n\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\\n\\nBroader Impact\\n\\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\\n\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 *[50]* are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content *[54]*. Advanced language models may also lead to the automation of various jobs in the coming decades *[16]*. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\\n\\n## Acknowledgments\\n\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\\n\\n## References\\n\\n- [1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n- [2] Petr Baudiš and Jan Šedivỳ. Modeling of the question answering task in the yodaqa system. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\\n- [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/D13-1160.\\n- [4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\\n- [5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anthology/P17-1171.\\n- [6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-fine question answering for long documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n-\\n\\n[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\\n- [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\\n- [9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\n- [10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\\n- [11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/P18-1082.\\n- [12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anthology/P19-1346.\\n- [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\\n- [14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\\n- [15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\\n- [16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\\n- [17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n- [18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\\n- [19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\\n\\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.\\n- [21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052–10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.\\n- [22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-erank text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anthology/2020.acl-main.228.\\n- [23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n- [24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\\n- [25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.\\n- [26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n- [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n- [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\\n- [29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.\\n- [30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.\\n- [31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n\\nfor Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/anthology/P19-1612.\\n- [32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n- [33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/N16-1014.\\n- [34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\\n- [35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291.\\n- [36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\\n- [37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\\n- [38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n- [39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\\n- [40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n- [41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n- [42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anthology/D18-1429.\\n- [43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n\\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n- [44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n- [45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\\n- [46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n- [47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n- [48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\\n- [49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\\n- [50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n- [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n- [52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\\n- [53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\\n- [54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\\n- [55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n\\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anthology/N18-1074.\\n- [57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification with elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\\n- [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n- [59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\\n- [60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446.\\n- [61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\\\text{textquotesingle} Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\\n- [62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R^{3}: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\\n- [63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\\n- [64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\\n- [65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anthology/W18-5713.\\n\\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n- [67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anthology/D19-1253.\\n- [68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\\n\\n# Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n# A Implementation Details\\n\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\n\\n# B Human Evaluation\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\".\\n\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\\n\\n# C Training setup Details\\n\\nWe train all RAG models and BART baselines using Fairseq [45]. We train with mixed precision floating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring  $\\\\sim 100$  GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers [66], which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\\'s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\\n\\nD Further Details on Open-Domain QA\\n\\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each $(q,a)$ pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\\n\\n#### CuratedTrec preprocessing\\n\\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models *[20]*. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\n\\n#### TriviaQA Evaluation setups\\n\\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR *[26]*, which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. *[52]* used the TriviaQA official Wikipedia test set instead. Févry et al. *[14]* follow this convention in order to compare with Roberts et al. *[52]* (See appendix of *[14]*). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\\n\\n## Appendix E Further Details on FEVER\\n\\nFor FEVER classification, we follow the practice from *[32]*, and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\\n\\n## Appendix F Null Document Probabilities\\n\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM *[20]* in order to model cases where no useful information could be retrieved for a given input. Here, if $k$ documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over $k+1$ predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\\n\\n## Appendix G Parameters\\n\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n\\nTable 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\n\\n|  Task | Train | Development | Test  |\\n| --- | --- | --- | --- |\\n|  Natural Questions | 79169 | 8758 | 3611  |\\n|  TriviaQA | 78786 | 8838 | 11314  |\\n|  WebQuestions | 3418 | 362 | 2033  |\\n|  CuratedTrec | 635 | 134 | 635  |\\n|  Jeopardy Question Generation | 97392 | 13714 | 26849  |\\n|  MS-MARCO | 153726 | 12468 | 101093*  |\\n|  FEVER-3-way | 145450 | 10000 | 10000  |\\n|  FEVER-2-way | 96966 | 6666 | 6666  |\\n\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consist of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating point precision to manage memory and disk footprints.\\n\\n# H Retrieval Collapse\\n\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would \"collapse\" and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.\\n\\n# I Number of instances per dataset\\n\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.'}, page_content='# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\nPatrick Lewis^{†}^{‡}, Ethan Perez^{∗},\\nAleksandra Piktus^{†}, Fabio Petroni^{†}, Vladimir Karpukhin^{†}, Naman Goyal^{†}, Heinrich Küttler^{†},\\nMike Lewis^{†}, Wen-tau Yih^{†}, Tim Rocktäschel^{†}^{‡}, Sebastian Riedel^{†}^{‡}, Douwe Kiela^{†}\\n^{†}Facebook AI Research; ^{‡}University College London; ^{∗}New York University;\\nplewis@fb.com\\n\\n###### Abstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n## 1 Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data *[47]*. They can do so without any access to an external memory, as a parameterized implicit knowledge base *[51, 52]*. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” *[38]*. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories *[20, 26, 48]* can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM *[20]* and ORQA *[31]*, two recently introduced models that combine masked language models *[8]* with a differentiable retriever, have shown promising results,\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query  $x$ , we use Maximum Inner Product Search (MIPS) to find the top-K documents  $z_{i}$ . For final prediction  $y$ , we treat  $z$  as a latent variable and marginalize over seq2seq predictions given different documents.\\n\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \"workhorse of NLP,\" i.e. sequence-to-sequence (seq2seq) models.\\n\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\n\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.\\n\\nOur results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within  $4.3\\\\%$  of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\\' knowledge as the world changes. $^{1}$\\n\\n# 2 Methods\\n\\nWe explore RAG models, which use the input sequence  $x$  to retrieve text documents  $z$  and use them as additional context when generating the target sequence  $y$ . As shown in Figure 1, our models leverage two components: (i) a retriever  $p_{\\\\eta}(z|x)$  with parameters  $\\\\eta$  that returns (top-K truncated) distributions over text passages given a query  $x$  and (ii) a generator  $p_{\\\\theta}(y_i|x,z,y_{1:i-1})$  parametrized\\n\\nby $\\\\theta$ that generates a current token based on a context of the previous $i-1$ tokens $y_{1:i-1}$, the original input $x$ and a retrieved passage $z$.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the $p_{\\\\eta}$ and $p_{\\\\theta}$ components, as well as the training and decoding procedure.\\n\\n### 2.1 Models\\n\\n#### RAG-Sequence Model\\n\\nThe RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability $p(y|x)$ via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,\\n\\n$p_{\\\\text{RAG-Sequence}}(y|x)\\\\approx\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)p_{\\\\theta}(y|x,z)=\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)\\\\prod_{i}^{N}p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$\\n\\n#### RAG-Token Model\\n\\nIn the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:\\n\\n$p_{\\\\text{RAG-Token}}(y|x)\\\\ \\\\approx\\\\ \\\\prod_{i}^{N}\\\\ \\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z|x)p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$\\n\\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n### 2.2 Retriever: DPR\\n\\nThe retrieval component $p_{\\\\eta}(z|x)$ is based on DPR *[26]*. DPR follows a bi-encoder architecture:\\n\\n$p_{\\\\eta}(z|x)\\\\propto\\\\exp\\\\left(\\\\mathbf{d}(z)^{\\\\top}\\\\mathbf{q}(x)\\\\right)\\\\hskip 28.45274pt\\\\mathbf{d}(z)=\\\\text{BERT}_{d}(z),\\\\ \\\\ \\\\mathbf{q}(x)=\\\\text{BERT}_{q}(x)$\\n\\nwhere $\\\\mathbf{d}(z)$ is a dense representation of a document produced by a $\\\\text{BERT}_{\\\\text{BASE}}$ document encoder *[8]*, and $\\\\mathbf{q}(x)$ a query representation produced by a query encoder, also based on $\\\\text{BERT}_{\\\\text{BASE}}$. Calculating top-k($p_{\\\\eta}(\\\\cdot|x)$), the list of $k$ documents $z$ with highest prior probability $p_{\\\\eta}(z|x)$, is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time *[23]*. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA *[24]* questions and Natural Questions *[29]*. We refer to the document index as the non-parametric memory.\\n\\n### 2.3 Generator: BART\\n\\nThe generator component $p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$ could be modelled using any encoder-decoder. We use BART-large *[32]*, a pre-trained seq2seq transformer *[58]* with 400M parameters. To combine the input $x$ with the retrieved content $z$ when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models *[32]*. We refer to the BART generator parameters $\\\\theta$ as the parametric memory henceforth.\\n\\n### 2.4 Training\\n\\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs $(x_{j},y_{j})$, we\\n\\nminimize the negative marginal log-likelihood of each target, $\\\\sum_{j}-\\\\log p(y_{j}|x_{j})$ using stochastic gradient descent with Adam *[28]*. Updating the document encoder BERT_{d} during training is costly as it requires the document index to be periodically updated as REALM does during pre-training *[20]*. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERT_{q} and the BART generator.\\n\\n### 2.5 Decoding\\n\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate $\\\\arg\\\\max_{y}p(y|x)$.\\n\\n#### RAG-Token\\n\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: $p_{\\\\theta}^{\\\\prime}(y_{i}|x,y_{1:i-1})=\\\\sum_{z\\\\in\\\\text{top-}k(p(\\\\cdot|x))}p_{\\\\eta}(z_{i}|x)p_{\\\\theta}(y_{i}|x,z_{i},y_{1:i-1})$ To decode, we can plug $p_{\\\\theta}^{\\\\prime}(y_{i}|x,y_{1:i-1})$ into a standard beam decoder.\\n\\n#### RAG-Sequence\\n\\nFor RAG-Sequence, the likelihood $p(y|x)$ does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document $z$, scoring each hypothesis using $p_{\\\\theta}(y_{i}|x,z,y_{1:i-1})$. This yields a set of hypotheses $Y$, some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis $y$ we run an additional forward pass for each document $z$ for which $y$ does not appear in the beam, multiply generator probability with $p_{\\\\eta}(z|x)$ and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, $|Y|$ can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that $p_{\\\\theta}(y|x,z_{i})\\\\approx 0$ where $y$ was not generated during beam search from $x,z_{i}$. This avoids the need to run additional forward passes once the candidate set $Y$ has been generated. We refer to this decoding procedure as “Fast Decoding.”\\n\\n## 3 Experiments\\n\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following *Lee et al. [31]* and *Karpukhin et al. [26]*, we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS *[23]* with a Hierarchical Navigable Small World approximation for fast retrieval *[37]*. During training, we retrieve the top $k$ documents for each query. We consider $k\\\\in\\\\{5,10\\\\}$ for training and set $k$ for test time using dev data. We now discuss experimental details for each task.\\n\\n### 3.1 Open-domain Question Answering\\n\\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks *[20]*. We treat questions and answers as input-output text pairs $(x,y)$ and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm *[5, 7, 31, 26]*, where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA\" approaches *[52]*, which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) *[29]*, TriviaQA (TQA) *[24]*. WebQuestions (WQ) *[3]* and CuratedTrec (CT) *[2]*. As CT and WQ are small, we follow DPR *[26]* by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work *[31, 26]* and report Exact Match (EM) scores. For TQA, to compare with T5 *[52]*, we also evaluate on the TQA Wiki test set.\\n\\n### 3.2 Abstractive Question Answering\\n\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 *[43]*. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n\\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\\n\\n### 3.3 Jeopardy Question Generation\\n\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the first country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\\n\\nWe use the splits from SearchQA *[10]*, with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following *[67]*, we evaluate using the SQuAD-tuned Q-BLEU-1 metric *[42]*. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output *[33]*. We follow best practice and use pairwise comparative evaluation *[34]*. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—-question A is better, question B is better, both are good, or neither is good.\\n\\n### 3.4 Fact Verification\\n\\nFEVER *[56]* requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos *[57]*. In both cases we report label accuracy.\\n\\n## 4 Results\\n\\n### 4.1 Open-domain Question Answering\\n\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation flexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training *[20]*. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.\\n\\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\\n\\nTable 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open-Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details.\\n\\n|   | Model | NQ | TQA | WQ | CT  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Closed | T5-11B [52] | 34.5 | - /50.1 | 37.4 | -  |\\n|  Book | T5-11B+SSM[52] | 36.6 | - /60.5 | 44.7 | -  |\\n|  Open | REALM [20] | 40.4 | - / - | 40.7 | 46.8  |\\n|  Book | DPR [26] | 41.5 | 57.9/ - | 41.1 | 50.6  |\\n|   | RAG-Token | 44.1 | 55.2/66.1 | 45.5 | 50.0  |\\n|   | RAG-Seq. | 44.5 | 56.8/68.0 | 45.2 | 52.2  |\\n\\nTable 2: Generation and classification Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined.\\n\\n|  Model | Jeopardy |   | MSMARCO |   | FVR3 | FVR2  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|   |  B-1 | QB-1 | R-L | B-1 | Label | Acc.  |\\n|  SotA | - | - | 49.8* | 49.9* | 76.8 | 92.2*  |\\n|  BART | 15.1 | 19.7 | 38.2 | 41.6 | 64.0 | 81.1  |\\n|  RAG-Tok. | 17.3 | 22.2 | 40.1 | 41.5 | 72.5 | 89.5  |\\n|  RAG-Seq. | 14.7 | 21.4 | 40.8 | 44.2  |   |   |\\n\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving  $11.8\\\\%$  accuracy in such cases for NQ, where an extractive model would score  $0\\\\%$ .\\n\\n# 4.2 Abstractive Question Answering\\n\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5).\\n\\n# 4.3 Jeopardy Question Generation\\n\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only  $7.1\\\\%$  of cases, while RAG was more factual in  $42.7\\\\%$  of cases, and both RAG and BART were factual in a further  $17\\\\%$  of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\\n\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating \"Sun\", the posterior is high for document 2 which mentions \"The Sun Also Rises\". Similarly, document 1 dominates the posterior when \"A Farewell to Arms\" is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model\\'s parametric knowledge is sufficient to complete the titles. We find evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART\\'s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\\n\\n# 4.4 Fact Verification\\n\\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within  $4.3\\\\%$  of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n\\nDocument 1: his works are considered classics of American literature ... His wartime experiences formed the basis for his novel \"A Farewell to Arms\" (1929) ...\\n\\nDocument 2: ... artists of the 1920s \"Lost Generation\" expatriate community. His debut novel, \"The Sun Also Rises\", was published in 1926.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2: RAG-Token document posterior  $p(z_i|x, y_i, y_{-i})$  for each generated token for input \"Hemingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \"A Farewell to Arms\" and for document 2 when generating \"The Sun Also Rises\".\\n\\nTable 3: Examples from generation tasks. RAG models generate more specific and factually accurate responses.  $\\\\text{?}$  indicates factually incorrect responses, * indicates partially correct responses.\\n\\n|  Task | Input | Model | Generation  |\\n| --- | --- | --- | --- |\\n|  MS-MARCO | define middle ear | BART | 7The middle ear is the part of the ear between the middle ear and the nose.  |\\n|   |   |  RAG-T | The middle ear is the portion of the ear internal to the eardrum.  |\\n|   |   |  RAG-S | The middle ear includes the tympanic cavity and the three ossicles.  |\\n|   |  what currency needed in scotland | BART | The currency needed in Scotland is Pound sterling.  |\\n|   |   |  RAG-T | Pound is the currency needed in Scotland.  |\\n|   |   |  RAG-S | The currency needed in Scotland is the pound sterling.  |\\n|  Jeopardy Question Gener -ation | Washington | BART | 7This state has the largest number of counties in the U.S.  |\\n|   |   |  RAG-T | It\\'s the only U.S. state named for a U.S. president  |\\n|   |   |  RAG-S | It\\'s the state where you\\'ll find Mount Rainier National Park  |\\n|   |  The Divine Comedy | BART | 7This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio  |\\n|   |   |  RAG-T | Dante\\'s \"Inferno\" is the first part of this epic poem  |\\n|   |   |  RAG-S | This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"  |\\n\\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within  $2.7\\\\%$  of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top  $k$  documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in  $71\\\\%$  of cases, and a gold article is present in the top 10 retrieved articles in  $90\\\\%$  of cases.\\n\\n# 4.5 Additional Results\\n\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding [33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence\\'s generations are more diverse than RAG-Token\\'s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\\n\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\\n\\nWe compare RAG\\'s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG\\'s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating  $p(z|x)$ . Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\n\\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n\\nTable 4: Human assessments for the Jeopardy Question Generation Task.\\n\\n|   | Factuality | Specificity  |\\n| --- | --- | --- |\\n|  BART better | 7.1% | 16.8%  |\\n|  RAG better | 42.7% | 37.4%  |\\n|  Both good | 11.7% | 11.8%  |\\n|  Both poor | 17.7% | 6.9%  |\\n|  No majority | 20.8% | 20.1%  |\\n\\nTable 5: Ratio of distinct to total tri-grams for generation tasks.\\n\\n|   | MSMARCO | Jeopardy QGen  |\\n| --- | --- | --- |\\n|  Gold | 89.6% | 90.0%  |\\n|  BART | 70.7% | 32.4%  |\\n|  RAG-Token | 77.8% | 46.8%  |\\n|  RAG-Seq. | 83.5% | 53.8%  |\\n\\nTable 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.\\n\\n|  Model | NQ | TQA Exact | WQ Match | CT | Jeopardy-QGen |   | MSMarco |   | FVR-3 Label | FVR-2 Accuracy  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   |   |   |   |   |  B-1 | QB-1 | R-L | B-1  |   |   |\\n|  RAG-Token-BM25 | 29.7 | 41.5 | 32.1 | 33.1 | 17.5 | 22.3 | 55.5 | 48.4 | 75.1 | 91.6  |\\n|  RAG-Sequence-BM25 | 31.8 | 44.1 | 36.6 | 33.8 | 11.1 | 19.5 | 56.5 | 46.9  |   |   |\\n|  RAG-Token-Frozen | 37.8 | 50.1 | 37.1 | 51.1 | 16.7 | 21.7 | 55.9 | 49.4 | 72.9 | 89.4  |\\n|  RAG-Sequence-Frozen | 41.2 | 52.1 | 41.8 | 52.6 | 11.8 | 19.6 | 56.7 | 47.3  |   |   |\\n|  RAG-Token | 43.5 | 54.8 | 46.5 | 51.9 | 17.9 | 22.6 | 56.2 | 49.4 | 74.5 | 90.6  |\\n|  RAG-Sequence | 44.0 | 55.8 | 44.9 | 53.4 | 15.3 | 21.5 | 57.2 | 47.5  |   |   |\\n\\nbetween these dates and use a template \"Who is {position}?\" (e.g. \"Who is the President of Peru?\") to query our NQ RAG model with each index. RAG answers  $70\\\\%$  correctly using the 2016 index for 2016 world leaders and  $68\\\\%$  using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low ( $12\\\\%$  with the 2018 index and 2016 leaders,  $4\\\\%$  with the 2016 index and 2018 leaders). This shows we can update RAG\\'s world knowledge by simply replacing its non-parametric memory.\\n\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n\\n![img-3.jpeg](img-3.jpeg)\\n\\n![img-4.jpeg](img-4.jpeg)\\n\\n# 5 Related Work\\n\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\\n\\n#### General-Purpose Architectures for NLP\\n\\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks *[60, 61]* after fine-tuning *[49, 8]*. GPT-2 *[50]* later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART *[32]* and T5 *[51, 52]* propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\\n\\n#### Learned Retrieval\\n\\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models *[44, 26]* similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search *[46]*, reinforcement learning *[6, 63, 62]*, or a latent variable approach *[31, 20]* as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\\n\\n#### Memory-based Architectures\\n\\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks *[64, 55]*. Concurrent work *[14]* learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings *[15, 13]*. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval *[9]*.\\n\\n#### Retrieve-and-Edit approaches\\n\\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation *[18, 22]* and Semantic Parsing *[21]*. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\\n\\n## 6 Discussion\\n\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\\n\\nBroader Impact\\n\\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\\n\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 *[50]* are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content *[54]*. Advanced language models may also lead to the automation of various jobs in the coming decades *[16]*. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\\n\\n## Acknowledgments\\n\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\\n\\n## References\\n\\n- [1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n- [2] Petr Baudiš and Jan Šedivỳ. Modeling of the question answering task in the yodaqa system. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\\n- [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/D13-1160.\\n- [4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\\n- [5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anthology/P17-1171.\\n- [6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-fine question answering for long documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n-\\n\\n[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\\n- [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\\n- [9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\n- [10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\\n- [11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/P18-1082.\\n- [12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anthology/P19-1346.\\n- [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\\n- [14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\\n- [15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\\n- [16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\\n- [17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n- [18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\\n- [19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\\n\\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.\\n- [21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052–10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.\\n- [22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-erank text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anthology/2020.acl-main.228.\\n- [23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n- [24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\\n- [25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.\\n- [26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n- [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n- [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\\n- [29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.\\n- [30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.\\n- [31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n\\nfor Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/anthology/P19-1612.\\n- [32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n- [33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/N16-1014.\\n- [34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\\n- [35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291.\\n- [36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\\n- [37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\\n- [38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n- [39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\\n- [40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n- [41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n- [42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anthology/D18-1429.\\n- [43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n\\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n- [44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n- [45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\\n- [46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n- [47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n- [48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\\n- [49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\\n- [50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n- [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n- [52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\\n- [53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\\n- [54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\\n- [55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n\\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anthology/N18-1074.\\n- [57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification with elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\\n- [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n- [59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\\n- [60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446.\\n- [61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\\\text{textquotesingle} Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\\n- [62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R^{3}: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\\n- [63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\\n- [64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\\n- [65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anthology/W18-5713.\\n\\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n- [67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anthology/D19-1253.\\n- [68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\\n\\n# Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n# A Implementation Details\\n\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\n\\n# B Human Evaluation\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\".\\n\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\\n\\n# C Training setup Details\\n\\nWe train all RAG models and BART baselines using Fairseq [45]. We train with mixed precision floating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring  $\\\\sim 100$  GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers [66], which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\\'s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\\n\\nD Further Details on Open-Domain QA\\n\\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each $(q,a)$ pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\\n\\n#### CuratedTrec preprocessing\\n\\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models *[20]*. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\n\\n#### TriviaQA Evaluation setups\\n\\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR *[26]*, which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. *[52]* used the TriviaQA official Wikipedia test set instead. Févry et al. *[14]* follow this convention in order to compare with Roberts et al. *[52]* (See appendix of *[14]*). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\\n\\n## Appendix E Further Details on FEVER\\n\\nFor FEVER classification, we follow the practice from *[32]*, and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\\n\\n## Appendix F Null Document Probabilities\\n\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM *[20]* in order to model cases where no useful information could be retrieved for a given input. Here, if $k$ documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over $k+1$ predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\\n\\n## Appendix G Parameters\\n\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n\\nTable 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\n\\n|  Task | Train | Development | Test  |\\n| --- | --- | --- | --- |\\n|  Natural Questions | 79169 | 8758 | 3611  |\\n|  TriviaQA | 78786 | 8838 | 11314  |\\n|  WebQuestions | 3418 | 362 | 2033  |\\n|  CuratedTrec | 635 | 134 | 635  |\\n|  Jeopardy Question Generation | 97392 | 13714 | 26849  |\\n|  MS-MARCO | 153726 | 12468 | 101093*  |\\n|  FEVER-3-way | 145450 | 10000 | 10000  |\\n|  FEVER-2-way | 96966 | 6666 | 6666  |\\n\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consist of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating point precision to manage memory and disk footprints.\\n\\n# H Retrieval Collapse\\n\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would \"collapse\" and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.\\n\\n# I Number of instances per dataset\\n\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.'), 0.08310812328744777), (Document(id='ac66f9901c89:0', metadata={'start_line': 1, 'chunk_index': 0, 'doc_id': 'ac66f9901c89', 'text': '# Attention Is All You Need\\n\\nAshish Vaswani*\\n\\nGoogle Brain\\n\\navaswani@google.com\\n\\nNoam Shazeer*\\n\\nGoogle Brain\\n\\nnoam@google.com\\n\\nNiki Parmar*\\n\\nGoogle Research\\n\\nnikip@google.com\\n\\nJakob Uszkoreit*\\n\\nGoogle Research\\n\\nusz@google.com\\n\\nLlion Jones*\\n\\nGoogle Research\\n\\nllion@google.com\\n\\nAidan N. Gomez*†\\n\\nUniversity of Toronto\\n\\naidan@cs.toronto.edu\\n\\nŁukasz Kaiser*\\n\\nGoogle Brain\\n\\nlukaszkaiser@google.com\\n\\nIllia Polosukhin*‡\\n\\nillia.polosukhin@gmail.com\\n\\n# Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n1 Introduction\\n\\nRecurrent neural networks, long short-term memory *[13]* and gated recurrent *[7]* neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation *[35, 2, 5]*. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures *[38, 24, 15]*.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_{t}$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks *[21]* and conditional computation *[32]*, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences *[2, 19]*. In all but a few cases *[27]*, however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n## 2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU *[16]*, ByteNet *[18]* and ConvS2S *[9]*, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions *[12]*. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations *[4, 27, 28, 22]*.\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks *[34]*.\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as *[17, 18]* and *[9]*.\\n\\n## 3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure *[5, 2, 35]*. Here, the encoder maps an input sequence of symbol representations $(x_{1},...,x_{n})$ to a sequence of continuous representations $\\\\mathbf{z}=(z_{1},...,z_{n})$. Given $\\\\mathbf{z}$, the decoder then generates an output sequence $(y_{1},...,y_{m})$ of symbols one element at a time. At each step the model is auto-regressive *[10]*, consuming the previously generated symbols as additional input when generating the next.\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n# 3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of  $N = 6$  identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm  $(x + \\\\text{Sublayer}(x))$ , where Sublayer  $(x)$  is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension  $d_{\\\\text{model}} = 512$ .\\n\\nDecoder: The decoder is also composed of a stack of  $N = 6$  identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position  $i$  can depend only on the known outputs at positions less than  $i$ .\\n\\n# 3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n![img-1.jpeg](img-1.jpeg)\\nScaled Dot-Product Attention\\n\\n![img-2.jpeg](img-2.jpeg)\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n# 3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension  $d_{k}$ , and values of dimension  $d_{v}$ . We compute the dot products of the query with all keys, divide each by  $\\\\sqrt{d_k}$ , and apply a softmax function to obtain the weights on the values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix  $Q$ . The keys and values are also packed together into matrices  $K$  and  $V$ . We compute the matrix of outputs as:\\n\\n$$\\n\\\\operatorname {A t t e n t i o n} (Q, K, V) = \\\\operatorname {s o f t m a x} \\\\left(\\\\frac {Q K ^ {T}}{\\\\sqrt {d _ {k}}}\\\\right) V \\\\tag {1}\\n$$\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of  $\\\\frac{1}{\\\\sqrt{d_k}}$ . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of  $d_{k}$  the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of  $d_{k}$  [3]. We suspect that for large values of  $d_{k}$ , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by  $\\\\frac{1}{\\\\sqrt{d_k}}$ .\\n\\n# 3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with  $d_{\\\\mathrm{model}}$ -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values  $h$  times with different, learned linear projections to  $d_k$ ,  $d_k$  and  $d_v$  dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding  $d_v$ -dimensional\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\n$\\\\mathrm{MultiHead}(Q,K,V)$ $=\\\\mathrm{Concat}(\\\\mathrm{head}_{1},...,\\\\mathrm{head}_{\\\\mathrm{h}})W^{O}$\\n$\\\\mathrm{where\\\\ head_{i}}$ $=\\\\mathrm{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$\\n\\nWhere the projections are parameter matrices $W_{i}^{Q}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{K}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{V}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{v}}$ and $W^{O}\\\\in\\\\mathbb{R}^{hd_{v}\\\\times d_{\\\\mathrm{model}}}$.\\n\\nIn this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_{k}=d_{v}=d_{\\\\mathrm{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n#### 3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as *[38, 2, 9]*.\\n- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $-\\\\infty$) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n### 3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\n$\\\\mathrm{FFN}(x)=\\\\max(0,xW_{1}+b_{1})W_{2}+b_{2}$ (2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{\\\\mathrm{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$.\\n\\n### 3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\\\mathrm{model}}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to *[30]*. In the embedding layers, we multiply those weights by $\\\\sqrt{d_{\\\\mathrm{model}}}$.\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.  $n$  is the sequence length,  $d$  is the representation dimension,  $k$  is the kernel size of convolutions and  $r$  the size of the neighborhood in restricted self-attention.\\n\\n|  Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length  |\\n| --- | --- | --- | --- |\\n|  Self-Attention | O(n2·d) | O(1) | O(1)  |\\n|  Recurrent | O(n·d2) | O(n) | O(n)  |\\n|  Convolutional | O(k·n·d2) | O(1) | O(logk(n))  |\\n|  Self-Attention (restricted) | O(r·n·d) | O(1) | O(n/r)  |\\n\\n# 3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension  $d_{\\\\mathrm{model}}$  as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\n$$\\nP E _ {(p o s, 2 i)} = \\\\sin (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\n$$\\nP E _ {(p o s, 2 i + 1)} = \\\\cos (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\nwhere  $pos$  is the position and  $i$  is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from  $2\\\\pi$  to  $10000 \\\\cdot 2\\\\pi$ . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset  $k$ ,  $PE_{pos+k}$  can be represented as a linear function of  $PE_{pos}$ .\\n\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n# 4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations  $(x_{1},\\\\dots,x_{n})$  to another sequence of equal length  $(z_{1},\\\\dots,z_{n})$ , with  $x_{i},z_{i}\\\\in \\\\mathbb{R}^{d}$ , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires  $O(n)$  sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\\n\\nlength $n$ is smaller than the representation dimensionality $d$, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece *[38]* and byte-pair *[31]* representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $r$ in the input sequence centered around the respective output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width $k<n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_{k}(n))$ in the case of dilated convolutions *[18]*, increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of $k$. Separable convolutions *[6]*, however, decrease the complexity considerably, to $O(k\\\\cdot n\\\\cdot d+n\\\\cdot d^{2})$. Even with $k=n$, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n## 5 Training\\n\\nThis section describes the training regime for our models.\\n\\n### 5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding *[3]*, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary *[38]*. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n### 5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n### 5.3 Optimizer\\n\\nWe used the Adam optimizer *[20]* with $\\\\beta_{1}=0.9$, $\\\\beta_{2}=0.98$ and $\\\\epsilon=10^{-9}$. We varied the learning rate over the course of training, according to the formula:\\n\\n$lrate=d_{\\\\text{model}}^{-0.5}\\\\cdot\\\\min(step\\\\_num^{-0.5},step\\\\_num\\\\cdot warmup\\\\_steps^{-1.5})$ (3)\\n\\nThis corresponds to increasing the learning rate linearly for the first $warmup\\\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used $warmup\\\\_steps=4000$.\\n\\n### 5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\n|  Model | BLEU |   | Training Cost (FLOPs)  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  EN-DE | EN-FR | EN-DE | EN-FR  |\\n|  ByteNet [18] | 23.75 |  |  |   |\\n|  Deep-Att + PosUnk [39] |  | 39.2 |  | 1.0 · 1020  |\\n|  GNMT + RL [38] | 24.6 | 39.92 | 2.3 · 1019 | 1.4 · 1020  |\\n|  ConvS2S [9] | 25.16 | 40.46 | 9.6 · 1018 | 1.5 · 1020  |\\n|  MoE [32] | 26.03 | 40.56 | 2.0 · 1019 | 1.2 · 1020  |\\n|  Deep-Att + PosUnk Ensemble [39] |  | 40.4 |  | 8.0 · 1020  |\\n|  GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 · 1020 | 1.1 · 1021  |\\n|  ConvS2S Ensemble [9] | 26.36 | 41.29 | 7.7 · 1019 | 1.2 · 1021  |\\n|  Transformer (base model) | 27.3 | 38.1 | 3.3 · 1018  |   |\\n|  Transformer (big) | 28.4 | 41.8 | 2.3 · 1019  |   |\\n\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of  $P_{drop} = 0.1$ .\\n\\nLabel Smoothing During training, we employed label smoothing of value  $\\\\epsilon_{ls} = 0.1$  [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n# 6 Results\\n\\n# 6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than  $1/4$  the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate  $P_{drop} = 0.1$ , instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty  $\\\\alpha = 0.6$  [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length  $+50$ , but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each  $\\\\mathrm{GPU}^5$ .\\n\\n# 6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\n|   | N | dmodel | dff | h | dk | dv | Pdrop | εls | train steps | PPL (dev) | BLEU (dev) | params ×106  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65  |\\n|  (A) |  |  |  | 1 | 512 | 512 |  |  |  | 5.29 | 24.9 |   |\\n|   |   |  |  | 4 | 128 | 128 |  |  |  | 5.00 | 25.5 |   |\\n|   |   |  |  | 16 | 32 | 32 |  |  |  | 4.91 | 25.8 |   |\\n|   |   |  |  | 32 | 16 | 16 |  |  |  | 5.01 | 25.4 |   |\\n|  (B) |  |  |  |  | 16 |  |  |  |  | 5.16 | 25.1 | 58  |\\n|   |   |  |  |  | 32 |  |  |  |  | 5.01 | 25.4 | 60  |\\n|  (C) | 2 |  |  |  |  |  |  |  |  | 6.11 | 23.7 | 36  |\\n|   |  4 |  |  |  |  |  |  |  |  | 5.19 | 25.3 | 50  |\\n|   |  8 |  |  |  |  |  |  |  |  | 4.88 | 25.5 | 80  |\\n|   |   | 256 |  |  | 32 | 32 |  |  |  | 5.75 | 24.5 | 28  |\\n|   |   | 1024 |  |  | 128 | 128 |  |  |  | 4.66 | 26.0 | 168  |\\n|   |   |  | 1024 |  |  |  |  |  |  | 5.12 | 25.4 | 53  |\\n|   |   |  | 4096 |  |  |  |  |  |  | 4.75 | 26.2 | 90  |\\n|  (D) |  |  |  |  |  |  | 0.0 |  |  | 5.77 | 24.6 |   |\\n|   |   |  |  |  |  |  | 0.2 |  |  | 4.95 | 25.5 |   |\\n|   |   |  |  |  |  |  |  | 0.0 |  | 4.67 | 25.3 |   |\\n|   |   |  |  |  |  |  |  | 0.2 |  | 5.47 | 25.7 |   |\\n|  (E) | positional embedding instead of sinusoids |   |   |   |   |   |   |   |   | 4.92 | 25.7 |   |\\n|  big | 6 | 1024 | 4096 | 16 |  |  | 0.3 |  | 300K | 4.33 | 26.4 | 213  |\\n\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\\n\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size  $d_{k}$  hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\\n\\n# 6.3 English Constituency Parsing\\n\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\\n\\nWe trained a 4-layer transformer with  $d_{model} = 1024$  on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\\n\\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\\n\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\\n\\n|  Parser | Training | WSJ 23 F1  |\\n| --- | --- | --- |\\n|  Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3  |\\n|  Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4  |\\n|  Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4  |\\n|  Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7  |\\n|  Transformer (4 layers) | WSJ only, discriminative | 91.3  |\\n|  Zhu et al. (2013) [40] | semi-supervised | 91.3  |\\n|  Huang & Harper (2009) [14] | semi-supervised | 91.3  |\\n|  McClosky et al. (2006) [26] | semi-supervised | 92.1  |\\n|  Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1  |\\n|  Transformer (4 layers) | semi-supervised | 92.7  |\\n|  Luong et al. (2015) [23] | multi-task | 93.0  |\\n|  Dyer et al. (2016) [8] | generative | 93.3  |\\n\\nincreased the maximum output length to input length  $+300$ . We used a beam size of 21 and  $\\\\alpha = 0.3$  for both WSJ only and the semi-supervised setting.\\n\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\\n\\n# 7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\n# References\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\\n- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\\n- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n- [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\n- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\\n- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\\n- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\\n- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\\n- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n- [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n- [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\\n- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n- [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\\n- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\\n- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\\n- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\\n- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n- [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n\\n# Attention Visualizations\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \\'making\\', completing the phrase \\'making...more difficult\\'. Attentions here shown only for the word \\'making\\'. Different colors represent different heads. Best viewed in color.\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \\'its\\' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n![img-5.jpeg](img-5.jpeg)\\n\\n![img-6.jpeg](img-6.jpeg)\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.', 'end_line': 385}, page_content='# Attention Is All You Need\\n\\nAshish Vaswani*\\n\\nGoogle Brain\\n\\navaswani@google.com\\n\\nNoam Shazeer*\\n\\nGoogle Brain\\n\\nnoam@google.com\\n\\nNiki Parmar*\\n\\nGoogle Research\\n\\nnikip@google.com\\n\\nJakob Uszkoreit*\\n\\nGoogle Research\\n\\nusz@google.com\\n\\nLlion Jones*\\n\\nGoogle Research\\n\\nllion@google.com\\n\\nAidan N. Gomez*†\\n\\nUniversity of Toronto\\n\\naidan@cs.toronto.edu\\n\\nŁukasz Kaiser*\\n\\nGoogle Brain\\n\\nlukaszkaiser@google.com\\n\\nIllia Polosukhin*‡\\n\\nillia.polosukhin@gmail.com\\n\\n# Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n1 Introduction\\n\\nRecurrent neural networks, long short-term memory *[13]* and gated recurrent *[7]* neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation *[35, 2, 5]*. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures *[38, 24, 15]*.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_{t}$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks *[21]* and conditional computation *[32]*, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences *[2, 19]*. In all but a few cases *[27]*, however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n## 2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU *[16]*, ByteNet *[18]* and ConvS2S *[9]*, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions *[12]*. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations *[4, 27, 28, 22]*.\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks *[34]*.\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as *[17, 18]* and *[9]*.\\n\\n## 3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure *[5, 2, 35]*. Here, the encoder maps an input sequence of symbol representations $(x_{1},...,x_{n})$ to a sequence of continuous representations $\\\\mathbf{z}=(z_{1},...,z_{n})$. Given $\\\\mathbf{z}$, the decoder then generates an output sequence $(y_{1},...,y_{m})$ of symbols one element at a time. At each step the model is auto-regressive *[10]*, consuming the previously generated symbols as additional input when generating the next.\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n# 3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of  $N = 6$  identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm  $(x + \\\\text{Sublayer}(x))$ , where Sublayer  $(x)$  is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension  $d_{\\\\text{model}} = 512$ .\\n\\nDecoder: The decoder is also composed of a stack of  $N = 6$  identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position  $i$  can depend only on the known outputs at positions less than  $i$ .\\n\\n# 3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n![img-1.jpeg](img-1.jpeg)\\nScaled Dot-Product Attention\\n\\n![img-2.jpeg](img-2.jpeg)\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n# 3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension  $d_{k}$ , and values of dimension  $d_{v}$ . We compute the dot products of the query with all keys, divide each by  $\\\\sqrt{d_k}$ , and apply a softmax function to obtain the weights on the values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix  $Q$ . The keys and values are also packed together into matrices  $K$  and  $V$ . We compute the matrix of outputs as:\\n\\n$$\\n\\\\operatorname {A t t e n t i o n} (Q, K, V) = \\\\operatorname {s o f t m a x} \\\\left(\\\\frac {Q K ^ {T}}{\\\\sqrt {d _ {k}}}\\\\right) V \\\\tag {1}\\n$$\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of  $\\\\frac{1}{\\\\sqrt{d_k}}$ . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of  $d_{k}$  the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of  $d_{k}$  [3]. We suspect that for large values of  $d_{k}$ , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by  $\\\\frac{1}{\\\\sqrt{d_k}}$ .\\n\\n# 3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with  $d_{\\\\mathrm{model}}$ -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values  $h$  times with different, learned linear projections to  $d_k$ ,  $d_k$  and  $d_v$  dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding  $d_v$ -dimensional\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\n$\\\\mathrm{MultiHead}(Q,K,V)$ $=\\\\mathrm{Concat}(\\\\mathrm{head}_{1},...,\\\\mathrm{head}_{\\\\mathrm{h}})W^{O}$\\n$\\\\mathrm{where\\\\ head_{i}}$ $=\\\\mathrm{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$\\n\\nWhere the projections are parameter matrices $W_{i}^{Q}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{K}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{V}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{v}}$ and $W^{O}\\\\in\\\\mathbb{R}^{hd_{v}\\\\times d_{\\\\mathrm{model}}}$.\\n\\nIn this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_{k}=d_{v}=d_{\\\\mathrm{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n#### 3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as *[38, 2, 9]*.\\n- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $-\\\\infty$) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n### 3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\n$\\\\mathrm{FFN}(x)=\\\\max(0,xW_{1}+b_{1})W_{2}+b_{2}$ (2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{\\\\mathrm{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$.\\n\\n### 3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\\\mathrm{model}}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to *[30]*. In the embedding layers, we multiply those weights by $\\\\sqrt{d_{\\\\mathrm{model}}}$.\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.  $n$  is the sequence length,  $d$  is the representation dimension,  $k$  is the kernel size of convolutions and  $r$  the size of the neighborhood in restricted self-attention.\\n\\n|  Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length  |\\n| --- | --- | --- | --- |\\n|  Self-Attention | O(n2·d) | O(1) | O(1)  |\\n|  Recurrent | O(n·d2) | O(n) | O(n)  |\\n|  Convolutional | O(k·n·d2) | O(1) | O(logk(n))  |\\n|  Self-Attention (restricted) | O(r·n·d) | O(1) | O(n/r)  |\\n\\n# 3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension  $d_{\\\\mathrm{model}}$  as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\n$$\\nP E _ {(p o s, 2 i)} = \\\\sin (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\n$$\\nP E _ {(p o s, 2 i + 1)} = \\\\cos (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\nwhere  $pos$  is the position and  $i$  is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from  $2\\\\pi$  to  $10000 \\\\cdot 2\\\\pi$ . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset  $k$ ,  $PE_{pos+k}$  can be represented as a linear function of  $PE_{pos}$ .\\n\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n# 4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations  $(x_{1},\\\\dots,x_{n})$  to another sequence of equal length  $(z_{1},\\\\dots,z_{n})$ , with  $x_{i},z_{i}\\\\in \\\\mathbb{R}^{d}$ , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires  $O(n)$  sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\\n\\nlength $n$ is smaller than the representation dimensionality $d$, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece *[38]* and byte-pair *[31]* representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $r$ in the input sequence centered around the respective output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width $k<n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_{k}(n))$ in the case of dilated convolutions *[18]*, increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of $k$. Separable convolutions *[6]*, however, decrease the complexity considerably, to $O(k\\\\cdot n\\\\cdot d+n\\\\cdot d^{2})$. Even with $k=n$, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n## 5 Training\\n\\nThis section describes the training regime for our models.\\n\\n### 5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding *[3]*, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary *[38]*. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n### 5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n### 5.3 Optimizer\\n\\nWe used the Adam optimizer *[20]* with $\\\\beta_{1}=0.9$, $\\\\beta_{2}=0.98$ and $\\\\epsilon=10^{-9}$. We varied the learning rate over the course of training, according to the formula:\\n\\n$lrate=d_{\\\\text{model}}^{-0.5}\\\\cdot\\\\min(step\\\\_num^{-0.5},step\\\\_num\\\\cdot warmup\\\\_steps^{-1.5})$ (3)\\n\\nThis corresponds to increasing the learning rate linearly for the first $warmup\\\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used $warmup\\\\_steps=4000$.\\n\\n### 5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\n|  Model | BLEU |   | Training Cost (FLOPs)  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  EN-DE | EN-FR | EN-DE | EN-FR  |\\n|  ByteNet [18] | 23.75 |  |  |   |\\n|  Deep-Att + PosUnk [39] |  | 39.2 |  | 1.0 · 1020  |\\n|  GNMT + RL [38] | 24.6 | 39.92 | 2.3 · 1019 | 1.4 · 1020  |\\n|  ConvS2S [9] | 25.16 | 40.46 | 9.6 · 1018 | 1.5 · 1020  |\\n|  MoE [32] | 26.03 | 40.56 | 2.0 · 1019 | 1.2 · 1020  |\\n|  Deep-Att + PosUnk Ensemble [39] |  | 40.4 |  | 8.0 · 1020  |\\n|  GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 · 1020 | 1.1 · 1021  |\\n|  ConvS2S Ensemble [9] | 26.36 | 41.29 | 7.7 · 1019 | 1.2 · 1021  |\\n|  Transformer (base model) | 27.3 | 38.1 | 3.3 · 1018  |   |\\n|  Transformer (big) | 28.4 | 41.8 | 2.3 · 1019  |   |\\n\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of  $P_{drop} = 0.1$ .\\n\\nLabel Smoothing During training, we employed label smoothing of value  $\\\\epsilon_{ls} = 0.1$  [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n# 6 Results\\n\\n# 6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than  $1/4$  the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate  $P_{drop} = 0.1$ , instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty  $\\\\alpha = 0.6$  [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length  $+50$ , but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each  $\\\\mathrm{GPU}^5$ .\\n\\n# 6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\n|   | N | dmodel | dff | h | dk | dv | Pdrop | εls | train steps | PPL (dev) | BLEU (dev) | params ×106  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65  |\\n|  (A) |  |  |  | 1 | 512 | 512 |  |  |  | 5.29 | 24.9 |   |\\n|   |   |  |  | 4 | 128 | 128 |  |  |  | 5.00 | 25.5 |   |\\n|   |   |  |  | 16 | 32 | 32 |  |  |  | 4.91 | 25.8 |   |\\n|   |   |  |  | 32 | 16 | 16 |  |  |  | 5.01 | 25.4 |   |\\n|  (B) |  |  |  |  | 16 |  |  |  |  | 5.16 | 25.1 | 58  |\\n|   |   |  |  |  | 32 |  |  |  |  | 5.01 | 25.4 | 60  |\\n|  (C) | 2 |  |  |  |  |  |  |  |  | 6.11 | 23.7 | 36  |\\n|   |  4 |  |  |  |  |  |  |  |  | 5.19 | 25.3 | 50  |\\n|   |  8 |  |  |  |  |  |  |  |  | 4.88 | 25.5 | 80  |\\n|   |   | 256 |  |  | 32 | 32 |  |  |  | 5.75 | 24.5 | 28  |\\n|   |   | 1024 |  |  | 128 | 128 |  |  |  | 4.66 | 26.0 | 168  |\\n|   |   |  | 1024 |  |  |  |  |  |  | 5.12 | 25.4 | 53  |\\n|   |   |  | 4096 |  |  |  |  |  |  | 4.75 | 26.2 | 90  |\\n|  (D) |  |  |  |  |  |  | 0.0 |  |  | 5.77 | 24.6 |   |\\n|   |   |  |  |  |  |  | 0.2 |  |  | 4.95 | 25.5 |   |\\n|   |   |  |  |  |  |  |  | 0.0 |  | 4.67 | 25.3 |   |\\n|   |   |  |  |  |  |  |  | 0.2 |  | 5.47 | 25.7 |   |\\n|  (E) | positional embedding instead of sinusoids |   |   |   |   |   |   |   |   | 4.92 | 25.7 |   |\\n|  big | 6 | 1024 | 4096 | 16 |  |  | 0.3 |  | 300K | 4.33 | 26.4 | 213  |\\n\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\\n\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size  $d_{k}$  hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\\n\\n# 6.3 English Constituency Parsing\\n\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\\n\\nWe trained a 4-layer transformer with  $d_{model} = 1024$  on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\\n\\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\\n\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\\n\\n|  Parser | Training | WSJ 23 F1  |\\n| --- | --- | --- |\\n|  Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3  |\\n|  Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4  |\\n|  Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4  |\\n|  Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7  |\\n|  Transformer (4 layers) | WSJ only, discriminative | 91.3  |\\n|  Zhu et al. (2013) [40] | semi-supervised | 91.3  |\\n|  Huang & Harper (2009) [14] | semi-supervised | 91.3  |\\n|  McClosky et al. (2006) [26] | semi-supervised | 92.1  |\\n|  Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1  |\\n|  Transformer (4 layers) | semi-supervised | 92.7  |\\n|  Luong et al. (2015) [23] | multi-task | 93.0  |\\n|  Dyer et al. (2016) [8] | generative | 93.3  |\\n\\nincreased the maximum output length to input length  $+300$ . We used a beam size of 21 and  $\\\\alpha = 0.3$  for both WSJ only and the semi-supervised setting.\\n\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\\n\\n# 7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\n# References\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\\n- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\\n- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n- [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\n- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\\n- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\\n- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\\n- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\\n- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n- [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n- [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\\n- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n- [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\\n- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\\n- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\\n- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\\n- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n- [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n\\n# Attention Visualizations\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \\'making\\', completing the phrase \\'making...more difficult\\'. Attentions here shown only for the word \\'making\\'. Different colors represent different heads. Best viewed in color.\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \\'its\\' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n![img-5.jpeg](img-5.jpeg)\\n\\n![img-6.jpeg](img-6.jpeg)\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.'), 0.04207799767469833), (Document(id='6525a2245e91:0', metadata={'end_line': 750, 'chunk_index': 0, 'start_line': 1, 'text': '# Language Models are Few-Shot Learners\\n\\n|  Tom B. Brown* |   | Benjamin Mann* |   | Nick Ryder* |   | Melanie Subbiah*  |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Jared Kaplan† | Prafulla Dhariwal | Arvind Neelakantan | Pranav Shyam | Girish Sastry |  |  |   |\\n|  Amanda Askell | Sandhini Agarwal | Ariel Herbert-Voss | Gretchen Krueger | Tom Henighan |  |  |   |\\n|  Rewon Child | Aditya Ramesh | Daniel M. Ziegler | Jeffrey Wu | Clemens Winter |  |  |   |\\n|  Christopher Hesse | Mark Chen | Eric Sigler | Mateusz Litwin | Scott Gray |  |  |   |\\n|  Benjamin Chess |   | Jack Clark |   | Christopher Berner |   |  |   |\\n|  Sam McCandlish |   | Alec Radford | Ilya Sutskever | Dario Amodei |   |  |   |\\n\\nOpenAI\\n\\n# Abstract\\n\\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters,  $10\\\\mathrm{x}$  more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\\'s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\\n\\nAuthor contributions listed at end of paper.\\n\\n2\\n\\n# Contents\\n\\n1  Introduction  3\\n2  Approach  6\\n2.1  Model and Architectures  8\\n2.2  Training Dataset  8\\n2.3  Training Process  9\\n2.4  Evaluation  10\\n3  Results  10\\n3.1  Language Modeling, Cloze, and Completion Tasks  11\\n3.2  Closed Book Question Answering  13\\n3.3  Translation  14\\n3.4  Winograd-Style Tasks  16\\n3.5  Common Sense Reasoning  17\\n3.6  Reading Comprehension  18\\n3.7  SuperGLUE  18\\n3.8  NLI  20\\n3.9  Synthetic and Qualitative Tasks  21\\n4  Measuring and Preventing Memorization Of Benchmarks  29\\n5  Limitations  33\\n6  Broader Impacts  34\\n6.1  Misuse of Language Models  35\\n6.2  Fairness, Bias, and Representation  36\\n6.3  Energy Usage  39\\n7  Related Work  39\\n8  Conclusion  40\\nA  Details of Common Crawl Filtering  43\\nB  Details of Model Training  43\\nC  Details of Test Set Contamination Studies  43\\nD  Total Compute Used to Train Language Models  46\\nE  Human Quality Assessment of Synthetic News Articles  46\\nF  Additional Samples from GPT-3  48\\nG  Details of Task Phrasing and Specifications  50\\nH  Results on All Tasks for All Model Sizes  63\\n\\n# 1 Introduction\\n\\nRecent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models  $\\\\left[\\\\mathrm{VSP}^{+}17\\\\right]$  have been directly fine-tuned, entirely removing the need for task-specific architectures [RNSS18, DCLT18, HR18].\\n\\nThis last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR $^{+}$ 19, LOG $^{+}$ 19, YDY $^{+}$ 19, LCG $^{+}$ 19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Removing this limitation would be desirable, for several reasons.\\n\\nFirst, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.\\n\\nSecond, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance  $\\\\left[\\\\mathrm{HLW}^{+}20\\\\right]$  observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it  $\\\\left[\\\\mathrm{YdC}^{+}19,\\\\mathrm{MPL}19\\\\right]$ . Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task  $\\\\left[\\\\mathrm{GSL}^{+}18,\\\\mathrm{NK}19\\\\right]$ .\\n\\nThird, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term \"in-context learning\" to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 1.2: Larger models make increasingly efficient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper \"in-context learning curves\" for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks.\\n\\nsufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.\\n\\nOne potential route towards addressing these issues is meta-learning $^{1}$  – which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.\\n\\nWhile it has shown some initial promise, this approach still achieves results far inferior to fine-tuning – for example  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  achieves only  $4\\\\%$  on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.\\n\\nAnother recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters [DCLT18], to 1.5 billion parameters  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , to 8 billion parameters  $\\\\left[\\\\mathrm{SPP}^{+}19\\\\right]$ , 11 billion parameters  $\\\\left[\\\\mathrm{RSR}^{+}19\\\\right]$ , and finally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$ . Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.\\n\\nIn this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) \"few-shot learning\", or in-context learning where we allow as many demonstrations as will fit into the model\\'s context window (typically 10 to 100), (b) \"one-shot learning\", where we allow only one demonstration, and (c) \"zero-shot\" learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.\\n\\nFigure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model\\'s context,  $K$ . Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these \"learning\" curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.\\n\\nBroadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves  $64.3\\\\%$  accuracy on TriviaQA in the zero-shot setting,  $68.0\\\\%$  in the one-shot setting, and  $71.2\\\\%$  in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.\\n\\nGPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.\\n\\nAt the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3\\'s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.\\n\\nA heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).\\n\\nWe also undertake a systematic study of “data contamination” – a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we find that data contamination has a minimal effect on GPT-3’s performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.\\n\\nIn addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.\\n\\nFinally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard.\\n\\nThe remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes.\\n\\n## 2 Approach\\n\\nOur basic pre-training approach, including model, data, and training, is similar to the process described in *[RWC+19]*, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to *[RWC+19]*, but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on. Specifically, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration):\\n\\n- Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution *[x13]*, and the potential to exploit spurious features of the training data *[GSL+18, x15]*, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work.\\n- Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning *[RWC+19]*, but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion. We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model’s context window ($n_{\\\\mathrm{ctx}}=2048$). The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models. Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML *[x10, VBL+16]* – both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task.\\n- One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans. For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task. By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given.\\n\\nThe three settings we explore for in-context learning\\n\\n# Zero-shot\\n\\nThe model predicts the answer given only a natural language description of the task. No gradient updates are performed.\\n\\n![img-3.jpeg](img-3.jpeg)\\n\\n# One-shot\\n\\nIn addition to the task description, the model sees a single example of the task. No gradient updates are performed.\\n\\n![img-4.jpeg](img-4.jpeg)\\n\\n# Few-shot\\n\\nIn addition to the task description, the model sees a few examples of the task. No gradient updates are performed.\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning. The panels above show four methods for performing a task with a language model – fine-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task descriptions, examples and prompts can be found in Appendix G.\\n\\n# Traditional fine-tuning (not used for GPT-3)\\n\\n# Fine-tuning\\n\\nThe model is trained via repeated gradient updates using a large corpus of example tasks.\\n\\n![img-6.jpeg](img-6.jpeg)\\n\\n- Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task. This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of pre-training data), but is also the most challenging setting. In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases \"unfairly hard\". For example, if someone is asked to \"make a table of world records for the 200m dash\", this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult). Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks – for example, in the translation example in Figure 2.1, a human would likely know what to do from just the text instruction.\\n\\nFigure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.\\n\\nSections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.\\n\\n|  Model Name | nparams | nlayers | dmodel | nheads | dhead | Batch Size | Learning Rate  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Small | 125M | 12 | 768 | 12 | 64 | 0.5M | 6.0 × 10-4  |\\n|  GPT-3 Medium | 350M | 24 | 1024 | 16 | 64 | 0.5M | 3.0 × 10-4  |\\n|  GPT-3 Large | 760M | 24 | 1536 | 16 | 96 | 0.5M | 2.5 × 10-4  |\\n|  GPT-3 XL | 1.3B | 24 | 2048 | 24 | 128 | 1M | 2.0 × 10-4  |\\n|  GPT-3 2.7B | 2.7B | 32 | 2560 | 32 | 80 | 1M | 1.6 × 10-4  |\\n|  GPT-3 6.7B | 6.7B | 32 | 4096 | 32 | 128 | 2M | 1.2 × 10-4  |\\n|  GPT-3 13B | 13.0B | 40 | 5140 | 40 | 128 | 2M | 1.0 × 10-4  |\\n|  GPT-3 175B or “GPT-3” | 175.0B | 96 | 12288 | 96 | 128 | 3.2M | 0.6 × 10-4  |\\n\\nTable 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens.\\n\\n# 2.1 Model and Architectures\\n\\nWe use the same model and architecture as GPT-2  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.\\n\\nTable 2.1 shows the sizes and architectures of our 8 models. Here  $n_{\\\\mathrm{params}}$  is the total number of trainable parameters,  $n_{\\\\mathrm{layers}}$  is the total number of layers,  $d_{\\\\mathrm{model}}$  is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer,  $d_{\\\\mathrm{ff}} = 4 * d_{\\\\mathrm{model}}$ ), and  $d_{\\\\mathrm{head}}$  is the dimension of each attention head. All models use a context window of  $n_{\\\\mathrm{ctx}} = 2048$  tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\\'s. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.\\n\\n# 2.2 Training Dataset\\n\\nDatasets for language models have rapidly expanded, culminating in the Common Crawl dataset $^2$  [RSR $^{+}$ 19] constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy dedduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.\\n\\nDetails of the first two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , collected by scraping links over a longer period of time, and first described in  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$ , two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.\\n\\nTable 2.2 shows the final mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.\\n\\n![img-7.jpeg](img-7.jpeg)\\nTotal Compute Used During Training\\nFigure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B is almost  $10\\\\mathrm{x}$  larger than RoBERTa-Large (355M params), both models took roughly 50 petaflop/s-days of compute during pre-training. Methodology for these calculations can be found in Appendix D.\\n\\n|  Dataset | Quantity (tokens) | Weight in training mix | Epochs elapsed when training for 300B tokens  |\\n| --- | --- | --- | --- |\\n|  Common Crawl (filtered) | 410 billion | 60% | 0.44  |\\n|  WebText2 | 19 billion | 22% | 2.9  |\\n|  Books1 | 12 billion | 8% | 1.9  |\\n|  Books2 | 55 billion | 8% | 0.43  |\\n|  Wikipedia | 3 billion | 3% | 3.4  |\\n\\nTable 2.2: Datasets used to train GPT-3. \"Weight in training mix\" refers to the fraction of examples during training that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once.\\n\\nA major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.\\n\\n# 2.3 Training Process\\n\\nAs found in  $\\\\left[\\\\mathrm{KMH}^{+}20, \\\\mathrm{MKAT18}\\\\right]$ , larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU\\'s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B.\\n\\n2.4 Evaluation\\n\\nFor few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.\\n\\n$K$ can be any value from 0 to the maximum amount allowed by the model’s context window, which is $n_{\\\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples. Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.\\n\\nOn tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\\\frac{P(\\\\mathrm{completion}|\\\\mathrm{context})}{P(\\\\mathrm{completion}|\\\\mathrm{answer\\\\_context})}$, where $\\\\mathrm{answer\\\\_context}$ is the string \"Answer: \" or \"A: \" and is used to prompt that the completion should be an answer but is otherwise generic.\\n\\nOn tasks that involve binary classification, we give the options more semantically meaningful names (e.g. “True” or “False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by *[RSR^{+}19]* (see Appendix G) for details.\\n\\nOn tasks with free-form completion, we use beam search with the same parameters as *[RSR^{+}19]*: a beam width of 4 and a length penalty of $\\\\alpha=0.6$. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.\\n\\nFinal results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.\\n\\n## 3 Results\\n\\nIn Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in *[KMH^{+}20]*, language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.\\n\\nBelow, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks.\\n\\nIn Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on “closed book” question answering tasks: tasks which require using the information stored in the model’s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model’s ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model’s performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we briefly explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities – these tasks focus on on-the-fly reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings.\\n\\n![img-8.jpeg](img-8.jpeg)\\nFigure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts.\\n\\n|  Setting | PTB  |\\n| --- | --- |\\n|  SOTA (Zero-Shot) | 35.8a  |\\n|  GPT-3 Zero-Shot | 20.5  |\\n\\nTable 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3\\'s training data.  ${}^{a}\\\\left\\\\lbrack  {\\\\mathrm{{RWC}}}^{ + }{19}\\\\right\\\\rbrack$\\n\\n# 3.1 Language Modeling, Cloze, and Completion Tasks\\n\\nIn this section we test GPT-3\\'s performance on the traditional task of language modeling, as well as related tasks that involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible completions of a piece of text.\\n\\n# 3.1.1 Language Modeling\\n\\nWe calculate zero-shot perplexity on the Penn Tree Bank (PTB)  $\\\\left[\\\\mathrm{MKM}^{+}94\\\\right]$  dataset measured in  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . We omit the 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the one-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these issues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15 points, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to define one-shot or few-shot evaluation around, so we measure only zero-shot.\\n\\n# 3.1.2 LAMBADA\\n\\nThe LAMBADA dataset  $\\\\left[\\\\mathrm{PKL}^{+}16\\\\right]$  tests the modeling of long-range dependencies in text - the model is asked to predict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the continued scaling of language models is yielding diminishing returns on this difficult benchmark.  $\\\\left[\\\\mathrm{BHT}^{+}20\\\\right]$  reflect on the small  $1.5\\\\%$  improvement achieved by a doubling of model size between two recent state of the art results  $\\\\left(\\\\left[\\\\mathrm{SPP}^{+}19\\\\right]\\\\right.$\\n\\n|  Setting | LAMBADA (acc) | LAMBADA (ppl) | StoryCloze (acc) | HellaSwag (acc)  |\\n| --- | --- | --- | --- | --- |\\n|  SOTA | 68.0a | 8.63b | 91.8c | 85.6d  |\\n|  GPT-3 Zero-Shot | 76.2 | 3.00 | 83.2 | 78.9  |\\n|  GPT-3 One-Shot | 72.5 | 3.35 | 84.7 | 78.1  |\\n|  GPT-3 Few-Shot | 86.4 | 1.92 | 87.7 | 79.3  |\\n\\nTable 3.2: Performance on cloze and completion tasks. GPT-3 significantly improves SOTA on LAMBADA while achieving respectable performance on two difficult completion prediction datasets.  ${}^{a}$  [Tur20]  ${}^{b}$  [RWC+19]  ${}^{c}$  [LDL19]  ${}^{d}$  [LCH+20]\\n\\n![img-9.jpeg](img-9.jpeg)\\nFigure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of the art by  $18\\\\%$ . Note zero-shot uses a different format from one-shot and few-shot as described in the text.\\n\\nand [Tur20]) and argue that \"continuing to expand hardware and data sizes by orders of magnitude is not the path forward\". We find that path is still promising and in a zero-shot setting GPT-3 achieves  $76\\\\%$  on LAMBADA, a gain of  $8\\\\%$  over the previous state of the art.\\n\\nLAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  (which ban \"continuation\" words). The few-shot setting instead allows us to \"frame\" the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following fill-in-the-blank format:\\n\\nAlice was friends with Bob. Alice went to visit her friend  $\\\\underline{\\\\hspace{1cm}}$ .  $\\\\rightarrow$  Bob\\n\\nGeorge bought some baseball equipment, a ball, a glove, and a  $\\\\underline{\\\\hspace{1cm}}$ .  $\\\\rightarrow$\\n\\nWhen presented with examples formatted this way, GPT-3 achieves  $86.4\\\\%$  accuracy in the few-shot setting, an increase of over  $18\\\\%$  from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost  $20\\\\%$ , for GPT-3 it improves accuracy by  $10\\\\%$ . Finally, the fill-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern.\\n\\n|  Setting | NaturalQS | WebQS | TriviaQA  |\\n| --- | --- | --- | --- |\\n|  RAG (Fine-tuned, Open-Domain) [LPP+20] | 44.5 | 45.5 | 68.0  |\\n|  T5-11B+SSM (Fine-tuned, Closed-Book) [RRS20] | 36.6 | 44.7 | 60.5  |\\n|  T5-11B (Fine-tuned, Closed-Book) | 34.5 | 37.4 | 50.1  |\\n|  GPT-3 Zero-Shot | 14.6 | 14.4 | 64.3  |\\n|  GPT-3 One-Shot | 23.0 | 25.3 | 68.0  |\\n|  GPT-3 Few-Shot | 29.9 | 41.5 | 71.2  |\\n\\nTable 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the wiki split test server.\\n\\nOne note of caution is that an analysis of test set contamination identified that a significant minority of the LAMBADA dataset appears to be present in our training data – however analysis performed in Section 4 suggests negligible impact on performance.\\n\\n# 3.1.3 HellaSwag\\n\\nThe HellaSwag dataset  $\\\\left[\\\\mathrm{ZHB}^{+}19\\\\right]$  involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be difficult for language models while remaining easy for humans (who achieve  $95.6\\\\%$  accuracy). GPT-3 achieves  $78.1\\\\%$  accuracy in the one-shot setting and  $79.3\\\\%$  accuracy in the few-shot setting, outperforming the  $75.4\\\\%$  accuracy of a fine-tuned 1.5B parameter language model  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$  but still a fair amount lower than the overall SOTA of  $85.6\\\\%$  achieved by the fine-tuned multi-task model ALUM.\\n\\n# 3.1.4 StoryCloze\\n\\nWe next evaluate GPT-3 on the StoryCloze 2016 dataset  $\\\\left[\\\\mathrm{MCH}^{+}16\\\\right]$ , which involves selecting the correct ending sentence for five-sentence long stories. Here GPT-3 achieves  $83.2\\\\%$  in the zero-shot setting and  $87.7\\\\%$  in the few-shot setting (with  $K = 70$ ). This is still  $4.1\\\\%$  lower than the fine-tuned SOTA using a BERT based model [LDL19] but improves over previous zero-shot results by roughly  $10\\\\%$ .\\n\\n# 3.2 Closed Book Question Answering\\n\\nIn this section we measure GPT-3\\'s ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to find relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted \"open-book\". [RRS20] recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxiliary information. They denote this more restrictive evaluation setting as \"closed-book\". Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions  $\\\\left[\\\\mathrm{KPR}^{+}19\\\\right]$ , WebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, fine-tuning on the Q&amp;A dataset itself is also not permitted.\\n\\nThe results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve  $64.3\\\\%$  in the zero-shot setting,  $68.0\\\\%$  in the one-shot setting, and  $71.2\\\\%$  in the few-shot setting. The zero-shot result already outperforms the fine-tuned T5-11B by  $14.2\\\\%$ , and also outperforms a version with Q&amp;A tailored span prediction during pre-training by  $3.8\\\\%$ . The one-shot result improves by  $3.7\\\\%$  and matches the SOTA for an open-domain QA system which not only fine-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents  $\\\\left[\\\\mathrm{LPP}^{+}20\\\\right]$ . GPT-3\\'s few-shot result further improves performance another  $3.2\\\\%$  beyond this.\\n\\nOn WebQuestions (WebQs), GPT-3 achieves  $14.4\\\\%$  in the zero-shot setting,  $25.3\\\\%$  in the one-shot setting, and  $41.5\\\\%$  in the few-shot setting. This compares to  $37.4\\\\%$  for fine-tuned T5-11B, and  $44.7\\\\%$  for fine-tuned T5-11B+SSM, which uses a Q&amp;A-specific pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art fine-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to few-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions\\n\\n![img-10.jpeg](img-10.jpeg)\\nFigure 3.3: On TriviaQA GPT3\\'s performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG  $\\\\left[\\\\mathrm{LPP}^{+}20\\\\right]$\\n\\nand/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance in the few-shot setting.\\n\\nOn Natural Questions (NQs) GPT-3 achieves  $14.6\\\\%$  in the zero-shot setting,  $23.0\\\\%$  in the one-shot setting, and  $29.9\\\\%$  in the few-shot setting, compared to  $36.6\\\\%$  for fine-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to TriviaQA and WebQS. In particular, the questions in NQs tend towards very fine-grained knowledge on Wikipedia specifically which could be testing the limits of GPT-3\\'s capacity and broad pretraining distribution.\\n\\nOverall, on one of the three datasets GPT-3\\'s one-shot matches the open-domain fine-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using fine-tuning. On all 3 datasets, we find that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reflecting the idea that model capacity translates directly to more \\'knowledge\\' absorbed in the parameters of the model.\\n\\n# 3.3 Translation\\n\\nFor GPT-2 a filter was used on a multilingual collection of documents to produce an English only dataset due to capacity concerns. Even with this filtering GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training dataset to include more representation of other languages, though this remains an area for further improvement. As discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based filtering. Although GPT-3\\'s training data is still primarily English (93% by word count), it also includes 7% of text in other languages. These languages are documented in the supplemental material. In order to better understand translation capability, we also expand our analysis to include two additional commonly studied languages, German and Romanian.\\n\\nExisting unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation [SHB15] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages together in a natural way, combining them on a word, sentence, and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in particular. However, our one / few-shot settings aren\\'t strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data.\\n\\nResults are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task, still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for\\n\\n|  Setting | En→Fr | Fr→En | En→De | De→En | En→Ro | Ro→En  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  SOTA (Supervised) | 45.6a | 35.0b | 41.2c | 40.2d | 38.5e | 39.9e  |\\n|  XLM [LC19] | 33.4 | 33.3 | 26.4 | 34.3 | 33.3 | 31.8  |\\n|  MASS [STQ+19] | 37.5 | 34.9 | 28.3 | 35.2 | 35.2 | 33.1  |\\n|  mBART [LGG+20] | - | - | 29.8 | 34.0 | 35.0 | 30.5  |\\n|  GPT-3 Zero-Shot | 25.2 | 21.2 | 24.6 | 27.2 | 14.1 | 19.9  |\\n|  GPT-3 One-Shot | 28.3 | 33.7 | 26.2 | 30.4 | 20.6 | 38.6  |\\n|  GPT-3 Few-Shot | 32.6 | 39.2 | 29.7 | 40.6 | 21.0 | 39.5  |\\n\\nTable 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating into English reflecting its strength as an English LM. We report BLEU scores on the WMT\\'14 Fr  $\\\\leftrightarrow$  En, WMT\\'16 De  $\\\\leftrightarrow$  En, and WMT\\'16 Ro  $\\\\leftrightarrow$  En datasets as measured by multi-bleu.perl with XLM\\'s tokenization in order to compare most closely with prior unsupervised NMT work. SacreBLEU\\' [Pos18] results reported in Appendix H. Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA with relative confidence.  $^a$  [EOAG18]  $^b$  [DHKH14]  $^c$  [WXH+18]  $^d$  [oR16]  $^e$  [LGG+20]  $^f$  [SacreBLEU signature: BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]\\n\\n![img-11.jpeg](img-11.jpeg)\\nFigure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent trend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be stronger than translation from English.\\n\\n|  Setting | Winograd | Winogrande (XL)  |\\n| --- | --- | --- |\\n|  Fine-tuned SOTA | 90.1a | 84.6b  |\\n|  GPT-3 Zero-Shot | 88.3* | 70.2  |\\n|  GPT-3 One-Shot | 89.7* | 73.2  |\\n|  GPT-3 Few-Shot | 88.6* | 77.7  |\\n\\nTable 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set.  ${}^{a}$  [SBBC19]  ${}^{b}$  [LYN+20]\\n\\n![img-12.jpeg](img-12.jpeg)\\nFigure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a fine-tuned RoBERTA-large.\\n\\neach translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could find but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised finetuning on 608K labeled examples, and backtranslation [LHCG19b].\\n\\nFinally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three settings is shown in Appendix H.\\n\\n# 3.4 Winograd-Style Tasks\\n\\nThe Winograd Schemas Challenge [LDM12] is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently fine-tuned language models have achieved near-human performance on the original Winograd dataset, but more difficult versions\\n\\n|  Setting | PIQA | ARC (Easy) | ARC (Challenge) | OpenBookQA  |\\n| --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 79.4 | 92.0[KKS+20] | 78.5[KKS+20] | 87.2[KKS+20]  |\\n|  GPT-3 Zero-Shot | 80.5* | 68.8 | 51.4 | 57.6  |\\n|  GPT-3 One-Shot | 80.5* | 71.2 | 53.2 | 58.8  |\\n|  GPT-3 Few-Shot | 82.8* | 70.1 | 51.5 | 65.4  |\\n\\nTable 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set.\\n\\n![img-13.jpeg](img-13.jpeg)\\nFigure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a score on the development set in all three conditions that exceeds the best recorded score on the task.\\n\\nsuch as the adversarially-mined Winogrande dataset [SBBC19] still significantly lag human performance. We test GPT-3\\'s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting.\\n\\nOn Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same \"partial evaluation\" method described in  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classification and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves  $88.3\\\\%$ ,  $89.7\\\\%$ , and  $88.6\\\\%$  in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4).\\n\\nOn the more difficult Winogrande dataset, we do find gains to in-context learning: GPT-3 achieves  $70.2\\\\%$  in the zero-shot setting,  $73.2\\\\%$  in the one-shot setting, and  $77.7\\\\%$  in the few-shot setting. For comparison a fine-tuned RoBERTA model achieves  $79\\\\%$ , state-of-the-art is  $84.6\\\\%$  achieved with a fine-tuned high capacity model (T5), and human performance on the task as reported by [SBBC19] is  $94.0\\\\%$ .\\n\\n# 3.5 Common Sense Reasoning\\n\\nNext we consider three datasets which attempt to capture physical or scientific reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering. The first, PhysicalQA (PIQA)  $\\\\left[\\\\mathrm{BZB}^{+}19\\\\right]$ , asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world. GPT-3 achieves  $81.0\\\\%$  accuracy zero-shot,  $80.5\\\\%$  accuracy one-shot, and  $82.8\\\\%$  accuracy few-shot (the last measured on PIQA\\'s test server). This compares favorably to the  $79.4\\\\%$  accuracy prior state-of-the-art of a\\n\\n|  Setting | CoQA | DROP | QuAC | SQuADv2 | RACE-h | RACE-m  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 90.7a | 89.1b | 74.4c | 93.0d | 90.0e | 93.1e  |\\n|  GPT-3 Zero-Shot | 81.5 | 23.6 | 41.5 | 59.5 | 45.5 | 58.4  |\\n|  GPT-3 One-Shot | 84.0 | 34.3 | 43.3 | 65.4 | 45.9 | 57.4  |\\n|  GPT-3 Few-Shot | 85.0 | 36.5 | 44.3 | 69.8 | 46.8 | 58.1  |\\n\\nTable 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy.  ${}^{a}\\\\left\\\\lbrack  {\\\\mathrm{{JZC}}}^{ + }{19}\\\\right\\\\rbrack  {}^{b}\\\\left\\\\lbrack  {\\\\mathrm{{JN20}}}\\\\right\\\\rbrack  {}^{c}\\\\left\\\\lbrack  {\\\\mathrm{{AI19}}}\\\\right\\\\rbrack  {}^{d}\\\\left\\\\lbrack  {\\\\mathrm{{QIA20}}}\\\\right\\\\rbrack  {}^{e}\\\\left\\\\lbrack  {\\\\mathrm{{SPP}}}^{ + }{19}\\\\right\\\\rbrack$\\n\\nfine-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over  $10\\\\%$  worse than human performance, but GPT-3\\'s few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis flagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk. See Section 4 for details.\\n\\nARC  $\\\\left[\\\\mathrm{CCE}^{+}18\\\\right]$  is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the \"Challenge\" version of the dataset which has been filtered to questions which simple statistical or information retrieval methods are unable to correctly answer, GPT-3 achieves  $51.4\\\\%$  accuracy in the zero-shot setting,  $53.2\\\\%$  in the one-shot setting, and  $51.5\\\\%$  in the few-shot setting. This is approaching the performance of a fine-tuned RoBERTa baseline  $(55.9\\\\%)$  from UnifiedQA  $\\\\left[\\\\mathrm{KKS}^{+}20\\\\right]$ . On the \"Easy\" version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves  $68.8\\\\%$ ,  $71.2\\\\%$ , and  $70.1\\\\%$  which slightly exceeds a fine-tuned RoBERTa baseline from  $\\\\left[\\\\mathrm{KKS}^{+}20\\\\right]$ . However, both of these results are still much worse than the overall SOTAs achieved by the UnifiedQA which exceeds GPT-3\\'s few-shot results by  $27\\\\%$  on the challenge set and  $22\\\\%$  on the easy set.\\n\\nOn OpenBookQA [MCKS18], GPT-3 improves significantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3\\'s few-shot performance is similar to a fine-tuned BERT Large baseline on the leaderboard.\\n\\nOverall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a significant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings.\\n\\n# 3.6 Reading Comprehension\\n\\nNext we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3\\'s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.\\n\\nGPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC  $\\\\left[\\\\mathrm{CHI}^{+}18\\\\right]$  a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP  $\\\\left[\\\\mathrm{DWD}^{+}19\\\\right]$ , a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems  $\\\\left[\\\\mathrm{RLL}^{+}19\\\\right]$ . On SQuAD 2.0 [RJL18], GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best fine-tuned result in the original paper. On RACE  $\\\\left[\\\\mathrm{LXL}^{+}17\\\\right]$ , a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still  $45\\\\%$  behind SOTA.\\n\\n# 3.7 SuperGLUE\\n\\nIn order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark  $\\\\left[\\\\mathrm{WPN}^{+}19\\\\right]$ $\\\\left[\\\\mathrm{WPN}^{+}19\\\\right]$ $\\\\left[\\\\mathrm{CLC}^{+}19\\\\right]$  [DMST19] [RBG11]  $\\\\left[\\\\mathrm{KCR}^{+}18\\\\right]$ $\\\\left[\\\\mathrm{ZLL}^{+}18\\\\right]$  [DGM06]  $\\\\left[\\\\mathrm{BHDD}^{+}06\\\\right]$  [GMDD07]  $\\\\left[\\\\mathrm{BDD}^{+}09\\\\right]$  [PCC18]  $\\\\left[\\\\mathrm{PHR}^{+}18\\\\right]$ . GPT-3\\'s test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC\\n\\n![img-14.jpeg](img-14.jpeg)\\nFigure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting, only a few points behind measured human performance and state-of-the-art fine-tuned models. Zero-shot and one-shot performance is a few points behind, with the gains to few-shot being largest for bigger models.\\n\\n|   | SuperGLUE Average | BoolQ Accuracy | CB Accuracy | CB F1 | COPA Accuracy | RTE Accuracy  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 89.0 | 91.0 | 96.9 | 93.9 | 94.8 | 92.5  |\\n|  Fine-tuned BERT-Large | 69.0 | 77.4 | 83.6 | 75.7 | 70.6 | 71.7  |\\n|  GPT-3 Few-Shot | 71.8 | 76.4 | 75.6 | 52.0 | 92.0 | 69.0  |\\n|   | WiC Accuracy | WSC Accuracy | MultiRC Accuracy | MultiRC F1a | ReCoRD Accuracy | ReCoRD F1  |\\n|  Fine-tuned SOTA | 76.1 | 93.8 | 62.3 | 88.2 | 92.5 | 93.3  |\\n|  Fine-tuned BERT-Large | 69.6 | 64.6 | 24.1 | 70.0 | 71.3 | 72.0  |\\n|  GPT-3 Few-Shot | 49.4 | 80.1 | 30.5 | 75.4 | 90.2 | 91.1  |\\n\\nTable 3.8: Performance of GPT-3 on SuperGLUE compared to fine-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates.\\n\\n![img-15.jpeg](img-15.jpeg)\\nFigure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value of  $K = 32$  means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table 3.8). The BERT-Large reference model was fine-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was first fine-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further fine-tuning on the SuperGLUE training set (for a total of 630K fine-tuning examples). We find the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context.\\n\\n![img-16.jpeg](img-16.jpeg)\\n\\nand MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we used the same set of randomly drawn examples from the training set as context for all of the problems we evaluated.\\n\\nWe observe a wide range in GPT-3\\'s performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving second place on the leaderboard, where first place is held by a fine-tuned 11 billion parameter model (T5). On WSC, performance is still relatively strong, achieving  $80.1\\\\%$  in the few-shot setting (note that GPT-3 achieves  $88.6\\\\%$  on the original Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable, roughly matching that of a fine-tuned BERT-Large. On CB, we see signs of life at  $75.6\\\\%$  in the few-shot setting.\\n\\nWiC is a notable weak spot with few-shot performance at  $49.4\\\\%$  (at random chance). We tried a number of different phrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two sentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer in the next section (which discusses the ANLI benchmark) – GPT-3 appears to be weak in the few-shot or one-shot setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same way in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another. This could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses, GPT-3 still outperforms a fine-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to the state-of-the-art held by a fine-tuned 11 billion parameter model.\\n\\nFinally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of examples in the context showing increasing benefits from in-context learning (Figure 3.8). We scale  $K$  up to 32 examples per task, after which point additional examples will not reliably fit into our context. When sweeping over values of  $K$ , we find that GPT-3 requires less than eight total examples per task to outperform a fine-tuned BERT-Large on overall SuperGLUE score.\\n\\n# 3.8 NLI\\n\\nNatural Language Inference (NLI) [Fyo00] concerns the ability to understand the relationship between two sentences. In practice, this task is usually structured as a two or three class classification problem where the model classifies\\n\\n![img-17.jpeg](img-17.jpeg)\\nFigure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of  $1.2\\\\%$ ). We find that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix.\\n\\nwhether the second sentence logically follows from the first, contradicts the first sentence, or is possibly true (neutral). SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task fine-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset  $\\\\left[\\\\mathrm{NWD}^{+}19\\\\right]$ . ANLI is a difficult dataset employing a series of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our models smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting ( $\\\\sim 33\\\\%$ ), whereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results for all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very difficult task for language models and they are only just beginning to show signs of progress.\\n\\n# 3.9 Synthetic and Qualitative Tasks\\n\\nOne way to probe GPT-3\\'s range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which require it to perform simple on-the-fly computational reasoning, recognize a novel pattern that is unlikely to have occurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we test GPT-3\\'s ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3\\'s ability to solve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models.\\n\\n# 3.9.1 Arithmetic\\n\\nTo test GPT-3\\'s ability to perform simple arithmetic operations without task-specific training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:\\n\\n- 2 digit addition  $(2\\\\mathbf{D}+)$  - The model is asked to add two integers sampled uniformly from  $[0,100)$ , phrased in the form of a question, e.g. \"Q: What is 48 plus 76? A: 124.\"\\n- 2 digit subtraction (2D-) - The model is asked to subtract two integers sampled uniformly from  $[0,100)$ ; the answer may be negative. Example: \"Q: What is 34 minus 53? A: -19\".\\n- 3 digit addition  $(3\\\\mathbf{D}+)$  - Same as 2 digit addition, except numbers are uniformly sampled from [0, 1000).\\n\\n![img-18.jpeg](img-18.jpeg)\\nFigure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a significant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being able to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a significant fraction of the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot are shown in the appendix.\\n\\n- 3 digit subtraction (3D-) - Same as 2 digit subtraction, except numbers are uniformly sampled from [0, 1000).\\n- 4 digit addition  $(4\\\\mathbf{D} + )$  - Same as 3 digit addition, except uniformly sampled from [0, 10000).\\n- 4 digit subtraction (4D-) - Same as 3 digit subtraction, except uniformly sampled from [0, 10000).\\n- 5 digit addition  $(5\\\\mathbf{D} + )$  - Same as 3 digit addition, except uniformly sampled from [0, 100000).\\n- 5 digit subtraction (5D-) - Same as 3 digit subtraction, except uniformly sampled from [0, 100000).\\n- 2 digit multiplication (2Dx) - The model is asked to multiply two integers sampled uniformly from [0, 100), e.g. \"Q: What is 24 times 42? A: 1008\".\\n- One-digit composite (1DC) - The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the last two. For example, \"Q: What is  $6 + (4^{*}8)$ ? A: 38\". The three 1 digit numbers are selected uniformly on [0, 10) and the operations are selected uniformly from  $\\\\{+, -, *\\\\}$ .\\n\\nIn all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances.\\n\\nFirst we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction, GPT-3 displays strong proficiency when the number of digits is small, achieving  $100\\\\%$  accuracy on 2 digit addition,  $98.9\\\\%$  at 2 digit subtraction,  $80.2\\\\%$  at 3 digit addition, and  $94.2\\\\%$  at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves  $25 - 26\\\\%$  accuracy on four digit operations and  $9 - 10\\\\%$  accuracy on five digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves  $29.2\\\\%$  accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves  $21.3\\\\%$  accuracy at single digit combined operations (for example,  $9^{*}(7 + 5)$ ), suggesting that it has some robustness beyond just single operations.\\n\\nAs Figure 3.10 makes clear, small models do poorly on all of these tasks – even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than  $10\\\\%$  of the time.\\n\\nOne-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly. Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 significantly\\n\\n|  Setting | 2D+ | 2D- | 3D+ | 3D- | 4D+ | 4D- | 5D+ | 5D- | 2Dx | 1DC  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Zero-shot | 76.9 | 58.0 | 34.2 | 48.3 | 4.0 | 7.5 | 0.7 | 0.8 | 19.8 | 9.8  |\\n|  GPT-3 One-shot | 99.6 | 86.4 | 65.5 | 78.7 | 14.0 | 14.0 | 3.5 | 3.8 | 27.4 | 14.3  |\\n|  GPT-3 Few-shot | 100.0 | 98.9 | 80.4 | 94.2 | 25.5 | 26.8 | 9.3 | 9.9 | 29.2 | 21.3  |\\n\\nTable 3.9: Results on basic arithmetic tasks for GPT-3 175B.  $\\\\{2,3,4,5\\\\} \\\\mathrm{D}\\\\{+, - \\\\}$  is 2, 3, 4, and 5 digit addition or subtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger moving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows significant arithmetic abilities.\\n\\n|  Setting | CL | A1 | A2 | RI | RW  |\\n| --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Zero-shot | 3.66 | 2.28 | 8.91 | 8.26 | 0.09  |\\n|  GPT-3 One-shot | 21.7 | 8.62 | 25.9 | 45.4 | 0.48  |\\n|  GPT-3 Few-shot | 37.9 | 15.1 | 39.7 | 67.2 | 0.44  |\\n\\nTable 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and few-shot settings. CL is \"cycle letters in word\", A1 is anagrams of but the first and last letters, A2 is anagrams of all but the first and last two letters, RI is \"Random insertion in word\", RW is \"reversed words\".\\n\\noutperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table 3.9, and model capacity scaling for all three settings is shown in Appendix H.\\n\\nTo spot-check whether the model is simply memorizing specific arithmetic problems, we took the 3-digit arithmetic problems in our test set and searched for them in our training data in both the forms \"<num1> + <num2> = \" and \"<num1> plus <num2>\". Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000 subtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers could have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes such as not carrying a \"1\", suggesting it is actually attempting to perform the relevant computation rather than memorizing a table.\\n\\nOverall, GPT-3 displays reasonable proficiency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings.\\n\\n# 3.9.2 Word Scrambling and Manipulation Tasks\\n\\nTo test GPT-3\\'s ability to learn novel symbolic manipulations from a few examples, we designed a small battery of 5 \"character manipulation\" tasks. Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are:\\n\\n- Cycle letters in word (CL) - The model is given a word with its letters cycled, then the “=” symbol, and is expected to generate the original word. For example, it might be given “lyinevitab” and should output “inevitably”.\\n- Anagrams of all but first and last characters (A1) - The model is given a word where every letter except the first and last have been scrambled randomly, and must output the original word. Example: criroptuon = corruption.\\n- Anagrams of all but first and last 2 characters (A2) - The model is given a word where every letter except the first 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt  $\\\\rightarrow$  opponent.\\n- Random insertion in word (RI) - A random punctuation or space character is inserted between each letter of a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession.\\n- Reversed words (RW) - The model is given a word spelled backwards, and must output the original word. Example: stcejbo  $\\\\rightarrow$  objects.\\n\\nFor each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by [Nor09] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11. Task performance tends to grow smoothly with model size, with the full GPT-3 model achieving  $66.9\\\\%$  on removing</num2></num1></num2></num1>\\n\\n![img-19.jpeg](img-19.jpeg)\\nFigure 3.11: Few-shot performance on the five word scrambling tasks for different sizes of model. There is generally smooth improvement with model size although the random insertion task shows an upward slope of improvement with the 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in the appendix. All tasks are done with  $K = 100$ .\\n\\nrandom insertions,  $38.6\\\\%$  on cycling letters,  $40.2\\\\%$  on the easier anagram task, and  $15.1\\\\%$  on the more difficult anagram task (where only the first and last letters are held fixed). None of the models can reverse the letters in a word.\\n\\nIn the one-shot setting, performance is significantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artificial nature makes them unlikely to appear in the pre-training data (although we cannot confirm this with certainty).\\n\\nWe can further quantify performance by plotting \"in-context learning curves\", which show task performance as a function of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task in Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions.\\n\\nFinally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on significant fractions of a word (on average  $\\\\sim 0.7$  words per token), so from the LM\\'s perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also, CL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word), requiring the model to perform some search to find the correct unscrambling. Thus, the skills involved appear to require non-trivial pattern-matching and computation.\\n\\n# 3.9.3 SAT Analogies\\n\\nTo test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 \"SAT analogy\" problems [TLBS03]. Analogies are a style of multiple choice question that constituted a section of the SAT college entrance exam before 2005. A typical example is \"audacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation\". The student is expected to choose which of the five word pairs has the same relationship as the original word pair; in this example the answer is \"sanctimonious is to hypocrisy\". On this task GPT-3 achieves  $65.2\\\\%$  in the few-shot setting,  $59.1\\\\%$  in the one-shot setting, and  $53.7\\\\%$  in the zero-shot setting, whereas the average score among college applicants was  $57\\\\%$  [TL05] (random guessing yields  $20\\\\%$ ). As shown in Figure 3.12, the results improve with scale, with the full 175 billion model improving by over  $10\\\\%$  compared to the 13 billion parameter model.\\n\\n![img-20.jpeg](img-20.jpeg)\\nFigure 3.12: Zero-, one-, and few-shot performance on SAT analogy tasks, for different sizes of model. The largest model achieves  $65\\\\%$  accuracy in the few-shot setting, and also demonstrates significant gains to in-context learning which are not present in smaller models.\\n\\n# 3.9.4 News Article Generation\\n\\nPrevious work on generative language models qualitatively tested their ability to generate synthetic \"news articles\" by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . Relative to  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , the dataset used to train GPT-3 is much less weighted towards news articles, so trying to generate news articles via raw unconditional samples is less effective – for example GPT-3 often interprets the proposed first sentence of a \"news article\" as a tweet and then posts synthetic responses or follow-up tweets. To solve this problem we employed GPT-3\\'s few-shot learning abilities by providing three previous news articles in the model\\'s context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the \"news\" genre.\\n\\nTo gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles from real ones. Similar work has been carried out by Kreps et al. [KMB20] and Zellers et al.  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$ . Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality.\\n\\nIn order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model. Participants were asked to select whether the article was \"very likely written by a human\", \"more likely written by a human\", \"I don\\'t know\", \"more likely written by a machine\", or \"very likely written by a machine\".\\n\\nThe articles we selected were not in the models\\' training data and the model outputs were formatted and selected programmatically to prevent human cherry-picking. All models used the same context to condition outputs on and were pre-trained with the same context size and the same article titles and subtitles were used as prompts for each model. However, we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a \"control model\": a 160M parameter model with no context and increased output randomness.\\n\\n|   | Mean accuracy | 95% Confidence Interval (low, hi) | t compared to control (p-value) | “I don’t know” assignments  |\\n| --- | --- | --- | --- | --- |\\n|  Control (deliberately bad model) | 86% | 83%-90% | - | 3.6 %  |\\n|  GPT-3 Small | 76% | 72%-80% | 3.9 (2e-4) | 4.9%  |\\n|  GPT-3 Medium | 61% | 58%-65% | 10.3 (7e-21) | 6.0%  |\\n|  GPT-3 Large | 68% | 64%-72% | 7.3 (3e-11) | 8.7%  |\\n|  GPT-3 XL | 62% | 59%-65% | 10.7 (1e-19) | 7.5%  |\\n|  GPT-3 2.7B | 62% | 58%-65% | 10.4 (5e-19) | 7.1%  |\\n|  GPT-3 6.7B | 60% | 56%-63% | 11.2 (3e-21) | 6.2%  |\\n|  GPT-3 13B | 55% | 52%-58% | 15.3 (1e-32) | 7.1%  |\\n|  GPT-3 175B | 52% | 49%-54% | 16.9 (1e-34) | 7.8%  |\\n\\nTable 3.11: Human accuracy in identifying whether short (~200 word) news articles are model generated. We find that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from  $86\\\\%$  on the control model to  $52\\\\%$  on GPT-3 175B. This table compares mean accuracy between five different models, and shows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model (an unconditional GPT-3 Small model with increased output randomness).\\n\\nMean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that the intentionally bad articles were model generated was  $\\\\sim 86\\\\%$  where  $50\\\\%$  is chance level performance. By contrast, mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance at  $\\\\sim 52\\\\%$  (see Table 3.11). Human abilities to detect model generated text appear to decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance. This is true despite the fact that participants spend more time on each output as model size increases (see Appendix E).\\n\\nExamples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15. Much of the text is—as indicated by the evaluations—difficult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors, the models have no access to the specific facts that the article titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are often subtle enough that they are not noticed.\\n\\nRelated work on language model detection by Ippolito et al. [IDCBE19] indicates that automatic discriminators like GROVER  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$  and GLTR [GSR19] may have greater success at detecting model generated text than human evaluators. Automatic detection of these models may be a promising area of future research.\\n\\nIppolito et al. [IDCBE19] also note that human accuracy at detecting model generated text increases as humans observe more tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial experiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to compare human abilities to detect the articles generated by GPT-3 and a control model.\\n\\nWe found that mean human accuracy at detecting the intentionally bad longer articles from the control model was  $\\\\sim 88\\\\%$ , while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at  $\\\\sim 52\\\\%$  (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles.\\n\\n# 3.9.5 Learning and Using Novel Words\\n\\nA task studied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a word in a sentence after seeing it defined only once, or conversely inferring a word\\'s meaning from only one usage. Here we qualitatively test GPT-3\\'s ability to do the former. Specifically, we give GPT-3 the definition of a nonexistent word, such as \"Gigamuru\", and then ask it to use it in a sentence. We provide one to five previous examples of a (separate)\\n\\n![img-21.jpeg](img-21.jpeg)\\nFigure 3.13: People\\'s ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately-bad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance  $(50\\\\%)$  is indicated with the dashed line at the bottom. Line of best fit is a power law with  $95\\\\%$  confidence intervals.\\n\\n|   | Mean accuracy | 95% Confidence Interval (low, hi) | t compared to control (p-value) | “I don’t know” assignments  |\\n| --- | --- | --- | --- | --- |\\n|  Control | 88% | 84%-91% | - | 2.7%  |\\n|  GPT-3 175B | 52% | 48%-57% | 12.7 (3.2e-23) | 10.6%  |\\n\\nTable 3.12: People\\'s ability to identify whether  $\\\\sim 500$  word articles are model generated (as measured by the ratio of correct assignments to non-neutral assignments) was  $88\\\\%$  on the control model and  $52\\\\%$  on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with increased output randomness).\\n\\nTitle: United Methodists Agree to Historic Split\\nSubtitle: Those who oppose gay marriage will form their own denomination\\nArticle: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church\\'s annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination.\\nThe Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the \"largest Protestant denomination in the U.S.,\" but that it has been shrinking in recent decades. The new split will be the second in the church\\'s history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church. The Post notes that the proposed split \"comes at a critical time for the church, which has been losing members for years,\" which has been \"pushed toward the brink of a schism over the role of LGBTQ people in the church.\" Gay marriage is not the only issue that has divided the church. In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban them from serving as clergy, and the South Pacific regional conference voting to allow them.\\n\\nFigure 3.14: The GPT-3 generated news article that humans had the greatest difficulty distinguishing from a human written article (accuracy:  $12\\\\%$ ).\\n\\nTitle: Star\\'s Tux Promise Draws Megyn Kelly\\'s Sarcasm\\nSubtitle: Joaquin Phoenix pledged to not change for each awards event\\nArticle: A year ago, Joaquin Phoenix made headlines when he appeared on the red carpet at the Golden Globes wearing a tuxedo with a paper bag over his head that read, \"I am a shape-shifter. I can\\'t change the world. I can only change myself.\" It was a promise to not change to fit into the Hollywood mold: \"I think that\\'s a really special thing, to not change yourself. I think it\\'s a really special thing to say, \\'This is what\\'s inside of me, I\\'m proud of it, and I\\'m not going to be ashamed because of the way that someone else thinks I should be.\\'\" Now, it\\'s the Oscars, and Phoenix is at it again. But this time, his publicist is saying he\\'ll be wearing a tux no matter what.\\nMegyn Kelly was not impressed, and she let him have it on The Tonight Show. \"You know, I feel like, I feel like you could have worn the tux,\" she says. \"But you\\'re saying you\\'re a shape-shifter. I don\\'t know if you can change your tux, but you can change your mind. You can change your mind. You can change your mind.\" Phoenix says he did, but it didn\\'t stick. \"I was like, \\'Okay, I\\'m going to wear a tuxedo to this thing.\\' And then I thought, \\'I don\\'t want to wear a tuxedo to this thing.\\'\" Kelly goes on to encourage him to change his mind again, but Phoenix says it\\'s too late: \"I\\'m committed to wearing this.\"\\n\\nFigure 3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written article (accuracy:  $61\\\\%$ ).\\n\\n![img-22.jpeg](img-22.jpeg)\\nFigure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is GPT-3\\'s completions, plain text is human prompts. In the first example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 other than the conditioning shown here.\\n\\nnonexistent word being defined and used in a sentence, so the task is few-shot in terms of previous examples of the broad task and one-shot in terms of the specific word. Table 3.16 shows the 6 examples we generated; all definitions were human-generated, and the first answer was human-generated as conditioning while the subsequent answers were generated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the final sentence the model generates a plausible conjugation for the word \"screeg\" (namely \"screeghed\"), although the use of the word is slightly awkward (\"screeghed at each other\") despite being plausible in the sense that it could describe a toy sword fight. Overall, GPT-3 appears to be at least proficient at the task of using novel words in a sentence.\\n\\n# 3.9.6 Correcting English Grammar\\n\\nAnother task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the few-shot setting by giving prompts of the form \"Poor English Input: <sentence>\\\\n Good English Output: <sentence>\". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats). Results are shown in Figure 3.17.\\n\\n# 4 Measuring and Preventing Memorization Of Benchmarks\\n\\nSince our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to.\\n\\nThis concern is not just hypothetical. One of the first papers to train a language model on Common Crawl data [TL18] detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  also conducted post-hoc overlap analysis. Their study was relatively encouraging, finding that</sentence></sentence>\\n\\n![img-23.jpeg](img-23.jpeg)\\nFigure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface is GPT-3\\'s completions, plain text is human prompts. In the first few examples example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 aside from the first few examples as conditioning and the \"Poor English input/Good English output\" framing. We note that the distinction between \"poor\" and \"good\" English (and the terms themselves) is complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what \"good\" is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word \"cheap\" in a way that alters meaning).\\n\\n![img-24.jpeg](img-24.jpeg)\\nFigure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difficulty rather than overfitting.\\n\\nalthough models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).\\n\\nGPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared.\\n\\nWe initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn\\'t feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results.\\n\\nFor each benchmark, we produce a \\'clean\\' version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix C.\\n\\nWe then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over  $50\\\\%$ ), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance.\\n\\nBelow, we review in more detail the few specific cases where either (1) the model performs significantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difficult.\\n\\nOur analysis flagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English\\n\\n![img-25.jpeg](img-25.jpeg)\\nFigure 4.2: Benchmark contamination analysis We constructed cleaned versions of each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high confidence to be clean, and the y-axis shows the difference in performance when evaluating only on the verified clean subset. Performance on most benchmarks changed negligibly, but some were flagged for further review. On inspection we find some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section 3 with an asterisk. We find no evidence that other benchmarks are affected.\\n\\ntranslation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false positives. We summarize the results for each group of tasks below:\\n\\n- Reading Comprehension: Our initial analysis flagged  $&gt;90\\\\%$  of task examples from QuAC, SQuAD2, and DROP as potentially contaminated, so large that even measuring the differential on a clean subset was difficult. Upon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source text was present in our training data but the question/answer pairs were not, meaning the model gains only background information and cannot memorize the answer to a specific question.\\n- German translation: We found  $25\\\\%$  of the examples in the WMT16 German-English test set were marked as potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the flagged examples contain paired sentences resembling NMT training data and collisions were monolingual matches mostly of snippets of events discussed in the news.\\n- Reversed Words and Anagrams: Recall that these tasks are of the form \"alaok = koala\". Due to the short length of these tasks, we used 2-grams for filtering (ignoring punctuation). After inspecting the flagged overlaps, we found that they were not typically instances of real reversals or unscramblings in the training set, but rather palindromes or trivial unscramblings, e.g. \"kayak = kayak\". The amount of overlap was small, but removing the trivial tasks lead to an increase in difficulty and thus a spurious signal. Related to this, the symbol insertion task shows high overlap but no effect on performance – this is because that task involves removing non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to many spurious matches.\\n- PIQA: The overlap analysis flagged  $29\\\\%$  of examples as contaminated, and observed a 3 percentage point absolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was released after our training set was created and its labels are hidden, some of the web pages used by the crowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller model with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias rather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot rigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential contamination.\\n- Winograd: The overlap analysis flagged  $45\\\\%$  of examples, and found a  $2.6\\\\%$  decrease in performance on the clean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in fact present in our training set, though presented in a different format than we present the task to the model. Although the decrease in performance is small, we mark our Winograd results in the main paper with an asterisk.\\n\\n- Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the Children’s Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark.\\n\\nWe also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our fill-in-the-blank format precludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section.\\n\\nAn important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset. It remains possible that memorization inflates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing.\\n\\nOverall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the field in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C.\\n\\n## 5 Limitations\\n\\nGPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work.\\n\\nFirst, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3’s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with “common sense physics”, despite doing well on some datasets (such as PIQA *[BZB^{+}19]*) that test this domain. Specifically GPT-3 has difficulty with questions of the type “If I put cheese into the fridge, will it melt?”. Quantitatively, GPT-3’s in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some “comparison” tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3’s strong few-shot performance on many other tasks.\\n\\nGPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models *[RSR^{+}19]*. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3’s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the “best of both worlds”.\\n\\nA more fundamental limitation of the general approach described in this paper – scaling up any LM-like model, whether autoregressive or bidirectional – is that it may eventually run into (or could already be running into) the limits of the\\n\\npretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. *[x20]* demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world *[BHT^{+}20]*. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans *[ZSW^{+}19a]*, fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world *[CLY^{+}19]*.\\n\\nAnother limitation broadly shared by language models is poor sample efficiency during pre-training. While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime *[x14]*. Improving pre-training sample efficiency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements.\\n\\nA limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identifies tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a specific style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or defining nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research.\\n\\nA limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation *[x13]* of large models down to a manageable size for specific tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a specific task, suggesting that in principle aggressive distillation may be possible. Distillation is well-explored in general *[x15]* but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size.\\n\\nFinally, GPT-3 shares some limitations common to most deep learning systems – its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This last issue – biases in the data that may lead the model to generate stereotyped or prejudiced content – is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section 6).\\n\\n## 6 Broader Impacts\\n\\nLanguage models have a wide range of beneficial applications for society, including code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. But they also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difficulty of distinguishing synthetic text from human-written text. It therefore has the potential to advance both the beneficial and harmful applications of language models.\\n\\nHere we focus on the potential harms of improved language models, not because we believe the harms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also briefly discuss issues of energy efficiency (Section 6.3).\\n\\n6.1 Misuse of Language Models\\n\\nMalicious uses of language models can be somewhat difficult to anticipate because they often involve repurposing language models in a very different environment or for a different purpose than researchers intended. To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact *[x20]*. We discuss three factors: potential misuse applications, threat actors, and external incentive structures.\\n\\n#### 6.1.1 Potential Misuse Applications\\n\\nAny socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufficiently high quality text. Language models that produce high quality text generation could lower existing barriers to carrying out these activities and increase their efficacy.\\n\\nThe misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to generate several paragraphs of synthetic content that people find difficult to distinguish from human-written text in 3.9.4 represents a concerning milestone in this regard.\\n\\n#### 6.1.2 Threat Actor Analysis\\n\\nThreat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors who may be able to build a malicious product to ‘advanced persistent threats’ (APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas *[SBC+19]*.\\n\\nTo understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did find significant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate, but significant improvements in reliability could change this.\\n\\nBecause APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about possible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible difference in operations that may see potential gains by using language models. The assessment was that language models may not be worth investing significant resources in because there has been no convincing demonstration that current language models are significantly better than current methods for generating text, and because methods for “targeting” or “controlling” the content of language models are still at a very early stage.\\n\\n#### 6.1.3 External Incentive Structures\\n\\nEach threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their agenda. TTPs are influenced by economic factors like scalability and ease of deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login credentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment.\\n\\nEase of use is another significant incentive. Having stable infrastructure has a large impact on the adoption of TTPs. The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot produces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor required in operating this bot. But a human is still needed to filter the outputs, which restricts how scalable the operation can be.\\n\\nBased on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will eventually develop language models that are sufficiently consistent and steerable that they will be of greater interest to malicious actors. We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers.\\n\\n####\\n\\n6.2 Fairness, Bias, and Representation\\n\\nBiases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms *[x10]*. We have conducted an analysis of biases in the model in order to better understand GPT-3’s limitations when it comes to fairness, bias, and representation.\\n\\nOur goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reflect all of the model’s biases even within the studied categories.\\n\\nBroadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data. Below we discuss our preliminary findings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension.\\n\\n#### 6.2.1 Gender\\n\\nIn our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3. We measured this by feeding the model a context such as \"The detective was a\" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identifiers include midwife, nurse, receptionist, housekeeper etc.\\n\\nWe also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation} was a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent {occupation} was a,\" the majority of occupations had an even higher probability of being followed by a male identifier than a female one than was the case with our original neutral prompt, \"The {occupation} was a\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as $\\\\frac{1}{n_{\\\\text{jobs}}}\\\\sum_{\\\\text{jobs}}\\\\log(\\\\frac{P(\\\\text{female}|\\\\text{Context})}{P(\\\\text{male}|\\\\text{Context})})$ - was $-1.11$ for the Neutral Variant, $-2.14$ for the Competent Variant and $-1.15$ for the Incompetent Variant.\\n\\nWe also carried out pronoun resolution on the Winogender dataset *[x23]* using two methods which further corroborated the model’s tendency to associate most occupations with males. One method measured the models ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \"The advisor met with the advisee because she wanted to get advice about job applications. ’She’ refers to the\" and found the option with the lowest probability between the two possible options (Choices between Occupation Option: advisor; Participant Option: advisee).\\n\\nOccupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models.\\n\\nWe also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-selected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature\\n\\nTable 6.1: Most Biased Descriptive Words in 175B Model\\n\\n|  Top 10 Most Biased Male Descriptive Words with Raw Co-Occurrence Counts | Top 10 Most Biased Female Descriptive Words with Raw Co-Occurrence Counts  |\\n| --- | --- |\\n|  Average Number of Co-Occurrences Across All Words: 17.5 | Average Number of Co-Occurrences Across All Words: 23.9  |\\n|  Large (16) | Optimistic (12)  |\\n|  Mostly (15) | Bubbly (12)  |\\n|  Lazy (14) | Naughty (12)  |\\n|  Fantastic (13) | Easy-going (12)  |\\n|  Eccentric (13) | Petite (10)  |\\n|  Protect (10) | Tight (10)  |\\n|  Jolly (10) | Pregnant (10)  |\\n|  Stable (9) | Gorgeous (28)  |\\n|  Personable (22) | Sucked (8)  |\\n|  Survive (7) | Beautiful (158)  |\\n\\nof 1 and top_p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\", \"She was very\", \"He would be described as\", \"She would be described as\". We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more often described using appearance oriented words such as \"beautiful\" and \"gorgeous\" as compared to men who were more often described using adjectives that span a greater spectrum.\\n\\nTable 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. \"Most Favored\" here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender.\\n\\n# 6.2.2 Race\\n\\nTo investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\", \"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation  $\\\\left[\\\\mathrm{HZJ}^{+}19\\\\right]$ , we explored how race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5, horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).\\n\\nIt should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology.\\n\\nAcross the models we analyzed, \\'Asian\\' had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, \\'Black\\' had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data.\\n\\n![img-26.jpeg](img-26.jpeg)\\nFigure 6.1: Racial Sentiment Across Models\\n\\n|  Religion | Most Favored Descriptive Words  |\\n| --- | --- |\\n|  Atheism | ‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’, ‘Characterized’  |\\n|  Buddhism | ‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘En- lightenment’, ‘Non-Violent’  |\\n|  Christianity | ‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Com- ments’, ‘Officially’  |\\n|  Hinduism | ‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated’, ‘Africa’  |\\n|  Islam | ‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’, ‘Levant’, ‘Allah’, ‘Prophet’  |\\n|  Judaism | ‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’, ‘Game’, ‘Russian’  |\\n\\nTable 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model.\\n\\n# 6.2.3 Religion\\n\\nWe studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length  $\\\\approx 50$  with a temperature of 1 and a top  $p$  of 0.9 for every prompt. Our prompts were of the nature \" {Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words.\\n\\nThe following is an example output from the model:\\n\\n\"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic.\"', 'doc_id': '6525a2245e91'}, page_content='# Language Models are Few-Shot Learners\\n\\n|  Tom B. Brown* |   | Benjamin Mann* |   | Nick Ryder* |   | Melanie Subbiah*  |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Jared Kaplan† | Prafulla Dhariwal | Arvind Neelakantan | Pranav Shyam | Girish Sastry |  |  |   |\\n|  Amanda Askell | Sandhini Agarwal | Ariel Herbert-Voss | Gretchen Krueger | Tom Henighan |  |  |   |\\n|  Rewon Child | Aditya Ramesh | Daniel M. Ziegler | Jeffrey Wu | Clemens Winter |  |  |   |\\n|  Christopher Hesse | Mark Chen | Eric Sigler | Mateusz Litwin | Scott Gray |  |  |   |\\n|  Benjamin Chess |   | Jack Clark |   | Christopher Berner |   |  |   |\\n|  Sam McCandlish |   | Alec Radford | Ilya Sutskever | Dario Amodei |   |  |   |\\n\\nOpenAI\\n\\n# Abstract\\n\\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters,  $10\\\\mathrm{x}$  more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\\'s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\\n\\nAuthor contributions listed at end of paper.\\n\\n2\\n\\n# Contents\\n\\n1  Introduction  3\\n2  Approach  6\\n2.1  Model and Architectures  8\\n2.2  Training Dataset  8\\n2.3  Training Process  9\\n2.4  Evaluation  10\\n3  Results  10\\n3.1  Language Modeling, Cloze, and Completion Tasks  11\\n3.2  Closed Book Question Answering  13\\n3.3  Translation  14\\n3.4  Winograd-Style Tasks  16\\n3.5  Common Sense Reasoning  17\\n3.6  Reading Comprehension  18\\n3.7  SuperGLUE  18\\n3.8  NLI  20\\n3.9  Synthetic and Qualitative Tasks  21\\n4  Measuring and Preventing Memorization Of Benchmarks  29\\n5  Limitations  33\\n6  Broader Impacts  34\\n6.1  Misuse of Language Models  35\\n6.2  Fairness, Bias, and Representation  36\\n6.3  Energy Usage  39\\n7  Related Work  39\\n8  Conclusion  40\\nA  Details of Common Crawl Filtering  43\\nB  Details of Model Training  43\\nC  Details of Test Set Contamination Studies  43\\nD  Total Compute Used to Train Language Models  46\\nE  Human Quality Assessment of Synthetic News Articles  46\\nF  Additional Samples from GPT-3  48\\nG  Details of Task Phrasing and Specifications  50\\nH  Results on All Tasks for All Model Sizes  63\\n\\n# 1 Introduction\\n\\nRecent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models  $\\\\left[\\\\mathrm{VSP}^{+}17\\\\right]$  have been directly fine-tuned, entirely removing the need for task-specific architectures [RNSS18, DCLT18, HR18].\\n\\nThis last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR $^{+}$ 19, LOG $^{+}$ 19, YDY $^{+}$ 19, LCG $^{+}$ 19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Removing this limitation would be desirable, for several reasons.\\n\\nFirst, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.\\n\\nSecond, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance  $\\\\left[\\\\mathrm{HLW}^{+}20\\\\right]$  observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it  $\\\\left[\\\\mathrm{YdC}^{+}19,\\\\mathrm{MPL}19\\\\right]$ . Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task  $\\\\left[\\\\mathrm{GSL}^{+}18,\\\\mathrm{NK}19\\\\right]$ .\\n\\nThird, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term \"in-context learning\" to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 1.2: Larger models make increasingly efficient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper \"in-context learning curves\" for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks.\\n\\nsufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.\\n\\nOne potential route towards addressing these issues is meta-learning $^{1}$  – which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.\\n\\nWhile it has shown some initial promise, this approach still achieves results far inferior to fine-tuning – for example  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  achieves only  $4\\\\%$  on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.\\n\\nAnother recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters [DCLT18], to 1.5 billion parameters  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , to 8 billion parameters  $\\\\left[\\\\mathrm{SPP}^{+}19\\\\right]$ , 11 billion parameters  $\\\\left[\\\\mathrm{RSR}^{+}19\\\\right]$ , and finally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$ . Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.\\n\\nIn this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) \"few-shot learning\", or in-context learning where we allow as many demonstrations as will fit into the model\\'s context window (typically 10 to 100), (b) \"one-shot learning\", where we allow only one demonstration, and (c) \"zero-shot\" learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.\\n\\nFigure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model\\'s context,  $K$ . Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these \"learning\" curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.\\n\\nBroadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves  $64.3\\\\%$  accuracy on TriviaQA in the zero-shot setting,  $68.0\\\\%$  in the one-shot setting, and  $71.2\\\\%$  in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.\\n\\nGPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.\\n\\nAt the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3\\'s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.\\n\\nA heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).\\n\\nWe also undertake a systematic study of “data contamination” – a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we find that data contamination has a minimal effect on GPT-3’s performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.\\n\\nIn addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.\\n\\nFinally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard.\\n\\nThe remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes.\\n\\n## 2 Approach\\n\\nOur basic pre-training approach, including model, data, and training, is similar to the process described in *[RWC+19]*, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to *[RWC+19]*, but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on. Specifically, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration):\\n\\n- Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution *[x13]*, and the potential to exploit spurious features of the training data *[GSL+18, x15]*, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work.\\n- Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning *[RWC+19]*, but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion. We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model’s context window ($n_{\\\\mathrm{ctx}}=2048$). The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models. Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML *[x10, VBL+16]* – both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task.\\n- One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans. For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task. By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given.\\n\\nThe three settings we explore for in-context learning\\n\\n# Zero-shot\\n\\nThe model predicts the answer given only a natural language description of the task. No gradient updates are performed.\\n\\n![img-3.jpeg](img-3.jpeg)\\n\\n# One-shot\\n\\nIn addition to the task description, the model sees a single example of the task. No gradient updates are performed.\\n\\n![img-4.jpeg](img-4.jpeg)\\n\\n# Few-shot\\n\\nIn addition to the task description, the model sees a few examples of the task. No gradient updates are performed.\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning. The panels above show four methods for performing a task with a language model – fine-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task descriptions, examples and prompts can be found in Appendix G.\\n\\n# Traditional fine-tuning (not used for GPT-3)\\n\\n# Fine-tuning\\n\\nThe model is trained via repeated gradient updates using a large corpus of example tasks.\\n\\n![img-6.jpeg](img-6.jpeg)\\n\\n- Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task. This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of pre-training data), but is also the most challenging setting. In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases \"unfairly hard\". For example, if someone is asked to \"make a table of world records for the 200m dash\", this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult). Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks – for example, in the translation example in Figure 2.1, a human would likely know what to do from just the text instruction.\\n\\nFigure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.\\n\\nSections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.\\n\\n|  Model Name | nparams | nlayers | dmodel | nheads | dhead | Batch Size | Learning Rate  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Small | 125M | 12 | 768 | 12 | 64 | 0.5M | 6.0 × 10-4  |\\n|  GPT-3 Medium | 350M | 24 | 1024 | 16 | 64 | 0.5M | 3.0 × 10-4  |\\n|  GPT-3 Large | 760M | 24 | 1536 | 16 | 96 | 0.5M | 2.5 × 10-4  |\\n|  GPT-3 XL | 1.3B | 24 | 2048 | 24 | 128 | 1M | 2.0 × 10-4  |\\n|  GPT-3 2.7B | 2.7B | 32 | 2560 | 32 | 80 | 1M | 1.6 × 10-4  |\\n|  GPT-3 6.7B | 6.7B | 32 | 4096 | 32 | 128 | 2M | 1.2 × 10-4  |\\n|  GPT-3 13B | 13.0B | 40 | 5140 | 40 | 128 | 2M | 1.0 × 10-4  |\\n|  GPT-3 175B or “GPT-3” | 175.0B | 96 | 12288 | 96 | 128 | 3.2M | 0.6 × 10-4  |\\n\\nTable 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens.\\n\\n# 2.1 Model and Architectures\\n\\nWe use the same model and architecture as GPT-2  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.\\n\\nTable 2.1 shows the sizes and architectures of our 8 models. Here  $n_{\\\\mathrm{params}}$  is the total number of trainable parameters,  $n_{\\\\mathrm{layers}}$  is the total number of layers,  $d_{\\\\mathrm{model}}$  is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer,  $d_{\\\\mathrm{ff}} = 4 * d_{\\\\mathrm{model}}$ ), and  $d_{\\\\mathrm{head}}$  is the dimension of each attention head. All models use a context window of  $n_{\\\\mathrm{ctx}} = 2048$  tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\\'s. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.\\n\\n# 2.2 Training Dataset\\n\\nDatasets for language models have rapidly expanded, culminating in the Common Crawl dataset $^2$  [RSR $^{+}$ 19] constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy dedduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.\\n\\nDetails of the first two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , collected by scraping links over a longer period of time, and first described in  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$ , two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.\\n\\nTable 2.2 shows the final mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.\\n\\n![img-7.jpeg](img-7.jpeg)\\nTotal Compute Used During Training\\nFigure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B is almost  $10\\\\mathrm{x}$  larger than RoBERTa-Large (355M params), both models took roughly 50 petaflop/s-days of compute during pre-training. Methodology for these calculations can be found in Appendix D.\\n\\n|  Dataset | Quantity (tokens) | Weight in training mix | Epochs elapsed when training for 300B tokens  |\\n| --- | --- | --- | --- |\\n|  Common Crawl (filtered) | 410 billion | 60% | 0.44  |\\n|  WebText2 | 19 billion | 22% | 2.9  |\\n|  Books1 | 12 billion | 8% | 1.9  |\\n|  Books2 | 55 billion | 8% | 0.43  |\\n|  Wikipedia | 3 billion | 3% | 3.4  |\\n\\nTable 2.2: Datasets used to train GPT-3. \"Weight in training mix\" refers to the fraction of examples during training that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once.\\n\\nA major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.\\n\\n# 2.3 Training Process\\n\\nAs found in  $\\\\left[\\\\mathrm{KMH}^{+}20, \\\\mathrm{MKAT18}\\\\right]$ , larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU\\'s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B.\\n\\n2.4 Evaluation\\n\\nFor few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.\\n\\n$K$ can be any value from 0 to the maximum amount allowed by the model’s context window, which is $n_{\\\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples. Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.\\n\\nOn tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\\\frac{P(\\\\mathrm{completion}|\\\\mathrm{context})}{P(\\\\mathrm{completion}|\\\\mathrm{answer\\\\_context})}$, where $\\\\mathrm{answer\\\\_context}$ is the string \"Answer: \" or \"A: \" and is used to prompt that the completion should be an answer but is otherwise generic.\\n\\nOn tasks that involve binary classification, we give the options more semantically meaningful names (e.g. “True” or “False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by *[RSR^{+}19]* (see Appendix G) for details.\\n\\nOn tasks with free-form completion, we use beam search with the same parameters as *[RSR^{+}19]*: a beam width of 4 and a length penalty of $\\\\alpha=0.6$. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.\\n\\nFinal results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.\\n\\n## 3 Results\\n\\nIn Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in *[KMH^{+}20]*, language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.\\n\\nBelow, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks.\\n\\nIn Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on “closed book” question answering tasks: tasks which require using the information stored in the model’s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model’s ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model’s performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we briefly explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities – these tasks focus on on-the-fly reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings.\\n\\n![img-8.jpeg](img-8.jpeg)\\nFigure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in  $\\\\left[\\\\mathrm{KMH}^{+}20\\\\right]$  continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts.\\n\\n|  Setting | PTB  |\\n| --- | --- |\\n|  SOTA (Zero-Shot) | 35.8a  |\\n|  GPT-3 Zero-Shot | 20.5  |\\n\\nTable 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3\\'s training data.  ${}^{a}\\\\left\\\\lbrack  {\\\\mathrm{{RWC}}}^{ + }{19}\\\\right\\\\rbrack$\\n\\n# 3.1 Language Modeling, Cloze, and Completion Tasks\\n\\nIn this section we test GPT-3\\'s performance on the traditional task of language modeling, as well as related tasks that involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible completions of a piece of text.\\n\\n# 3.1.1 Language Modeling\\n\\nWe calculate zero-shot perplexity on the Penn Tree Bank (PTB)  $\\\\left[\\\\mathrm{MKM}^{+}94\\\\right]$  dataset measured in  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . We omit the 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the one-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these issues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15 points, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to define one-shot or few-shot evaluation around, so we measure only zero-shot.\\n\\n# 3.1.2 LAMBADA\\n\\nThe LAMBADA dataset  $\\\\left[\\\\mathrm{PKL}^{+}16\\\\right]$  tests the modeling of long-range dependencies in text - the model is asked to predict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the continued scaling of language models is yielding diminishing returns on this difficult benchmark.  $\\\\left[\\\\mathrm{BHT}^{+}20\\\\right]$  reflect on the small  $1.5\\\\%$  improvement achieved by a doubling of model size between two recent state of the art results  $\\\\left(\\\\left[\\\\mathrm{SPP}^{+}19\\\\right]\\\\right.$\\n\\n|  Setting | LAMBADA (acc) | LAMBADA (ppl) | StoryCloze (acc) | HellaSwag (acc)  |\\n| --- | --- | --- | --- | --- |\\n|  SOTA | 68.0a | 8.63b | 91.8c | 85.6d  |\\n|  GPT-3 Zero-Shot | 76.2 | 3.00 | 83.2 | 78.9  |\\n|  GPT-3 One-Shot | 72.5 | 3.35 | 84.7 | 78.1  |\\n|  GPT-3 Few-Shot | 86.4 | 1.92 | 87.7 | 79.3  |\\n\\nTable 3.2: Performance on cloze and completion tasks. GPT-3 significantly improves SOTA on LAMBADA while achieving respectable performance on two difficult completion prediction datasets.  ${}^{a}$  [Tur20]  ${}^{b}$  [RWC+19]  ${}^{c}$  [LDL19]  ${}^{d}$  [LCH+20]\\n\\n![img-9.jpeg](img-9.jpeg)\\nFigure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of the art by  $18\\\\%$ . Note zero-shot uses a different format from one-shot and few-shot as described in the text.\\n\\nand [Tur20]) and argue that \"continuing to expand hardware and data sizes by orders of magnitude is not the path forward\". We find that path is still promising and in a zero-shot setting GPT-3 achieves  $76\\\\%$  on LAMBADA, a gain of  $8\\\\%$  over the previous state of the art.\\n\\nLAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  (which ban \"continuation\" words). The few-shot setting instead allows us to \"frame\" the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following fill-in-the-blank format:\\n\\nAlice was friends with Bob. Alice went to visit her friend  $\\\\underline{\\\\hspace{1cm}}$ .  $\\\\rightarrow$  Bob\\n\\nGeorge bought some baseball equipment, a ball, a glove, and a  $\\\\underline{\\\\hspace{1cm}}$ .  $\\\\rightarrow$\\n\\nWhen presented with examples formatted this way, GPT-3 achieves  $86.4\\\\%$  accuracy in the few-shot setting, an increase of over  $18\\\\%$  from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost  $20\\\\%$ , for GPT-3 it improves accuracy by  $10\\\\%$ . Finally, the fill-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern.\\n\\n|  Setting | NaturalQS | WebQS | TriviaQA  |\\n| --- | --- | --- | --- |\\n|  RAG (Fine-tuned, Open-Domain) [LPP+20] | 44.5 | 45.5 | 68.0  |\\n|  T5-11B+SSM (Fine-tuned, Closed-Book) [RRS20] | 36.6 | 44.7 | 60.5  |\\n|  T5-11B (Fine-tuned, Closed-Book) | 34.5 | 37.4 | 50.1  |\\n|  GPT-3 Zero-Shot | 14.6 | 14.4 | 64.3  |\\n|  GPT-3 One-Shot | 23.0 | 25.3 | 68.0  |\\n|  GPT-3 Few-Shot | 29.9 | 41.5 | 71.2  |\\n\\nTable 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the wiki split test server.\\n\\nOne note of caution is that an analysis of test set contamination identified that a significant minority of the LAMBADA dataset appears to be present in our training data – however analysis performed in Section 4 suggests negligible impact on performance.\\n\\n# 3.1.3 HellaSwag\\n\\nThe HellaSwag dataset  $\\\\left[\\\\mathrm{ZHB}^{+}19\\\\right]$  involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be difficult for language models while remaining easy for humans (who achieve  $95.6\\\\%$  accuracy). GPT-3 achieves  $78.1\\\\%$  accuracy in the one-shot setting and  $79.3\\\\%$  accuracy in the few-shot setting, outperforming the  $75.4\\\\%$  accuracy of a fine-tuned 1.5B parameter language model  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$  but still a fair amount lower than the overall SOTA of  $85.6\\\\%$  achieved by the fine-tuned multi-task model ALUM.\\n\\n# 3.1.4 StoryCloze\\n\\nWe next evaluate GPT-3 on the StoryCloze 2016 dataset  $\\\\left[\\\\mathrm{MCH}^{+}16\\\\right]$ , which involves selecting the correct ending sentence for five-sentence long stories. Here GPT-3 achieves  $83.2\\\\%$  in the zero-shot setting and  $87.7\\\\%$  in the few-shot setting (with  $K = 70$ ). This is still  $4.1\\\\%$  lower than the fine-tuned SOTA using a BERT based model [LDL19] but improves over previous zero-shot results by roughly  $10\\\\%$ .\\n\\n# 3.2 Closed Book Question Answering\\n\\nIn this section we measure GPT-3\\'s ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to find relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted \"open-book\". [RRS20] recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxiliary information. They denote this more restrictive evaluation setting as \"closed-book\". Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions  $\\\\left[\\\\mathrm{KPR}^{+}19\\\\right]$ , WebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, fine-tuning on the Q&amp;A dataset itself is also not permitted.\\n\\nThe results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve  $64.3\\\\%$  in the zero-shot setting,  $68.0\\\\%$  in the one-shot setting, and  $71.2\\\\%$  in the few-shot setting. The zero-shot result already outperforms the fine-tuned T5-11B by  $14.2\\\\%$ , and also outperforms a version with Q&amp;A tailored span prediction during pre-training by  $3.8\\\\%$ . The one-shot result improves by  $3.7\\\\%$  and matches the SOTA for an open-domain QA system which not only fine-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents  $\\\\left[\\\\mathrm{LPP}^{+}20\\\\right]$ . GPT-3\\'s few-shot result further improves performance another  $3.2\\\\%$  beyond this.\\n\\nOn WebQuestions (WebQs), GPT-3 achieves  $14.4\\\\%$  in the zero-shot setting,  $25.3\\\\%$  in the one-shot setting, and  $41.5\\\\%$  in the few-shot setting. This compares to  $37.4\\\\%$  for fine-tuned T5-11B, and  $44.7\\\\%$  for fine-tuned T5-11B+SSM, which uses a Q&amp;A-specific pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art fine-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to few-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions\\n\\n![img-10.jpeg](img-10.jpeg)\\nFigure 3.3: On TriviaQA GPT3\\'s performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG  $\\\\left[\\\\mathrm{LPP}^{+}20\\\\right]$\\n\\nand/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance in the few-shot setting.\\n\\nOn Natural Questions (NQs) GPT-3 achieves  $14.6\\\\%$  in the zero-shot setting,  $23.0\\\\%$  in the one-shot setting, and  $29.9\\\\%$  in the few-shot setting, compared to  $36.6\\\\%$  for fine-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to TriviaQA and WebQS. In particular, the questions in NQs tend towards very fine-grained knowledge on Wikipedia specifically which could be testing the limits of GPT-3\\'s capacity and broad pretraining distribution.\\n\\nOverall, on one of the three datasets GPT-3\\'s one-shot matches the open-domain fine-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using fine-tuning. On all 3 datasets, we find that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reflecting the idea that model capacity translates directly to more \\'knowledge\\' absorbed in the parameters of the model.\\n\\n# 3.3 Translation\\n\\nFor GPT-2 a filter was used on a multilingual collection of documents to produce an English only dataset due to capacity concerns. Even with this filtering GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training dataset to include more representation of other languages, though this remains an area for further improvement. As discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based filtering. Although GPT-3\\'s training data is still primarily English (93% by word count), it also includes 7% of text in other languages. These languages are documented in the supplemental material. In order to better understand translation capability, we also expand our analysis to include two additional commonly studied languages, German and Romanian.\\n\\nExisting unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation [SHB15] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages together in a natural way, combining them on a word, sentence, and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in particular. However, our one / few-shot settings aren\\'t strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data.\\n\\nResults are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task, still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for\\n\\n|  Setting | En→Fr | Fr→En | En→De | De→En | En→Ro | Ro→En  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  SOTA (Supervised) | 45.6a | 35.0b | 41.2c | 40.2d | 38.5e | 39.9e  |\\n|  XLM [LC19] | 33.4 | 33.3 | 26.4 | 34.3 | 33.3 | 31.8  |\\n|  MASS [STQ+19] | 37.5 | 34.9 | 28.3 | 35.2 | 35.2 | 33.1  |\\n|  mBART [LGG+20] | - | - | 29.8 | 34.0 | 35.0 | 30.5  |\\n|  GPT-3 Zero-Shot | 25.2 | 21.2 | 24.6 | 27.2 | 14.1 | 19.9  |\\n|  GPT-3 One-Shot | 28.3 | 33.7 | 26.2 | 30.4 | 20.6 | 38.6  |\\n|  GPT-3 Few-Shot | 32.6 | 39.2 | 29.7 | 40.6 | 21.0 | 39.5  |\\n\\nTable 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating into English reflecting its strength as an English LM. We report BLEU scores on the WMT\\'14 Fr  $\\\\leftrightarrow$  En, WMT\\'16 De  $\\\\leftrightarrow$  En, and WMT\\'16 Ro  $\\\\leftrightarrow$  En datasets as measured by multi-bleu.perl with XLM\\'s tokenization in order to compare most closely with prior unsupervised NMT work. SacreBLEU\\' [Pos18] results reported in Appendix H. Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA with relative confidence.  $^a$  [EOAG18]  $^b$  [DHKH14]  $^c$  [WXH+18]  $^d$  [oR16]  $^e$  [LGG+20]  $^f$  [SacreBLEU signature: BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]\\n\\n![img-11.jpeg](img-11.jpeg)\\nFigure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent trend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be stronger than translation from English.\\n\\n|  Setting | Winograd | Winogrande (XL)  |\\n| --- | --- | --- |\\n|  Fine-tuned SOTA | 90.1a | 84.6b  |\\n|  GPT-3 Zero-Shot | 88.3* | 70.2  |\\n|  GPT-3 One-Shot | 89.7* | 73.2  |\\n|  GPT-3 Few-Shot | 88.6* | 77.7  |\\n\\nTable 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set.  ${}^{a}$  [SBBC19]  ${}^{b}$  [LYN+20]\\n\\n![img-12.jpeg](img-12.jpeg)\\nFigure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a fine-tuned RoBERTA-large.\\n\\neach translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could find but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised finetuning on 608K labeled examples, and backtranslation [LHCG19b].\\n\\nFinally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three settings is shown in Appendix H.\\n\\n# 3.4 Winograd-Style Tasks\\n\\nThe Winograd Schemas Challenge [LDM12] is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently fine-tuned language models have achieved near-human performance on the original Winograd dataset, but more difficult versions\\n\\n|  Setting | PIQA | ARC (Easy) | ARC (Challenge) | OpenBookQA  |\\n| --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 79.4 | 92.0[KKS+20] | 78.5[KKS+20] | 87.2[KKS+20]  |\\n|  GPT-3 Zero-Shot | 80.5* | 68.8 | 51.4 | 57.6  |\\n|  GPT-3 One-Shot | 80.5* | 71.2 | 53.2 | 58.8  |\\n|  GPT-3 Few-Shot | 82.8* | 70.1 | 51.5 | 65.4  |\\n\\nTable 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set.\\n\\n![img-13.jpeg](img-13.jpeg)\\nFigure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a score on the development set in all three conditions that exceeds the best recorded score on the task.\\n\\nsuch as the adversarially-mined Winogrande dataset [SBBC19] still significantly lag human performance. We test GPT-3\\'s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting.\\n\\nOn Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same \"partial evaluation\" method described in  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classification and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves  $88.3\\\\%$ ,  $89.7\\\\%$ , and  $88.6\\\\%$  in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4).\\n\\nOn the more difficult Winogrande dataset, we do find gains to in-context learning: GPT-3 achieves  $70.2\\\\%$  in the zero-shot setting,  $73.2\\\\%$  in the one-shot setting, and  $77.7\\\\%$  in the few-shot setting. For comparison a fine-tuned RoBERTA model achieves  $79\\\\%$ , state-of-the-art is  $84.6\\\\%$  achieved with a fine-tuned high capacity model (T5), and human performance on the task as reported by [SBBC19] is  $94.0\\\\%$ .\\n\\n# 3.5 Common Sense Reasoning\\n\\nNext we consider three datasets which attempt to capture physical or scientific reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering. The first, PhysicalQA (PIQA)  $\\\\left[\\\\mathrm{BZB}^{+}19\\\\right]$ , asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world. GPT-3 achieves  $81.0\\\\%$  accuracy zero-shot,  $80.5\\\\%$  accuracy one-shot, and  $82.8\\\\%$  accuracy few-shot (the last measured on PIQA\\'s test server). This compares favorably to the  $79.4\\\\%$  accuracy prior state-of-the-art of a\\n\\n|  Setting | CoQA | DROP | QuAC | SQuADv2 | RACE-h | RACE-m  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 90.7a | 89.1b | 74.4c | 93.0d | 90.0e | 93.1e  |\\n|  GPT-3 Zero-Shot | 81.5 | 23.6 | 41.5 | 59.5 | 45.5 | 58.4  |\\n|  GPT-3 One-Shot | 84.0 | 34.3 | 43.3 | 65.4 | 45.9 | 57.4  |\\n|  GPT-3 Few-Shot | 85.0 | 36.5 | 44.3 | 69.8 | 46.8 | 58.1  |\\n\\nTable 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy.  ${}^{a}\\\\left\\\\lbrack  {\\\\mathrm{{JZC}}}^{ + }{19}\\\\right\\\\rbrack  {}^{b}\\\\left\\\\lbrack  {\\\\mathrm{{JN20}}}\\\\right\\\\rbrack  {}^{c}\\\\left\\\\lbrack  {\\\\mathrm{{AI19}}}\\\\right\\\\rbrack  {}^{d}\\\\left\\\\lbrack  {\\\\mathrm{{QIA20}}}\\\\right\\\\rbrack  {}^{e}\\\\left\\\\lbrack  {\\\\mathrm{{SPP}}}^{ + }{19}\\\\right\\\\rbrack$\\n\\nfine-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over  $10\\\\%$  worse than human performance, but GPT-3\\'s few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis flagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk. See Section 4 for details.\\n\\nARC  $\\\\left[\\\\mathrm{CCE}^{+}18\\\\right]$  is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the \"Challenge\" version of the dataset which has been filtered to questions which simple statistical or information retrieval methods are unable to correctly answer, GPT-3 achieves  $51.4\\\\%$  accuracy in the zero-shot setting,  $53.2\\\\%$  in the one-shot setting, and  $51.5\\\\%$  in the few-shot setting. This is approaching the performance of a fine-tuned RoBERTa baseline  $(55.9\\\\%)$  from UnifiedQA  $\\\\left[\\\\mathrm{KKS}^{+}20\\\\right]$ . On the \"Easy\" version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves  $68.8\\\\%$ ,  $71.2\\\\%$ , and  $70.1\\\\%$  which slightly exceeds a fine-tuned RoBERTa baseline from  $\\\\left[\\\\mathrm{KKS}^{+}20\\\\right]$ . However, both of these results are still much worse than the overall SOTAs achieved by the UnifiedQA which exceeds GPT-3\\'s few-shot results by  $27\\\\%$  on the challenge set and  $22\\\\%$  on the easy set.\\n\\nOn OpenBookQA [MCKS18], GPT-3 improves significantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3\\'s few-shot performance is similar to a fine-tuned BERT Large baseline on the leaderboard.\\n\\nOverall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a significant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings.\\n\\n# 3.6 Reading Comprehension\\n\\nNext we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3\\'s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.\\n\\nGPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC  $\\\\left[\\\\mathrm{CHI}^{+}18\\\\right]$  a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP  $\\\\left[\\\\mathrm{DWD}^{+}19\\\\right]$ , a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems  $\\\\left[\\\\mathrm{RLL}^{+}19\\\\right]$ . On SQuAD 2.0 [RJL18], GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best fine-tuned result in the original paper. On RACE  $\\\\left[\\\\mathrm{LXL}^{+}17\\\\right]$ , a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still  $45\\\\%$  behind SOTA.\\n\\n# 3.7 SuperGLUE\\n\\nIn order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark  $\\\\left[\\\\mathrm{WPN}^{+}19\\\\right]$ $\\\\left[\\\\mathrm{WPN}^{+}19\\\\right]$ $\\\\left[\\\\mathrm{CLC}^{+}19\\\\right]$  [DMST19] [RBG11]  $\\\\left[\\\\mathrm{KCR}^{+}18\\\\right]$ $\\\\left[\\\\mathrm{ZLL}^{+}18\\\\right]$  [DGM06]  $\\\\left[\\\\mathrm{BHDD}^{+}06\\\\right]$  [GMDD07]  $\\\\left[\\\\mathrm{BDD}^{+}09\\\\right]$  [PCC18]  $\\\\left[\\\\mathrm{PHR}^{+}18\\\\right]$ . GPT-3\\'s test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC\\n\\n![img-14.jpeg](img-14.jpeg)\\nFigure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting, only a few points behind measured human performance and state-of-the-art fine-tuned models. Zero-shot and one-shot performance is a few points behind, with the gains to few-shot being largest for bigger models.\\n\\n|   | SuperGLUE Average | BoolQ Accuracy | CB Accuracy | CB F1 | COPA Accuracy | RTE Accuracy  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  Fine-tuned SOTA | 89.0 | 91.0 | 96.9 | 93.9 | 94.8 | 92.5  |\\n|  Fine-tuned BERT-Large | 69.0 | 77.4 | 83.6 | 75.7 | 70.6 | 71.7  |\\n|  GPT-3 Few-Shot | 71.8 | 76.4 | 75.6 | 52.0 | 92.0 | 69.0  |\\n|   | WiC Accuracy | WSC Accuracy | MultiRC Accuracy | MultiRC F1a | ReCoRD Accuracy | ReCoRD F1  |\\n|  Fine-tuned SOTA | 76.1 | 93.8 | 62.3 | 88.2 | 92.5 | 93.3  |\\n|  Fine-tuned BERT-Large | 69.6 | 64.6 | 24.1 | 70.0 | 71.3 | 72.0  |\\n|  GPT-3 Few-Shot | 49.4 | 80.1 | 30.5 | 75.4 | 90.2 | 91.1  |\\n\\nTable 3.8: Performance of GPT-3 on SuperGLUE compared to fine-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates.\\n\\n![img-15.jpeg](img-15.jpeg)\\nFigure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value of  $K = 32$  means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table 3.8). The BERT-Large reference model was fine-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was first fine-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further fine-tuning on the SuperGLUE training set (for a total of 630K fine-tuning examples). We find the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context.\\n\\n![img-16.jpeg](img-16.jpeg)\\n\\nand MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we used the same set of randomly drawn examples from the training set as context for all of the problems we evaluated.\\n\\nWe observe a wide range in GPT-3\\'s performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving second place on the leaderboard, where first place is held by a fine-tuned 11 billion parameter model (T5). On WSC, performance is still relatively strong, achieving  $80.1\\\\%$  in the few-shot setting (note that GPT-3 achieves  $88.6\\\\%$  on the original Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable, roughly matching that of a fine-tuned BERT-Large. On CB, we see signs of life at  $75.6\\\\%$  in the few-shot setting.\\n\\nWiC is a notable weak spot with few-shot performance at  $49.4\\\\%$  (at random chance). We tried a number of different phrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two sentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer in the next section (which discusses the ANLI benchmark) – GPT-3 appears to be weak in the few-shot or one-shot setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same way in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another. This could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses, GPT-3 still outperforms a fine-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to the state-of-the-art held by a fine-tuned 11 billion parameter model.\\n\\nFinally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of examples in the context showing increasing benefits from in-context learning (Figure 3.8). We scale  $K$  up to 32 examples per task, after which point additional examples will not reliably fit into our context. When sweeping over values of  $K$ , we find that GPT-3 requires less than eight total examples per task to outperform a fine-tuned BERT-Large on overall SuperGLUE score.\\n\\n# 3.8 NLI\\n\\nNatural Language Inference (NLI) [Fyo00] concerns the ability to understand the relationship between two sentences. In practice, this task is usually structured as a two or three class classification problem where the model classifies\\n\\n![img-17.jpeg](img-17.jpeg)\\nFigure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of  $1.2\\\\%$ ). We find that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix.\\n\\nwhether the second sentence logically follows from the first, contradicts the first sentence, or is possibly true (neutral). SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task fine-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset  $\\\\left[\\\\mathrm{NWD}^{+}19\\\\right]$ . ANLI is a difficult dataset employing a series of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our models smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting ( $\\\\sim 33\\\\%$ ), whereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results for all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very difficult task for language models and they are only just beginning to show signs of progress.\\n\\n# 3.9 Synthetic and Qualitative Tasks\\n\\nOne way to probe GPT-3\\'s range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which require it to perform simple on-the-fly computational reasoning, recognize a novel pattern that is unlikely to have occurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we test GPT-3\\'s ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3\\'s ability to solve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models.\\n\\n# 3.9.1 Arithmetic\\n\\nTo test GPT-3\\'s ability to perform simple arithmetic operations without task-specific training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:\\n\\n- 2 digit addition  $(2\\\\mathbf{D}+)$  - The model is asked to add two integers sampled uniformly from  $[0,100)$ , phrased in the form of a question, e.g. \"Q: What is 48 plus 76? A: 124.\"\\n- 2 digit subtraction (2D-) - The model is asked to subtract two integers sampled uniformly from  $[0,100)$ ; the answer may be negative. Example: \"Q: What is 34 minus 53? A: -19\".\\n- 3 digit addition  $(3\\\\mathbf{D}+)$  - Same as 2 digit addition, except numbers are uniformly sampled from [0, 1000).\\n\\n![img-18.jpeg](img-18.jpeg)\\nFigure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a significant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being able to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a significant fraction of the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot are shown in the appendix.\\n\\n- 3 digit subtraction (3D-) - Same as 2 digit subtraction, except numbers are uniformly sampled from [0, 1000).\\n- 4 digit addition  $(4\\\\mathbf{D} + )$  - Same as 3 digit addition, except uniformly sampled from [0, 10000).\\n- 4 digit subtraction (4D-) - Same as 3 digit subtraction, except uniformly sampled from [0, 10000).\\n- 5 digit addition  $(5\\\\mathbf{D} + )$  - Same as 3 digit addition, except uniformly sampled from [0, 100000).\\n- 5 digit subtraction (5D-) - Same as 3 digit subtraction, except uniformly sampled from [0, 100000).\\n- 2 digit multiplication (2Dx) - The model is asked to multiply two integers sampled uniformly from [0, 100), e.g. \"Q: What is 24 times 42? A: 1008\".\\n- One-digit composite (1DC) - The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the last two. For example, \"Q: What is  $6 + (4^{*}8)$ ? A: 38\". The three 1 digit numbers are selected uniformly on [0, 10) and the operations are selected uniformly from  $\\\\{+, -, *\\\\}$ .\\n\\nIn all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances.\\n\\nFirst we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction, GPT-3 displays strong proficiency when the number of digits is small, achieving  $100\\\\%$  accuracy on 2 digit addition,  $98.9\\\\%$  at 2 digit subtraction,  $80.2\\\\%$  at 3 digit addition, and  $94.2\\\\%$  at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves  $25 - 26\\\\%$  accuracy on four digit operations and  $9 - 10\\\\%$  accuracy on five digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves  $29.2\\\\%$  accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves  $21.3\\\\%$  accuracy at single digit combined operations (for example,  $9^{*}(7 + 5)$ ), suggesting that it has some robustness beyond just single operations.\\n\\nAs Figure 3.10 makes clear, small models do poorly on all of these tasks – even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than  $10\\\\%$  of the time.\\n\\nOne-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly. Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 significantly\\n\\n|  Setting | 2D+ | 2D- | 3D+ | 3D- | 4D+ | 4D- | 5D+ | 5D- | 2Dx | 1DC  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Zero-shot | 76.9 | 58.0 | 34.2 | 48.3 | 4.0 | 7.5 | 0.7 | 0.8 | 19.8 | 9.8  |\\n|  GPT-3 One-shot | 99.6 | 86.4 | 65.5 | 78.7 | 14.0 | 14.0 | 3.5 | 3.8 | 27.4 | 14.3  |\\n|  GPT-3 Few-shot | 100.0 | 98.9 | 80.4 | 94.2 | 25.5 | 26.8 | 9.3 | 9.9 | 29.2 | 21.3  |\\n\\nTable 3.9: Results on basic arithmetic tasks for GPT-3 175B.  $\\\\{2,3,4,5\\\\} \\\\mathrm{D}\\\\{+, - \\\\}$  is 2, 3, 4, and 5 digit addition or subtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger moving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows significant arithmetic abilities.\\n\\n|  Setting | CL | A1 | A2 | RI | RW  |\\n| --- | --- | --- | --- | --- | --- |\\n|  GPT-3 Zero-shot | 3.66 | 2.28 | 8.91 | 8.26 | 0.09  |\\n|  GPT-3 One-shot | 21.7 | 8.62 | 25.9 | 45.4 | 0.48  |\\n|  GPT-3 Few-shot | 37.9 | 15.1 | 39.7 | 67.2 | 0.44  |\\n\\nTable 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and few-shot settings. CL is \"cycle letters in word\", A1 is anagrams of but the first and last letters, A2 is anagrams of all but the first and last two letters, RI is \"Random insertion in word\", RW is \"reversed words\".\\n\\noutperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table 3.9, and model capacity scaling for all three settings is shown in Appendix H.\\n\\nTo spot-check whether the model is simply memorizing specific arithmetic problems, we took the 3-digit arithmetic problems in our test set and searched for them in our training data in both the forms \"<num1> + <num2> = \" and \"<num1> plus <num2>\". Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000 subtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers could have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes such as not carrying a \"1\", suggesting it is actually attempting to perform the relevant computation rather than memorizing a table.\\n\\nOverall, GPT-3 displays reasonable proficiency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings.\\n\\n# 3.9.2 Word Scrambling and Manipulation Tasks\\n\\nTo test GPT-3\\'s ability to learn novel symbolic manipulations from a few examples, we designed a small battery of 5 \"character manipulation\" tasks. Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are:\\n\\n- Cycle letters in word (CL) - The model is given a word with its letters cycled, then the “=” symbol, and is expected to generate the original word. For example, it might be given “lyinevitab” and should output “inevitably”.\\n- Anagrams of all but first and last characters (A1) - The model is given a word where every letter except the first and last have been scrambled randomly, and must output the original word. Example: criroptuon = corruption.\\n- Anagrams of all but first and last 2 characters (A2) - The model is given a word where every letter except the first 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt  $\\\\rightarrow$  opponent.\\n- Random insertion in word (RI) - A random punctuation or space character is inserted between each letter of a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession.\\n- Reversed words (RW) - The model is given a word spelled backwards, and must output the original word. Example: stcejbo  $\\\\rightarrow$  objects.\\n\\nFor each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by [Nor09] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11. Task performance tends to grow smoothly with model size, with the full GPT-3 model achieving  $66.9\\\\%$  on removing</num2></num1></num2></num1>\\n\\n![img-19.jpeg](img-19.jpeg)\\nFigure 3.11: Few-shot performance on the five word scrambling tasks for different sizes of model. There is generally smooth improvement with model size although the random insertion task shows an upward slope of improvement with the 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in the appendix. All tasks are done with  $K = 100$ .\\n\\nrandom insertions,  $38.6\\\\%$  on cycling letters,  $40.2\\\\%$  on the easier anagram task, and  $15.1\\\\%$  on the more difficult anagram task (where only the first and last letters are held fixed). None of the models can reverse the letters in a word.\\n\\nIn the one-shot setting, performance is significantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artificial nature makes them unlikely to appear in the pre-training data (although we cannot confirm this with certainty).\\n\\nWe can further quantify performance by plotting \"in-context learning curves\", which show task performance as a function of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task in Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions.\\n\\nFinally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on significant fractions of a word (on average  $\\\\sim 0.7$  words per token), so from the LM\\'s perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also, CL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word), requiring the model to perform some search to find the correct unscrambling. Thus, the skills involved appear to require non-trivial pattern-matching and computation.\\n\\n# 3.9.3 SAT Analogies\\n\\nTo test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 \"SAT analogy\" problems [TLBS03]. Analogies are a style of multiple choice question that constituted a section of the SAT college entrance exam before 2005. A typical example is \"audacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation\". The student is expected to choose which of the five word pairs has the same relationship as the original word pair; in this example the answer is \"sanctimonious is to hypocrisy\". On this task GPT-3 achieves  $65.2\\\\%$  in the few-shot setting,  $59.1\\\\%$  in the one-shot setting, and  $53.7\\\\%$  in the zero-shot setting, whereas the average score among college applicants was  $57\\\\%$  [TL05] (random guessing yields  $20\\\\%$ ). As shown in Figure 3.12, the results improve with scale, with the full 175 billion model improving by over  $10\\\\%$  compared to the 13 billion parameter model.\\n\\n![img-20.jpeg](img-20.jpeg)\\nFigure 3.12: Zero-, one-, and few-shot performance on SAT analogy tasks, for different sizes of model. The largest model achieves  $65\\\\%$  accuracy in the few-shot setting, and also demonstrates significant gains to in-context learning which are not present in smaller models.\\n\\n# 3.9.4 News Article Generation\\n\\nPrevious work on generative language models qualitatively tested their ability to generate synthetic \"news articles\" by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ . Relative to  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$ , the dataset used to train GPT-3 is much less weighted towards news articles, so trying to generate news articles via raw unconditional samples is less effective – for example GPT-3 often interprets the proposed first sentence of a \"news article\" as a tweet and then posts synthetic responses or follow-up tweets. To solve this problem we employed GPT-3\\'s few-shot learning abilities by providing three previous news articles in the model\\'s context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the \"news\" genre.\\n\\nTo gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles from real ones. Similar work has been carried out by Kreps et al. [KMB20] and Zellers et al.  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$ . Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality.\\n\\nIn order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model. Participants were asked to select whether the article was \"very likely written by a human\", \"more likely written by a human\", \"I don\\'t know\", \"more likely written by a machine\", or \"very likely written by a machine\".\\n\\nThe articles we selected were not in the models\\' training data and the model outputs were formatted and selected programmatically to prevent human cherry-picking. All models used the same context to condition outputs on and were pre-trained with the same context size and the same article titles and subtitles were used as prompts for each model. However, we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a \"control model\": a 160M parameter model with no context and increased output randomness.\\n\\n|   | Mean accuracy | 95% Confidence Interval (low, hi) | t compared to control (p-value) | “I don’t know” assignments  |\\n| --- | --- | --- | --- | --- |\\n|  Control (deliberately bad model) | 86% | 83%-90% | - | 3.6 %  |\\n|  GPT-3 Small | 76% | 72%-80% | 3.9 (2e-4) | 4.9%  |\\n|  GPT-3 Medium | 61% | 58%-65% | 10.3 (7e-21) | 6.0%  |\\n|  GPT-3 Large | 68% | 64%-72% | 7.3 (3e-11) | 8.7%  |\\n|  GPT-3 XL | 62% | 59%-65% | 10.7 (1e-19) | 7.5%  |\\n|  GPT-3 2.7B | 62% | 58%-65% | 10.4 (5e-19) | 7.1%  |\\n|  GPT-3 6.7B | 60% | 56%-63% | 11.2 (3e-21) | 6.2%  |\\n|  GPT-3 13B | 55% | 52%-58% | 15.3 (1e-32) | 7.1%  |\\n|  GPT-3 175B | 52% | 49%-54% | 16.9 (1e-34) | 7.8%  |\\n\\nTable 3.11: Human accuracy in identifying whether short (~200 word) news articles are model generated. We find that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from  $86\\\\%$  on the control model to  $52\\\\%$  on GPT-3 175B. This table compares mean accuracy between five different models, and shows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model (an unconditional GPT-3 Small model with increased output randomness).\\n\\nMean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that the intentionally bad articles were model generated was  $\\\\sim 86\\\\%$  where  $50\\\\%$  is chance level performance. By contrast, mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance at  $\\\\sim 52\\\\%$  (see Table 3.11). Human abilities to detect model generated text appear to decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance. This is true despite the fact that participants spend more time on each output as model size increases (see Appendix E).\\n\\nExamples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15. Much of the text is—as indicated by the evaluations—difficult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors, the models have no access to the specific facts that the article titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are often subtle enough that they are not noticed.\\n\\nRelated work on language model detection by Ippolito et al. [IDCBE19] indicates that automatic discriminators like GROVER  $\\\\left[\\\\mathrm{ZHR}^{+}19\\\\right]$  and GLTR [GSR19] may have greater success at detecting model generated text than human evaluators. Automatic detection of these models may be a promising area of future research.\\n\\nIppolito et al. [IDCBE19] also note that human accuracy at detecting model generated text increases as humans observe more tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial experiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to compare human abilities to detect the articles generated by GPT-3 and a control model.\\n\\nWe found that mean human accuracy at detecting the intentionally bad longer articles from the control model was  $\\\\sim 88\\\\%$ , while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at  $\\\\sim 52\\\\%$  (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles.\\n\\n# 3.9.5 Learning and Using Novel Words\\n\\nA task studied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a word in a sentence after seeing it defined only once, or conversely inferring a word\\'s meaning from only one usage. Here we qualitatively test GPT-3\\'s ability to do the former. Specifically, we give GPT-3 the definition of a nonexistent word, such as \"Gigamuru\", and then ask it to use it in a sentence. We provide one to five previous examples of a (separate)\\n\\n![img-21.jpeg](img-21.jpeg)\\nFigure 3.13: People\\'s ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately-bad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance  $(50\\\\%)$  is indicated with the dashed line at the bottom. Line of best fit is a power law with  $95\\\\%$  confidence intervals.\\n\\n|   | Mean accuracy | 95% Confidence Interval (low, hi) | t compared to control (p-value) | “I don’t know” assignments  |\\n| --- | --- | --- | --- | --- |\\n|  Control | 88% | 84%-91% | - | 2.7%  |\\n|  GPT-3 175B | 52% | 48%-57% | 12.7 (3.2e-23) | 10.6%  |\\n\\nTable 3.12: People\\'s ability to identify whether  $\\\\sim 500$  word articles are model generated (as measured by the ratio of correct assignments to non-neutral assignments) was  $88\\\\%$  on the control model and  $52\\\\%$  on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with increased output randomness).\\n\\nTitle: United Methodists Agree to Historic Split\\nSubtitle: Those who oppose gay marriage will form their own denomination\\nArticle: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church\\'s annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination.\\nThe Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the \"largest Protestant denomination in the U.S.,\" but that it has been shrinking in recent decades. The new split will be the second in the church\\'s history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church. The Post notes that the proposed split \"comes at a critical time for the church, which has been losing members for years,\" which has been \"pushed toward the brink of a schism over the role of LGBTQ people in the church.\" Gay marriage is not the only issue that has divided the church. In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban them from serving as clergy, and the South Pacific regional conference voting to allow them.\\n\\nFigure 3.14: The GPT-3 generated news article that humans had the greatest difficulty distinguishing from a human written article (accuracy:  $12\\\\%$ ).\\n\\nTitle: Star\\'s Tux Promise Draws Megyn Kelly\\'s Sarcasm\\nSubtitle: Joaquin Phoenix pledged to not change for each awards event\\nArticle: A year ago, Joaquin Phoenix made headlines when he appeared on the red carpet at the Golden Globes wearing a tuxedo with a paper bag over his head that read, \"I am a shape-shifter. I can\\'t change the world. I can only change myself.\" It was a promise to not change to fit into the Hollywood mold: \"I think that\\'s a really special thing, to not change yourself. I think it\\'s a really special thing to say, \\'This is what\\'s inside of me, I\\'m proud of it, and I\\'m not going to be ashamed because of the way that someone else thinks I should be.\\'\" Now, it\\'s the Oscars, and Phoenix is at it again. But this time, his publicist is saying he\\'ll be wearing a tux no matter what.\\nMegyn Kelly was not impressed, and she let him have it on The Tonight Show. \"You know, I feel like, I feel like you could have worn the tux,\" she says. \"But you\\'re saying you\\'re a shape-shifter. I don\\'t know if you can change your tux, but you can change your mind. You can change your mind. You can change your mind.\" Phoenix says he did, but it didn\\'t stick. \"I was like, \\'Okay, I\\'m going to wear a tuxedo to this thing.\\' And then I thought, \\'I don\\'t want to wear a tuxedo to this thing.\\'\" Kelly goes on to encourage him to change his mind again, but Phoenix says it\\'s too late: \"I\\'m committed to wearing this.\"\\n\\nFigure 3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written article (accuracy:  $61\\\\%$ ).\\n\\n![img-22.jpeg](img-22.jpeg)\\nFigure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is GPT-3\\'s completions, plain text is human prompts. In the first example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 other than the conditioning shown here.\\n\\nnonexistent word being defined and used in a sentence, so the task is few-shot in terms of previous examples of the broad task and one-shot in terms of the specific word. Table 3.16 shows the 6 examples we generated; all definitions were human-generated, and the first answer was human-generated as conditioning while the subsequent answers were generated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the final sentence the model generates a plausible conjugation for the word \"screeg\" (namely \"screeghed\"), although the use of the word is slightly awkward (\"screeghed at each other\") despite being plausible in the sense that it could describe a toy sword fight. Overall, GPT-3 appears to be at least proficient at the task of using novel words in a sentence.\\n\\n# 3.9.6 Correcting English Grammar\\n\\nAnother task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the few-shot setting by giving prompts of the form \"Poor English Input: <sentence>\\\\n Good English Output: <sentence>\". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats). Results are shown in Figure 3.17.\\n\\n# 4 Measuring and Preventing Memorization Of Benchmarks\\n\\nSince our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to.\\n\\nThis concern is not just hypothetical. One of the first papers to train a language model on Common Crawl data [TL18] detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2  $\\\\left[\\\\mathrm{RWC}^{+}19\\\\right]$  also conducted post-hoc overlap analysis. Their study was relatively encouraging, finding that</sentence></sentence>\\n\\n![img-23.jpeg](img-23.jpeg)\\nFigure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface is GPT-3\\'s completions, plain text is human prompts. In the first few examples example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 aside from the first few examples as conditioning and the \"Poor English input/Good English output\" framing. We note that the distinction between \"poor\" and \"good\" English (and the terms themselves) is complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what \"good\" is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word \"cheap\" in a way that alters meaning).\\n\\n![img-24.jpeg](img-24.jpeg)\\nFigure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difficulty rather than overfitting.\\n\\nalthough models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).\\n\\nGPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared.\\n\\nWe initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn\\'t feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results.\\n\\nFor each benchmark, we produce a \\'clean\\' version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix C.\\n\\nWe then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over  $50\\\\%$ ), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance.\\n\\nBelow, we review in more detail the few specific cases where either (1) the model performs significantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difficult.\\n\\nOur analysis flagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English\\n\\n![img-25.jpeg](img-25.jpeg)\\nFigure 4.2: Benchmark contamination analysis We constructed cleaned versions of each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high confidence to be clean, and the y-axis shows the difference in performance when evaluating only on the verified clean subset. Performance on most benchmarks changed negligibly, but some were flagged for further review. On inspection we find some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section 3 with an asterisk. We find no evidence that other benchmarks are affected.\\n\\ntranslation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false positives. We summarize the results for each group of tasks below:\\n\\n- Reading Comprehension: Our initial analysis flagged  $&gt;90\\\\%$  of task examples from QuAC, SQuAD2, and DROP as potentially contaminated, so large that even measuring the differential on a clean subset was difficult. Upon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source text was present in our training data but the question/answer pairs were not, meaning the model gains only background information and cannot memorize the answer to a specific question.\\n- German translation: We found  $25\\\\%$  of the examples in the WMT16 German-English test set were marked as potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the flagged examples contain paired sentences resembling NMT training data and collisions were monolingual matches mostly of snippets of events discussed in the news.\\n- Reversed Words and Anagrams: Recall that these tasks are of the form \"alaok = koala\". Due to the short length of these tasks, we used 2-grams for filtering (ignoring punctuation). After inspecting the flagged overlaps, we found that they were not typically instances of real reversals or unscramblings in the training set, but rather palindromes or trivial unscramblings, e.g. \"kayak = kayak\". The amount of overlap was small, but removing the trivial tasks lead to an increase in difficulty and thus a spurious signal. Related to this, the symbol insertion task shows high overlap but no effect on performance – this is because that task involves removing non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to many spurious matches.\\n- PIQA: The overlap analysis flagged  $29\\\\%$  of examples as contaminated, and observed a 3 percentage point absolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was released after our training set was created and its labels are hidden, some of the web pages used by the crowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller model with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias rather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot rigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential contamination.\\n- Winograd: The overlap analysis flagged  $45\\\\%$  of examples, and found a  $2.6\\\\%$  decrease in performance on the clean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in fact present in our training set, though presented in a different format than we present the task to the model. Although the decrease in performance is small, we mark our Winograd results in the main paper with an asterisk.\\n\\n- Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the Children’s Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark.\\n\\nWe also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our fill-in-the-blank format precludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section.\\n\\nAn important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset. It remains possible that memorization inflates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing.\\n\\nOverall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the field in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C.\\n\\n## 5 Limitations\\n\\nGPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work.\\n\\nFirst, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3’s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with “common sense physics”, despite doing well on some datasets (such as PIQA *[BZB^{+}19]*) that test this domain. Specifically GPT-3 has difficulty with questions of the type “If I put cheese into the fridge, will it melt?”. Quantitatively, GPT-3’s in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some “comparison” tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3’s strong few-shot performance on many other tasks.\\n\\nGPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models *[RSR^{+}19]*. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3’s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the “best of both worlds”.\\n\\nA more fundamental limitation of the general approach described in this paper – scaling up any LM-like model, whether autoregressive or bidirectional – is that it may eventually run into (or could already be running into) the limits of the\\n\\npretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. *[x20]* demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world *[BHT^{+}20]*. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans *[ZSW^{+}19a]*, fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world *[CLY^{+}19]*.\\n\\nAnother limitation broadly shared by language models is poor sample efficiency during pre-training. While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime *[x14]*. Improving pre-training sample efficiency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements.\\n\\nA limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identifies tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a specific style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or defining nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research.\\n\\nA limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation *[x13]* of large models down to a manageable size for specific tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a specific task, suggesting that in principle aggressive distillation may be possible. Distillation is well-explored in general *[x15]* but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size.\\n\\nFinally, GPT-3 shares some limitations common to most deep learning systems – its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This last issue – biases in the data that may lead the model to generate stereotyped or prejudiced content – is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section 6).\\n\\n## 6 Broader Impacts\\n\\nLanguage models have a wide range of beneficial applications for society, including code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. But they also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difficulty of distinguishing synthetic text from human-written text. It therefore has the potential to advance both the beneficial and harmful applications of language models.\\n\\nHere we focus on the potential harms of improved language models, not because we believe the harms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also briefly discuss issues of energy efficiency (Section 6.3).\\n\\n6.1 Misuse of Language Models\\n\\nMalicious uses of language models can be somewhat difficult to anticipate because they often involve repurposing language models in a very different environment or for a different purpose than researchers intended. To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact *[x20]*. We discuss three factors: potential misuse applications, threat actors, and external incentive structures.\\n\\n#### 6.1.1 Potential Misuse Applications\\n\\nAny socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufficiently high quality text. Language models that produce high quality text generation could lower existing barriers to carrying out these activities and increase their efficacy.\\n\\nThe misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to generate several paragraphs of synthetic content that people find difficult to distinguish from human-written text in 3.9.4 represents a concerning milestone in this regard.\\n\\n#### 6.1.2 Threat Actor Analysis\\n\\nThreat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors who may be able to build a malicious product to ‘advanced persistent threats’ (APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas *[SBC+19]*.\\n\\nTo understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did find significant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate, but significant improvements in reliability could change this.\\n\\nBecause APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about possible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible difference in operations that may see potential gains by using language models. The assessment was that language models may not be worth investing significant resources in because there has been no convincing demonstration that current language models are significantly better than current methods for generating text, and because methods for “targeting” or “controlling” the content of language models are still at a very early stage.\\n\\n#### 6.1.3 External Incentive Structures\\n\\nEach threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their agenda. TTPs are influenced by economic factors like scalability and ease of deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login credentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment.\\n\\nEase of use is another significant incentive. Having stable infrastructure has a large impact on the adoption of TTPs. The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot produces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor required in operating this bot. But a human is still needed to filter the outputs, which restricts how scalable the operation can be.\\n\\nBased on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will eventually develop language models that are sufficiently consistent and steerable that they will be of greater interest to malicious actors. We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers.\\n\\n####\\n\\n6.2 Fairness, Bias, and Representation\\n\\nBiases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms *[x10]*. We have conducted an analysis of biases in the model in order to better understand GPT-3’s limitations when it comes to fairness, bias, and representation.\\n\\nOur goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reflect all of the model’s biases even within the studied categories.\\n\\nBroadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data. Below we discuss our preliminary findings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension.\\n\\n#### 6.2.1 Gender\\n\\nIn our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3. We measured this by feeding the model a context such as \"The detective was a\" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identifiers include midwife, nurse, receptionist, housekeeper etc.\\n\\nWe also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation} was a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent {occupation} was a,\" the majority of occupations had an even higher probability of being followed by a male identifier than a female one than was the case with our original neutral prompt, \"The {occupation} was a\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as $\\\\frac{1}{n_{\\\\text{jobs}}}\\\\sum_{\\\\text{jobs}}\\\\log(\\\\frac{P(\\\\text{female}|\\\\text{Context})}{P(\\\\text{male}|\\\\text{Context})})$ - was $-1.11$ for the Neutral Variant, $-2.14$ for the Competent Variant and $-1.15$ for the Incompetent Variant.\\n\\nWe also carried out pronoun resolution on the Winogender dataset *[x23]* using two methods which further corroborated the model’s tendency to associate most occupations with males. One method measured the models ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \"The advisor met with the advisee because she wanted to get advice about job applications. ’She’ refers to the\" and found the option with the lowest probability between the two possible options (Choices between Occupation Option: advisor; Participant Option: advisee).\\n\\nOccupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models.\\n\\nWe also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-selected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature\\n\\nTable 6.1: Most Biased Descriptive Words in 175B Model\\n\\n|  Top 10 Most Biased Male Descriptive Words with Raw Co-Occurrence Counts | Top 10 Most Biased Female Descriptive Words with Raw Co-Occurrence Counts  |\\n| --- | --- |\\n|  Average Number of Co-Occurrences Across All Words: 17.5 | Average Number of Co-Occurrences Across All Words: 23.9  |\\n|  Large (16) | Optimistic (12)  |\\n|  Mostly (15) | Bubbly (12)  |\\n|  Lazy (14) | Naughty (12)  |\\n|  Fantastic (13) | Easy-going (12)  |\\n|  Eccentric (13) | Petite (10)  |\\n|  Protect (10) | Tight (10)  |\\n|  Jolly (10) | Pregnant (10)  |\\n|  Stable (9) | Gorgeous (28)  |\\n|  Personable (22) | Sucked (8)  |\\n|  Survive (7) | Beautiful (158)  |\\n\\nof 1 and top_p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\", \"She was very\", \"He would be described as\", \"She would be described as\". We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more often described using appearance oriented words such as \"beautiful\" and \"gorgeous\" as compared to men who were more often described using adjectives that span a greater spectrum.\\n\\nTable 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. \"Most Favored\" here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender.\\n\\n# 6.2.2 Race\\n\\nTo investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\", \"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation  $\\\\left[\\\\mathrm{HZJ}^{+}19\\\\right]$ , we explored how race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5, horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).\\n\\nIt should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology.\\n\\nAcross the models we analyzed, \\'Asian\\' had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, \\'Black\\' had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data.\\n\\n![img-26.jpeg](img-26.jpeg)\\nFigure 6.1: Racial Sentiment Across Models\\n\\n|  Religion | Most Favored Descriptive Words  |\\n| --- | --- |\\n|  Atheism | ‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’, ‘Characterized’  |\\n|  Buddhism | ‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘En- lightenment’, ‘Non-Violent’  |\\n|  Christianity | ‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Com- ments’, ‘Officially’  |\\n|  Hinduism | ‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated’, ‘Africa’  |\\n|  Islam | ‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’, ‘Levant’, ‘Allah’, ‘Prophet’  |\\n|  Judaism | ‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’, ‘Game’, ‘Russian’  |\\n\\nTable 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model.\\n\\n# 6.2.3 Religion\\n\\nWe studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length  $\\\\approx 50$  with a temperature of 1 and a top  $p$  of 0.9 for every prompt. Our prompts were of the nature \" {Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words.\\n\\nThe following is an example output from the model:\\n\\n\"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic.\"'), 0.032961634341792), (Document(id='6525a2245e91:2', metadata={'text': '|  Target Completion → | 2844  |\\n\\nFigure G.48: Formatted dataset example for Arithmetic 4D-\\n\\n|  Context → | Q: What is 9923 plus 617?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | 10540  |\\n\\nFigure G.49: Formatted dataset example for Arithmetic 4D+\\n\\n|  Context → | Q: What is 40649 minus 78746?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | -38097  |\\n\\nFigure G.50: Formatted dataset example for Arithmetic 5D-\\n\\n|  Context → | Q: What is 65360 plus 16204?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | 81564  |\\n\\nFigure G.51: Formatted dataset example for Arithmetic 5D+\\n\\nH Results on All Tasks for All Model Sizes\\n\\n|  Name | Metric | Splits | Fine-tune |   | Zero-Shot |   |   |   |   | One-Shot |   |   |   |   | Few-Shot |   |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   |   |   |  SOTA | K | Small Med Large | XL | 2.7B | 6.7B | 13B | 175B | Small Med Large | XL | 2.7B | 6.7B | 13B | 175B | Small Med Large | XL | 2.7B | 6.7B  |\\n|  HellaSwag | acc | dev | 85.6 | 20 | 33.7 | 43.6 | 51.0 | 54.7 | 62.8 | 67.4 | 70.9 | 78.9 | 33.0 | 42.9 | 50.5 | 53.5 | 61.9 | 66.5 | 70.0 | 78.1  |\\n|  LAMBADA | acc | test | 68.0 | 15 | 42.7 | 54.3 | 60.4 | 63.6 | 67.1 | 70.3 | 72.5 | 76.2 | 22.0 | 47.1 | 52.6 | 58.3 | 61.1 | 65.4 | 69.0 | 72.5  |\\n|  LAMBADA | ppl | test | 8.63 | 15 | 18.6 | 9.09 | 6.53 | 5.44 | 4.60 | 4.00 | 3.56 | 3.00 | 165.0 | 11.6 | 8.29 | 6.46 | 5.53 | 4.61 | 4.06 | 3.35  |\\n|  StoryCloze | acc | test | 91.8 | 70 | 63.3 | 68.5 | 72.4 | 73.4 | 77.2 | 77.7 | 79.5 | 83.2 | 62.3 | 68.7 | 72.3 | 74.2 | 77.3 | 78.7 | 79.7 | 84.7  |\\n|  NQs | acc | test | 44.5 | 64 | 0.64 | 1.75 | 2.71 | 4.40 | 6.01 | 5.79 | 7.84 | 14.6 | 1.19 | 3.07 | 4.79 | 5.43 | 8.73 | 9.78 | 13.7 | 23.0  |\\n|  TriviaQA | acc | dev | 68.0 | 64 | 4.15 | 7.61 | 14.0 | 19.7 | 31.3 | 38.7 | 41.8 | 64.3 | 4.19 | 12.9 | 20.5 | 26.5 | 35.9 | 44.4 | 51.3 | 68.0  |\\n|  WebQs | acc | test | 45.5 | 64 | 1.77 | 3.20 | 4.33 | 4.63 | 7.92 | 7.73 | 8.22 | 14.4 | 2.56 | 6.20 | 8.51 | 9.15 | 14.5 | 15.1 | 19.0 | 25.3  |\\n|  Ro→En 16 | BLEU-mb | test | 39.9 | 64 | 2.08 | 2.71 | 3.09 | 3.15 | 16.3 | 8.34 | 20.2 | 19.9 | 0.55 | 15.4 | 23.0 | 26.3 | 30.6 | 33.2 | 35.6 | 38.6  |\\n|  Ro→En 16 | BLEU-sb | test |  | 64 | 2.39 | 3.08 | 3.49 | 3.56 | 16.8 | 8.75 | 20.8 | 20.9 | 0.65 | 15.9 | 23.6 | 26.8 | 31.3 | 34.2 | 36.7 | 40.0  |\\n|  En→Ro 16 | BLEU-mb | test | 38.5 | 64 | 2.14 | 2.65 | 2.53 | 2.50 | 3.46 | 4.24 | 5.32 | 14.1 | 0.35 | 3.30 | 7.89 | 8.72 | 13.2 | 15.1 | 17.3 | 20.6  |\\n|  En→Ro 16 | BLEU-sb | test |  | 64 | 2.61 | 3.11 | 3.07 | 3.09 | 4.26 | 5.31 | 6.43 | 18.0 | 0.55 | 3.90 | 9.15 | 10.3 | 15.7 | 18.2 | 20.8 | 24.9  |\\n|  Fr→En 14 | BLEU-mb | test | 35.0 | 64 | 1.81 | 2.53 | 3.47 | 3.13 | 20.6 | 15.1 | 21.8 | 21.2 | 1.28 | 15.9 | 23.7 | 26.3 | 29.0 | 30.5 | 30.2 | 33.7  |\\n|  Fr→En 14 | BLEU-sb | test |  | 64 | 2.29 | 2.99 | 3.90 | 3.60 | 21.2 | 15.5 | 22.4 | 21.9 | 1.50 | 16.3 | 24.4 | 27.0 | 30.0 | 31.6 | 31.4 | 35.6  |\\n|  En→Fr 14 | BLEU-mb | test | 45.6 | 64 | 1.74 | 2.16 | 2.73 | 2.15 | 15.1 | 8.82 | 12.0 | 25.2 | 0.49 | 8.00 | 14.8 | 15.9 | 20.3 | 23.3 | 24.9 | 28.3  |\\n|  En→Fr 14 | BLEU-sb | test | 45.9 | 64 | 2.44 | 2.75 | 3.54 | 2.82 | 19.3 | 11.4 | 15.3 | 31.3 | 0.81 | 10.0 | 18.2 | 19.3 | 24.7 | 28.3 | 30.1 | 34.1  |\\n|  De→En 16 | BLEU-mb | test | 40.2 | 64 | 2.06 | 2.87 | 3.41 | 3.63 | 21.5 | 17.3 | 23.0 | 27.2 | 0.83 | 16.2 | 22.5 | 24.7 | 28.2 | 30.7 | 33.0 | 30.4  |\\n|  De→En 16 | BLEU-sb | test |  | 64 | 2.39 | 3.27 | 3.85 | 4.04 | 22.5 | 18.2 | 24.4 | 28.6 | 0.93 | 17.1 | 23.4 | 25.8 | 29.2 | 31.9 | 34.5 | 32.1  |\\n|  En→De 16 | BLEU-mb | test | 41.2 | 64 | 1.70 | 2.27 | 2.31 | 2.43 | 12.9 | 8.66 | 10.4 | 24.6 | 0.50 | 7.00 | 12.9 | 13.1 | 18.3 | 20.9 | 22.5 | 26.2  |\\n|  En→De 16 | BLEU-sb | test | 41.2 | 64 | 2.09 | 2.65 | 2.75 | 2.92 | 13.7 | 9.36 | 11.0 | 25.3 | 0.54 | 7.40 | 13.4 | 13.4 | 18.8 | 21.7 | 23.3 | 27.3  |\\n|  Winegrad | acc | test | 93.8 | 7 | 66.3 | 72.9 | 74.7 | 76.9 | 82.4 | 85.7 | 87.9 | 88.3 | 63.4 | 68.5 | 72.9 | 76.9 | 82.4 | 84.6 | 86.1 | 89.7  |\\n|  Winegranule | acc | dev | 84.6 | 50 | 52.0 | 52.1 | 57.4 | 58.7 | 62.3 | 64.5 | 67.9 | 70.2 | 51.3 | 53.0 | 58.3 | 59.1 | 61.7 | 65.8 | 66.9 | 73.2  |\\n|  PIQA | acc | dev | 77.1 | 50 | 64.6 | 70.2 | 72.9 | 75.1 | 75.6 | 78.0 | 78.5 | 81.0 | 64.3 | 69.3 | 71.8 | 74.4 | 74.3 | 76.3 | 77.8 | 80.5  |\\n|  ARC (Challenge) | acc | test | 78.5 | 50 | 26.6 | 29.5 | 31.8 | 35.5 | 38.0 | 41.4 | 43.7 | 51.4 | 25.5 | 30.2 | 31.6 | 36.4 | 38.4 | 41.5 | 43.1 | 53.2  |\\n|  ARC (Easy) | acc | test | 92.0 | 50 | 43.6 | 46.5 | 53.0 | 53.8 | 58.2 | 60.2 | 63.8 | 68.8 | 42.7 | 48.2 | 54.6 | 55.9 | 60.3 | 62.6 | 66.8 | 71.2  |\\n|  OpenBookQA | acc | test | 87.2 | 100 | 35.6 | 43.2 | 45.2 | 46.8 | 53.0 | 50.4 | 55.6 | 57.0 | 39.8 | 46.2 | 46.4 | 53.4 | 53.0 | 55.8 | 58.8 | 37.0  |\\n|  Quac | f1 | dev | 74.4 | 5 | 21.2 | 26.8 | 31.0 | 30.1 | 34.7 | 36.1 | 38.4 | 41.5 | 21.1 | 26.9 | 31.9 | 32.3 | 37.4 | 39.0 | 40.6 | 43.4  |\\n|  RACE-h | acc | test | 90.0 | 10 | 35.2 | 37.9 | 40.1 | 40.9 | 42.4 | 44.1 | 44.6 | 45.5 | 34.3 | 37.7 | 40.0 | 42.0 | 43.8 | 44.3 | 44.6 | 45.9  |\\n|  RACE-m | acc | test | 93.1 | 10 | 42.1 | 47.2 | 52.1 | 52.3 | 54.7 | 54.4 | 56.7 | 58.4 | 42.3 | 47.3 | 51.7 | 55.2 | 56.1 | 54.7 | 56.9 | 57.4  |\\n|  SQuAD-2 | em | dev | 90.7 | 16 | 22.6 | 32.8 | 33.9 | 43.1 | 43.6 | 45.4 | 49.0 | 52.6 | 25.1 | 37.5 | 37.9 | 47.9 | 47.9 | 51.1 | 56.0 | 60.1  |\\n|  SQuADv2 | f1 | dev | 93.0 | 16 | 28.3 | 40.2 | 41.4 | 50.3 | 51.0 | 52.7 | 56.3 | 59.5 | 30.1 | 43.6 | 44.1 | 54.0 | 54.1 | 57.1 | 61.8 | 65.4  |\\n|  CoQA | f1 | dev | 90.7 | 5 | 34.5 | 55.0 | 61.8 | 65.3 | 71.1 | 72.8 | 76.3 | 81.5 | 30.6 | 52.1 | 61.6 | 66.1 | 71.8 | 75.1 | 77.9 | 84.0  |\\n|  DROP | f1 | dev | 89.1 | 20 | 9.40 | 13.6 | 14.4 | 16.4 | 19.7 | 17.0 | 24.0 | 23.6 | 11.7 | 18.1 | 20.9 | 23.0 | 26.4 | 27.3 | 29.2 | 34.3  |\\n|  BoxQ | acc | dev | 91.0 | 32 | 49.7 | 60.3 | 58.9 | 62.4 | 67.1 | 65.4 | 66.2 | 60.5 | 52.6 | 61.7 | 60.4 | 63.7 | 68.4 | 68.7 | 69.0 | 76.7  |\\n|  CB | acc | dev | 96.9 | 32 | 0.00 | 32.1 | 8.93 | 19.6 | 19.6 | 28.6 | 19.6 | 46.4 | 55.4 | 53.6 | 53.6 | 48.2 | 57.1 | 33.9 | 55.4 | 64.3  |\\n|  CB | f1 | dev | 93.9 | 32 | 0.00 | 29.3 | 11.4 | 17.4 | 22.4 | 25.1 | 20.3 | 42.8 | 60.1 | 39.8 | 45.6 | 37.5 | 45.7 | 28.5 | 44.6 | 52.5  |\\n|  Copa | acc | dev | 94.8 | 32 | 66.0 | 68.0 | 73.0 | 77.0 | 76.0 | 80.0 | 84.0 | 91.0 | 62.0 | 64.0 | 66.0 | 74.0 | 76.0 | 82.0 | 86.0 | 87.0  |\\n|  RTE | acc | dev | 92.5 | 32 | 47.7 | 49.8 | 48.4 | 56.0 | 46.6 | 55.2 | 62.8 | 63.5 | 53.1 | 47.3 | 49.5 | 49.5 | 54.9 | 56.3 | 70.4 |   |\\n|  WcC | acc | dev | 76.1 | 32 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 50.0 | 50.3 | 50.3 | 49.2 | 49.4 | 50.3 | 50.0 | 48.6  |\\n|  WSC | acc | dev | 93.8 | 32 | 59.6 | 56.7 | 65.4 | 61.5 | 66.3 | 60.6 | 64.4 | 65.4 | 58.7 | 58.7 | 60.6 | 62.5 | 66.3 | 60.6 | 66.3 | 69.2  |\\n|  MultiRC | acc | dev | 62.3 | 32 | 4.72 | 9.65 | 12.3 | 13.6 | 14.3 | 18.4 | 24.2 | 27.6 | 6.09 | 11.8 | 16.8 | 20.8 | 24.7 | 23.8 | 25.0 | 32.5  |\\n|  MultiRC | f1a | dev | 88.2 | 32 | 57.0 | 59.7 | 60.4 | 59.9 | 60.0 | 64.5 | 71.4 | 72.9 | 57.0 | 59.7 | 60.4 | 59.9 | 60.0 | 64.5 | 71.4 | 72.9  |\\n|  ReCoRD | acc | dev | 92.5 | 32 | 70.8 | 78.5 | 82.1 | 84.1 | 86.2 | 86.6 | 89.0 | 90.2 | 69.8 | 77.0 | 80.7 | 83.0 | 85.9 | 88.0 | 88.8 | 90.2  |\\n|  ReCoRD | f1 | dev | 93.3 | 32 | 71.9 | 79.2 | 82.8 | 85.2 | 87.3 | 89.5 | 90.4 | 91.0 | 70.7 | 77.8 | 81.6 | 83.9 | 86.8 | 88.8 | 89.7 | 91.2  |\\n|  SuperGLUE | average | dev | 89.0 |  | 40.6 | 47.4 | 46.8 | 49.6 | 50.1 | 52.3 | 54.4 | 58.2 | 54.4 | 55.1 | 56.7 | 57.8 | 61.2 | 59.7 | 64.3 | 68.9  |\\n|  ANLI R1 | acc | test | 73.8 | 50 | 33.4 | 34.2 | 33.4 | 33.4 | 34.2 | 32.3 | 33.2 | 34.6 | 32.1 | 31.6 | 31.9 | 34.6 | 30.6 | 31.6 | 32.7 | 32.0  |\\n|  ANLI R2 | acc | test | 50.7 | 50 | 33.2 | 31.9 | 33.3 | 33.3 | 33.8 | 33.5 | 33.5 | 35.4 | 35.7 | 33.7 | 33.2 | 32.7 | 32.7 | 33.9 | 33.9 | 33.9  |\\n|  ANLI R3 | acc | test | 48.5 | 50 | 33.6 | 34.0 | 33.8 | 33.4 | 35.3 | 34.8 | 34.4 | 34.5 | 35.0 | 32.6 | 33.0 | 33.9 | 34.1 | 33.1 | 32.5 | 35.1  |\\n|  2D+ | acc | n/a |  | 50 | 0.70 | 0.65 | 0.70 | 0.85 | 1.10 | 2.54 | 15.4 | 76.9 | 2.00 | 0.55 | 3.15 | 4.00 | 12.1 | 19.6 | 73.0 | 99.6  |\\n|  2D- | acc | n/a |  | 50 | 1.25 | 1.25 | 1.25 | 1.25 | 1.60 | 7.60 | 12.6 | 58.0 | 1.15 | 0.95 | 1.45 | 1.95 | 3.85 | 11.5 | 44.6 | 86.4  |\\n|  3D+ | acc | n/a |  | 50 | 0.10 | 0.10 | 0.05 | 0.10 | 0.10 | 0.25 | 1.40 | 34.2 | 0.15 | 0.00 | 0.10 | 0.30 | 0.45 | 0.95 | 15.4 | 65.5  |\\n|  3D- | acc | n/a |  | 50 | 0.05 | 0.05 | 0.05 | 0.05 | 0.05 | 0.45 | 1.35 | 48.3 | 0.05 | 0.15 | 0.25 | 0.30 | 0.55 | 1.60 | 61.5 | 78.7  |\\n|  4D+ | acc | n/a |  | 50 | 0.05 | 0.05 | 0.05 | 0.05 | 0.05 | 0.15 | 4.00 |  |  |  |  |  |  |  |  |   |\\n|  4D- | acc | n/a |  | 50 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |  |  |  |  |  |  |  |  |   |\\n|  5D+ | acc | n/a |  | 50 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |  |  |  |  |  |  |  |  |  |   |\\n\\nTable H.1: Scores for every task, setting and model that we investigate in this paper.\\n\\n![img-28.jpeg](img-28.jpeg)\\n\\n![img-29.jpeg](img-29.jpeg)\\n\\n![img-30.jpeg](img-30.jpeg)\\n\\n![img-31.jpeg](img-31.jpeg)\\n\\n![img-32.jpeg](img-32.jpeg)\\n\\n![img-33.jpeg](img-33.jpeg)\\n\\n![img-34.jpeg](img-34.jpeg)\\n\\n![img-35.jpeg](img-35.jpeg)\\n\\n![img-36.jpeg](img-36.jpeg)\\n\\n![img-37.jpeg](img-37.jpeg)\\nFigure H.1: All results for all SuperGLUE tasks.\\n\\n![img-38.jpeg](img-38.jpeg)\\n\\n![img-39.jpeg](img-39.jpeg)\\nFigure H.2: Results for SAT task.\\n\\n![img-40.jpeg](img-40.jpeg)\\nFigure H.3: All results for all Winograd tasks.\\n\\n![img-41.jpeg](img-41.jpeg)\\n\\n![img-42.jpeg](img-42.jpeg)\\n\\n![img-43.jpeg](img-43.jpeg)\\n\\n![img-44.jpeg](img-44.jpeg)\\n\\n![img-45.jpeg](img-45.jpeg)\\n\\n![img-46.jpeg](img-46.jpeg)\\n\\n![img-47.jpeg](img-47.jpeg)\\n\\n![img-48.jpeg](img-48.jpeg)\\n\\n![img-49.jpeg](img-49.jpeg)\\n\\n![img-50.jpeg](img-50.jpeg)\\n\\n![img-51.jpeg](img-51.jpeg)\\nFigure H.4: All results for all Arithmetic tasks.\\n\\n![img-52.jpeg](img-52.jpeg)\\n\\n![img-53.jpeg](img-53.jpeg)\\nFigure H.5: All results for all Cloze and Completion tasks.\\n\\n![img-54.jpeg](img-54.jpeg)\\n\\n![img-55.jpeg](img-55.jpeg)\\n\\n![img-56.jpeg](img-56.jpeg)\\nFigure H.6: All results for all Common Sense Reasoning tasks.\\n\\n![img-57.jpeg](img-57.jpeg)\\n\\n![img-58.jpeg](img-58.jpeg)\\n\\n![img-59.jpeg](img-59.jpeg)\\nFigure H.7: All results for all QA tasks.\\n\\n![img-60.jpeg](img-60.jpeg)\\n\\n![img-61.jpeg](img-61.jpeg)\\n\\n![img-62.jpeg](img-62.jpeg)\\n\\n![img-63.jpeg](img-63.jpeg)\\n\\n![img-64.jpeg](img-64.jpeg)\\n\\n![img-65.jpeg](img-65.jpeg)\\nFigure H.8: All results for all Reading Comprehension tasks.\\n\\n![img-66.jpeg](img-66.jpeg)\\n\\n![img-67.jpeg](img-67.jpeg)\\n\\n![img-68.jpeg](img-68.jpeg)\\nFigure H.9: All results for all ANLI rounds.\\n\\n![img-69.jpeg](img-69.jpeg)\\n\\n![img-70.jpeg](img-70.jpeg)\\n\\n![img-71.jpeg](img-71.jpeg)\\n\\n![img-72.jpeg](img-72.jpeg)\\n\\n![img-73.jpeg](img-73.jpeg)\\n\\n![img-74.jpeg](img-74.jpeg)\\nFigure H.10: All results for all Scramble tasks.\\n\\n![img-75.jpeg](img-75.jpeg)\\n\\n![img-76.jpeg](img-76.jpeg)\\n\\n![img-77.jpeg](img-77.jpeg)\\n\\n![img-78.jpeg](img-78.jpeg)\\n\\n![img-79.jpeg](img-79.jpeg)\\n\\n![img-80.jpeg](img-80.jpeg)\\n\\n![img-81.jpeg](img-81.jpeg)\\n\\n![img-82.jpeg](img-82.jpeg)\\n\\n![img-83.jpeg](img-83.jpeg)\\n\\n![img-84.jpeg](img-84.jpeg)\\n\\n![img-85.jpeg](img-85.jpeg)\\n\\n![img-86.jpeg](img-86.jpeg)\\nFigure H.11: All results for all Translation tasks.\\n\\n![img-87.jpeg](img-87.jpeg)\\n\\n![img-88.jpeg](img-88.jpeg)\\n\\nReferences\\n\\n- [ADG^{+}16] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in neural information processing systems, pages 3981–3989, 2016.\\n- [AI19] WeChat AI. Tr-mt (ensemble), December 2019.\\n- [AJF19] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\\n- [BBDIW20] Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. Language (technology) is power: A critical survey of “bias” in nlp. arXiv preprint arXiv:2005.14050, 2020.\\n- [BCFL13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013.\\n- [BDD^{+}09] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The fifth PASCAL recognizing textual entailment challenge. 2009.\\n- [BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining. In Lrec, volume 10, pages 2200–2204, 2010.\\n- [BHDD^{+}06] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second PASCAL recognising textual entailment challenge. 2006.\\n- [BHT^{+}20] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. Experience grounds language. arXiv preprint arXiv:2004.10151, 2020.\\n- [BLC13] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. Arxiv, 2013.\\n- [BZB^{+}19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. arXiv preprint arXiv:1911.11641, 2019.\\n- [Car97] Rich Caruana. Multitask learning. Machine learning, 28(1), 1997.\\n- [CB78] Susan Carey and Elsa Bartlett. Acquiring a single new word. Proceedings of the Stanford Child Language Conference, 1978.\\n- [CCE^{+}18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.\\n- [CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019.\\n- [CHI^{+}18] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac : Question answering in context. Arxiv, 2018.\\n- [CLC^{+}19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\\n- [CLY^{+}19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019.\\n- [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote, 2017.\\n- [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n-\\n\\n[DGM06] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising textual entailment, pages 177–190. Springer, 2006.\\n- [DGV^{+}18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. Arxiv, 2018.\\n- [DHKH14] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. Edinburgh’s phrase-based machine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, 2014.\\n- [DL15] Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In Advances in neural information processing systems, 2015.\\n- [DMST19] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigating projection in naturally occurring discourse. 2019. To appear in proceedings of Sinn und Bedeutung 23. Data can be found at https://github.com/mcdm/CommitmentBank/.\\n- [DSC^{+}16] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl^{2}: Fast reinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016.\\n- [DWD^{+}19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019.\\n- [DYY^{+}19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. Arxiv, 2019.\\n- [EOAG18] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. arXiv preprint arXiv:1808.09381, 2018.\\n- [FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. ArXiv, abs/1703.03400, 2017.\\n- [Fyo00] Yaroslav Fyodorov. A natural logic inference system, 2000.\\n- [GG19] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. arXiv preprint arXiv:1903.03862, 2019.\\n- [GLT^{+}20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.\\n- [GMDD07] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9. Association for Computational Linguistics, 2007.\\n- [Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv, 2016.\\n- [GSL^{+}18] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018.\\n- [GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical detection and visualization of generated text. arXiv preprint arXiv: 1906.04043, 2019.\\n- [GWC^{+}18] Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for low-resource neural machine translation. arXiv preprint arXiv:1808.08437, 2018.\\n- [HB20] Daniel Hernandez and Tom Brown. Ai and efficiency, May 2020.\\n- [HBFC19] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. CoRR, abs/1904.09751, 2019.\\n- [HLW^{+}20] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out of distribution robustness. arXiv preprint arXiv:2004.06100, 2020.\\n- [JL17]\\n\\n- [HNA^{+}17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\\n- [HR18] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.\\n- [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\\n- [HYC01] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using Gradient Descent. In International Conference on Artificial Neural Networks, pages 87–94. Springer, 2001.\\n- [HZJ^{+}19] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in language models via counterfactual evaluation. arXiv preprint arXiv:1911.03064, 2019.\\n- [IBGC^{+}14] Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daumé III. A neural network for factoid question answering over paragraphs. In Empirical Methods in Natural Language Processing, 2014.\\n- [IDCBE19] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. arXiv preprint arXiv:1911.00650, 2019.\\n- [JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\\n- [JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020.\\n- [JVS^{+}16] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n- [JYS^{+}19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for natural language understanding. arXiv preprint arXiv:1909.10351, 2019.\\n- [JZC^{+}19] Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yunfeng Liu. Technical report on conversational question answering. arXiv preprint arXiv:1909.10772, 2019.\\n- [KCR^{+}18] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), 2018.\\n- [KKS^{+}20] Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020.\\n- [KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that’s fit to fabricate: Ai-generated text as a tool of media misinformation, 2020.\\n- [KMH^{+}20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\\n- [KPR^{+}19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.\\n- [KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv, 2016.\\n- [LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002.\\n- [LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291, 2019.\\n\\n- [LCG^{+}19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\\n- [LCH^{+}20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020.\\n- [LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. arXiv preprint arXiv:1905.07504, 2019.\\n- [LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012.\\n- [LGG^{+}20] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint arXiv:2001.08210, 2020.\\n- [LGH^{+}15] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representation learning using multi-task deep neural networks for semantic classification and information retrieval. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2015.\\n- [LH17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\\n- [LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural networks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482, 2019.\\n- [LHCG19b] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.\\n- [Lin20] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint arXiv:2005.00955, 2020.\\n- [LLG^{+}19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\\n- [LM17] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017.\\n- [LOG^{+}19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n- [LPP^{+}20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Kiela Douwe. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020.\\n- [LSP^{+}18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.\\n- [LWS^{+}20] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez. Train large, then compress: Rethinking model size for efficient training and inference of transformers, 2020.\\n- [LXL^{+}17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\\n- [LYN^{+}20] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin. Tttttackling winogrande schemas. arXiv preprint arXiv:2003.08380, 2020.\\n- [Mac92] David. MacKay. Information-based objective functions for active data selection. Neural Computation, 1992.\\n\\n[MBXS17] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pages 6294–6305, 2017.\\n- [MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n- [MCH^{+}16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696, 2016.\\n- [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. ArXiv, abs/1809.02789, 2018.\\n- [MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training, 2018.\\n- [MKM^{+}94] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: annotating predicate argument structure. In Proceedings of the workshop on Human Language Technology, pages 114–119. Association for Computational Linguistics, 1994.\\n- [MKXS18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\\n- [MPL19] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019.\\n- [MWZ^{+}18] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting, 2018.\\n- [NBR20] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020.\\n- [NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language arguments. arXiv preprint arXiv:1907.07355, 2019.\\n- [Nor09] Peter Norvig. Natural language corpus data, 2009.\\n- [NvNvdG19] Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational: Man is to doctor as woman is to doctor. arXiv preprint arXiv:1905.09866, 2019.\\n- [NWD^{+}19] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019.\\n- [oR16] University of Regensburg. Fascha, 2016.\\n- [PCC18] Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for evaluating context-sensitive representations. arXiv preprint arXiv:1808.09121, 2018.\\n- [PFB18] Jason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018.\\n- [PHR^{+}18] Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of EMNLP, 2018.\\n- [PKL^{+}16] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\\n- [PNZtY18] Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen tau Yih. Dissecting contextual word embeddings: Architecture and representation, 2018.\\n- [Pos18] Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771, 2018.\\n\\n[PSM14] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014.\\n- [QIA20] QIANXIN. Sa-net on albert (ensemble), April 2020.\\n- [QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-level language models with a gender-equalizing loss function. arXiv preprint arXiv:1905.12801, 2019.\\n- [RBG11] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.\\n- [RCM19] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249–266, 2019.\\n- [RCP^{+}17] Scott Reed, Yutian Chen, Thomas Paine, Aäron van den Oord, SM Eslami, Danilo Rezende, Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn distributions. arXiv preprint arXiv:1710.10304, 2017.\\n- [RJL18] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.\\n- [RL16] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR 2017 (oral), 2016.\\n- [RLL^{+}19] Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading comprehension with numerical reasoning. In Proceedings of EMNLP, 2019.\\n- [RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301, 2018.\\n- [RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018.\\n- [Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication, 2012.\\n- [RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales, 2019.\\n- [RRS20] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\\n- [RSR^{+}19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2019.\\n- [RWC^{+}19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019.\\n- [SBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.\\n- [SBC^{+}19] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang. Release strategies and the social impacts of language models, 2019.\\n- [SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326, 2019.\\n- [SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\\n- [SDSE19] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR, abs/1907.10597, 2019.\\n- [SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709, 2015.\\n\\n- [SMM^{+}17] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n- [SPP^{+}19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019.\\n- [SS20] Timo Schick and Hinrich Schütze. Exploiting cloze questions for few-shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020.\\n- [STQ^{+}19] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.\\n- [TFR^{+}17] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23–30. IEEE, 2017.\\n- [TL05] Peter D. Turney and Michael L. Littman. Corpus-based learning of analogies and semantic relations. CoRR, abs/cs/0508103, 2005.\\n- [TL18] Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018.\\n- [TLBS03] Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. Combining independent modules to solve multiple-choice synonym and analogy problems. CoRR, cs.CL/0309035, 2003.\\n- [Tur20] Project Turing. Microsoft research blog, Feb 2020.\\n- [VBL^{+}16] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching Networks for One Shot Learning. In Advances in neural information processing systems, pages 3630–3638, 2016.\\n- [VSP^{+}17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 2017.\\n- [WPN^{+}19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, pages 3261–3275, 2019.\\n- [WXH^{+}18] Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-agent dual learning. ICLR 2019, 2018.\\n- [XDH^{+}19] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data augmentation for consistency training, 2019.\\n- [YdC^{+}19] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019.\\n- [YDY^{+}19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.\\n- [ZHB^{+}19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\\n- [ZHR^{+}19] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019.\\n- [ZLL^{+}18] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018.\\n- [ZSW^{+}19a] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2019.\\n\\n[ZSW^{+}19b] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593, 2019.', 'doc_id': '6525a2245e91', 'chunk_index': 2, 'start_line': 1365, 'end_line': 1739}, page_content='|  Target Completion → | 2844  |\\n\\nFigure G.48: Formatted dataset example for Arithmetic 4D-\\n\\n|  Context → | Q: What is 9923 plus 617?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | 10540  |\\n\\nFigure G.49: Formatted dataset example for Arithmetic 4D+\\n\\n|  Context → | Q: What is 40649 minus 78746?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | -38097  |\\n\\nFigure G.50: Formatted dataset example for Arithmetic 5D-\\n\\n|  Context → | Q: What is 65360 plus 16204?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | 81564  |\\n\\nFigure G.51: Formatted dataset example for Arithmetic 5D+\\n\\nH Results on All Tasks for All Model Sizes\\n\\n|  Name | Metric | Splits | Fine-tune |   | Zero-Shot |   |   |   |   | One-Shot |   |   |   |   | Few-Shot |   |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   |   |   |  SOTA | K | Small Med Large | XL | 2.7B | 6.7B | 13B | 175B | Small Med Large | XL | 2.7B | 6.7B | 13B | 175B | Small Med Large | XL | 2.7B | 6.7B  |\\n|  HellaSwag | acc | dev | 85.6 | 20 | 33.7 | 43.6 | 51.0 | 54.7 | 62.8 | 67.4 | 70.9 | 78.9 | 33.0 | 42.9 | 50.5 | 53.5 | 61.9 | 66.5 | 70.0 | 78.1  |\\n|  LAMBADA | acc | test | 68.0 | 15 | 42.7 | 54.3 | 60.4 | 63.6 | 67.1 | 70.3 | 72.5 | 76.2 | 22.0 | 47.1 | 52.6 | 58.3 | 61.1 | 65.4 | 69.0 | 72.5  |\\n|  LAMBADA | ppl | test | 8.63 | 15 | 18.6 | 9.09 | 6.53 | 5.44 | 4.60 | 4.00 | 3.56 | 3.00 | 165.0 | 11.6 | 8.29 | 6.46 | 5.53 | 4.61 | 4.06 | 3.35  |\\n|  StoryCloze | acc | test | 91.8 | 70 | 63.3 | 68.5 | 72.4 | 73.4 | 77.2 | 77.7 | 79.5 | 83.2 | 62.3 | 68.7 | 72.3 | 74.2 | 77.3 | 78.7 | 79.7 | 84.7  |\\n|  NQs | acc | test | 44.5 | 64 | 0.64 | 1.75 | 2.71 | 4.40 | 6.01 | 5.79 | 7.84 | 14.6 | 1.19 | 3.07 | 4.79 | 5.43 | 8.73 | 9.78 | 13.7 | 23.0  |\\n|  TriviaQA | acc | dev | 68.0 | 64 | 4.15 | 7.61 | 14.0 | 19.7 | 31.3 | 38.7 | 41.8 | 64.3 | 4.19 | 12.9 | 20.5 | 26.5 | 35.9 | 44.4 | 51.3 | 68.0  |\\n|  WebQs | acc | test | 45.5 | 64 | 1.77 | 3.20 | 4.33 | 4.63 | 7.92 | 7.73 | 8.22 | 14.4 | 2.56 | 6.20 | 8.51 | 9.15 | 14.5 | 15.1 | 19.0 | 25.3  |\\n|  Ro→En 16 | BLEU-mb | test | 39.9 | 64 | 2.08 | 2.71 | 3.09 | 3.15 | 16.3 | 8.34 | 20.2 | 19.9 | 0.55 | 15.4 | 23.0 | 26.3 | 30.6 | 33.2 | 35.6 | 38.6  |\\n|  Ro→En 16 | BLEU-sb | test |  | 64 | 2.39 | 3.08 | 3.49 | 3.56 | 16.8 | 8.75 | 20.8 | 20.9 | 0.65 | 15.9 | 23.6 | 26.8 | 31.3 | 34.2 | 36.7 | 40.0  |\\n|  En→Ro 16 | BLEU-mb | test | 38.5 | 64 | 2.14 | 2.65 | 2.53 | 2.50 | 3.46 | 4.24 | 5.32 | 14.1 | 0.35 | 3.30 | 7.89 | 8.72 | 13.2 | 15.1 | 17.3 | 20.6  |\\n|  En→Ro 16 | BLEU-sb | test |  | 64 | 2.61 | 3.11 | 3.07 | 3.09 | 4.26 | 5.31 | 6.43 | 18.0 | 0.55 | 3.90 | 9.15 | 10.3 | 15.7 | 18.2 | 20.8 | 24.9  |\\n|  Fr→En 14 | BLEU-mb | test | 35.0 | 64 | 1.81 | 2.53 | 3.47 | 3.13 | 20.6 | 15.1 | 21.8 | 21.2 | 1.28 | 15.9 | 23.7 | 26.3 | 29.0 | 30.5 | 30.2 | 33.7  |\\n|  Fr→En 14 | BLEU-sb | test |  | 64 | 2.29 | 2.99 | 3.90 | 3.60 | 21.2 | 15.5 | 22.4 | 21.9 | 1.50 | 16.3 | 24.4 | 27.0 | 30.0 | 31.6 | 31.4 | 35.6  |\\n|  En→Fr 14 | BLEU-mb | test | 45.6 | 64 | 1.74 | 2.16 | 2.73 | 2.15 | 15.1 | 8.82 | 12.0 | 25.2 | 0.49 | 8.00 | 14.8 | 15.9 | 20.3 | 23.3 | 24.9 | 28.3  |\\n|  En→Fr 14 | BLEU-sb | test | 45.9 | 64 | 2.44 | 2.75 | 3.54 | 2.82 | 19.3 | 11.4 | 15.3 | 31.3 | 0.81 | 10.0 | 18.2 | 19.3 | 24.7 | 28.3 | 30.1 | 34.1  |\\n|  De→En 16 | BLEU-mb | test | 40.2 | 64 | 2.06 | 2.87 | 3.41 | 3.63 | 21.5 | 17.3 | 23.0 | 27.2 | 0.83 | 16.2 | 22.5 | 24.7 | 28.2 | 30.7 | 33.0 | 30.4  |\\n|  De→En 16 | BLEU-sb | test |  | 64 | 2.39 | 3.27 | 3.85 | 4.04 | 22.5 | 18.2 | 24.4 | 28.6 | 0.93 | 17.1 | 23.4 | 25.8 | 29.2 | 31.9 | 34.5 | 32.1  |\\n|  En→De 16 | BLEU-mb | test | 41.2 | 64 | 1.70 | 2.27 | 2.31 | 2.43 | 12.9 | 8.66 | 10.4 | 24.6 | 0.50 | 7.00 | 12.9 | 13.1 | 18.3 | 20.9 | 22.5 | 26.2  |\\n|  En→De 16 | BLEU-sb | test | 41.2 | 64 | 2.09 | 2.65 | 2.75 | 2.92 | 13.7 | 9.36 | 11.0 | 25.3 | 0.54 | 7.40 | 13.4 | 13.4 | 18.8 | 21.7 | 23.3 | 27.3  |\\n|  Winegrad | acc | test | 93.8 | 7 | 66.3 | 72.9 | 74.7 | 76.9 | 82.4 | 85.7 | 87.9 | 88.3 | 63.4 | 68.5 | 72.9 | 76.9 | 82.4 | 84.6 | 86.1 | 89.7  |\\n|  Winegranule | acc | dev | 84.6 | 50 | 52.0 | 52.1 | 57.4 | 58.7 | 62.3 | 64.5 | 67.9 | 70.2 | 51.3 | 53.0 | 58.3 | 59.1 | 61.7 | 65.8 | 66.9 | 73.2  |\\n|  PIQA | acc | dev | 77.1 | 50 | 64.6 | 70.2 | 72.9 | 75.1 | 75.6 | 78.0 | 78.5 | 81.0 | 64.3 | 69.3 | 71.8 | 74.4 | 74.3 | 76.3 | 77.8 | 80.5  |\\n|  ARC (Challenge) | acc | test | 78.5 | 50 | 26.6 | 29.5 | 31.8 | 35.5 | 38.0 | 41.4 | 43.7 | 51.4 | 25.5 | 30.2 | 31.6 | 36.4 | 38.4 | 41.5 | 43.1 | 53.2  |\\n|  ARC (Easy) | acc | test | 92.0 | 50 | 43.6 | 46.5 | 53.0 | 53.8 | 58.2 | 60.2 | 63.8 | 68.8 | 42.7 | 48.2 | 54.6 | 55.9 | 60.3 | 62.6 | 66.8 | 71.2  |\\n|  OpenBookQA | acc | test | 87.2 | 100 | 35.6 | 43.2 | 45.2 | 46.8 | 53.0 | 50.4 | 55.6 | 57.0 | 39.8 | 46.2 | 46.4 | 53.4 | 53.0 | 55.8 | 58.8 | 37.0  |\\n|  Quac | f1 | dev | 74.4 | 5 | 21.2 | 26.8 | 31.0 | 30.1 | 34.7 | 36.1 | 38.4 | 41.5 | 21.1 | 26.9 | 31.9 | 32.3 | 37.4 | 39.0 | 40.6 | 43.4  |\\n|  RACE-h | acc | test | 90.0 | 10 | 35.2 | 37.9 | 40.1 | 40.9 | 42.4 | 44.1 | 44.6 | 45.5 | 34.3 | 37.7 | 40.0 | 42.0 | 43.8 | 44.3 | 44.6 | 45.9  |\\n|  RACE-m | acc | test | 93.1 | 10 | 42.1 | 47.2 | 52.1 | 52.3 | 54.7 | 54.4 | 56.7 | 58.4 | 42.3 | 47.3 | 51.7 | 55.2 | 56.1 | 54.7 | 56.9 | 57.4  |\\n|  SQuAD-2 | em | dev | 90.7 | 16 | 22.6 | 32.8 | 33.9 | 43.1 | 43.6 | 45.4 | 49.0 | 52.6 | 25.1 | 37.5 | 37.9 | 47.9 | 47.9 | 51.1 | 56.0 | 60.1  |\\n|  SQuADv2 | f1 | dev | 93.0 | 16 | 28.3 | 40.2 | 41.4 | 50.3 | 51.0 | 52.7 | 56.3 | 59.5 | 30.1 | 43.6 | 44.1 | 54.0 | 54.1 | 57.1 | 61.8 | 65.4  |\\n|  CoQA | f1 | dev | 90.7 | 5 | 34.5 | 55.0 | 61.8 | 65.3 | 71.1 | 72.8 | 76.3 | 81.5 | 30.6 | 52.1 | 61.6 | 66.1 | 71.8 | 75.1 | 77.9 | 84.0  |\\n|  DROP | f1 | dev | 89.1 | 20 | 9.40 | 13.6 | 14.4 | 16.4 | 19.7 | 17.0 | 24.0 | 23.6 | 11.7 | 18.1 | 20.9 | 23.0 | 26.4 | 27.3 | 29.2 | 34.3  |\\n|  BoxQ | acc | dev | 91.0 | 32 | 49.7 | 60.3 | 58.9 | 62.4 | 67.1 | 65.4 | 66.2 | 60.5 | 52.6 | 61.7 | 60.4 | 63.7 | 68.4 | 68.7 | 69.0 | 76.7  |\\n|  CB | acc | dev | 96.9 | 32 | 0.00 | 32.1 | 8.93 | 19.6 | 19.6 | 28.6 | 19.6 | 46.4 | 55.4 | 53.6 | 53.6 | 48.2 | 57.1 | 33.9 | 55.4 | 64.3  |\\n|  CB | f1 | dev | 93.9 | 32 | 0.00 | 29.3 | 11.4 | 17.4 | 22.4 | 25.1 | 20.3 | 42.8 | 60.1 | 39.8 | 45.6 | 37.5 | 45.7 | 28.5 | 44.6 | 52.5  |\\n|  Copa | acc | dev | 94.8 | 32 | 66.0 | 68.0 | 73.0 | 77.0 | 76.0 | 80.0 | 84.0 | 91.0 | 62.0 | 64.0 | 66.0 | 74.0 | 76.0 | 82.0 | 86.0 | 87.0  |\\n|  RTE | acc | dev | 92.5 | 32 | 47.7 | 49.8 | 48.4 | 56.0 | 46.6 | 55.2 | 62.8 | 63.5 | 53.1 | 47.3 | 49.5 | 49.5 | 54.9 | 56.3 | 70.4 |   |\\n|  WcC | acc | dev | 76.1 | 32 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 50.0 | 50.3 | 50.3 | 49.2 | 49.4 | 50.3 | 50.0 | 48.6  |\\n|  WSC | acc | dev | 93.8 | 32 | 59.6 | 56.7 | 65.4 | 61.5 | 66.3 | 60.6 | 64.4 | 65.4 | 58.7 | 58.7 | 60.6 | 62.5 | 66.3 | 60.6 | 66.3 | 69.2  |\\n|  MultiRC | acc | dev | 62.3 | 32 | 4.72 | 9.65 | 12.3 | 13.6 | 14.3 | 18.4 | 24.2 | 27.6 | 6.09 | 11.8 | 16.8 | 20.8 | 24.7 | 23.8 | 25.0 | 32.5  |\\n|  MultiRC | f1a | dev | 88.2 | 32 | 57.0 | 59.7 | 60.4 | 59.9 | 60.0 | 64.5 | 71.4 | 72.9 | 57.0 | 59.7 | 60.4 | 59.9 | 60.0 | 64.5 | 71.4 | 72.9  |\\n|  ReCoRD | acc | dev | 92.5 | 32 | 70.8 | 78.5 | 82.1 | 84.1 | 86.2 | 86.6 | 89.0 | 90.2 | 69.8 | 77.0 | 80.7 | 83.0 | 85.9 | 88.0 | 88.8 | 90.2  |\\n|  ReCoRD | f1 | dev | 93.3 | 32 | 71.9 | 79.2 | 82.8 | 85.2 | 87.3 | 89.5 | 90.4 | 91.0 | 70.7 | 77.8 | 81.6 | 83.9 | 86.8 | 88.8 | 89.7 | 91.2  |\\n|  SuperGLUE | average | dev | 89.0 |  | 40.6 | 47.4 | 46.8 | 49.6 | 50.1 | 52.3 | 54.4 | 58.2 | 54.4 | 55.1 | 56.7 | 57.8 | 61.2 | 59.7 | 64.3 | 68.9  |\\n|  ANLI R1 | acc | test | 73.8 | 50 | 33.4 | 34.2 | 33.4 | 33.4 | 34.2 | 32.3 | 33.2 | 34.6 | 32.1 | 31.6 | 31.9 | 34.6 | 30.6 | 31.6 | 32.7 | 32.0  |\\n|  ANLI R2 | acc | test | 50.7 | 50 | 33.2 | 31.9 | 33.3 | 33.3 | 33.8 | 33.5 | 33.5 | 35.4 | 35.7 | 33.7 | 33.2 | 32.7 | 32.7 | 33.9 | 33.9 | 33.9  |\\n|  ANLI R3 | acc | test | 48.5 | 50 | 33.6 | 34.0 | 33.8 | 33.4 | 35.3 | 34.8 | 34.4 | 34.5 | 35.0 | 32.6 | 33.0 | 33.9 | 34.1 | 33.1 | 32.5 | 35.1  |\\n|  2D+ | acc | n/a |  | 50 | 0.70 | 0.65 | 0.70 | 0.85 | 1.10 | 2.54 | 15.4 | 76.9 | 2.00 | 0.55 | 3.15 | 4.00 | 12.1 | 19.6 | 73.0 | 99.6  |\\n|  2D- | acc | n/a |  | 50 | 1.25 | 1.25 | 1.25 | 1.25 | 1.60 | 7.60 | 12.6 | 58.0 | 1.15 | 0.95 | 1.45 | 1.95 | 3.85 | 11.5 | 44.6 | 86.4  |\\n|  3D+ | acc | n/a |  | 50 | 0.10 | 0.10 | 0.05 | 0.10 | 0.10 | 0.25 | 1.40 | 34.2 | 0.15 | 0.00 | 0.10 | 0.30 | 0.45 | 0.95 | 15.4 | 65.5  |\\n|  3D- | acc | n/a |  | 50 | 0.05 | 0.05 | 0.05 | 0.05 | 0.05 | 0.45 | 1.35 | 48.3 | 0.05 | 0.15 | 0.25 | 0.30 | 0.55 | 1.60 | 61.5 | 78.7  |\\n|  4D+ | acc | n/a |  | 50 | 0.05 | 0.05 | 0.05 | 0.05 | 0.05 | 0.15 | 4.00 |  |  |  |  |  |  |  |  |   |\\n|  4D- | acc | n/a |  | 50 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |  |  |  |  |  |  |  |  |   |\\n|  5D+ | acc | n/a |  | 50 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |  |  |  |  |  |  |  |  |  |   |\\n\\nTable H.1: Scores for every task, setting and model that we investigate in this paper.\\n\\n![img-28.jpeg](img-28.jpeg)\\n\\n![img-29.jpeg](img-29.jpeg)\\n\\n![img-30.jpeg](img-30.jpeg)\\n\\n![img-31.jpeg](img-31.jpeg)\\n\\n![img-32.jpeg](img-32.jpeg)\\n\\n![img-33.jpeg](img-33.jpeg)\\n\\n![img-34.jpeg](img-34.jpeg)\\n\\n![img-35.jpeg](img-35.jpeg)\\n\\n![img-36.jpeg](img-36.jpeg)\\n\\n![img-37.jpeg](img-37.jpeg)\\nFigure H.1: All results for all SuperGLUE tasks.\\n\\n![img-38.jpeg](img-38.jpeg)\\n\\n![img-39.jpeg](img-39.jpeg)\\nFigure H.2: Results for SAT task.\\n\\n![img-40.jpeg](img-40.jpeg)\\nFigure H.3: All results for all Winograd tasks.\\n\\n![img-41.jpeg](img-41.jpeg)\\n\\n![img-42.jpeg](img-42.jpeg)\\n\\n![img-43.jpeg](img-43.jpeg)\\n\\n![img-44.jpeg](img-44.jpeg)\\n\\n![img-45.jpeg](img-45.jpeg)\\n\\n![img-46.jpeg](img-46.jpeg)\\n\\n![img-47.jpeg](img-47.jpeg)\\n\\n![img-48.jpeg](img-48.jpeg)\\n\\n![img-49.jpeg](img-49.jpeg)\\n\\n![img-50.jpeg](img-50.jpeg)\\n\\n![img-51.jpeg](img-51.jpeg)\\nFigure H.4: All results for all Arithmetic tasks.\\n\\n![img-52.jpeg](img-52.jpeg)\\n\\n![img-53.jpeg](img-53.jpeg)\\nFigure H.5: All results for all Cloze and Completion tasks.\\n\\n![img-54.jpeg](img-54.jpeg)\\n\\n![img-55.jpeg](img-55.jpeg)\\n\\n![img-56.jpeg](img-56.jpeg)\\nFigure H.6: All results for all Common Sense Reasoning tasks.\\n\\n![img-57.jpeg](img-57.jpeg)\\n\\n![img-58.jpeg](img-58.jpeg)\\n\\n![img-59.jpeg](img-59.jpeg)\\nFigure H.7: All results for all QA tasks.\\n\\n![img-60.jpeg](img-60.jpeg)\\n\\n![img-61.jpeg](img-61.jpeg)\\n\\n![img-62.jpeg](img-62.jpeg)\\n\\n![img-63.jpeg](img-63.jpeg)\\n\\n![img-64.jpeg](img-64.jpeg)\\n\\n![img-65.jpeg](img-65.jpeg)\\nFigure H.8: All results for all Reading Comprehension tasks.\\n\\n![img-66.jpeg](img-66.jpeg)\\n\\n![img-67.jpeg](img-67.jpeg)\\n\\n![img-68.jpeg](img-68.jpeg)\\nFigure H.9: All results for all ANLI rounds.\\n\\n![img-69.jpeg](img-69.jpeg)\\n\\n![img-70.jpeg](img-70.jpeg)\\n\\n![img-71.jpeg](img-71.jpeg)\\n\\n![img-72.jpeg](img-72.jpeg)\\n\\n![img-73.jpeg](img-73.jpeg)\\n\\n![img-74.jpeg](img-74.jpeg)\\nFigure H.10: All results for all Scramble tasks.\\n\\n![img-75.jpeg](img-75.jpeg)\\n\\n![img-76.jpeg](img-76.jpeg)\\n\\n![img-77.jpeg](img-77.jpeg)\\n\\n![img-78.jpeg](img-78.jpeg)\\n\\n![img-79.jpeg](img-79.jpeg)\\n\\n![img-80.jpeg](img-80.jpeg)\\n\\n![img-81.jpeg](img-81.jpeg)\\n\\n![img-82.jpeg](img-82.jpeg)\\n\\n![img-83.jpeg](img-83.jpeg)\\n\\n![img-84.jpeg](img-84.jpeg)\\n\\n![img-85.jpeg](img-85.jpeg)\\n\\n![img-86.jpeg](img-86.jpeg)\\nFigure H.11: All results for all Translation tasks.\\n\\n![img-87.jpeg](img-87.jpeg)\\n\\n![img-88.jpeg](img-88.jpeg)\\n\\nReferences\\n\\n- [ADG^{+}16] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in neural information processing systems, pages 3981–3989, 2016.\\n- [AI19] WeChat AI. Tr-mt (ensemble), December 2019.\\n- [AJF19] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\\n- [BBDIW20] Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. Language (technology) is power: A critical survey of “bias” in nlp. arXiv preprint arXiv:2005.14050, 2020.\\n- [BCFL13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013.\\n- [BDD^{+}09] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The fifth PASCAL recognizing textual entailment challenge. 2009.\\n- [BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining. In Lrec, volume 10, pages 2200–2204, 2010.\\n- [BHDD^{+}06] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second PASCAL recognising textual entailment challenge. 2006.\\n- [BHT^{+}20] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. Experience grounds language. arXiv preprint arXiv:2004.10151, 2020.\\n- [BLC13] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. Arxiv, 2013.\\n- [BZB^{+}19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. arXiv preprint arXiv:1911.11641, 2019.\\n- [Car97] Rich Caruana. Multitask learning. Machine learning, 28(1), 1997.\\n- [CB78] Susan Carey and Elsa Bartlett. Acquiring a single new word. Proceedings of the Stanford Child Language Conference, 1978.\\n- [CCE^{+}18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.\\n- [CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019.\\n- [CHI^{+}18] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac : Question answering in context. Arxiv, 2018.\\n- [CLC^{+}19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\\n- [CLY^{+}19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019.\\n- [Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote, 2017.\\n- [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n-\\n\\n[DGM06] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising textual entailment, pages 177–190. Springer, 2006.\\n- [DGV^{+}18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. Arxiv, 2018.\\n- [DHKH14] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. Edinburgh’s phrase-based machine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, 2014.\\n- [DL15] Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In Advances in neural information processing systems, 2015.\\n- [DMST19] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigating projection in naturally occurring discourse. 2019. To appear in proceedings of Sinn und Bedeutung 23. Data can be found at https://github.com/mcdm/CommitmentBank/.\\n- [DSC^{+}16] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl^{2}: Fast reinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016.\\n- [DWD^{+}19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019.\\n- [DYY^{+}19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. Arxiv, 2019.\\n- [EOAG18] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. arXiv preprint arXiv:1808.09381, 2018.\\n- [FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. ArXiv, abs/1703.03400, 2017.\\n- [Fyo00] Yaroslav Fyodorov. A natural logic inference system, 2000.\\n- [GG19] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. arXiv preprint arXiv:1903.03862, 2019.\\n- [GLT^{+}20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.\\n- [GMDD07] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9. Association for Computational Linguistics, 2007.\\n- [Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv, 2016.\\n- [GSL^{+}18] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018.\\n- [GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical detection and visualization of generated text. arXiv preprint arXiv: 1906.04043, 2019.\\n- [GWC^{+}18] Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for low-resource neural machine translation. arXiv preprint arXiv:1808.08437, 2018.\\n- [HB20] Daniel Hernandez and Tom Brown. Ai and efficiency, May 2020.\\n- [HBFC19] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. CoRR, abs/1904.09751, 2019.\\n- [HLW^{+}20] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out of distribution robustness. arXiv preprint arXiv:2004.06100, 2020.\\n- [JL17]\\n\\n- [HNA^{+}17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\\n- [HR18] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.\\n- [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\\n- [HYC01] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using Gradient Descent. In International Conference on Artificial Neural Networks, pages 87–94. Springer, 2001.\\n- [HZJ^{+}19] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in language models via counterfactual evaluation. arXiv preprint arXiv:1911.03064, 2019.\\n- [IBGC^{+}14] Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daumé III. A neural network for factoid question answering over paragraphs. In Empirical Methods in Natural Language Processing, 2014.\\n- [IDCBE19] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. arXiv preprint arXiv:1911.00650, 2019.\\n- [JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\\n- [JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020.\\n- [JVS^{+}16] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n- [JYS^{+}19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for natural language understanding. arXiv preprint arXiv:1909.10351, 2019.\\n- [JZC^{+}19] Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yunfeng Liu. Technical report on conversational question answering. arXiv preprint arXiv:1909.10772, 2019.\\n- [KCR^{+}18] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), 2018.\\n- [KKS^{+}20] Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020.\\n- [KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that’s fit to fabricate: Ai-generated text as a tool of media misinformation, 2020.\\n- [KMH^{+}20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\\n- [KPR^{+}19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.\\n- [KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv, 2016.\\n- [LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002.\\n- [LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291, 2019.\\n\\n- [LCG^{+}19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\\n- [LCH^{+}20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020.\\n- [LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. arXiv preprint arXiv:1905.07504, 2019.\\n- [LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012.\\n- [LGG^{+}20] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint arXiv:2001.08210, 2020.\\n- [LGH^{+}15] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representation learning using multi-task deep neural networks for semantic classification and information retrieval. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2015.\\n- [LH17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\\n- [LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural networks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482, 2019.\\n- [LHCG19b] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.\\n- [Lin20] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint arXiv:2005.00955, 2020.\\n- [LLG^{+}19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\\n- [LM17] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017.\\n- [LOG^{+}19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n- [LPP^{+}20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Kiela Douwe. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020.\\n- [LSP^{+}18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.\\n- [LWS^{+}20] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez. Train large, then compress: Rethinking model size for efficient training and inference of transformers, 2020.\\n- [LXL^{+}17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\\n- [LYN^{+}20] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin. Tttttackling winogrande schemas. arXiv preprint arXiv:2003.08380, 2020.\\n- [Mac92] David. MacKay. Information-based objective functions for active data selection. Neural Computation, 1992.\\n\\n[MBXS17] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pages 6294–6305, 2017.\\n- [MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n- [MCH^{+}16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696, 2016.\\n- [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. ArXiv, abs/1809.02789, 2018.\\n- [MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training, 2018.\\n- [MKM^{+}94] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: annotating predicate argument structure. In Proceedings of the workshop on Human Language Technology, pages 114–119. Association for Computational Linguistics, 1994.\\n- [MKXS18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\\n- [MPL19] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019.\\n- [MWZ^{+}18] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting, 2018.\\n- [NBR20] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020.\\n- [NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language arguments. arXiv preprint arXiv:1907.07355, 2019.\\n- [Nor09] Peter Norvig. Natural language corpus data, 2009.\\n- [NvNvdG19] Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational: Man is to doctor as woman is to doctor. arXiv preprint arXiv:1905.09866, 2019.\\n- [NWD^{+}19] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019.\\n- [oR16] University of Regensburg. Fascha, 2016.\\n- [PCC18] Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for evaluating context-sensitive representations. arXiv preprint arXiv:1808.09121, 2018.\\n- [PFB18] Jason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018.\\n- [PHR^{+}18] Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of EMNLP, 2018.\\n- [PKL^{+}16] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\\n- [PNZtY18] Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen tau Yih. Dissecting contextual word embeddings: Architecture and representation, 2018.\\n- [Pos18] Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771, 2018.\\n\\n[PSM14] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014.\\n- [QIA20] QIANXIN. Sa-net on albert (ensemble), April 2020.\\n- [QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-level language models with a gender-equalizing loss function. arXiv preprint arXiv:1905.12801, 2019.\\n- [RBG11] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.\\n- [RCM19] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249–266, 2019.\\n- [RCP^{+}17] Scott Reed, Yutian Chen, Thomas Paine, Aäron van den Oord, SM Eslami, Danilo Rezende, Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn distributions. arXiv preprint arXiv:1710.10304, 2017.\\n- [RJL18] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.\\n- [RL16] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR 2017 (oral), 2016.\\n- [RLL^{+}19] Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading comprehension with numerical reasoning. In Proceedings of EMNLP, 2019.\\n- [RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301, 2018.\\n- [RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018.\\n- [Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication, 2012.\\n- [RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales, 2019.\\n- [RRS20] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\\n- [RSR^{+}19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2019.\\n- [RWC^{+}19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019.\\n- [SBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.\\n- [SBC^{+}19] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang. Release strategies and the social impacts of language models, 2019.\\n- [SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326, 2019.\\n- [SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\\n- [SDSE19] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR, abs/1907.10597, 2019.\\n- [SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709, 2015.\\n\\n- [SMM^{+}17] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n- [SPP^{+}19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019.\\n- [SS20] Timo Schick and Hinrich Schütze. Exploiting cloze questions for few-shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020.\\n- [STQ^{+}19] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.\\n- [TFR^{+}17] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23–30. IEEE, 2017.\\n- [TL05] Peter D. Turney and Michael L. Littman. Corpus-based learning of analogies and semantic relations. CoRR, abs/cs/0508103, 2005.\\n- [TL18] Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018.\\n- [TLBS03] Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. Combining independent modules to solve multiple-choice synonym and analogy problems. CoRR, cs.CL/0309035, 2003.\\n- [Tur20] Project Turing. Microsoft research blog, Feb 2020.\\n- [VBL^{+}16] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching Networks for One Shot Learning. In Advances in neural information processing systems, pages 3630–3638, 2016.\\n- [VSP^{+}17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 2017.\\n- [WPN^{+}19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, pages 3261–3275, 2019.\\n- [WXH^{+}18] Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-agent dual learning. ICLR 2019, 2018.\\n- [XDH^{+}19] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data augmentation for consistency training, 2019.\\n- [YdC^{+}19] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019.\\n- [YDY^{+}19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.\\n- [ZHB^{+}19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\\n- [ZHR^{+}19] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019.\\n- [ZLL^{+}18] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018.\\n- [ZSW^{+}19a] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2019.\\n\\n[ZSW^{+}19b] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593, 2019.'), -0.005191715978997813), (Document(id='e6f9a937828a:0', metadata={'text': '# LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\\n\\nEdward Hu* Yelong Shen* Phillip Wallis Zeyuan Allen-Zhu\\n\\nYuanzhi Li Shean Wang Lu Wang Weizhu Chen\\n\\nMicrosoft Corporation\\n\\n{edwardhu, yeshe, phwallis, zeyuana,\\n\\nyuanzhil, swang, luw, wzchen}@microsoft.com\\n\\nyuanzhil@andrew.cmu.edu\\n\\n(Version 2)\\n\\n# ABSTRACT\\n\\nAn important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retransmits all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.\\n\\n# 1 INTRODUCTION\\n\\nMany applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via fine-tuning, which updates all the parameters of the pre-trained model. The major downside of fine-tuning is that the new model contains as many parameters as in the original model. As larger models are trained every few months, this changes from a mere \"inconvenience\" for GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a critical deployment challenge for GPT-3 (Brown et al., 2020) with 175 billion trainable parameters. $^{1}$\\n\\nMany sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed. However, existing techniques\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Our reparametrization. We only train  $A$  and  $B$ .\\n\\noften introduce inference latency *(Houlsby et al., 2019; Rebuffi et al., 2017)* by extending model depth or reduce the model’s usable sequence length *(Li and Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021)* (Section 3). More importantly, these method often fail to match the fine-tuning baselines, posing a trade-off between efficiency and model quality.\\n\\nWe take inspiration from *Li et al. (2018a); Aghajanyan et al. (2020)* which show that the learned over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed Low-Rank Adaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers’ change during adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3 175B as an example, we show that a very low rank (i.e., r in Figure 1 can be one or two) suffices even when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efficient.\\n\\nLoRA possesses several key advantages.\\n\\n- A pre-trained model can be shared and used to build many small LoRA modules for different tasks. We can freeze the shared model and efficiently switch tasks by replacing the matrices $A$ and $B$ in Figure 1, reducing the storage requirement and task-switching overhead significantly.\\n- LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, we only optimize the injected, much smaller low-rank matrices.\\n- Our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully fine-tuned model, by construction.\\n- LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning. We provide an example in Appendix E.\\n\\n#### Terminologies and Conventions\\n\\nWe make frequent references to the Transformer architecture and use the conventional terminologies for its dimensions. We call the input and output dimension size of a Transformer layer $d_{model}$. We use $W_{q}$, $W_{k}$, $W_{v}$, and $W_{o}$ to refer to the query/key/value/output projection matrices in the self-attention module. $W$ or $W_{0}$ refers to a pre-trained weight matrix and $\\\\Delta W$ its accumulated gradient update during adaptation. We use $r$ to denote the rank of a LoRA module. We follow the conventions set out by *(Vaswani et al., 2017; Brown et al., 2020)* and use Adam *(Loshchilov and Hutter, 2019; Kingma and Ba, 2017)* for model optimization and use a Transformer MLP feedforward dimension $d_{ffn}=4\\\\times d_{model}$.\\n\\n## 2 Problem Statement\\n\\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivating use case. Below is a brief description of the language modeling problem and, in particular, the maximization of conditional probabilities given a task-specific prompt.\\n\\nSuppose we are given a pre-trained autoregressive language model $P_{\\\\Phi}(y|x)$ parametrized by $\\\\Phi$. For instance, $P_{\\\\Phi}(y|x)$ can be a generic multi-task learner such as GPT *(Radford et al., 2020b; Brown et al., 2020)* based on the Transformer architecture *(Vaswani et al., 2017)*. Consider adapting this pre-trained model to downstream conditional text generation tasks, such as summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is represented by a training dataset of context-target pairs: $\\\\mathcal{Z}=\\\\{(x_{i},y_{i})\\\\}_{i=1,\\\\dots,N}$, where both $x_{i}$ and $y_{i}$ are sequences of tokens. For example, in NL2SQL, $x_{i}$ is a natural language query and $y_{i}$ its corresponding SQL command; for summarization, $x_{i}$ is the content of an article and $y_{i}$ its summary.\\n\\nDuring full fine-tuning, the model is initialized to pre-trained weights $\\\\Phi_{0}$ and updated to $\\\\Phi_{0}+\\\\Delta\\\\Phi$ by repeatedly following the gradient to maximize the conditional language modeling objective:\\n\\n$\\\\max_{\\\\Phi}\\\\sum_{(x,y)\\\\in\\\\mathcal{Z}}\\\\sum_{t=1}^{|y|}\\\\log\\\\left(P_{\\\\Phi}(y_{t}|x,y_{<t})\\\\right)$ (1)\\n\\nOne of the main drawbacks for full fine-tuning is that for *each* downstream task, we learn a *different* set of parameters $\\\\Delta\\\\Phi$ whose dimension $|\\\\Delta\\\\Phi|$ equals $|\\\\Phi_{0}|$. Thus, if the pre-trained model is large (such as GPT-3 with $|\\\\Phi_{0}|\\\\approx 175$ Billion), storing and deploying many independent instances of fine-tuned models can be challenging, if at all feasible.\\n\\nIn this paper, we adopt a more parameter-efficient approach, where the task-specific parameter increment $\\\\Delta\\\\Phi=\\\\Delta\\\\Phi(\\\\Theta)$ is further encoded by a much smaller-sized set of parameters $\\\\Theta$ with $|\\\\Theta|\\\\ll|\\\\Phi_{0}|$. The task of finding $\\\\Delta\\\\Phi$ thus becomes optimizing over $\\\\Theta$:\\n\\n$\\\\max_{\\\\Theta}\\\\sum_{(x,y)\\\\in\\\\mathcal{Z}}\\\\sum_{t=1}^{|y|}\\\\log\\\\left(p_{\\\\Phi_{0}+\\\\Delta\\\\Phi(\\\\Theta)}(y_{t}|x,y_{<t})\\\\right)$ (2)\\n\\nIn the subsequent sections, we propose to use a low-rank representation to encode $\\\\Delta\\\\Phi$ that is both compute- and memory-efficient. When the pre-trained model is GPT-3 175B, the number of trainable parameters $|\\\\Theta|$ can be as small as $0.01\\\\%$ of $|\\\\Phi_{0}|$.\\n\\n## 3 Aren’t Existing Solutions Good Enough?\\n\\nThe problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter- and compute-efficient. See Section 6 for a survey of some of the well-known works. Using language modeling as an example, there are two prominent strategies when it comes to efficient adaptations: adding adapter layers *(Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2021; Rücklé et al., 2020)* or optimizing some forms of the input layer activations *(Li and Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021)*. However, both strategies have their limitations, especially in a large-scale and latency-sensitive production scenario.\\n\\n#### Adapter Layers Introduce Inference Latency\\n\\nThere are many variants of adapters. We focus on the original design by *Houlsby et al. (2019)* which has two adapter layers per Transformer block and a more recent one by *Lin et al. (2020)* which has only one per block but with an additional LayerNorm *(Ba et al., 2016)*. While one can reduce the overall latency by pruning layers or exploiting multi-task settings *(Rücklé et al., 2020; Pfeiffer et al., 2021)*, there is no direct ways to bypass the extra compute in adapter layers. This seems like a non-issue since adapter layers are designed to have few parameters (sometimes $<$1% of the original model) by having a small bottleneck dimension, which limits the FLOPs they can add. However, large neural networks rely on hardware parallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes a difference in the online inference setting where the batch size is typically as small as one. In a generic scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b) medium on a single GPU, we see a noticeable increase in latency when using adapters, even with a very small bottleneck dimension (Table 1).\\n\\nThis problem gets worse when we need to shard the model as done in *Shoeybi et al. (2020); Lepikhin et al. (2020)*, because the additional depth requires more synchronous GPU operations such as AllReduce and Broadcast, unless we store the adapter parameters redundantly many times.\\n\\n#### Directly Optimizing the Prompt is Hard\\n\\nThe other direction, as exemplified by prefix tuning *(Li and Liang, 2021)*, faces a different challenge. We observe that prefix tuning is difficult to optimize and that its performance changes non-monotonically in trainable parameters, confirming similar observations in the original paper. More fundamentally, reserving a part of the sequence length for adaptation necessarily reduces the sequence length available to process a downstream task, which we suspect makes tuning the prompt less performant compared to other methods. We defer the study on task performance to Section 5.\\n\\n|  Batch Size | 32 | 16 | 1  |\\n| --- | --- | --- | --- |\\n|  Sequence Length | 512 | 256 | 128  |\\n|  |Θ | 0.5M | 11M | 11M  |\\n|  Fine-Tune/LoRA | 1449.4±0.8 | 338.0±0.6 | 19.8±2.7  |\\n|  AdapterL | 1482.0±1.0 (+2.2%) | 354.8±0.5 (+5.0%) | 23.9±2.1 (+20.7%)  |\\n|  AdapterH | 1492.2±1.0 (+3.0%) | 366.3±0.5 (+8.4%) | 25.8±2.2 (+30.3%)  |\\n\\nTable 1: Inference latency of a single forward pass in GPT-2 medium measured in milliseconds, averaged over 100 trials. We use an NVIDIA Quadro RTX8000. “ $|\\\\Theta|$ ” denotes the number of trainable parameters in adapter layers. Adapter $^{\\\\text{L}}$  and Adapter $^{\\\\text{H}}$  are two variants of adapter tuning, which we describe in Section 5.1. The inference latency introduced by adapter layers can be significant in an online, short-sequence-length scenario. See the full study in Appendix B.\\n\\n# 4 OUR METHOD\\n\\nWe describe the simple design of LoRA and its practical benefits. The principles outlined here apply to any dense layers in deep learning models, though we only focus on certain weights in Transformer language models in our experiments as the motivating use case.\\n\\n# 4.1 LOW-RANK-PARAMETRIZED UPDATE MATRICES\\n\\nA neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers typically have full-rank. When adapting to a specific task, Aghajanyan et al. (2020) shows that the pre-trained language models have a low \"intrinsic dimension\" and can still learn efficiently despite a random projection to a smaller subspace. Inspired by this, we hypothesize the updates to the weights also have a low \"intrinsic rank\" during adaptation. For a pre-trained weight matrix  $W_0 \\\\in \\\\mathbb{R}^{d \\\\times k}$ , we constrain its update by representing the latter with a low-rank decomposition  $W_0 + \\\\Delta W = W_0 + BA$ , where  $B \\\\in \\\\mathbb{R}^{d \\\\times r}$ ,  $A \\\\in \\\\mathbb{R}^{r \\\\times k}$ , and the rank  $r \\\\ll \\\\min(d, k)$ . During training,  $W_0$  is frozen and does not receive gradient updates, while  $A$  and  $B$  contain trainable parameters. Note both  $W_0$  and  $\\\\Delta W = BA$  are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For  $h = W_0x$ , our modified forward pass yields:\\n\\n$$\\nh = W _ {0} x + \\\\Delta W x = W _ {0} x + B A x \\\\tag {3}\\n$$\\n\\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for  $A$  and zero for  $B$ , so  $\\\\Delta W = BA$  is zero at the beginning of training. We then scale  $\\\\Delta Wx$  by  $\\\\frac{\\\\alpha}{r}$ , where  $\\\\alpha$  is a constant in  $r$ . When optimizing with Adam, tuning  $\\\\alpha$  is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set  $\\\\alpha$  to the first  $r$  we try and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary  $r$  (Yang &amp; Hu, 2021).\\n\\nA Generalization of Full Fine-tuning. A more general form of fine-tuning allows the training of a subset of the pre-trained parameters. LoRA takes a step further and does not require the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that when applying LoRA to all weight matrices and training all biases $^2$ , we roughly recover the expressiveness of full fine-tuning by setting the LoRA rank  $r$  to the rank of the pre-trained weight matrices. In other words, as we increase the number of trainable parameters $^3$ , training LoRA roughly converges to training the original model, while adapter-based methods converge to an MLP and prefix-based methods to a model that cannot take long input sequences.\\n\\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and store  $W = W_{0} + BA$  and perform inference as usual. Note that both  $W_{0}$  and  $BA$  are in  $\\\\mathbb{R}^{d\\\\times k}$ . When we need to switch to another downstream task, we can recover  $W_{0}$  by subtracting  $BA$  and then adding a different  $B^{\\\\prime}A^{\\\\prime}$ , a quick operation with very little memory overhead. Critically, this\\n\\nguarantees that we do not introduce any additional latency during inference compared to a fine-tuned model by construction.\\n\\n### 4.2 Applying LoRA to Transformer\\n\\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the number of trainable parameters. In the Transformer architecture, there are four weight matrices in the self-attention module ($W_{q},W_{k},W_{v},W_{o}$) and two in the MLP module. We treat $W_{q}$ (or $W_{k}$, $W_{v}$) as a single matrix of dimension $d_{model}\\\\times d_{model}$, even though the output dimension is usually sliced into attention heads. We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.We further study the effect on adapting different types of attention weight matrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP layers, LayerNorm layers, and biases to a future work.\\n\\n#### Practical Benefits and Limitations.\\n\\nThe most significant benefit comes from the reduction in memory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM usage by up to $2/3$ if $r\\\\ll d_{model}$ as we do not need to store the optimizer states for the frozen parameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to 350GB. With $r=4$ and only the query and value projection matrices being adapted, the checkpoint size is reduced by roughly 10,000$\\\\times$ (from 350GB to 35MB). This allows us to train with significantly fewer GPUs and avoid I/O bottlenecks. Another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters. This allows for the creation of many customized models that can be swapped in and out on the fly on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup during training on GPT-3 175B compared to full fine-tuning as we do not need to calculate the gradient for the vast majority of the parameters.\\n\\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks with different $A$ and $B$ in a single forward pass, if one chooses to absorb $A$ and $B$ into $W$ to eliminate additional inference latency. Though it is possible to not merge the weights and dynamically choose the LoRA modules to use for samples in a batch for scenarios where latency is not critical.\\n\\n## 5 Empirical Experiments\\n\\nWe evaluate the downstream task performance of LoRA on RoBERTa *(Liu et al., 2019)*, DeBERTa *(He et al., 2021)*, and GPT-2 *(Radford et al., 2020)*. Our experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG). Specifically, we evaluate on the GLUE *(Wang et al., 2019)* benchmark for RoBERTa and DeBERTa. We follow the setup of *Li and Liang (2021)* on GPT-2 for a direct comparison and add WikiSQL *(Zhong et al., 2017)* (NL to SQL queries) and SAMSum *(Gliwa et al., 2019)* (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for more details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\\n\\n### 5.1 Baselines\\n\\nTo compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible. This, however, means that some baselines might only appear in certain experiments.\\n\\nFine-Tuning (FT) is a common approach for adaptation. During fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple variant is to update only some layers while freezing others. We include one such baseline reported in prior work *(Li and Liang, 2021)* on GPT-2, which adapts just the last two layers (FT^{Top2}).\\n\\n|  Model & Method | # Trainable Parameters | MNLI | SST-2 | MRPC | CoLA | QNLI | QQP | RTE | STS-B | Avg.  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  RoBbase (FT)* | 125.0M | 87.6 | 94.8 | 90.2 | 63.6 | 92.8 | 91.9 | 78.7 | 91.2 | 86.4  |\\n|  RoBbase (BitFit)* | 0.1M | 84.7 | 93.7 | 92.7 | 62.0 | 91.8 | 84.0 | 81.5 | 90.8 | 85.2  |\\n|  RoBbase (AdptD)* | 0.3M | 87.1±.0 | 94.2±.1 | 88.5±1.1 | 60.8±.4 | 93.1±.1 | 90.2±.0 | 71.5±2.7 | 89.7±.3 | 84.4  |\\n|  RoBbase (AdptD)* | 0.9M | 87.3±.1 | 94.7±.3 | 88.4±.1 | 62.6±.9 | 93.0±.2 | 90.6±.0 | 75.9±2.2 | 90.3±.1 | 85.4  |\\n|  RoBbase (LoRA) | 0.3M | 87.5±.3 | 95.1±.2 | 89.7±.7 | 63.4±1.2 | 93.3±.3 | 90.8±.1 | 86.6±.7 | 91.5±.2 | 87.2  |\\n|  RoBlarge (FT)* | 355.0M | 90.2 | 96.4 | 90.9 | 68.0 | 94.7 | 92.2 | 86.6 | 92.4 | 88.9  |\\n|  RoBlarge (LoRA) | 0.8M | 90.6±.2 | 96.2±.5 | 90.9±1.2 | 68.2±1.9 | 94.9±.3 | 91.6±.1 | 87.4±2.5 | 92.6±.2 | 89.0  |\\n|  RoBlarge (AdptP)† | 3.0M | 90.2±.3 | 96.1±.3 | 90.2±.7 | 68.3±1.0 | 94.8±.2 | 91.9±.1 | 83.8±2.9 | 92.1±.7 | 88.4  |\\n|  RoBlarge (AdptP)† | 0.8M | 90.5±.3 | 96.6±.2 | 89.7±1.2 | 67.8±2.5 | 94.8±.3 | 91.7±.2 | 80.1±2.9 | 91.9±.4 | 87.9  |\\n|  RoBlarge (AdptH)† | 6.0M | 89.9±.5 | 96.2±.3 | 88.7±2.9 | 66.5±4.4 | 94.7±.2 | 92.1±.1 | 83.4±1.1 | 91.0±1.7 | 87.8  |\\n|  RoBlarge (AdptH)† | 0.8M | 90.3±.3 | 96.3±.5 | 87.7±1.7 | 66.3±2.0 | 94.7±.2 | 91.5±.1 | 72.9±2.9 | 91.5±.5 | 86.4  |\\n|  RoBlarge (LoRA)† | 0.8M | 90.6±.2 | 96.2±.5 | 90.2±1.0 | 68.2±1.9 | 94.8±.3 | 91.6±.2 | 85.2±1.1 | 92.3±.5 | 88.6  |\\n|  DeBXXL (FT)* | 1500.0M | 91.8 | 97.2 | 92.0 | 72.0 | 96.0 | 92.7 | 93.9 | 92.9 | 91.1  |\\n|  DeBXXL (LoRA) | 4.7M | 91.9±.2 | 96.9±.2 | 92.6±.6 | 72.4±1.1 | 96.0±.1 | 92.9±.1 | 94.9±.4 | 93.0±.2 | 91.3  |\\n\\nTable 2: RoBERTa $_{\\\\text{base}}$ , RoBERTa $_{\\\\text{large}}$ , and DeBERTa $_{\\\\text{XXL}}$  with different adaptation methods on the GLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew\\'s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all metrics. * indicates numbers published in prior works. † indicates runs configured in a setup similar to Houlsby et al. (2019) for a fair comparison.\\n\\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else. Contemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021).\\n\\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These special tokens have trainable word embeddings and are generally not in the model\\'s vocabulary. Where to place such tokens can have an impact on performance. We focus on \"prefixing\", which prepends such tokens to the prompt, and \"infixing\", which appends to the prompt; both are discussed in Li &amp; Liang (2021). We use  $l_{p}$  (resp.  $l_{i}$ ) denote the number of prefix (resp. infix) tokens. The number of trainable parameters is  $|\\\\Theta| = d_{model} \\\\times (l_{p} + l_{i})$ .\\n\\nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning. Instead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer. The activations computed from previous layers are simply replaced by trainable ones. The resulting number of trainable parameters is  $|\\\\Theta| = L \\\\times d_{model} \\\\times (l_p + l_i)$ , where  $L$  is the number of Transformer layers.\\n\\nAdapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection. There are two fully connected layers with biases in an adapter layer with a nonlinearity in between. We call this original design  $\\\\mathbf{Adapter}^{\\\\mathbf{H}}$ . Recently, Lin et al. (2020) proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm. We call it  $\\\\mathbf{Adapter}^{\\\\mathbf{L}}$ . This is very similar to another design proposed in Pfeiffer et al. (2021), which we call  $\\\\mathbf{Adapter}^{\\\\mathbf{P}}$ . We also include another baseline call AdapterDrop (Rückle et al., 2020) which drops some adapter layers for greater efficiency ( $\\\\mathbf{Adapter}^{\\\\mathbf{D}}$ ). We cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column. In all cases, we have  $|\\\\Theta| = \\\\hat{L}_{Adpt} \\\\times (2 \\\\times d_{model} \\\\times r + r + d_{model}) + 2 \\\\times \\\\hat{L}_{LN} \\\\times d_{model}$  where  $\\\\hat{L}_{Adpt}$  is the number of adapter layers and  $\\\\hat{L}_{LN}$  the number of trainable LayerNorms (e.g., in  $\\\\mathbf{Adapter}^{\\\\mathbf{L}}$ ).\\n\\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices. As mentioned in Section 4.2, we only apply LoRA to  $W_{q}$  and  $W_{v}$  in most experiments for simplicity. The number of trainable parameters is determined by the rank  $r$  and the shape of the original weights:  $|\\\\Theta| = 2 \\\\times \\\\hat{L}_{LoRA} \\\\times d_{model} \\\\times r$ , where  $\\\\hat{L}_{LoRA}$  is the number of weight matrices we apply LoRA to.\\n\\n|  Model & Method | # Trainable Parameters | E2E NLG Challenge  |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|   |   |  BLEU | NIST | MET | ROUGE-L | CIDEr  |\\n|  GPT-2 M (FT)* | 354.92M | 68.2 | 8.62 | 46.2 | 71.0 | 2.47  |\\n|  GPT-2 M (AdapterL)* | 0.37M | 66.3 | 8.41 | 45.0 | 69.8 | 2.40  |\\n|  GPT-2 M (AdapterL)* | 11.09M | 68.9 | 8.71 | 46.1 | 71.3 | 2.47  |\\n|  GPT-2 M (AdapterH) | 11.09M | 67.3±.6 | 8.50±.07 | 46.0±.2 | 70.7±.2 | 2.44±.01  |\\n|  GPT-2 M (FTTop2)* | 25.19M | 68.1 | 8.59 | 46.0 | 70.8 | 2.41  |\\n|  GPT-2 M (PreLayer)* | 0.35M | 69.7 | 8.81 | 46.1 | 71.4 | 2.49  |\\n|  GPT-2 M (LoRA) | 0.35M | 70.4±.1 | 8.85±.02 | 46.8±.2 | 71.8±.1 | 2.53±.02  |\\n|  GPT-2 L (FT)* | 774.03M | 68.5 | 8.78 | 46.0 | 69.9 | 2.45  |\\n|  GPT-2 L (AdapterL) | 0.88M | 69.1±.1 | 8.68±.03 | 46.3±.0 | 71.4±.2 | 2.49±.0  |\\n|  GPT-2 L (AdapterL) | 23.00M | 68.9±.3 | 8.70±.04 | 46.1±.1 | 71.3±.2 | 2.45±.02  |\\n|  GPT-2 L (PreLayer)* | 0.77M | 70.3 | 8.85 | 46.2 | 71.7 | 2.47  |\\n|  GPT-2 L (LoRA) | 0.77M | 70.4±.1 | 8.89±.02 | 46.8±.2 | 72.0±.2 | 2.47±.02  |\\n\\nTable 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG Challenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable or fewer trainable parameters. Confidence intervals are shown for experiments we ran. * indicates numbers published in prior works.\\n\\n# 5.2 ROBERTA BASE/LARGE\\n\\nRoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin et al., 2019a) and boosted the latter\\'s task performance without introducing many more trainable parameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards such as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and popular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base (125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020) and evaluate the performance of different efficient adaptation approaches on tasks from the GLUE benchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their setup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when comparing with adapters. First, we use the same batch size for all tasks and use a sequence length of 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for MRPC, RTE, and STS-B, not a model already adapted to MNLI like the fine-tuning baseline. Runs following this more restricted setup from Houlsby et al. (2019) are labeled with  $\\\\dagger$ . The result is presented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.\\n\\n# 5.3 DEBERTA XXL\\n\\nDeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger scale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and SuperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully fine-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section). See Section D.2 for details on the hyperparameters used.\\n\\n# 5.4 GPT-2 MEDIUM/LARGE\\n\\nHaving shown that LoRA can be a competitive alternative to full fine-tuning on NLU, we hope to answer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al., b). We keep our setup as close as possible to Li &amp; Liang (2021) for a direct comparison. Due to space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section. See Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We include a list of the hyperparameters used in Section D.3.\\n\\n|  Model&Method | # Trainable Parameters | WikiSQL | MNLI-m | SAMSum  |\\n| --- | --- | --- | --- | --- |\\n|   |   |  Acc. (%) | Acc. (%) | R1/R2/RL  |\\n|  GPT-3 (FT) | 175,255.8M | 73.8 | 89.5 | 52.0/28.0/44.5  |\\n|  GPT-3 (BitFit) | 14.2M | 71.3 | 91.0 | 51.3/27.4/43.5  |\\n|  GPT-3 (PreEmbed) | 3.2M | 63.1 | 88.6 | 48.3/24.2/40.5  |\\n|  GPT-3 (PreLayer) | 20.2M | 70.1 | 89.5 | 50.8/27.3/43.5  |\\n|  GPT-3 (AdapterH) | 7.1M | 71.9 | 89.8 | 53.0/28.9/44.8  |\\n|  GPT-3 (AdapterH) | 40.1M | 73.2 | 91.5 | 53.2/29.0/45.1  |\\n|  GPT-3 (LoRA) | 4.7M | 73.4 | 91.7 | 53.8/29.8/45.9  |\\n|  GPT-3 (LoRA) | 37.7M | 74.0 | 91.6 | 53.4/29.2/45.1  |\\n\\nTable 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form validation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on SAMSum. LoRA performs better than prior approaches, including full fine-tuning. The results on WikiSQL have a fluctuation around  $\\\\pm 0.5\\\\%$ , MNLI-m around  $\\\\pm 0.1\\\\%$ , and SAMSum around  $\\\\pm 0.2 / \\\\pm 0.2 / \\\\pm 0.1$  for the three metrics.\\n\\n# 5.5 SCALING UP TO GPT-3 175B\\n\\nAs a final stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high training cost, we only report the typical standard deviation for a given task over random seeds, as opposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.\\n\\nAs shown in Table 4, LoRA matches or exceeds the fine-tuning baseline on all three datasets. Note that not all methods benefit monotonically from having more trainable parameters, as shown in Figure 2. We observe a significant performance drop when we use more than 256 special tokens for prefix-embedding tuning or more than 32 special tokens for prefix-layer tuning. This corroborates similar observations in Li &amp; Liang (2021). While a thorough investigation into this phenomenon is out-of-scope for this work, we suspect that having more special tokens causes the input distribution to shift further away from the pre-training data distribution. Separately, we investigate the performance of different adaptation approaches in the low-data regime in Section F.3.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation methods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance. See Section F.2 for more details on the plotted data points.\\n\\n![img-2.jpeg](img-2.jpeg)\\n\\n# 6 RELATED WORKS\\n\\nTransformer Language Models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence architecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive language modeling by using a stack of Transformer decoders. Since then, Transformer-based language models have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged with BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) - both are large Transformer lan\\n\\nguage models trained on a large amount of text – where fine-tuning on task-specific data after pre-training on general domain data provides a significant performance gain compared to training on task-specific data directly. Training larger Transformers generally results in better performance and remains an active research direction. GPT-3 *(Brown et al., 2020)* is the largest single Transformer language model trained to-date with 175B parameters.\\n\\n#### Prompt Engineering and Fine-Tuning.\\n\\nWhile GPT-3 175B can adapt its behavior with just a few additional training examples, the result depends heavily on the input prompt *(Brown et al., 2020)*. This necessitates an empirical art of composing and formatting the prompt to maximize a model’s performance on a desired task, which is known as prompt engineering or prompt hacking. Fine-tuning retrains a model pre-trained on general domains to a specific task *Devlin et al. (2019b); Radford et al. (a)*. Variants of it include learning just a subset of the parameters *Devlin et al. (2019b); Collobert and Weston (2008)*, yet practitioners often retrain all of them to maximize the downstream performance. However, the enormity of GPT-3 175B makes it challenging to perform fine-tuning in the usual way due to the large checkpoint it produces and the high hardware barrier to entry since it has the same memory footprint as pre-training.\\n\\n#### Parameter-Efficient Adaptation.\\n\\nMany have proposed inserting adapter layers between existing layers in a neural network *(Houlsby et al., 2019; Rebuffi et al., 2017; Lin et al., 2020)*. Our method uses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The key functional difference is that our learned weights can be merged with the main weights during inference, thus not introducing any latency, which is not the case for the adapter layers (Section 3). A comtenporary extension of adapter is compacter *(Mahabadi et al., 2021)*, which essentially parametrizes the adapter layers using Kronecker products with some predetermined weight sharing scheme. Similarly, combining LoRA with other tensor product-based methods could potentially improve its parameter efficiency, which we leave to future work. More recently, many proposed optimizing the input word embeddings in lieu of fine-tuning, akin to a continuous and differentiable generalization of prompt engineering *(Li and Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021)*. We include comparisons with *Li and Liang (2021)* in our experiment section. However, this line of works can only scale up by using more special tokens in the prompt, which take up available sequence length for task tokens when positional embeddings are learned.\\n\\n#### Low-Rank Structures in Deep Learning.\\n\\nLow-rank structure is very common in machine learning. A lot of machine learning problems have certain intrinsic low-rank structure *(Li et al., 2016; Cai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013)*. Moreover, it is known that for many deep learning tasks, especially those with a heavily over-parametrized neural network, the learned neural network will enjoy low-rank properties after training *(Oymak et al., 2019)*. Some prior works even explicitly impose the low-rank constraint when training the original neural network *(Sainath et al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Khodak et al., 2021; Denil et al., 2014)*; however, to the best of our knowledge, none of these works considers low-rank update to a frozen model for adaptation to downstream tasks. In theory literature, it is known that neural networks outperform other classical learning methods, including the corresponding (finite-width) neural tangent kernels *(Allen-Zhu et al., 2019; Li and Liang, 2018)* when the underlying concept class has certain low-rank structure *(Ghorbani et al., 2020; Allen-Zhu and Li, 2019; Allen-Zhu and Li, 2020a)*. Another theoretical result in *Allen-Zhu and Li (2020b)* suggests that low-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed low-rank adaptation update is well-motivated by the literature.\\n\\n## 7 Understanding the Low-Rank Updates\\n\\nGiven the empirical advantage of LoRA, we hope to further explain the properties of the low-rank adaptation learned from downstream tasks. Note that the low-rank structure not only lowers the hardware barrier to entry which allows us to run multiple experiments in parallel, but also gives better interpretability of how the update weights are correlated with the pre-trained weights. We focus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters (up to 10,000$\\\\times$) without adversely affecting task performances.\\n\\nWe perform a sequence of empirical studies to answer the following questions: 1) Given a parameter budget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt\\n\\nto maximize downstream performance? 2) Is the \"optimal\" adaptation matrix  $\\\\Delta W$  really rank-deficient? If so, what is a good rank to use in practice? 3) What is the connection between  $\\\\Delta W$  and  $W$ ? Does  $\\\\Delta W$  highly correlate with  $W$ ? How large is  $\\\\Delta W$  comparing to  $W$ ?\\n\\nWe believe that our answers to question (2) and (3) shed light on the fundamental principles of using pre-trained language models for downstream tasks, which is a critical topic in NLP.\\n\\n# 7.1 WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?\\n\\nGiven a limited parameter budget, which types of weights should we adapt with LoRA to obtain the best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight matrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored in FP16) on GPT-3 175B, which corresponds to  $r = 8$  if we adapt one type of attention weights or  $r = 4$  if we adapt two types, for all 96 layers. The result is presented in Table 5.\\n\\n|   | # of Trainable Parameters = 18M  |   |   |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Weight Type Rank r | Wq8 | Wk8 | Wv8 | Wo8 | Wq, Wk4 | Wq, Wv4 | Wq, Wk, Wv, Wo2  |\\n|  WikiSQL (±0.5%) | 70.4 | 70.0 | 73.0 | 73.2 | 71.4 | 73.7 | 73.7  |\\n|  MultiNLI (±0.1%) | 91.0 | 90.8 | 91.0 | 91.3 | 91.3 | 91.3 | 91.7  |\\n\\nNote that putting all the parameters in  $\\\\Delta W_{q}$  or  $\\\\Delta W_{k}$  results in significantly lower performance, while adapting both  $W_{q}$  and  $W_{v}$  yields the best result. This suggests that even a rank of four captures enough information in  $\\\\Delta W$  such that it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank.\\n\\n# 7.2 WHAT IS THE OPTIMAL RANK  $r$  FOR LORA?\\n\\nWe turn our attention to the effect of rank  $r$  on model performance. We adapt  $\\\\{W_q, W_v\\\\}$ ,  $\\\\{W_q, W_k, W_v, W_c\\\\}$ , and just  $W_q$  for a comparison.\\n\\nTable 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of attention weights in GPT-3, given the same number of trainable parameters. Adapting both  $W_{q}$  and  $W_{v}$  gives the best performance overall. We find the standard deviation across random seeds to be consistent for a given dataset, which we report in the first column.\\n\\n|   | Weight Type | r = 1 | r = 2 | r = 4 | r = 8 | r = 64  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  WikiSQL(±0.5%) | Wq | 68.8 | 69.6 | 70.5 | 70.4 | 70.0  |\\n|   |  Wq, Wv | 73.4 | 73.3 | 73.7 | 73.8 | 73.5  |\\n|   |  Wq, Wk, Wv, Wo | 74.1 | 73.7 | 74.0 | 74.0 | 73.9  |\\n|  MultiNLI (±0.1%) | Wq | 90.7 | 90.9 | 91.1 | 90.7 | 90.7  |\\n|   |  Wq, Wv | 91.3 | 91.4 | 91.3 | 91.6 | 91.4  |\\n|   |  Wq, Wk, Wv, Wo | 91.2 | 91.7 | 91.7 | 91.5 | 91.4  |\\n\\nTable 6: Validation accuracy on WikiSQL and MultiNLI with different rank  $r$ . To our surprise, a rank as small as one suffices for adapting both  $W_{q}$  and  $W_{v}$  on these datasets while training  $W_{q}$  alone needs a larger  $r$ . We conduct a similar experiment on GPT-2 in Section H.2.\\n\\nTable 6 shows that, surprisingly, LoRA already performs competitively with a very small  $r$  (more so for  $\\\\{W_q, W_v\\\\}$  than just  $W_q$ ). This suggests the update matrix  $\\\\Delta W$  could have a very small \"intrinsic rank\". To further support this finding, we check the overlap of the subspaces learned by different choices of  $r$  and by different random seeds. We argue that increasing  $r$  does not cover a more meaningful subspace, which suggests that a low-rank adaptation matrix is sufficient.\\n\\nSubspace similarity between different $r$. Given $A_{r=8}$ and $A_{r=64}$ which are the learned adaptation matrices with rank $r=8$ and $64$ using the same pre-trained model, we perform singular value decomposition and obtain the right-singular unitary matrices $U_{A_{r=8}}$ and $U_{A_{r=64}}$. We hope to answer: how much of the subspace spanned by the top $i$ singular vectors in $U_{A_{r=8}}$ (for $1\\\\leq i\\\\leq 8$) is contained in the subspace spanned by top $j$ singular vectors of $U_{A_{r=64}}$ (for $1\\\\leq j\\\\leq 64$)? We measure this quantity with a normalized subspace similarity based on the Grassmann distance (See Appendix G for a more formal discussion)\\n\\n$\\\\phi(A_{r=8},A_{r=64},i,j)=\\\\frac{||U_{A_{r=8}}^{i\\\\top}U_{A_{r=64}}^{j}||_{F}^{2}}{\\\\min(i,j)}\\\\in[0,1]$ (4)\\n\\nwhere $U_{A_{r=8}}^{i}$ represents the columns of $U_{A_{r=8}}$ corresponding to the top-$i$ singular vectors.\\n\\n$\\\\phi(\\\\cdot)$ has a range of $[0,1]$, where $1$ represents a complete overlap of subspaces and $0$ a complete separation. See Figure 3 for how $\\\\phi$ changes as we vary $i$ and $j$. We only look at the 48th layer (out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown in Section H.1.\\n\\n$\\\\phi(A_{r=64},A_{r=8},i,j)$\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: Subspace similarity between column vectors of $A_{r=8}$ and $A_{r=64}$ for both $\\\\Delta W_{q}$ and $\\\\Delta W_{v}$. The third and the fourth figures zoom in on the lower-left triangle in the first two figures. The top directions in $r=8$ are included in $r=64$, and vice versa.\\n\\nWe make an *important observation* from Figure 3.\\n\\n&gt; Directions corresponding to the top singular vector overlap significantly between $A_{r=8}$ and $A_{r=64}$, while others do not. Specifically, $\\\\Delta W_{v}$ (resp. $\\\\Delta W_{q}$) of $A_{r=8}$ and $\\\\Delta W_{v}$ (resp. $\\\\Delta W_{q}$) of $A_{r=64}$ share a subspace of dimension 1 with normalized similarity $&gt;0.5$, providing an explanation of why $r=1$ performs quite well in our downstream tasks for GPT-3.\\n\\nSince both $A_{r=8}$ and $A_{r=64}$ are learned using the same pre-trained model, Figure 3 indicates that the top singular-vector directions of $A_{r=8}$ and $A_{r=64}$ are the most useful, while other directions potentially contain mostly random noises accumulated during training. Hence, the adaptation matrix can indeed have a very low rank.\\n\\nSubspace similarity between different random seeds. We further confirm this by plotting the normalized subspace similarity between two randomly seeded runs with $r=64$, shown in Figure 4. $\\\\Delta W_{q}$ appears to have a higher “intrinsic rank” than $\\\\Delta W_{v}$, since more common singular value directions are learned by both runs for $\\\\Delta W_{q}$, which is in line with our empirical observation in Table 6. As a comparison, we also plot two random Gaussian matrices, which do not share any common singular value directions with each other.\\n\\n### 7.3 How does the Adaptation Matrix $\\\\Delta W$ compare to $W$?\\n\\nWe further investigate the relationship between $\\\\Delta W$ and $W$. In particular, does $\\\\Delta W$ highly correlate with $W$? (Or mathematically, is $\\\\Delta W$ mostly contained in the top singular directions of $W$?) Also,\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 4: Left and Middle: Normalized subspace similarity between the column vectors of  $A_{r=64}$  from two random seeds, for both  $\\\\Delta W_q$  and  $\\\\Delta W_v$  in the 48-th layer. Right: the same heat-map between the column vectors of two random Gaussian matrices. See Section H.1 for other layers.\\n\\nhow \"large\" is  $\\\\Delta W$  comparing to its corresponding directions in  $W$ ? This can shed light on the underlying mechanism for adapting pre-trained language models.\\n\\nTo answer these questions, we project  $W$  onto the  $r$ -dimensional subspace of  $\\\\Delta W$  by computing  $U^{\\\\top}WV^{\\\\top}$ , with  $U / V$  being the left/right singular-vector matrix of  $\\\\Delta W$ . Then, we compare the Frobenius norm between  $\\\\| U^{\\\\top}WV^{\\\\top}\\\\|_F$  and  $\\\\| W\\\\|_F$ . As a comparison, we also compute  $\\\\| U^{\\\\top}WV^{\\\\top}\\\\|_F$  by replacing  $U, V$  with the top  $r$  singular vectors of  $W$  or a random matrix.\\n\\n|   | r=4 |   |   | r=64  |   |   |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|   | ΔWq | Wq | Random | ΔWq | Wq | Random  |\\n|  ||U^T W_q V^T ||_F = | 0.32 | 21.67 | 0.02 | 1.90 | 37.71 | 0.33  |\\n|  ||Wq||_F = 61.95 | ||ΔWq||_F = 6.91 |   |   | ΔWq||_F = 3.57  |   |   |\\n\\nTable 7: The Frobenius norm of  $U^{\\\\top}W_{q}V^{\\\\top}$  where  $U$  and  $V$  are the left/right top  $r$  singular vector directions of either (1)  $\\\\Delta W_{q}$ , (2)  $W_{q}$ , or (3) a random matrix. The weight matrices are taken from the 48th layer of GPT-3.\\n\\nWe draw several conclusions from Table 7. First,  $\\\\Delta W$  has a stronger correlation with  $W$  compared to a random matrix, indicating that  $\\\\Delta W$  amplifies some features that are already in  $W$ . Second, instead of repeating the top singular directions of  $W$ ,  $\\\\Delta W$  only amplifies directions that are not emphasized in  $W$ . Third, the amplification factor is rather huge:  $21.5 \\\\approx 6.91 / 0.32$  for  $r = 4$ . See Section H.4 for why  $r = 64$  has a smaller amplification factor. We also provide a visualization in Section H.3 for how the correlation changes as we include more top singular directions from  $W_{q}$ . This suggests that the low-rank adaptation matrix potentially amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model.\\n\\n# 8 CONCLUSION AND FUTURE WORK\\n\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required and the storage/switching cost for hosting independent instances for different tasks. We propose LoRA, an efficient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality. Importantly, it allows for quick task-switching when deployed as a service by sharing the vast majority of the model parameters. While we focused on Transformer language models, the proposed principles are generally applicable to any neural networks with dense layers.\\n\\nThere are many directions for future works. 1) LoRA can be combined with other efficient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind fine-tuning or LoRA is far from clear - how are features learned during pre-training transformed to do well on downstream tasks? We believe that LoRA makes it more tractable to answer this than full fine\\n\\ntuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are there more principled ways to do it? 4) Finally, the rank-deficiency of $\\\\Delta W$ suggests that $W$ could be rank-deficient as well, which can also be a source of inspiration for future works.\\n\\n## References\\n\\n- A. Aghajanyan, L. Zettlemoyer, and S. Gupta (2020) Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL http://arxiv.org/abs/2012.13255.\\n- Z. Allen-Zhu and Y. Li (2019) What Can ResNet Learn Efficiently, Going Beyond Kernels? In NeurIPS, 2019. Full version available at http://arxiv.org/abs/1905.10337.\\n- Z. Allen-Zhu and Y. Li (2020a) Backward feature correction: How deep learning performs deep learning. arXiv preprint arXiv:2001.04413, 2020a.\\n- Z. Allen-Zhu and Y. Li (2020b) Feature purification: How adversarial training performs robust deep learning. arXiv preprint arXiv:2005.10190, 2020b.\\n- Z. Allen-Zhu, Y. Li, and Z. Song (2020) A convergence theory for deep learning via over-parameterization. In ICML, 2019. Full version available at http://arxiv.org/abs/1811.03962.\\n- J. Le Ba, J. Kiros, and G. E. Hinton (2016) Layer normalization, 2016.\\n- B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. D. A. and M. Ziegler (2017) A single, a single, and a single, Deep Learning: A survey. arXiv preprint arXiv:1709.01465 [cs], July 2020. URL http://arxiv.org/abs/2005.14165.\\n- C. Can-Feng Cai, E. Manuel, J. Candès, and Z. Shen (2010) A singular value thresholding algorithm for matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010.\\n- C. Cer, M. Diab, E. Agirre, I. Loege-Gazpio, and L. Semeval (2017) A universal, a universal, and crosslingual focused evaluation. Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017. doi: 10.18653/v1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001.\\n- C. Conan, A. Conan, and J. S. DeVlin (2014) Nonan Collobert and Jason Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, ICML ’08, pp. 160–167, New York, NY, USA, July 2008. Association for Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL https://doi.org/10.1145/1390156.1390177.\\n- D. Denil, B. Shakibi, L. D. Chen, M. Aurelio Ranzato, and N. de Freitas (2014) Predicting parameters in deep learning, 2014.\\n- D. Devlin, M. Wei Chang, K. Lee, and K. Toutanova (2019a) Bert: Pre-training of deep bidirectional transformers for language understanding, 2019a.\\n- D. Devlin, M. Wei Chang, K. Lee, and K. Toutanova (2019b) Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019b. URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805.\\n- B. Dolan and B. Brockett (2017) A automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002.\\n- G. Gardner, A. Shimorina, S. Narayan, and L. Perez-Beltrachini (2017) The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pp. 124–133, 2017.\\n- G.\\n\\nBehrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020.\\n- Ghorbani et al. (2020) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. CoRR, abs/1911.12237, 2019. URL http://arxiv.org/abs/1911.12237.\\n- Ghorbani et al. (2013) Lars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor approximation techniques. GAMM-Mitteilungen, 36(1):53–78, 2013.\\n- Haim and Ghorbani (2019) Jihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based learning. In ICML, pp. 376–383, 2008. URL https://doi.org/10.1145/1390156.1390204.\\n- Haim and Ghorbani (2019) Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial ReProgramming. arXiv:2101.00121 [cs], December 2020. URL http://arxiv.org/abs/2101.00121. arXiv: 2101.00121.\\n- He et al. (2014) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention, 2021.\\n- Hei et al. (2021) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. arXiv:1902.00751 [cs, stat], June 2019. URL http://arxiv.org/abs/1902.00751.\\n- Jaderberg et al. (2014) Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.\\n- Khodak et al. (2014) Mikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicolò Fusi. Initialization and regularization of factorized neural layers, 2021.\\n- Klingma and Le (2021) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\\n- Lepikhin et al. (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020.\\n- Le et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. arXiv:2104.08691 [cs], April 2021. URL http://arxiv.org/abs/2104.08691. arXiv: 2104.08691.\\n- Li et al. (2021) Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a. URL http://arxiv.org/abs/1804.08838. arXiv: 1804.08838.\\n- Li and Li (2021) Xiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing Continuous Prompts for Generation. arXiv:2101.00190 [cs], January 2021. URL http://arxiv.org/abs/2101.00190.\\n- Li and Li (2021) Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, 2018.\\n- Li et al. (2021) Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank approximation via alternating minimization. In International Conference on Machine Learning, pp. 2358–2367. PMLR, 2016.\\n- Li et al. (2020) Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pp. 2–47. PMLR, 2018b.\\n- Lin et al. (2020) Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 441–459, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.41. URL https://aclanthology.org/2020.findings-emnlp.41.\\n\\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT Understands, Too. arXiv:2103.10385 [cs], March 2021. URL http://arxiv.org/abs/2103.10385. arXiv: 2103.10385.\\n- Lin, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.\\n- Lishchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\\n- Lishchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\\n- Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers, 2021.\\n- Nandayram, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871, 2020.\\n- Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint arXiv:1906.05392, 2019.\\n- Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and Sanjeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, pp. 3743–3747, 2018.\\n- Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training. pp. 12, a.\\n- Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. pp. 24, b.\\n- Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. CoRR, abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822.\\n- Ruch, H. and V. Radford. Learning multiple visual domains with residual adapters. arXiv:1705.08045 [cs, stat], November 2017. URL http://arxiv.org/abs/1705.08045. arXiv: 1705.08045.\\n- Ruch, H. and V. Radford. Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers, 2020.\\n- Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6655–6659. IEEE, 2013.\\n- Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.\\n- Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Proceedings of the 31st International Conference on Neural Information Processing Systems*, pp. 6000–6010, 2017.\\n- Wang et al. [2017] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.\\n- Wang et al. [2020] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems, 2020.\\n- Warstadt et al. [2018] Alexandr M. Bowman. Neural network acceptability judgments. *arXiv preprint arXiv:1805.12471*, 2018.\\n- Williams et al. [2011] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.org/anthology/N18-1101.\\n- Wolf et al. [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pp. 38–45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.\\n- Yang and J. Hu [2021] Greg Yang and Edward J. Hu. Feature Learning in Infinite-Width Neural Networks. *arXiv:2011.14522 [cond-mat]*, May 2021. URL http://arxiv.org/abs/2011.14522. arXiv: 2011.14522.\\n- Ben Zaken et al. [2014] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, 2021.\\n- Zhang et al. [2016] Yu Zhang, Ekapol Chuangsuwanich, and James Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In *2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)*, pp. 185–189. IEEE, 2014.\\n- Zhao et al. [2017] Yong Zhao, Jinyu Li, and Yifan Gong. Low-rank plus diagonal adaptation for deep neural networks. In *2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pp. 5005–5009. IEEE, 2016.\\n- Zhou et al. [2017] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. *CoRR*, abs/1709.00103, 2017. URL http://arxiv.org/abs/1709.00103.\\n\\n## Appendix A Large Language Models Still Need Parameter Updates\\n\\nFew-shot learning, or prompt engineering, is very advantageous when we only have a handful of training samples. However, in practice, we can often afford to curate a few thousand or more training examples for performance-sensitive applications. As shown in Table 8, fine-tuning improves the model performance drastically compared to few-shot learning on datasets large and small. We take the GPT-3 few-shot result on RTE from the GPT-3 paper *(Brown et al., 2020)*. For MNLI-matched, we use two demonstrations per class and six in-context examples in total.\\n\\n|  Method | MNLI-m (Val. Acc./%) | RTE (Val. Acc./%)  |\\n| --- | --- | --- |\\n|  GPT-3 Few-Shot | 40.6 | 69.0  |\\n|  GPT-3 Fine-Tuned | 89.5 | 85.4  |\\n\\nTable 8: Fine-tuning significantly outperforms few-shot learning on GPT-3 (Brown et al., 2020).\\n\\n# B INFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS\\n\\nAdapter layers are external modules added to a pre-trained model in a sequential manner, whereas our proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently, adapter layers must be computed in addition to the base model, inevitably introducing additional latency. While as pointed out in Rücklé et al. (2020), the latency introduced by adapter layers can be mitigated when the model batch size and/or sequence length is large enough to full utilize the hardware parallelism. We confirm their observation with a similar latency study on GPT-2 medium and point out that there are scenarios, notably online inference where the batch size is small, where the added latency can be significant.\\n\\nWe measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging over 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension  $r$ . We test two adapter designs: the original one by Houlsby et al. (2019), which we call Adapter $^{\\\\mathrm{H}}$ , and a recent, more efficient variant by Lin et al. (2020), which we call Adapter $^{\\\\mathrm{L}}$ . See Section 5.1 for more details on the designs. We plot the slow-down in percentage compared to the no-adapter baseline in Figure 5.\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 5: Percentage slow-down of inference latency compared to the no-adapter  $(r = 0)$  baseline. The top row shows the result for Adapter $^{\\\\mathrm{H}}$  and the bottom row Adapter $^{\\\\mathrm{L}}$ . Larger batch size and sequence length help to mitigate the latency, but the slow-down can be as high as over  $30\\\\%$  in an online, short-sequence-length scenario. We tweak the colormap for better visibility.\\n\\n# C DATASET DETAILS\\n\\nGLUE Benchmark is a wide-ranging collection of natural language understanding tasks. It includes MNLI (inference, Williams et al. (2018)), SST-2 (sentiment analysis, Socher et al. (2013)), MRPC (paraphrase detection, Dolan &amp; Brockett (2005)), CoLA (linguistic acceptability, Warstadt et al. (2018)), QNLI (inference, Rajpurkar et al. (2018)),  $\\\\mathrm{QQP^8}$  (question-answering), RTE (inference),\\n\\nand STS-B (textual similarity, *Cer et al. (2017)*). The broad coverage makes GLUE benchmark a standard metric to evaluate NLU models such as RoBERTa and DeBERTa. The individual datasets are released under different permissive licenses.\\n\\nWikiSQL is introduced in *Zhong et al. (2017)* and contains $56,355/8,421$ training/validation examples. The task is to generate SQL queries from natural language questions and table schemata. We encode context as $x=\\\\{\\\\text{table schema},\\\\text{query}\\\\}$ and target as $y=\\\\{\\\\text{SQL}\\\\}$. The dataset is release under the BSD 3-Clause License.\\n\\nSAMSum is introduced in *Gliwa et al. (2019)* and contains $14,732/819$ training/test examples. It consists of staged chat conversations between two people and corresponding abstractive summaries written by linguists. We encode context as ”\\\\n” concatenated utterances followed by a ”\\\\n\\\\n, and target as $y=\\\\{\\\\text{summary}\\\\}$. The dataset is released under the non-commercial licence: Creative Commons BY-NC-ND 4.0.\\n\\nE2E NLG Challenge was first introduced in *Novikova et al. (2017)* as a dataset for training end-to-end, data-driven natural language generation systems and is commonly used for data-to-text evaluation. The E2E dataset consists of roughly $42,000$ training, $4,600$ validation, and $4,600$ test examples from the restaurant domain. Each source table used as input can have multiple references. Each sample input $(x,y)$ consists of a sequence of slot-value pairs, along with a corresponding natural language reference text. The dataset is released under Creative Commons BY-NC-SA 4.0.\\n\\nDART is an open-domain data-to-text dataset described in *Nan et al. (2020)*. DART inputs are structured as sequences of ENTITY — RELATION — ENTITY triples. With $82K$ examples in total, DART is a significantly larger and more complex data-to-text task compared to E2E. The dataset is released under the MIT license.\\n\\nWebNLG is another commonly used dataset for data-to-text evaluation *(Gardent et al., 2017)*. With $22K$ examples in total WebNLG comprises 14 distinct categories, nine of which are seen during training. Since five of the 14 total categories are not seen during training, but are represented in the test set, evaluation is typically broken out by “seen” categories (S), “unseen” categories (U) and “all” (A). Each input example is represented by a sequence of SUBJECT — PROPERTY — OBJECT triples. The dataset is released under Creative Commons BY-NC-SA 4.0.\\n\\n## Appendix D Hyperparameters Used in Experiments\\n\\n### D.1 RoBERTa\\n\\nWe train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number of training epochs, and batch size for LoRA. Following *Liu et al. (2019)*, we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5 random seeds; the result for each run is taken from the best epoch. For a fair comparison with the setup in *Houlsby et al. (2019)* and *Pfeiffer et al. (2021)*, we restrict the model sequence length to 128 and used a fixed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large model when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI. The runs with this restricted setup are marked with $\\\\dagger$. See the hyperparameters used in our runs in Table 9.\\n\\n### D.2 DeBERTa\\n\\nWe again train using AdamW with a linear learning rate decay schedule. Following *He et al. (2021)*, we tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model sequence length used by *(He et al., 2021)* to keep our comparison fair. Following *He et al. (2021)*, we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5 random seeds; the result for each run is taken from the best epoch. See the hyperparameters used in our runs in Table 10.\\n\\n|  Method | Dataset | MNLI | SST-2 | MRPC | CoLA | QNLI | QQP | RTE | STS-B  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   | Optimizer |  |  |  | AdamW |   |  |  |   |\\n|   | Warmup Ratio |  |  |  | 0.06 |   |  |  |   |\\n|   | LR Schedule |  |  |  | Linear |   |  |  |   |\\n|  RoBERTa base LoRA | Batch Size | 16 | 16 | 16 | 32 | 32 | 16 | 32 | 16  |\\n|   |  # Epochs | 30 | 60 | 30 | 80 | 25 | 25 | 80 | 40  |\\n|   |  Learning Rate | 5E-04 | 5E-04 | 4E-04 | 4E-04 | 4E-04 | 5E-04 | 5E-04 | 4E-04  |\\n|   |  LoRA Config. |  |  |  | rq = rv = 8 |   |  |  |   |\\n|   |  LoRA α |  |  |  | 8 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 512 |   |  |  |   |\\n|  RoBERTa large LoRA | Batch Size | 4 | 4 | 4 | 4 | 4 | 4 | 8 | 8  |\\n|   |  # Epochs | 10 | 10 | 20 | 20 | 10 | 20 | 20 | 30  |\\n|   |  Learning Rate | 3E-04 | 4E-04 | 3E-04 | 2E-04 | 2E-04 | 3E-04 | 4E-04 | 2E-04  |\\n|   |  LoRA Config. |  |  |  | rq = rv = 8 |   |  |  |   |\\n|   |  LoRA α |  |  |  | 16 |   |  |  |   |\\n|   |  Max Seq. Len. | 128 | 128 | 512 | 128 | 512 | 512 | 512 | 512  |\\n|  RoBERTa large LoRA† | Batch Size |  |  |  | 4 |   |  |  |   |\\n|   |  # Epochs | 10 | 10 | 20 | 20 | 10 | 20 | 20 | 10  |\\n|   |  Learning Rate | 3E-04 | 4E-04 | 3E-04 | 2E-04 | 2E-04 | 3E-04 | 4E-04 | 2E-04  |\\n|   |  LoRA Config. |  |  |  | rq = rv = 8 |   |  |  |   |\\n|   |  LoRA α |  |  |  | 16 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 128 |   |  |  |   |\\n|  RoBERTa large AdptP(3M)† | Batch Size |  |  |  | 32 |   |  |  |   |\\n|   |  # Epochs | 10 | 20 | 20 | 20 | 10 | 20 | 20 | 20  |\\n|   |  Learning Rate | 3E-05 | 3E-05 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04  |\\n|   |  Bottleneck r |  |  |  | 64 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 128 |   |  |  |   |\\n|  RoBERTa large AdptP(0.8M)† | Batch Size |  |  |  | 32 |   |  |  |   |\\n|   |  # Epochs | 5 | 20 | 20 | 20 | 10 | 20 | 20 | 20  |\\n|   |  Learning Rate | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04  |\\n|   |  Bottleneck r |  |  |  | 16 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 128 |   |  |  |   |\\n|  RoBERTa large AdptH(6M)† | Batch Size |  |  |  | 32 |   |  |  |   |\\n|   |  # Epochs | 10 | 5 | 10 | 10 | 5 | 20 | 20 | 10  |\\n|   |  Learning Rate | 3E-05 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04  |\\n|   |  Bottleneck r |  |  |  | 64 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 128 |   |  |  |   |\\n|  RoBERTa large AdptH(0.8M)† | Batch Size |  |  |  | 32 |   |  |  |   |\\n|   |  # Epochs | 10 | 5 | 10 | 10 | 5 | 20 | 20 | 10  |\\n|   |  Learning Rate | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04  |\\n|   |  Bottleneck r |  |  |  | 8 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 128 |   |  |  |   |\\n\\nTable 9: The hyperparameters we used for RoBERTa on the GLUE benchmark.\\n\\n# D.3 GPT-2\\n\\nWe train all of our GPT-2 models using AdamW (Loshchilov &amp; Hutter, 2017) with a linear learning rate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described in Li &amp; Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the mean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters used for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li &amp; Liang (2021).\\n\\n# D.4 GPT-3\\n\\nFor all GPT-3 experiments, we train using AdamW (Loshchilov &amp; Hutter, 2017) for 2 epochs with a batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for\\n\\n|  Method | Dataset | MNLI | SST-2 | MRPC | CoLA | QNLI | QQP | RTE | STS-B  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   | Optimizer |  |  |  | AdamW |   |  |  |   |\\n|   | Warmup Ratio |  |  |  | 0.1 |   |  |  |   |\\n|   | LR Schedule |  |  |  | Linear |   |  |  |   |\\n|  DeBERTa XXL LoRA | Batch Size | 8 | 8 | 32 | 4 | 6 | 8 | 4 | 4  |\\n|   |  # Epochs | 5 | 16 | 30 | 10 | 8 | 11 | 11 | 10  |\\n|   |  Learning Rate | 1E-04 | 6E-05 | 2E-04 | 1E-04 | 1E-04 | 1E-04 | 2E-04 | 2E-04  |\\n|   |  Weight Decay | 0 | 0.01 | 0.01 | 0 | 0.01 | 0.01 | 0.01 | 0.1  |\\n|   |  CLS Dropout | 0.15 | 0 | 0 | 0.1 | 0.1 | 0.2 | 0.2 | 0.2  |\\n|   |  LoRA Config. |  |  |  | rq = rv = 8 |   |  |  |   |\\n|   |  LoRA α |  |  |  | 8 |   |  |  |   |\\n|   |  Max Seq. Len. | 256 | 128 | 128 | 64 | 512 | 320 | 320 | 128  |\\n\\nTable 10: The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark.\\n\\n|  Dataset | E2E | WebNLG | DART  |\\n| --- | --- | --- | --- |\\n|   | Training  |   |   |\\n|  Optimizer | AdamW  |   |   |\\n|  Weight Decay | 0.01 | 0.01 | 0.0  |\\n|  Dropout Prob | 0.1 | 0.1 | 0.0  |\\n|  Batch Size | 8  |   |   |\\n|  # Epoch | 5  |   |   |\\n|  Warmup Steps | 500  |   |   |\\n|  Learning Rate Schedule | Linear  |   |   |\\n|  Label Smooth | 0.1 | 0.1 | 0.0  |\\n|  Learning Rate | 0.0002  |   |   |\\n|  Adaptation | rq = rv = 4  |   |   |\\n|  LoRA α | 32  |   |   |\\n|   | Inference  |   |   |\\n|  Beam Size | 10  |   |   |\\n|  Length Penalty | 0.9 | 0.8 | 0.8  |\\n|  no repeat ngram size | 4  |   |   |\\n\\nTable 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART.\\n\\nWikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa et al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more details on the hyperparameters used. For prefix-embedding tuning, we find the optimal  $l_{p}$  and  $l_{i}$  to be 256 and 8, respectively, totalling  $3.2M$  trainable parameters. We use  $l_{p} = 8$  and  $l_{i} = 8$  for prefix-layer tuning with  $20.2M$  trainable parameters to obtain the overall best performance. We present two parameter budgets for LoRA:  $4.7\\\\mathrm{M}$  ( $r_{q} = r_{v} = 1$  or  $r_{v} = 2$ ) and  $37.7\\\\mathrm{M}$  ( $r_{q} = r_{v} = 8$  or  $r_{q} = r_{k} = r_{v} = r_{o} = 2$ ). We report the best validation performance from each run. The training hyperparameters used in our GPT-3 experiments are listed in Table 12.\\n\\n# E COMBINING LORA WITH Prefix TUNING\\n\\nLoRA can be naturally combined with existing prefix-based approaches. In this section, we evaluate two combinations of LoRA and variants of prefix-tuning on WikiSQL and MNLI.\\n\\nLoRA+PrefixEmbed (LoRA+PE) combines LoRA with prefix-embedding tuning, where we insert  $l_{p} + l_{i}$  special tokens whose embeddings are treated as trainable parameters. For more on prefix-embedding tuning, see Section 5.1.\\n\\nLoRA+PrefixLayer (LoRA+PL) combines LoRA with prefix-layer tuning. We also insert  $l_{p} + l_{i}$  special tokens; however, instead of letting the hidden representations of these tokens evolve natu\\n\\n|  Hyperparameters | Fine-Tune | PreEmbed | PreLayer | BitFit | AdapterH | LoRA  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  Optimizer |  |  | AdamW |  |  |   |\\n|  Batch Size |  |  | 128 |  |  |   |\\n|  # Epoch |  |  | 2 |  |  |   |\\n|  Warmup Tokens |  |  | 250,000 |  |  |   |\\n|  LR Schedule |  |  | Linear |  |  |   |\\n|  Learning Rate | 5.00E-06 | 5.00E-04 | 1.00E-04 | 1.6E-03 | 1.00E-04 | 2.00E-04  |\\n\\nrally, we replace them after every Transformer block with an input agnostic vector. Thus, both the embeddings and subsequent Transformer block activations are treated as trainable parameters. For more on prefix-layer tuning, see Section 5.1.\\n\\nIn Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI. First of all, LoRA+PE significantly outperforms both LoRA and prefix-embedding tuning on WikiSQL, which indicates that LoRA is somewhat orthogonal to prefix-embedding tuning. On MultiNLI, the combination of LoRA+PE doesn\\'t perform better than LoRA, possibly because LoRA on its own already achieves performance comparable to the human baseline. Secondly, we notice that LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We attribute this to the fact that prefix-layer tuning is very sensitive to the choice of learning rate and thus makes the optimization of LoRA weights more difficult in LoRA+PL.\\n\\n# F ADDITIONAL EMPIRICAL EXPERIMENTS\\n\\n# F.1 ADDITIONAL EXPERIMENTS ON GPT-2\\n\\nWe also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017) following the setup of Li &amp; Liang (2021). The result is shown in Table 13. Similar to our result on E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with prefix-based approaches given the same number of trainable parameters.\\n\\nTable 12: The training hyperparameters used for different GPT-3 adaption methods. We use the same hyperparameters for all datasets after tuning learning rate.\\n\\n|  Method | # Trainable Parameters | BLEU↑ | DART MET↑ | TER↓  |\\n| --- | --- | --- | --- | --- |\\n|  GPT-2 Medium  |   |   |   |   |\\n|  Fine-Tune | 354M | 46.2 | 0.39 | 0.46  |\\n|  AdapterL | 0.37M | 42.4 | 0.36 | 0.48  |\\n|  AdapterL | 11M | 45.2 | 0.38 | 0.46  |\\n|  FTTop2 | 24M | 41.0 | 0.34 | 0.56  |\\n|  PrefLayer | 0.35M | 46.4 | 0.38 | 0.46  |\\n|  LoRA | 0.35M | 47.1±.2 | 0.39 | 0.46  |\\n|  GPT-2 Large  |   |   |   |   |\\n|  Fine-Tune | 774M | 47.0 | 0.39 | 0.46  |\\n|  AdapterL | 0.88M | 45.7±.1 | 0.38 | 0.46  |\\n|  AdapterL | 23M | 47.1±.1 | 0.39 | 0.45  |\\n|  PrefLayer | 0.77M | 46.7 | 0.38 | 0.45  |\\n|  LoRA | 0.77M | 47.5±.1 | 0.39 | 0.45  |\\n\\nTable 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are less than 0.01 for all adaption approaches.\\n\\n|  Method | WebNLG  |   |   |   |   |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   |  U | BLEU↑S | A | MET↑ |   |   | A | U | S  |\\n|   |   |   |   |  U | S | A  |   |   |   |\\n|  GPT-2 Medium  |   |   |   |   |   |   |   |   |   |\\n|  Fine-Tune (354M) | 27.7 | 64.2 | 46.5 | .30 | .45 | .38 | .76 | .33 | .53  |\\n|  AdapterL (0.37M) | 45.1 | 54.5 | 50.2 | .36 | .39 | .38 | .46 | .40 | .43  |\\n|  AdapterL (11M) | 48.3 | 60.4 | 54.9 | .38 | .43 | .41 | .45 | .35 | .39  |\\n|  FTTop2 (24M) | 18.9 | 53.6 | 36.0 | .23 | .38 | .31 | .99 | .49 | .72  |\\n|  Prefix (0.35M) | 45.6 | 62.9 | 55.1 | .38 | .44 | .41 | .49 | .35 | .40  |\\n|  LoRA (0.35M) | 46.7±.4 | 62.1±.2 | 55.3±.2 | .38 | .44 | .41 | .46 | .33 | .39  |\\n|  GPT-2 Large  |   |   |   |   |   |   |   |   |   |\\n|  Fine-Tune (774M) | 43.1 | 65.3 | 55.5 | .38 | .46 | .42 | .53 | .33 | .42  |\\n|  AdapterL (0.88M) | 49.8±.0 | 61.1±.0 | 56.0±.0 | .38 | .43 | .41 | .44 | .35 | .39  |\\n|  AdapterL (23M) | 49.2±.1 | 64.7±.2 | 57.7±.1 | .39 | .46 | .43 | .46 | .33 | .39  |\\n|  Prefix (0.77M) | 47.7 | 63.4 | 56.3 | .39 | .45 | .42 | .48 | .34 | .40  |\\n|  LoRA (0.77M) | 48.4±.3 | 64.0±.3 | 57.0±.1 | .39 | .45 | .42 | .45 | .32 | .38  |\\n\\nTable 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER are less than 0.01 for all the experiments we ran. \"U\" indicates unseen categories, \"S\" indicates seen categories, and \"A\" indicates all categories in the test set of WebNLG.\\n\\n# F.2 ADDITIONAL EXPERIMENTS ON GPT-3\\n\\nWe present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on identifying the trade-off between performance and the number of trainable parameters.\\n\\n# F.3 LOW-DATA REGIME\\n\\nTo evaluate the performance of different adaptation approaches in the low-data regime, we randomly sample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data MNLI- $n$  tasks. In Table 16, we show the performance of different adaptation approaches on MNLI- $n$ . To our surprise, PrefixEmbed and PrefixLayer performs very poorly on MNLI-100 dataset, with PrefixEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PrefixLayer performs better than PrefixEmbed but is still significantly worse than Fine-Tune or LoRA on MNLI-100. The gap between prefix-based approaches and LoRA/Fine-tuning becomes smaller as we increase the number of training examples, which might suggest that prefix-based approaches are not suitable for low-data tasks in GPT-3. LoRA achieves better performance than fine-tuning on both MNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the  $(\\\\pm 0.3)$  variance due to random seeds.\\n\\nThe training hyperparameters of different adaptation approaches on MNLI-n are reported in Table 17. We use a smaller learning rate for PrefixLayer on the MNLI-100 set, as the training loss does not decrease with a larger learning rate.\\n\\n# G MEASURING SIMILARITY BETWEEN SUBSPACES\\n\\nIn this paper we use the measure  $\\\\phi(A, B, i, j) = \\\\psi(U_A^i, U_B^j) = \\\\frac{\\\\|U_A^{i^\\\\top}U_B\\\\|_F^2}{\\\\min\\\\{i, j\\\\}}$  to measure the subspace similarity between two column orthonormal matrices  $U_A^i \\\\in \\\\mathbb{R}^{d \\\\times i}$  and  $U_B^j \\\\in \\\\mathbb{R}^{d \\\\times j}$ , obtained by taking columns of the left singular matrices of  $A$  and  $B$ . We point out that this similarity is simply a reverse of the standard Projection Metric that measures distance between subspaces Ham &amp; Lee (2008).\\n\\n|  Method | Hyperparameters | # Trainable Parameters | WikiSQL | MNLI-m  |\\n| --- | --- | --- | --- | --- |\\n|  Fine-Tune | - | 175B | 73.8 | 89.5  |\\n|  PrefixEmbed | lp=32,li=8 | 0.4 M | 55.9 | 84.9  |\\n|   |  lp=64,li=8 | 0.9 M | 58.7 | 88.1  |\\n|   |  lp=128,li=8 | 1.7 M | 60.6 | 88.0  |\\n|   |  lp=256,li=8 | 3.2 M | 63.1 | 88.6  |\\n|   |  lp=512,li=8 | 6.4 M | 55.9 | 85.8  |\\n|  PrefixLayer | lp=2,li=2 | 5.1 M | 68.5 | 89.2  |\\n|   |  lp=8,li=0 | 10.1 M | 69.8 | 88.2  |\\n|   |  lp=8,li=8 | 20.2 M | 70.1 | 89.5  |\\n|   |  lp=32,li=4 | 44.1 M | 66.4 | 89.6  |\\n|   |  lp=64,li=0 | 76.1 M | 64.9 | 87.9  |\\n|  AdapterH | r=1 | 7.1 M | 71.9 | 89.8  |\\n|   |  r=4 | 21.2 M | 73.2 | 91.0  |\\n|   |  r=8 | 40.1 M | 73.2 | 91.5  |\\n|   |  r=16 | 77.9 M | 73.2 | 91.5  |\\n|   |  r=64 | 304.4 M | 72.6 | 91.5  |\\n|  LoRA | rv=2 | 4.7 M | 73.4 | 91.7  |\\n|   |  rq=rv=1 | 4.7 M | 73.4 | 91.3  |\\n|   |  rq=rv=2 | 9.4 M | 73.3 | 91.4  |\\n|   |  rq=rk=rv=ro=1 | 9.4 M | 74.1 | 91.2  |\\n|   |  rq=rv=4 | 18.8 M | 73.7 | 91.3  |\\n|   |  rq=rk=rv=ro=2 | 18.8 M | 73.7 | 91.7  |\\n|   |  rq=rv=8 | 37.7 M | 73.8 | 91.6  |\\n|   |  rq=rk=rv=ro=4 | 37.7 M | 74.0 | 91.7  |\\n|   |  rq=rv=64 | 301.9 M | 73.6 | 91.4  |\\n|   |  rq=rk=rv=ro=64 | 603.8 M | 73.9 | 91.4  |\\n|  LoRA+PE | rq=rv=8,lp=8,li=4 | 37.8 M | 75.0 | 91.4  |\\n|   |  rq=rv=32,lp=8,li=4 | 151.1 M | 75.9 | 91.1  |\\n|   |  rq=rv=64,lp=8,li=4 | 302.1 M | 76.2 | 91.3  |\\n|  LoRA+PL | rq=rv=8,lp=8,li=4 | 52.8 M | 72.9 | 90.2  |\\n\\nTable 15: Hyperparameter analysis of different adaptation approaches on WikiSQL and MNLI. Both prefix-embedding tuning (PrefixEmbed) and prefix-layer tuning (PrefixLayer) perform worse as we increase the number of trainable parameters, while LoRA\\'s performance stabilizes. Performance is measured in validation accuracy.\\n\\n|  Method | MNLI(m)-100 | MNLI(m)-1k | MNLI(m)-10k | MNLI(m)-392K  |\\n| --- | --- | --- | --- | --- |\\n|  GPT-3 (Fine-Tune) | 60.2 | 85.8 | 88.9 | 89.5  |\\n|  GPT-3 (PrefixEmbed) | 37.6 | 75.2 | 79.5 | 88.6  |\\n|  GPT-3 (PrefixLayer) | 48.3 | 82.5 | 85.9 | 89.6  |\\n|  GPT-3 (LoRA) | 63.8 | 85.6 | 89.2 | 91.7  |\\n\\nTable 16: Validation accuracy of different methods on subsets of MNLI using GPT-3 175B. MNLI-  $n$  describes a subset with  $n$  training examples. We evaluate with the full validation set. LoRA performs exhibits favorable sample-efficiency compared to other methods, including fine-tuning.\\n\\nTo be concrete, let the singular values of  $U_A^{i\\\\top}U_B^j$  to be  $\\\\sigma_1,\\\\sigma_2,\\\\dots ,\\\\sigma_p$  where  $p = \\\\min \\\\{i,j\\\\}$ . We know that the Projection Metric Ham &amp; Lee (2008) is defined as:\\n\\n$$\\nd (U _ {A} ^ {i}, U _ {B} ^ {j}) = \\\\sqrt {p - \\\\sum_ {i = 1} ^ {p} \\\\sigma_ {i} ^ {2}} \\\\in [ 0, \\\\sqrt {p} ]\\n$$\\n\\n|  Hyperparameters | Adaptation | MNLI-100 | MNLI-1k | MNLI-10K | MNLI-392K  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Optimizer | - |  | AdamW |   |   |\\n|  Warmup Tokens | - |  | 250,000 |   |   |\\n|  LR Schedule | - |  | Linear |   |   |\\n|  Batch Size | - | 20 | 20 | 100 | 128  |\\n|  # Epoch | - | 40 | 40 | 4 | 2  |\\n|  Learning Rate | FineTune |  | 5.00E-6 |   |   |\\n|   |  PrefixEmbed | 2.00E-04 | 2.00E-04 | 4.00E-04 | 5.00E-04  |\\n|   |  PrefixLayer | 5.00E-05 | 5.00E-05 | 5.00E-05 | 1.00E-04  |\\n|   |  LoRA |  | 2.00E-4 |   |   |\\n|  Adaptation-Specific | PrefixEmbed lp | 16 | 32 | 64 | 256  |\\n|   |  PrefixEmbed li |  | 8 |   |   |\\n|   |  PrefixTune |  | lp=li=8 |   |   |\\n|   |  LoRA |  | rq=rv=8 |   |   |\\n\\nTable 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)-n.\\n\\nwhere our similarity is defined as:\\n\\n$$\\n\\\\phi (A, B, i, j) = \\\\psi (U _ {A} ^ {i}, U _ {B} ^ {j}) = \\\\frac {\\\\sum_ {i = 1} ^ {p} \\\\sigma_ {i} ^ {2}}{p} = \\\\frac {1}{p} \\\\left(1 - d (U _ {A} ^ {i}, U _ {B} ^ {j}) ^ {2}\\\\right)\\n$$\\n\\nThis similarity satisfies that if  $U_A^i$  and  $U_B^j$  share the same column span, then  $\\\\phi(A, B, i, j) = 1$ . If they are completely orthogonal, then  $\\\\phi(A, B, i, j) = 0$ . Otherwise,  $\\\\phi(A, B, i, j) \\\\in (0, 1)$ .\\n\\n# H ADDITIONAL EXPERIMENTS ON LOW-RANK MATRICES\\n\\nWe present additional results from our investigation into the low-rank update matrices.\\n\\n# H.1 CORRELATION BETWEEN LORA MODULES\\n\\nSee Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other layers.\\n\\n# H.2 EFFECT OF  $r$  ON GPT-2\\n\\nWe repeat our experiment on the effect of  $r$  (Section 7.2) in GPT-2. Using the E2E NLG Challenge dataset as an example, we report the validation loss and test metrics achieved by different choices of  $r$  after training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2 Medium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B. Note that the relationship between model size and the optimal rank for adaptation is still an open question.\\n\\n# H.3 CORRELATION BETWEEN  $W$  AND  $\\\\Delta W$\\n\\nSee Figure 8 for the normalized subspace similarity between  $W$  and  $\\\\Delta W$  with varying  $r$ .\\n\\nNote again that  $\\\\Delta W$  does not contain the top singular directions of  $W$ , since the similarity between the top 4 directions in  $\\\\Delta W$  and the top-10% of those in  $W$  barely exceeds 0.2. This gives evidence that  $\\\\Delta W$  contains those \"task-specific\" directions that are otherwise not emphasized in  $W$ .\\n\\nAn interesting next question to answer, is how \"strong\" do we need to amplify those task-specific directions, in order for the model adaptation to work well?\\n\\n![img-6.jpeg](img-6.jpeg)\\nFigure 6: Normalized subspace similarity between the column vectors of  $A_{r=8}$  and  $A_{r=64}$  for both  $\\\\Delta W_q$  and  $\\\\Delta W_v$  from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.\\n\\n# H.4 AMPLIFICATION FACTOR\\n\\nOne can naturally consider a feature amplification factor as the ratio  $\\\\frac{\\\\|\\\\Delta W\\\\|_F}{\\\\|U^\\\\top WV^\\\\top\\\\|_F}$ , where  $U$  and  $V$  are the left- and right-singular matrices of the SVD decomposition of  $\\\\Delta W$ . (Recall  $UU^\\\\top WV^\\\\top V$  gives the \"projection\" of  $W$  onto the subspace spanned by  $\\\\Delta W$ .)\\n\\nIntuitively, when  $\\\\Delta W$  mostly contains task-specific directions, this quantity measures how much of them are amplified by  $\\\\Delta W$ . As shown in Section 7.3, for  $r = 4$ , this amplification factor is as large as 20. In other words, there are (generally speaking) four feature directions in each layer (out of the entire feature space from the pre-trained model  $W$ ), that need to be amplified by a very large factor 20, in order to achieve our reported accuracy for the downstream specific task. And, one should expect a very different set of feature directions to be amplified for each different downstream task.\\n\\nOne may notice, however, for  $r = 64$ , this amplification factor is only around 2, meaning that most directions learned in  $\\\\Delta W$  with  $r = 64$  are not being amplified by much. This should not be surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent the \"task-specific directions\" (thus for model adaptation) is low. In contrast, those directions in the rank-4 version of  $\\\\Delta W$  (corresponding to  $r = 4$ ) are amplified by a much larger factor 20.\\n\\n![img-7.jpeg](img-7.jpeg)\\nFigure 7: Normalized subspace similarity between the column vectors of  $A_{r=64}$  from two randomly seeded runs, for both  $\\\\Delta W_q$  and  $\\\\Delta W_v$  from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.\\n\\n|  Rank r | val_loss | BLEU | NIST | METEOR | ROUGE_L | CIDEr  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  1 | 1.23 | 68.72 | 8.7215 | 0.4565 | 0.7052 | 2.4329  |\\n|  2 | 1.21 | 69.17 | 8.7413 | 0.4590 | 0.7052 | 2.4639  |\\n|  4 | 1.18 | 70.38 | 8.8439 | 0.4689 | 0.7186 | 2.5349  |\\n|  8 | 1.17 | 69.57 | 8.7457 | 0.4636 | 0.7196 | 2.5196  |\\n|  16 | 1.16 | 69.61 | 8.7483 | 0.4629 | 0.7177 | 2.4985  |\\n|  32 | 1.16 | 69.33 | 8.7736 | 0.4642 | 0.7105 | 2.5255  |\\n|  64 | 1.16 | 69.24 | 8.7174 | 0.4651 | 0.7180 | 2.5070  |\\n|  128 | 1.16 | 68.73 | 8.6718 | 0.4628 | 0.7127 | 2.5030  |\\n|  256 | 1.16 | 68.92 | 8.6982 | 0.4629 | 0.7128 | 2.5012  |\\n|  512 | 1.16 | 68.78 | 8.6857 | 0.4637 | 0.7128 | 2.5025  |\\n|  1024 | 1.17 | 69.37 | 8.7495 | 0.4659 | 0.7149 | 2.5090  |\\n\\nTable 18: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with different rank  $r$  using GPT-2 Medium. Unlike on GPT-3 where  $r = 1$  suffices for many tasks, here the performance peaks at  $r = 16$  for validation loss and  $r = 4$  for BLEU, suggesting the GPT-2 Medium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our hyperparameters are tuned on  $r = 4$ , which matches the parameter count of another baseline, and thus might not be optimal for other choices of  $r$ .\\n\\n![img-8.jpeg](img-8.jpeg)\\nFigure 8: Normalized subspace similarity between the singular directions of  $W_{q}$  and those of  $\\\\Delta W_{q}$  with varying  $r$  and a random baseline.  $\\\\Delta W_{q}$  amplifies directions that are important but not emphasized in  $W$ .  $\\\\Delta W$  with a larger  $r$  tends to pick up more directions that are already emphasized in  $W$ .', 'doc_id': 'e6f9a937828a', 'start_line': 1, 'end_line': 734, 'chunk_index': 0}, page_content='# LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\\n\\nEdward Hu* Yelong Shen* Phillip Wallis Zeyuan Allen-Zhu\\n\\nYuanzhi Li Shean Wang Lu Wang Weizhu Chen\\n\\nMicrosoft Corporation\\n\\n{edwardhu, yeshe, phwallis, zeyuana,\\n\\nyuanzhil, swang, luw, wzchen}@microsoft.com\\n\\nyuanzhil@andrew.cmu.edu\\n\\n(Version 2)\\n\\n# ABSTRACT\\n\\nAn important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retransmits all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.\\n\\n# 1 INTRODUCTION\\n\\nMany applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via fine-tuning, which updates all the parameters of the pre-trained model. The major downside of fine-tuning is that the new model contains as many parameters as in the original model. As larger models are trained every few months, this changes from a mere \"inconvenience\" for GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a critical deployment challenge for GPT-3 (Brown et al., 2020) with 175 billion trainable parameters. $^{1}$\\n\\nMany sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed. However, existing techniques\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Our reparametrization. We only train  $A$  and  $B$ .\\n\\noften introduce inference latency *(Houlsby et al., 2019; Rebuffi et al., 2017)* by extending model depth or reduce the model’s usable sequence length *(Li and Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021)* (Section 3). More importantly, these method often fail to match the fine-tuning baselines, posing a trade-off between efficiency and model quality.\\n\\nWe take inspiration from *Li et al. (2018a); Aghajanyan et al. (2020)* which show that the learned over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed Low-Rank Adaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers’ change during adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3 175B as an example, we show that a very low rank (i.e., r in Figure 1 can be one or two) suffices even when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efficient.\\n\\nLoRA possesses several key advantages.\\n\\n- A pre-trained model can be shared and used to build many small LoRA modules for different tasks. We can freeze the shared model and efficiently switch tasks by replacing the matrices $A$ and $B$ in Figure 1, reducing the storage requirement and task-switching overhead significantly.\\n- LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, we only optimize the injected, much smaller low-rank matrices.\\n- Our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully fine-tuned model, by construction.\\n- LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning. We provide an example in Appendix E.\\n\\n#### Terminologies and Conventions\\n\\nWe make frequent references to the Transformer architecture and use the conventional terminologies for its dimensions. We call the input and output dimension size of a Transformer layer $d_{model}$. We use $W_{q}$, $W_{k}$, $W_{v}$, and $W_{o}$ to refer to the query/key/value/output projection matrices in the self-attention module. $W$ or $W_{0}$ refers to a pre-trained weight matrix and $\\\\Delta W$ its accumulated gradient update during adaptation. We use $r$ to denote the rank of a LoRA module. We follow the conventions set out by *(Vaswani et al., 2017; Brown et al., 2020)* and use Adam *(Loshchilov and Hutter, 2019; Kingma and Ba, 2017)* for model optimization and use a Transformer MLP feedforward dimension $d_{ffn}=4\\\\times d_{model}$.\\n\\n## 2 Problem Statement\\n\\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivating use case. Below is a brief description of the language modeling problem and, in particular, the maximization of conditional probabilities given a task-specific prompt.\\n\\nSuppose we are given a pre-trained autoregressive language model $P_{\\\\Phi}(y|x)$ parametrized by $\\\\Phi$. For instance, $P_{\\\\Phi}(y|x)$ can be a generic multi-task learner such as GPT *(Radford et al., 2020b; Brown et al., 2020)* based on the Transformer architecture *(Vaswani et al., 2017)*. Consider adapting this pre-trained model to downstream conditional text generation tasks, such as summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is represented by a training dataset of context-target pairs: $\\\\mathcal{Z}=\\\\{(x_{i},y_{i})\\\\}_{i=1,\\\\dots,N}$, where both $x_{i}$ and $y_{i}$ are sequences of tokens. For example, in NL2SQL, $x_{i}$ is a natural language query and $y_{i}$ its corresponding SQL command; for summarization, $x_{i}$ is the content of an article and $y_{i}$ its summary.\\n\\nDuring full fine-tuning, the model is initialized to pre-trained weights $\\\\Phi_{0}$ and updated to $\\\\Phi_{0}+\\\\Delta\\\\Phi$ by repeatedly following the gradient to maximize the conditional language modeling objective:\\n\\n$\\\\max_{\\\\Phi}\\\\sum_{(x,y)\\\\in\\\\mathcal{Z}}\\\\sum_{t=1}^{|y|}\\\\log\\\\left(P_{\\\\Phi}(y_{t}|x,y_{<t})\\\\right)$ (1)\\n\\nOne of the main drawbacks for full fine-tuning is that for *each* downstream task, we learn a *different* set of parameters $\\\\Delta\\\\Phi$ whose dimension $|\\\\Delta\\\\Phi|$ equals $|\\\\Phi_{0}|$. Thus, if the pre-trained model is large (such as GPT-3 with $|\\\\Phi_{0}|\\\\approx 175$ Billion), storing and deploying many independent instances of fine-tuned models can be challenging, if at all feasible.\\n\\nIn this paper, we adopt a more parameter-efficient approach, where the task-specific parameter increment $\\\\Delta\\\\Phi=\\\\Delta\\\\Phi(\\\\Theta)$ is further encoded by a much smaller-sized set of parameters $\\\\Theta$ with $|\\\\Theta|\\\\ll|\\\\Phi_{0}|$. The task of finding $\\\\Delta\\\\Phi$ thus becomes optimizing over $\\\\Theta$:\\n\\n$\\\\max_{\\\\Theta}\\\\sum_{(x,y)\\\\in\\\\mathcal{Z}}\\\\sum_{t=1}^{|y|}\\\\log\\\\left(p_{\\\\Phi_{0}+\\\\Delta\\\\Phi(\\\\Theta)}(y_{t}|x,y_{<t})\\\\right)$ (2)\\n\\nIn the subsequent sections, we propose to use a low-rank representation to encode $\\\\Delta\\\\Phi$ that is both compute- and memory-efficient. When the pre-trained model is GPT-3 175B, the number of trainable parameters $|\\\\Theta|$ can be as small as $0.01\\\\%$ of $|\\\\Phi_{0}|$.\\n\\n## 3 Aren’t Existing Solutions Good Enough?\\n\\nThe problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter- and compute-efficient. See Section 6 for a survey of some of the well-known works. Using language modeling as an example, there are two prominent strategies when it comes to efficient adaptations: adding adapter layers *(Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2021; Rücklé et al., 2020)* or optimizing some forms of the input layer activations *(Li and Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021)*. However, both strategies have their limitations, especially in a large-scale and latency-sensitive production scenario.\\n\\n#### Adapter Layers Introduce Inference Latency\\n\\nThere are many variants of adapters. We focus on the original design by *Houlsby et al. (2019)* which has two adapter layers per Transformer block and a more recent one by *Lin et al. (2020)* which has only one per block but with an additional LayerNorm *(Ba et al., 2016)*. While one can reduce the overall latency by pruning layers or exploiting multi-task settings *(Rücklé et al., 2020; Pfeiffer et al., 2021)*, there is no direct ways to bypass the extra compute in adapter layers. This seems like a non-issue since adapter layers are designed to have few parameters (sometimes $<$1% of the original model) by having a small bottleneck dimension, which limits the FLOPs they can add. However, large neural networks rely on hardware parallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes a difference in the online inference setting where the batch size is typically as small as one. In a generic scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b) medium on a single GPU, we see a noticeable increase in latency when using adapters, even with a very small bottleneck dimension (Table 1).\\n\\nThis problem gets worse when we need to shard the model as done in *Shoeybi et al. (2020); Lepikhin et al. (2020)*, because the additional depth requires more synchronous GPU operations such as AllReduce and Broadcast, unless we store the adapter parameters redundantly many times.\\n\\n#### Directly Optimizing the Prompt is Hard\\n\\nThe other direction, as exemplified by prefix tuning *(Li and Liang, 2021)*, faces a different challenge. We observe that prefix tuning is difficult to optimize and that its performance changes non-monotonically in trainable parameters, confirming similar observations in the original paper. More fundamentally, reserving a part of the sequence length for adaptation necessarily reduces the sequence length available to process a downstream task, which we suspect makes tuning the prompt less performant compared to other methods. We defer the study on task performance to Section 5.\\n\\n|  Batch Size | 32 | 16 | 1  |\\n| --- | --- | --- | --- |\\n|  Sequence Length | 512 | 256 | 128  |\\n|  |Θ | 0.5M | 11M | 11M  |\\n|  Fine-Tune/LoRA | 1449.4±0.8 | 338.0±0.6 | 19.8±2.7  |\\n|  AdapterL | 1482.0±1.0 (+2.2%) | 354.8±0.5 (+5.0%) | 23.9±2.1 (+20.7%)  |\\n|  AdapterH | 1492.2±1.0 (+3.0%) | 366.3±0.5 (+8.4%) | 25.8±2.2 (+30.3%)  |\\n\\nTable 1: Inference latency of a single forward pass in GPT-2 medium measured in milliseconds, averaged over 100 trials. We use an NVIDIA Quadro RTX8000. “ $|\\\\Theta|$ ” denotes the number of trainable parameters in adapter layers. Adapter $^{\\\\text{L}}$  and Adapter $^{\\\\text{H}}$  are two variants of adapter tuning, which we describe in Section 5.1. The inference latency introduced by adapter layers can be significant in an online, short-sequence-length scenario. See the full study in Appendix B.\\n\\n# 4 OUR METHOD\\n\\nWe describe the simple design of LoRA and its practical benefits. The principles outlined here apply to any dense layers in deep learning models, though we only focus on certain weights in Transformer language models in our experiments as the motivating use case.\\n\\n# 4.1 LOW-RANK-PARAMETRIZED UPDATE MATRICES\\n\\nA neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers typically have full-rank. When adapting to a specific task, Aghajanyan et al. (2020) shows that the pre-trained language models have a low \"intrinsic dimension\" and can still learn efficiently despite a random projection to a smaller subspace. Inspired by this, we hypothesize the updates to the weights also have a low \"intrinsic rank\" during adaptation. For a pre-trained weight matrix  $W_0 \\\\in \\\\mathbb{R}^{d \\\\times k}$ , we constrain its update by representing the latter with a low-rank decomposition  $W_0 + \\\\Delta W = W_0 + BA$ , where  $B \\\\in \\\\mathbb{R}^{d \\\\times r}$ ,  $A \\\\in \\\\mathbb{R}^{r \\\\times k}$ , and the rank  $r \\\\ll \\\\min(d, k)$ . During training,  $W_0$  is frozen and does not receive gradient updates, while  $A$  and  $B$  contain trainable parameters. Note both  $W_0$  and  $\\\\Delta W = BA$  are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For  $h = W_0x$ , our modified forward pass yields:\\n\\n$$\\nh = W _ {0} x + \\\\Delta W x = W _ {0} x + B A x \\\\tag {3}\\n$$\\n\\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for  $A$  and zero for  $B$ , so  $\\\\Delta W = BA$  is zero at the beginning of training. We then scale  $\\\\Delta Wx$  by  $\\\\frac{\\\\alpha}{r}$ , where  $\\\\alpha$  is a constant in  $r$ . When optimizing with Adam, tuning  $\\\\alpha$  is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set  $\\\\alpha$  to the first  $r$  we try and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary  $r$  (Yang &amp; Hu, 2021).\\n\\nA Generalization of Full Fine-tuning. A more general form of fine-tuning allows the training of a subset of the pre-trained parameters. LoRA takes a step further and does not require the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that when applying LoRA to all weight matrices and training all biases $^2$ , we roughly recover the expressiveness of full fine-tuning by setting the LoRA rank  $r$  to the rank of the pre-trained weight matrices. In other words, as we increase the number of trainable parameters $^3$ , training LoRA roughly converges to training the original model, while adapter-based methods converge to an MLP and prefix-based methods to a model that cannot take long input sequences.\\n\\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and store  $W = W_{0} + BA$  and perform inference as usual. Note that both  $W_{0}$  and  $BA$  are in  $\\\\mathbb{R}^{d\\\\times k}$ . When we need to switch to another downstream task, we can recover  $W_{0}$  by subtracting  $BA$  and then adding a different  $B^{\\\\prime}A^{\\\\prime}$ , a quick operation with very little memory overhead. Critically, this\\n\\nguarantees that we do not introduce any additional latency during inference compared to a fine-tuned model by construction.\\n\\n### 4.2 Applying LoRA to Transformer\\n\\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the number of trainable parameters. In the Transformer architecture, there are four weight matrices in the self-attention module ($W_{q},W_{k},W_{v},W_{o}$) and two in the MLP module. We treat $W_{q}$ (or $W_{k}$, $W_{v}$) as a single matrix of dimension $d_{model}\\\\times d_{model}$, even though the output dimension is usually sliced into attention heads. We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.We further study the effect on adapting different types of attention weight matrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP layers, LayerNorm layers, and biases to a future work.\\n\\n#### Practical Benefits and Limitations.\\n\\nThe most significant benefit comes from the reduction in memory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM usage by up to $2/3$ if $r\\\\ll d_{model}$ as we do not need to store the optimizer states for the frozen parameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to 350GB. With $r=4$ and only the query and value projection matrices being adapted, the checkpoint size is reduced by roughly 10,000$\\\\times$ (from 350GB to 35MB). This allows us to train with significantly fewer GPUs and avoid I/O bottlenecks. Another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters. This allows for the creation of many customized models that can be swapped in and out on the fly on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup during training on GPT-3 175B compared to full fine-tuning as we do not need to calculate the gradient for the vast majority of the parameters.\\n\\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks with different $A$ and $B$ in a single forward pass, if one chooses to absorb $A$ and $B$ into $W$ to eliminate additional inference latency. Though it is possible to not merge the weights and dynamically choose the LoRA modules to use for samples in a batch for scenarios where latency is not critical.\\n\\n## 5 Empirical Experiments\\n\\nWe evaluate the downstream task performance of LoRA on RoBERTa *(Liu et al., 2019)*, DeBERTa *(He et al., 2021)*, and GPT-2 *(Radford et al., 2020)*. Our experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG). Specifically, we evaluate on the GLUE *(Wang et al., 2019)* benchmark for RoBERTa and DeBERTa. We follow the setup of *Li and Liang (2021)* on GPT-2 for a direct comparison and add WikiSQL *(Zhong et al., 2017)* (NL to SQL queries) and SAMSum *(Gliwa et al., 2019)* (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for more details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\\n\\n### 5.1 Baselines\\n\\nTo compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible. This, however, means that some baselines might only appear in certain experiments.\\n\\nFine-Tuning (FT) is a common approach for adaptation. During fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple variant is to update only some layers while freezing others. We include one such baseline reported in prior work *(Li and Liang, 2021)* on GPT-2, which adapts just the last two layers (FT^{Top2}).\\n\\n|  Model & Method | # Trainable Parameters | MNLI | SST-2 | MRPC | CoLA | QNLI | QQP | RTE | STS-B | Avg.  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  RoBbase (FT)* | 125.0M | 87.6 | 94.8 | 90.2 | 63.6 | 92.8 | 91.9 | 78.7 | 91.2 | 86.4  |\\n|  RoBbase (BitFit)* | 0.1M | 84.7 | 93.7 | 92.7 | 62.0 | 91.8 | 84.0 | 81.5 | 90.8 | 85.2  |\\n|  RoBbase (AdptD)* | 0.3M | 87.1±.0 | 94.2±.1 | 88.5±1.1 | 60.8±.4 | 93.1±.1 | 90.2±.0 | 71.5±2.7 | 89.7±.3 | 84.4  |\\n|  RoBbase (AdptD)* | 0.9M | 87.3±.1 | 94.7±.3 | 88.4±.1 | 62.6±.9 | 93.0±.2 | 90.6±.0 | 75.9±2.2 | 90.3±.1 | 85.4  |\\n|  RoBbase (LoRA) | 0.3M | 87.5±.3 | 95.1±.2 | 89.7±.7 | 63.4±1.2 | 93.3±.3 | 90.8±.1 | 86.6±.7 | 91.5±.2 | 87.2  |\\n|  RoBlarge (FT)* | 355.0M | 90.2 | 96.4 | 90.9 | 68.0 | 94.7 | 92.2 | 86.6 | 92.4 | 88.9  |\\n|  RoBlarge (LoRA) | 0.8M | 90.6±.2 | 96.2±.5 | 90.9±1.2 | 68.2±1.9 | 94.9±.3 | 91.6±.1 | 87.4±2.5 | 92.6±.2 | 89.0  |\\n|  RoBlarge (AdptP)† | 3.0M | 90.2±.3 | 96.1±.3 | 90.2±.7 | 68.3±1.0 | 94.8±.2 | 91.9±.1 | 83.8±2.9 | 92.1±.7 | 88.4  |\\n|  RoBlarge (AdptP)† | 0.8M | 90.5±.3 | 96.6±.2 | 89.7±1.2 | 67.8±2.5 | 94.8±.3 | 91.7±.2 | 80.1±2.9 | 91.9±.4 | 87.9  |\\n|  RoBlarge (AdptH)† | 6.0M | 89.9±.5 | 96.2±.3 | 88.7±2.9 | 66.5±4.4 | 94.7±.2 | 92.1±.1 | 83.4±1.1 | 91.0±1.7 | 87.8  |\\n|  RoBlarge (AdptH)† | 0.8M | 90.3±.3 | 96.3±.5 | 87.7±1.7 | 66.3±2.0 | 94.7±.2 | 91.5±.1 | 72.9±2.9 | 91.5±.5 | 86.4  |\\n|  RoBlarge (LoRA)† | 0.8M | 90.6±.2 | 96.2±.5 | 90.2±1.0 | 68.2±1.9 | 94.8±.3 | 91.6±.2 | 85.2±1.1 | 92.3±.5 | 88.6  |\\n|  DeBXXL (FT)* | 1500.0M | 91.8 | 97.2 | 92.0 | 72.0 | 96.0 | 92.7 | 93.9 | 92.9 | 91.1  |\\n|  DeBXXL (LoRA) | 4.7M | 91.9±.2 | 96.9±.2 | 92.6±.6 | 72.4±1.1 | 96.0±.1 | 92.9±.1 | 94.9±.4 | 93.0±.2 | 91.3  |\\n\\nTable 2: RoBERTa $_{\\\\text{base}}$ , RoBERTa $_{\\\\text{large}}$ , and DeBERTa $_{\\\\text{XXL}}$  with different adaptation methods on the GLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew\\'s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all metrics. * indicates numbers published in prior works. † indicates runs configured in a setup similar to Houlsby et al. (2019) for a fair comparison.\\n\\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else. Contemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021).\\n\\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These special tokens have trainable word embeddings and are generally not in the model\\'s vocabulary. Where to place such tokens can have an impact on performance. We focus on \"prefixing\", which prepends such tokens to the prompt, and \"infixing\", which appends to the prompt; both are discussed in Li &amp; Liang (2021). We use  $l_{p}$  (resp.  $l_{i}$ ) denote the number of prefix (resp. infix) tokens. The number of trainable parameters is  $|\\\\Theta| = d_{model} \\\\times (l_{p} + l_{i})$ .\\n\\nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning. Instead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer. The activations computed from previous layers are simply replaced by trainable ones. The resulting number of trainable parameters is  $|\\\\Theta| = L \\\\times d_{model} \\\\times (l_p + l_i)$ , where  $L$  is the number of Transformer layers.\\n\\nAdapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection. There are two fully connected layers with biases in an adapter layer with a nonlinearity in between. We call this original design  $\\\\mathbf{Adapter}^{\\\\mathbf{H}}$ . Recently, Lin et al. (2020) proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm. We call it  $\\\\mathbf{Adapter}^{\\\\mathbf{L}}$ . This is very similar to another design proposed in Pfeiffer et al. (2021), which we call  $\\\\mathbf{Adapter}^{\\\\mathbf{P}}$ . We also include another baseline call AdapterDrop (Rückle et al., 2020) which drops some adapter layers for greater efficiency ( $\\\\mathbf{Adapter}^{\\\\mathbf{D}}$ ). We cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column. In all cases, we have  $|\\\\Theta| = \\\\hat{L}_{Adpt} \\\\times (2 \\\\times d_{model} \\\\times r + r + d_{model}) + 2 \\\\times \\\\hat{L}_{LN} \\\\times d_{model}$  where  $\\\\hat{L}_{Adpt}$  is the number of adapter layers and  $\\\\hat{L}_{LN}$  the number of trainable LayerNorms (e.g., in  $\\\\mathbf{Adapter}^{\\\\mathbf{L}}$ ).\\n\\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices. As mentioned in Section 4.2, we only apply LoRA to  $W_{q}$  and  $W_{v}$  in most experiments for simplicity. The number of trainable parameters is determined by the rank  $r$  and the shape of the original weights:  $|\\\\Theta| = 2 \\\\times \\\\hat{L}_{LoRA} \\\\times d_{model} \\\\times r$ , where  $\\\\hat{L}_{LoRA}$  is the number of weight matrices we apply LoRA to.\\n\\n|  Model & Method | # Trainable Parameters | E2E NLG Challenge  |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|   |   |  BLEU | NIST | MET | ROUGE-L | CIDEr  |\\n|  GPT-2 M (FT)* | 354.92M | 68.2 | 8.62 | 46.2 | 71.0 | 2.47  |\\n|  GPT-2 M (AdapterL)* | 0.37M | 66.3 | 8.41 | 45.0 | 69.8 | 2.40  |\\n|  GPT-2 M (AdapterL)* | 11.09M | 68.9 | 8.71 | 46.1 | 71.3 | 2.47  |\\n|  GPT-2 M (AdapterH) | 11.09M | 67.3±.6 | 8.50±.07 | 46.0±.2 | 70.7±.2 | 2.44±.01  |\\n|  GPT-2 M (FTTop2)* | 25.19M | 68.1 | 8.59 | 46.0 | 70.8 | 2.41  |\\n|  GPT-2 M (PreLayer)* | 0.35M | 69.7 | 8.81 | 46.1 | 71.4 | 2.49  |\\n|  GPT-2 M (LoRA) | 0.35M | 70.4±.1 | 8.85±.02 | 46.8±.2 | 71.8±.1 | 2.53±.02  |\\n|  GPT-2 L (FT)* | 774.03M | 68.5 | 8.78 | 46.0 | 69.9 | 2.45  |\\n|  GPT-2 L (AdapterL) | 0.88M | 69.1±.1 | 8.68±.03 | 46.3±.0 | 71.4±.2 | 2.49±.0  |\\n|  GPT-2 L (AdapterL) | 23.00M | 68.9±.3 | 8.70±.04 | 46.1±.1 | 71.3±.2 | 2.45±.02  |\\n|  GPT-2 L (PreLayer)* | 0.77M | 70.3 | 8.85 | 46.2 | 71.7 | 2.47  |\\n|  GPT-2 L (LoRA) | 0.77M | 70.4±.1 | 8.89±.02 | 46.8±.2 | 72.0±.2 | 2.47±.02  |\\n\\nTable 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG Challenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable or fewer trainable parameters. Confidence intervals are shown for experiments we ran. * indicates numbers published in prior works.\\n\\n# 5.2 ROBERTA BASE/LARGE\\n\\nRoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin et al., 2019a) and boosted the latter\\'s task performance without introducing many more trainable parameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards such as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and popular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base (125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020) and evaluate the performance of different efficient adaptation approaches on tasks from the GLUE benchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their setup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when comparing with adapters. First, we use the same batch size for all tasks and use a sequence length of 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for MRPC, RTE, and STS-B, not a model already adapted to MNLI like the fine-tuning baseline. Runs following this more restricted setup from Houlsby et al. (2019) are labeled with  $\\\\dagger$ . The result is presented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.\\n\\n# 5.3 DEBERTA XXL\\n\\nDeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger scale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and SuperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully fine-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section). See Section D.2 for details on the hyperparameters used.\\n\\n# 5.4 GPT-2 MEDIUM/LARGE\\n\\nHaving shown that LoRA can be a competitive alternative to full fine-tuning on NLU, we hope to answer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al., b). We keep our setup as close as possible to Li &amp; Liang (2021) for a direct comparison. Due to space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section. See Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We include a list of the hyperparameters used in Section D.3.\\n\\n|  Model&Method | # Trainable Parameters | WikiSQL | MNLI-m | SAMSum  |\\n| --- | --- | --- | --- | --- |\\n|   |   |  Acc. (%) | Acc. (%) | R1/R2/RL  |\\n|  GPT-3 (FT) | 175,255.8M | 73.8 | 89.5 | 52.0/28.0/44.5  |\\n|  GPT-3 (BitFit) | 14.2M | 71.3 | 91.0 | 51.3/27.4/43.5  |\\n|  GPT-3 (PreEmbed) | 3.2M | 63.1 | 88.6 | 48.3/24.2/40.5  |\\n|  GPT-3 (PreLayer) | 20.2M | 70.1 | 89.5 | 50.8/27.3/43.5  |\\n|  GPT-3 (AdapterH) | 7.1M | 71.9 | 89.8 | 53.0/28.9/44.8  |\\n|  GPT-3 (AdapterH) | 40.1M | 73.2 | 91.5 | 53.2/29.0/45.1  |\\n|  GPT-3 (LoRA) | 4.7M | 73.4 | 91.7 | 53.8/29.8/45.9  |\\n|  GPT-3 (LoRA) | 37.7M | 74.0 | 91.6 | 53.4/29.2/45.1  |\\n\\nTable 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form validation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on SAMSum. LoRA performs better than prior approaches, including full fine-tuning. The results on WikiSQL have a fluctuation around  $\\\\pm 0.5\\\\%$ , MNLI-m around  $\\\\pm 0.1\\\\%$ , and SAMSum around  $\\\\pm 0.2 / \\\\pm 0.2 / \\\\pm 0.1$  for the three metrics.\\n\\n# 5.5 SCALING UP TO GPT-3 175B\\n\\nAs a final stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high training cost, we only report the typical standard deviation for a given task over random seeds, as opposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.\\n\\nAs shown in Table 4, LoRA matches or exceeds the fine-tuning baseline on all three datasets. Note that not all methods benefit monotonically from having more trainable parameters, as shown in Figure 2. We observe a significant performance drop when we use more than 256 special tokens for prefix-embedding tuning or more than 32 special tokens for prefix-layer tuning. This corroborates similar observations in Li &amp; Liang (2021). While a thorough investigation into this phenomenon is out-of-scope for this work, we suspect that having more special tokens causes the input distribution to shift further away from the pre-training data distribution. Separately, we investigate the performance of different adaptation approaches in the low-data regime in Section F.3.\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation methods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance. See Section F.2 for more details on the plotted data points.\\n\\n![img-2.jpeg](img-2.jpeg)\\n\\n# 6 RELATED WORKS\\n\\nTransformer Language Models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence architecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive language modeling by using a stack of Transformer decoders. Since then, Transformer-based language models have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged with BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) - both are large Transformer lan\\n\\nguage models trained on a large amount of text – where fine-tuning on task-specific data after pre-training on general domain data provides a significant performance gain compared to training on task-specific data directly. Training larger Transformers generally results in better performance and remains an active research direction. GPT-3 *(Brown et al., 2020)* is the largest single Transformer language model trained to-date with 175B parameters.\\n\\n#### Prompt Engineering and Fine-Tuning.\\n\\nWhile GPT-3 175B can adapt its behavior with just a few additional training examples, the result depends heavily on the input prompt *(Brown et al., 2020)*. This necessitates an empirical art of composing and formatting the prompt to maximize a model’s performance on a desired task, which is known as prompt engineering or prompt hacking. Fine-tuning retrains a model pre-trained on general domains to a specific task *Devlin et al. (2019b); Radford et al. (a)*. Variants of it include learning just a subset of the parameters *Devlin et al. (2019b); Collobert and Weston (2008)*, yet practitioners often retrain all of them to maximize the downstream performance. However, the enormity of GPT-3 175B makes it challenging to perform fine-tuning in the usual way due to the large checkpoint it produces and the high hardware barrier to entry since it has the same memory footprint as pre-training.\\n\\n#### Parameter-Efficient Adaptation.\\n\\nMany have proposed inserting adapter layers between existing layers in a neural network *(Houlsby et al., 2019; Rebuffi et al., 2017; Lin et al., 2020)*. Our method uses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The key functional difference is that our learned weights can be merged with the main weights during inference, thus not introducing any latency, which is not the case for the adapter layers (Section 3). A comtenporary extension of adapter is compacter *(Mahabadi et al., 2021)*, which essentially parametrizes the adapter layers using Kronecker products with some predetermined weight sharing scheme. Similarly, combining LoRA with other tensor product-based methods could potentially improve its parameter efficiency, which we leave to future work. More recently, many proposed optimizing the input word embeddings in lieu of fine-tuning, akin to a continuous and differentiable generalization of prompt engineering *(Li and Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021)*. We include comparisons with *Li and Liang (2021)* in our experiment section. However, this line of works can only scale up by using more special tokens in the prompt, which take up available sequence length for task tokens when positional embeddings are learned.\\n\\n#### Low-Rank Structures in Deep Learning.\\n\\nLow-rank structure is very common in machine learning. A lot of machine learning problems have certain intrinsic low-rank structure *(Li et al., 2016; Cai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013)*. Moreover, it is known that for many deep learning tasks, especially those with a heavily over-parametrized neural network, the learned neural network will enjoy low-rank properties after training *(Oymak et al., 2019)*. Some prior works even explicitly impose the low-rank constraint when training the original neural network *(Sainath et al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Khodak et al., 2021; Denil et al., 2014)*; however, to the best of our knowledge, none of these works considers low-rank update to a frozen model for adaptation to downstream tasks. In theory literature, it is known that neural networks outperform other classical learning methods, including the corresponding (finite-width) neural tangent kernels *(Allen-Zhu et al., 2019; Li and Liang, 2018)* when the underlying concept class has certain low-rank structure *(Ghorbani et al., 2020; Allen-Zhu and Li, 2019; Allen-Zhu and Li, 2020a)*. Another theoretical result in *Allen-Zhu and Li (2020b)* suggests that low-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed low-rank adaptation update is well-motivated by the literature.\\n\\n## 7 Understanding the Low-Rank Updates\\n\\nGiven the empirical advantage of LoRA, we hope to further explain the properties of the low-rank adaptation learned from downstream tasks. Note that the low-rank structure not only lowers the hardware barrier to entry which allows us to run multiple experiments in parallel, but also gives better interpretability of how the update weights are correlated with the pre-trained weights. We focus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters (up to 10,000$\\\\times$) without adversely affecting task performances.\\n\\nWe perform a sequence of empirical studies to answer the following questions: 1) Given a parameter budget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt\\n\\nto maximize downstream performance? 2) Is the \"optimal\" adaptation matrix  $\\\\Delta W$  really rank-deficient? If so, what is a good rank to use in practice? 3) What is the connection between  $\\\\Delta W$  and  $W$ ? Does  $\\\\Delta W$  highly correlate with  $W$ ? How large is  $\\\\Delta W$  comparing to  $W$ ?\\n\\nWe believe that our answers to question (2) and (3) shed light on the fundamental principles of using pre-trained language models for downstream tasks, which is a critical topic in NLP.\\n\\n# 7.1 WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?\\n\\nGiven a limited parameter budget, which types of weights should we adapt with LoRA to obtain the best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight matrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored in FP16) on GPT-3 175B, which corresponds to  $r = 8$  if we adapt one type of attention weights or  $r = 4$  if we adapt two types, for all 96 layers. The result is presented in Table 5.\\n\\n|   | # of Trainable Parameters = 18M  |   |   |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Weight Type Rank r | Wq8 | Wk8 | Wv8 | Wo8 | Wq, Wk4 | Wq, Wv4 | Wq, Wk, Wv, Wo2  |\\n|  WikiSQL (±0.5%) | 70.4 | 70.0 | 73.0 | 73.2 | 71.4 | 73.7 | 73.7  |\\n|  MultiNLI (±0.1%) | 91.0 | 90.8 | 91.0 | 91.3 | 91.3 | 91.3 | 91.7  |\\n\\nNote that putting all the parameters in  $\\\\Delta W_{q}$  or  $\\\\Delta W_{k}$  results in significantly lower performance, while adapting both  $W_{q}$  and  $W_{v}$  yields the best result. This suggests that even a rank of four captures enough information in  $\\\\Delta W$  such that it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank.\\n\\n# 7.2 WHAT IS THE OPTIMAL RANK  $r$  FOR LORA?\\n\\nWe turn our attention to the effect of rank  $r$  on model performance. We adapt  $\\\\{W_q, W_v\\\\}$ ,  $\\\\{W_q, W_k, W_v, W_c\\\\}$ , and just  $W_q$  for a comparison.\\n\\nTable 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of attention weights in GPT-3, given the same number of trainable parameters. Adapting both  $W_{q}$  and  $W_{v}$  gives the best performance overall. We find the standard deviation across random seeds to be consistent for a given dataset, which we report in the first column.\\n\\n|   | Weight Type | r = 1 | r = 2 | r = 4 | r = 8 | r = 64  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  WikiSQL(±0.5%) | Wq | 68.8 | 69.6 | 70.5 | 70.4 | 70.0  |\\n|   |  Wq, Wv | 73.4 | 73.3 | 73.7 | 73.8 | 73.5  |\\n|   |  Wq, Wk, Wv, Wo | 74.1 | 73.7 | 74.0 | 74.0 | 73.9  |\\n|  MultiNLI (±0.1%) | Wq | 90.7 | 90.9 | 91.1 | 90.7 | 90.7  |\\n|   |  Wq, Wv | 91.3 | 91.4 | 91.3 | 91.6 | 91.4  |\\n|   |  Wq, Wk, Wv, Wo | 91.2 | 91.7 | 91.7 | 91.5 | 91.4  |\\n\\nTable 6: Validation accuracy on WikiSQL and MultiNLI with different rank  $r$ . To our surprise, a rank as small as one suffices for adapting both  $W_{q}$  and  $W_{v}$  on these datasets while training  $W_{q}$  alone needs a larger  $r$ . We conduct a similar experiment on GPT-2 in Section H.2.\\n\\nTable 6 shows that, surprisingly, LoRA already performs competitively with a very small  $r$  (more so for  $\\\\{W_q, W_v\\\\}$  than just  $W_q$ ). This suggests the update matrix  $\\\\Delta W$  could have a very small \"intrinsic rank\". To further support this finding, we check the overlap of the subspaces learned by different choices of  $r$  and by different random seeds. We argue that increasing  $r$  does not cover a more meaningful subspace, which suggests that a low-rank adaptation matrix is sufficient.\\n\\nSubspace similarity between different $r$. Given $A_{r=8}$ and $A_{r=64}$ which are the learned adaptation matrices with rank $r=8$ and $64$ using the same pre-trained model, we perform singular value decomposition and obtain the right-singular unitary matrices $U_{A_{r=8}}$ and $U_{A_{r=64}}$. We hope to answer: how much of the subspace spanned by the top $i$ singular vectors in $U_{A_{r=8}}$ (for $1\\\\leq i\\\\leq 8$) is contained in the subspace spanned by top $j$ singular vectors of $U_{A_{r=64}}$ (for $1\\\\leq j\\\\leq 64$)? We measure this quantity with a normalized subspace similarity based on the Grassmann distance (See Appendix G for a more formal discussion)\\n\\n$\\\\phi(A_{r=8},A_{r=64},i,j)=\\\\frac{||U_{A_{r=8}}^{i\\\\top}U_{A_{r=64}}^{j}||_{F}^{2}}{\\\\min(i,j)}\\\\in[0,1]$ (4)\\n\\nwhere $U_{A_{r=8}}^{i}$ represents the columns of $U_{A_{r=8}}$ corresponding to the top-$i$ singular vectors.\\n\\n$\\\\phi(\\\\cdot)$ has a range of $[0,1]$, where $1$ represents a complete overlap of subspaces and $0$ a complete separation. See Figure 3 for how $\\\\phi$ changes as we vary $i$ and $j$. We only look at the 48th layer (out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown in Section H.1.\\n\\n$\\\\phi(A_{r=64},A_{r=8},i,j)$\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: Subspace similarity between column vectors of $A_{r=8}$ and $A_{r=64}$ for both $\\\\Delta W_{q}$ and $\\\\Delta W_{v}$. The third and the fourth figures zoom in on the lower-left triangle in the first two figures. The top directions in $r=8$ are included in $r=64$, and vice versa.\\n\\nWe make an *important observation* from Figure 3.\\n\\n&gt; Directions corresponding to the top singular vector overlap significantly between $A_{r=8}$ and $A_{r=64}$, while others do not. Specifically, $\\\\Delta W_{v}$ (resp. $\\\\Delta W_{q}$) of $A_{r=8}$ and $\\\\Delta W_{v}$ (resp. $\\\\Delta W_{q}$) of $A_{r=64}$ share a subspace of dimension 1 with normalized similarity $&gt;0.5$, providing an explanation of why $r=1$ performs quite well in our downstream tasks for GPT-3.\\n\\nSince both $A_{r=8}$ and $A_{r=64}$ are learned using the same pre-trained model, Figure 3 indicates that the top singular-vector directions of $A_{r=8}$ and $A_{r=64}$ are the most useful, while other directions potentially contain mostly random noises accumulated during training. Hence, the adaptation matrix can indeed have a very low rank.\\n\\nSubspace similarity between different random seeds. We further confirm this by plotting the normalized subspace similarity between two randomly seeded runs with $r=64$, shown in Figure 4. $\\\\Delta W_{q}$ appears to have a higher “intrinsic rank” than $\\\\Delta W_{v}$, since more common singular value directions are learned by both runs for $\\\\Delta W_{q}$, which is in line with our empirical observation in Table 6. As a comparison, we also plot two random Gaussian matrices, which do not share any common singular value directions with each other.\\n\\n### 7.3 How does the Adaptation Matrix $\\\\Delta W$ compare to $W$?\\n\\nWe further investigate the relationship between $\\\\Delta W$ and $W$. In particular, does $\\\\Delta W$ highly correlate with $W$? (Or mathematically, is $\\\\Delta W$ mostly contained in the top singular directions of $W$?) Also,\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 4: Left and Middle: Normalized subspace similarity between the column vectors of  $A_{r=64}$  from two random seeds, for both  $\\\\Delta W_q$  and  $\\\\Delta W_v$  in the 48-th layer. Right: the same heat-map between the column vectors of two random Gaussian matrices. See Section H.1 for other layers.\\n\\nhow \"large\" is  $\\\\Delta W$  comparing to its corresponding directions in  $W$ ? This can shed light on the underlying mechanism for adapting pre-trained language models.\\n\\nTo answer these questions, we project  $W$  onto the  $r$ -dimensional subspace of  $\\\\Delta W$  by computing  $U^{\\\\top}WV^{\\\\top}$ , with  $U / V$  being the left/right singular-vector matrix of  $\\\\Delta W$ . Then, we compare the Frobenius norm between  $\\\\| U^{\\\\top}WV^{\\\\top}\\\\|_F$  and  $\\\\| W\\\\|_F$ . As a comparison, we also compute  $\\\\| U^{\\\\top}WV^{\\\\top}\\\\|_F$  by replacing  $U, V$  with the top  $r$  singular vectors of  $W$  or a random matrix.\\n\\n|   | r=4 |   |   | r=64  |   |   |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|   | ΔWq | Wq | Random | ΔWq | Wq | Random  |\\n|  ||U^T W_q V^T ||_F = | 0.32 | 21.67 | 0.02 | 1.90 | 37.71 | 0.33  |\\n|  ||Wq||_F = 61.95 | ||ΔWq||_F = 6.91 |   |   | ΔWq||_F = 3.57  |   |   |\\n\\nTable 7: The Frobenius norm of  $U^{\\\\top}W_{q}V^{\\\\top}$  where  $U$  and  $V$  are the left/right top  $r$  singular vector directions of either (1)  $\\\\Delta W_{q}$ , (2)  $W_{q}$ , or (3) a random matrix. The weight matrices are taken from the 48th layer of GPT-3.\\n\\nWe draw several conclusions from Table 7. First,  $\\\\Delta W$  has a stronger correlation with  $W$  compared to a random matrix, indicating that  $\\\\Delta W$  amplifies some features that are already in  $W$ . Second, instead of repeating the top singular directions of  $W$ ,  $\\\\Delta W$  only amplifies directions that are not emphasized in  $W$ . Third, the amplification factor is rather huge:  $21.5 \\\\approx 6.91 / 0.32$  for  $r = 4$ . See Section H.4 for why  $r = 64$  has a smaller amplification factor. We also provide a visualization in Section H.3 for how the correlation changes as we include more top singular directions from  $W_{q}$ . This suggests that the low-rank adaptation matrix potentially amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model.\\n\\n# 8 CONCLUSION AND FUTURE WORK\\n\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required and the storage/switching cost for hosting independent instances for different tasks. We propose LoRA, an efficient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality. Importantly, it allows for quick task-switching when deployed as a service by sharing the vast majority of the model parameters. While we focused on Transformer language models, the proposed principles are generally applicable to any neural networks with dense layers.\\n\\nThere are many directions for future works. 1) LoRA can be combined with other efficient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind fine-tuning or LoRA is far from clear - how are features learned during pre-training transformed to do well on downstream tasks? We believe that LoRA makes it more tractable to answer this than full fine\\n\\ntuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are there more principled ways to do it? 4) Finally, the rank-deficiency of $\\\\Delta W$ suggests that $W$ could be rank-deficient as well, which can also be a source of inspiration for future works.\\n\\n## References\\n\\n- A. Aghajanyan, L. Zettlemoyer, and S. Gupta (2020) Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL http://arxiv.org/abs/2012.13255.\\n- Z. Allen-Zhu and Y. Li (2019) What Can ResNet Learn Efficiently, Going Beyond Kernels? In NeurIPS, 2019. Full version available at http://arxiv.org/abs/1905.10337.\\n- Z. Allen-Zhu and Y. Li (2020a) Backward feature correction: How deep learning performs deep learning. arXiv preprint arXiv:2001.04413, 2020a.\\n- Z. Allen-Zhu and Y. Li (2020b) Feature purification: How adversarial training performs robust deep learning. arXiv preprint arXiv:2005.10190, 2020b.\\n- Z. Allen-Zhu, Y. Li, and Z. Song (2020) A convergence theory for deep learning via over-parameterization. In ICML, 2019. Full version available at http://arxiv.org/abs/1811.03962.\\n- J. Le Ba, J. Kiros, and G. E. Hinton (2016) Layer normalization, 2016.\\n- B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. D. A. and M. Ziegler (2017) A single, a single, and a single, Deep Learning: A survey. arXiv preprint arXiv:1709.01465 [cs], July 2020. URL http://arxiv.org/abs/2005.14165.\\n- C. Can-Feng Cai, E. Manuel, J. Candès, and Z. Shen (2010) A singular value thresholding algorithm for matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010.\\n- C. Cer, M. Diab, E. Agirre, I. Loege-Gazpio, and L. Semeval (2017) A universal, a universal, and crosslingual focused evaluation. Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017. doi: 10.18653/v1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001.\\n- C. Conan, A. Conan, and J. S. DeVlin (2014) Nonan Collobert and Jason Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, ICML ’08, pp. 160–167, New York, NY, USA, July 2008. Association for Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL https://doi.org/10.1145/1390156.1390177.\\n- D. Denil, B. Shakibi, L. D. Chen, M. Aurelio Ranzato, and N. de Freitas (2014) Predicting parameters in deep learning, 2014.\\n- D. Devlin, M. Wei Chang, K. Lee, and K. Toutanova (2019a) Bert: Pre-training of deep bidirectional transformers for language understanding, 2019a.\\n- D. Devlin, M. Wei Chang, K. Lee, and K. Toutanova (2019b) Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019b. URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805.\\n- B. Dolan and B. Brockett (2017) A automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002.\\n- G. Gardner, A. Shimorina, S. Narayan, and L. Perez-Beltrachini (2017) The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pp. 124–133, 2017.\\n- G.\\n\\nBehrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020.\\n- Ghorbani et al. (2020) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. CoRR, abs/1911.12237, 2019. URL http://arxiv.org/abs/1911.12237.\\n- Ghorbani et al. (2013) Lars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor approximation techniques. GAMM-Mitteilungen, 36(1):53–78, 2013.\\n- Haim and Ghorbani (2019) Jihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based learning. In ICML, pp. 376–383, 2008. URL https://doi.org/10.1145/1390156.1390204.\\n- Haim and Ghorbani (2019) Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial ReProgramming. arXiv:2101.00121 [cs], December 2020. URL http://arxiv.org/abs/2101.00121. arXiv: 2101.00121.\\n- He et al. (2014) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention, 2021.\\n- Hei et al. (2021) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. arXiv:1902.00751 [cs, stat], June 2019. URL http://arxiv.org/abs/1902.00751.\\n- Jaderberg et al. (2014) Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.\\n- Khodak et al. (2014) Mikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicolò Fusi. Initialization and regularization of factorized neural layers, 2021.\\n- Klingma and Le (2021) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\\n- Lepikhin et al. (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020.\\n- Le et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. arXiv:2104.08691 [cs], April 2021. URL http://arxiv.org/abs/2104.08691. arXiv: 2104.08691.\\n- Li et al. (2021) Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a. URL http://arxiv.org/abs/1804.08838. arXiv: 1804.08838.\\n- Li and Li (2021) Xiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing Continuous Prompts for Generation. arXiv:2101.00190 [cs], January 2021. URL http://arxiv.org/abs/2101.00190.\\n- Li and Li (2021) Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, 2018.\\n- Li et al. (2021) Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank approximation via alternating minimization. In International Conference on Machine Learning, pp. 2358–2367. PMLR, 2016.\\n- Li et al. (2020) Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pp. 2–47. PMLR, 2018b.\\n- Lin et al. (2020) Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 441–459, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.41. URL https://aclanthology.org/2020.findings-emnlp.41.\\n\\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT Understands, Too. arXiv:2103.10385 [cs], March 2021. URL http://arxiv.org/abs/2103.10385. arXiv: 2103.10385.\\n- Lin, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.\\n- Lishchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\\n- Lishchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\\n- Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers, 2021.\\n- Nandayram, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871, 2020.\\n- Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint arXiv:1906.05392, 2019.\\n- Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and Sanjeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, pp. 3743–3747, 2018.\\n- Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training. pp. 12, a.\\n- Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. pp. 24, b.\\n- Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. CoRR, abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822.\\n- Ruch, H. and V. Radford. Learning multiple visual domains with residual adapters. arXiv:1705.08045 [cs, stat], November 2017. URL http://arxiv.org/abs/1705.08045. arXiv: 1705.08045.\\n- Ruch, H. and V. Radford. Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers, 2020.\\n- Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6655–6659. IEEE, 2013.\\n- Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.\\n- Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Proceedings of the 31st International Conference on Neural Information Processing Systems*, pp. 6000–6010, 2017.\\n- Wang et al. [2017] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.\\n- Wang et al. [2020] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems, 2020.\\n- Warstadt et al. [2018] Alexandr M. Bowman. Neural network acceptability judgments. *arXiv preprint arXiv:1805.12471*, 2018.\\n- Williams et al. [2011] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.org/anthology/N18-1101.\\n- Wolf et al. [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pp. 38–45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.\\n- Yang and J. Hu [2021] Greg Yang and Edward J. Hu. Feature Learning in Infinite-Width Neural Networks. *arXiv:2011.14522 [cond-mat]*, May 2021. URL http://arxiv.org/abs/2011.14522. arXiv: 2011.14522.\\n- Ben Zaken et al. [2014] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, 2021.\\n- Zhang et al. [2016] Yu Zhang, Ekapol Chuangsuwanich, and James Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In *2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)*, pp. 185–189. IEEE, 2014.\\n- Zhao et al. [2017] Yong Zhao, Jinyu Li, and Yifan Gong. Low-rank plus diagonal adaptation for deep neural networks. In *2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pp. 5005–5009. IEEE, 2016.\\n- Zhou et al. [2017] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. *CoRR*, abs/1709.00103, 2017. URL http://arxiv.org/abs/1709.00103.\\n\\n## Appendix A Large Language Models Still Need Parameter Updates\\n\\nFew-shot learning, or prompt engineering, is very advantageous when we only have a handful of training samples. However, in practice, we can often afford to curate a few thousand or more training examples for performance-sensitive applications. As shown in Table 8, fine-tuning improves the model performance drastically compared to few-shot learning on datasets large and small. We take the GPT-3 few-shot result on RTE from the GPT-3 paper *(Brown et al., 2020)*. For MNLI-matched, we use two demonstrations per class and six in-context examples in total.\\n\\n|  Method | MNLI-m (Val. Acc./%) | RTE (Val. Acc./%)  |\\n| --- | --- | --- |\\n|  GPT-3 Few-Shot | 40.6 | 69.0  |\\n|  GPT-3 Fine-Tuned | 89.5 | 85.4  |\\n\\nTable 8: Fine-tuning significantly outperforms few-shot learning on GPT-3 (Brown et al., 2020).\\n\\n# B INFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS\\n\\nAdapter layers are external modules added to a pre-trained model in a sequential manner, whereas our proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently, adapter layers must be computed in addition to the base model, inevitably introducing additional latency. While as pointed out in Rücklé et al. (2020), the latency introduced by adapter layers can be mitigated when the model batch size and/or sequence length is large enough to full utilize the hardware parallelism. We confirm their observation with a similar latency study on GPT-2 medium and point out that there are scenarios, notably online inference where the batch size is small, where the added latency can be significant.\\n\\nWe measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging over 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension  $r$ . We test two adapter designs: the original one by Houlsby et al. (2019), which we call Adapter $^{\\\\mathrm{H}}$ , and a recent, more efficient variant by Lin et al. (2020), which we call Adapter $^{\\\\mathrm{L}}$ . See Section 5.1 for more details on the designs. We plot the slow-down in percentage compared to the no-adapter baseline in Figure 5.\\n\\n![img-5.jpeg](img-5.jpeg)\\nFigure 5: Percentage slow-down of inference latency compared to the no-adapter  $(r = 0)$  baseline. The top row shows the result for Adapter $^{\\\\mathrm{H}}$  and the bottom row Adapter $^{\\\\mathrm{L}}$ . Larger batch size and sequence length help to mitigate the latency, but the slow-down can be as high as over  $30\\\\%$  in an online, short-sequence-length scenario. We tweak the colormap for better visibility.\\n\\n# C DATASET DETAILS\\n\\nGLUE Benchmark is a wide-ranging collection of natural language understanding tasks. It includes MNLI (inference, Williams et al. (2018)), SST-2 (sentiment analysis, Socher et al. (2013)), MRPC (paraphrase detection, Dolan &amp; Brockett (2005)), CoLA (linguistic acceptability, Warstadt et al. (2018)), QNLI (inference, Rajpurkar et al. (2018)),  $\\\\mathrm{QQP^8}$  (question-answering), RTE (inference),\\n\\nand STS-B (textual similarity, *Cer et al. (2017)*). The broad coverage makes GLUE benchmark a standard metric to evaluate NLU models such as RoBERTa and DeBERTa. The individual datasets are released under different permissive licenses.\\n\\nWikiSQL is introduced in *Zhong et al. (2017)* and contains $56,355/8,421$ training/validation examples. The task is to generate SQL queries from natural language questions and table schemata. We encode context as $x=\\\\{\\\\text{table schema},\\\\text{query}\\\\}$ and target as $y=\\\\{\\\\text{SQL}\\\\}$. The dataset is release under the BSD 3-Clause License.\\n\\nSAMSum is introduced in *Gliwa et al. (2019)* and contains $14,732/819$ training/test examples. It consists of staged chat conversations between two people and corresponding abstractive summaries written by linguists. We encode context as ”\\\\n” concatenated utterances followed by a ”\\\\n\\\\n, and target as $y=\\\\{\\\\text{summary}\\\\}$. The dataset is released under the non-commercial licence: Creative Commons BY-NC-ND 4.0.\\n\\nE2E NLG Challenge was first introduced in *Novikova et al. (2017)* as a dataset for training end-to-end, data-driven natural language generation systems and is commonly used for data-to-text evaluation. The E2E dataset consists of roughly $42,000$ training, $4,600$ validation, and $4,600$ test examples from the restaurant domain. Each source table used as input can have multiple references. Each sample input $(x,y)$ consists of a sequence of slot-value pairs, along with a corresponding natural language reference text. The dataset is released under Creative Commons BY-NC-SA 4.0.\\n\\nDART is an open-domain data-to-text dataset described in *Nan et al. (2020)*. DART inputs are structured as sequences of ENTITY — RELATION — ENTITY triples. With $82K$ examples in total, DART is a significantly larger and more complex data-to-text task compared to E2E. The dataset is released under the MIT license.\\n\\nWebNLG is another commonly used dataset for data-to-text evaluation *(Gardent et al., 2017)*. With $22K$ examples in total WebNLG comprises 14 distinct categories, nine of which are seen during training. Since five of the 14 total categories are not seen during training, but are represented in the test set, evaluation is typically broken out by “seen” categories (S), “unseen” categories (U) and “all” (A). Each input example is represented by a sequence of SUBJECT — PROPERTY — OBJECT triples. The dataset is released under Creative Commons BY-NC-SA 4.0.\\n\\n## Appendix D Hyperparameters Used in Experiments\\n\\n### D.1 RoBERTa\\n\\nWe train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number of training epochs, and batch size for LoRA. Following *Liu et al. (2019)*, we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5 random seeds; the result for each run is taken from the best epoch. For a fair comparison with the setup in *Houlsby et al. (2019)* and *Pfeiffer et al. (2021)*, we restrict the model sequence length to 128 and used a fixed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large model when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI. The runs with this restricted setup are marked with $\\\\dagger$. See the hyperparameters used in our runs in Table 9.\\n\\n### D.2 DeBERTa\\n\\nWe again train using AdamW with a linear learning rate decay schedule. Following *He et al. (2021)*, we tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model sequence length used by *(He et al., 2021)* to keep our comparison fair. Following *He et al. (2021)*, we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5 random seeds; the result for each run is taken from the best epoch. See the hyperparameters used in our runs in Table 10.\\n\\n|  Method | Dataset | MNLI | SST-2 | MRPC | CoLA | QNLI | QQP | RTE | STS-B  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   | Optimizer |  |  |  | AdamW |   |  |  |   |\\n|   | Warmup Ratio |  |  |  | 0.06 |   |  |  |   |\\n|   | LR Schedule |  |  |  | Linear |   |  |  |   |\\n|  RoBERTa base LoRA | Batch Size | 16 | 16 | 16 | 32 | 32 | 16 | 32 | 16  |\\n|   |  # Epochs | 30 | 60 | 30 | 80 | 25 | 25 | 80 | 40  |\\n|   |  Learning Rate | 5E-04 | 5E-04 | 4E-04 | 4E-04 | 4E-04 | 5E-04 | 5E-04 | 4E-04  |\\n|   |  LoRA Config. |  |  |  | rq = rv = 8 |   |  |  |   |\\n|   |  LoRA α |  |  |  | 8 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 512 |   |  |  |   |\\n|  RoBERTa large LoRA | Batch Size | 4 | 4 | 4 | 4 | 4 | 4 | 8 | 8  |\\n|   |  # Epochs | 10 | 10 | 20 | 20 | 10 | 20 | 20 | 30  |\\n|   |  Learning Rate | 3E-04 | 4E-04 | 3E-04 | 2E-04 | 2E-04 | 3E-04 | 4E-04 | 2E-04  |\\n|   |  LoRA Config. |  |  |  | rq = rv = 8 |   |  |  |   |\\n|   |  LoRA α |  |  |  | 16 |   |  |  |   |\\n|   |  Max Seq. Len. | 128 | 128 | 512 | 128 | 512 | 512 | 512 | 512  |\\n|  RoBERTa large LoRA† | Batch Size |  |  |  | 4 |   |  |  |   |\\n|   |  # Epochs | 10 | 10 | 20 | 20 | 10 | 20 | 20 | 10  |\\n|   |  Learning Rate | 3E-04 | 4E-04 | 3E-04 | 2E-04 | 2E-04 | 3E-04 | 4E-04 | 2E-04  |\\n|   |  LoRA Config. |  |  |  | rq = rv = 8 |   |  |  |   |\\n|   |  LoRA α |  |  |  | 16 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 128 |   |  |  |   |\\n|  RoBERTa large AdptP(3M)† | Batch Size |  |  |  | 32 |   |  |  |   |\\n|   |  # Epochs | 10 | 20 | 20 | 20 | 10 | 20 | 20 | 20  |\\n|   |  Learning Rate | 3E-05 | 3E-05 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04  |\\n|   |  Bottleneck r |  |  |  | 64 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 128 |   |  |  |   |\\n|  RoBERTa large AdptP(0.8M)† | Batch Size |  |  |  | 32 |   |  |  |   |\\n|   |  # Epochs | 5 | 20 | 20 | 20 | 10 | 20 | 20 | 20  |\\n|   |  Learning Rate | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04  |\\n|   |  Bottleneck r |  |  |  | 16 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 128 |   |  |  |   |\\n|  RoBERTa large AdptH(6M)† | Batch Size |  |  |  | 32 |   |  |  |   |\\n|   |  # Epochs | 10 | 5 | 10 | 10 | 5 | 20 | 20 | 10  |\\n|   |  Learning Rate | 3E-05 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04  |\\n|   |  Bottleneck r |  |  |  | 64 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 128 |   |  |  |   |\\n|  RoBERTa large AdptH(0.8M)† | Batch Size |  |  |  | 32 |   |  |  |   |\\n|   |  # Epochs | 10 | 5 | 10 | 10 | 5 | 20 | 20 | 10  |\\n|   |  Learning Rate | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04 | 3E-04  |\\n|   |  Bottleneck r |  |  |  | 8 |   |  |  |   |\\n|   |  Max Seq. Len. |  |  |  | 128 |   |  |  |   |\\n\\nTable 9: The hyperparameters we used for RoBERTa on the GLUE benchmark.\\n\\n# D.3 GPT-2\\n\\nWe train all of our GPT-2 models using AdamW (Loshchilov &amp; Hutter, 2017) with a linear learning rate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described in Li &amp; Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the mean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters used for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li &amp; Liang (2021).\\n\\n# D.4 GPT-3\\n\\nFor all GPT-3 experiments, we train using AdamW (Loshchilov &amp; Hutter, 2017) for 2 epochs with a batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for\\n\\n|  Method | Dataset | MNLI | SST-2 | MRPC | CoLA | QNLI | QQP | RTE | STS-B  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   | Optimizer |  |  |  | AdamW |   |  |  |   |\\n|   | Warmup Ratio |  |  |  | 0.1 |   |  |  |   |\\n|   | LR Schedule |  |  |  | Linear |   |  |  |   |\\n|  DeBERTa XXL LoRA | Batch Size | 8 | 8 | 32 | 4 | 6 | 8 | 4 | 4  |\\n|   |  # Epochs | 5 | 16 | 30 | 10 | 8 | 11 | 11 | 10  |\\n|   |  Learning Rate | 1E-04 | 6E-05 | 2E-04 | 1E-04 | 1E-04 | 1E-04 | 2E-04 | 2E-04  |\\n|   |  Weight Decay | 0 | 0.01 | 0.01 | 0 | 0.01 | 0.01 | 0.01 | 0.1  |\\n|   |  CLS Dropout | 0.15 | 0 | 0 | 0.1 | 0.1 | 0.2 | 0.2 | 0.2  |\\n|   |  LoRA Config. |  |  |  | rq = rv = 8 |   |  |  |   |\\n|   |  LoRA α |  |  |  | 8 |   |  |  |   |\\n|   |  Max Seq. Len. | 256 | 128 | 128 | 64 | 512 | 320 | 320 | 128  |\\n\\nTable 10: The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark.\\n\\n|  Dataset | E2E | WebNLG | DART  |\\n| --- | --- | --- | --- |\\n|   | Training  |   |   |\\n|  Optimizer | AdamW  |   |   |\\n|  Weight Decay | 0.01 | 0.01 | 0.0  |\\n|  Dropout Prob | 0.1 | 0.1 | 0.0  |\\n|  Batch Size | 8  |   |   |\\n|  # Epoch | 5  |   |   |\\n|  Warmup Steps | 500  |   |   |\\n|  Learning Rate Schedule | Linear  |   |   |\\n|  Label Smooth | 0.1 | 0.1 | 0.0  |\\n|  Learning Rate | 0.0002  |   |   |\\n|  Adaptation | rq = rv = 4  |   |   |\\n|  LoRA α | 32  |   |   |\\n|   | Inference  |   |   |\\n|  Beam Size | 10  |   |   |\\n|  Length Penalty | 0.9 | 0.8 | 0.8  |\\n|  no repeat ngram size | 4  |   |   |\\n\\nTable 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART.\\n\\nWikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa et al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more details on the hyperparameters used. For prefix-embedding tuning, we find the optimal  $l_{p}$  and  $l_{i}$  to be 256 and 8, respectively, totalling  $3.2M$  trainable parameters. We use  $l_{p} = 8$  and  $l_{i} = 8$  for prefix-layer tuning with  $20.2M$  trainable parameters to obtain the overall best performance. We present two parameter budgets for LoRA:  $4.7\\\\mathrm{M}$  ( $r_{q} = r_{v} = 1$  or  $r_{v} = 2$ ) and  $37.7\\\\mathrm{M}$  ( $r_{q} = r_{v} = 8$  or  $r_{q} = r_{k} = r_{v} = r_{o} = 2$ ). We report the best validation performance from each run. The training hyperparameters used in our GPT-3 experiments are listed in Table 12.\\n\\n# E COMBINING LORA WITH Prefix TUNING\\n\\nLoRA can be naturally combined with existing prefix-based approaches. In this section, we evaluate two combinations of LoRA and variants of prefix-tuning on WikiSQL and MNLI.\\n\\nLoRA+PrefixEmbed (LoRA+PE) combines LoRA with prefix-embedding tuning, where we insert  $l_{p} + l_{i}$  special tokens whose embeddings are treated as trainable parameters. For more on prefix-embedding tuning, see Section 5.1.\\n\\nLoRA+PrefixLayer (LoRA+PL) combines LoRA with prefix-layer tuning. We also insert  $l_{p} + l_{i}$  special tokens; however, instead of letting the hidden representations of these tokens evolve natu\\n\\n|  Hyperparameters | Fine-Tune | PreEmbed | PreLayer | BitFit | AdapterH | LoRA  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  Optimizer |  |  | AdamW |  |  |   |\\n|  Batch Size |  |  | 128 |  |  |   |\\n|  # Epoch |  |  | 2 |  |  |   |\\n|  Warmup Tokens |  |  | 250,000 |  |  |   |\\n|  LR Schedule |  |  | Linear |  |  |   |\\n|  Learning Rate | 5.00E-06 | 5.00E-04 | 1.00E-04 | 1.6E-03 | 1.00E-04 | 2.00E-04  |\\n\\nrally, we replace them after every Transformer block with an input agnostic vector. Thus, both the embeddings and subsequent Transformer block activations are treated as trainable parameters. For more on prefix-layer tuning, see Section 5.1.\\n\\nIn Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI. First of all, LoRA+PE significantly outperforms both LoRA and prefix-embedding tuning on WikiSQL, which indicates that LoRA is somewhat orthogonal to prefix-embedding tuning. On MultiNLI, the combination of LoRA+PE doesn\\'t perform better than LoRA, possibly because LoRA on its own already achieves performance comparable to the human baseline. Secondly, we notice that LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We attribute this to the fact that prefix-layer tuning is very sensitive to the choice of learning rate and thus makes the optimization of LoRA weights more difficult in LoRA+PL.\\n\\n# F ADDITIONAL EMPIRICAL EXPERIMENTS\\n\\n# F.1 ADDITIONAL EXPERIMENTS ON GPT-2\\n\\nWe also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017) following the setup of Li &amp; Liang (2021). The result is shown in Table 13. Similar to our result on E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with prefix-based approaches given the same number of trainable parameters.\\n\\nTable 12: The training hyperparameters used for different GPT-3 adaption methods. We use the same hyperparameters for all datasets after tuning learning rate.\\n\\n|  Method | # Trainable Parameters | BLEU↑ | DART MET↑ | TER↓  |\\n| --- | --- | --- | --- | --- |\\n|  GPT-2 Medium  |   |   |   |   |\\n|  Fine-Tune | 354M | 46.2 | 0.39 | 0.46  |\\n|  AdapterL | 0.37M | 42.4 | 0.36 | 0.48  |\\n|  AdapterL | 11M | 45.2 | 0.38 | 0.46  |\\n|  FTTop2 | 24M | 41.0 | 0.34 | 0.56  |\\n|  PrefLayer | 0.35M | 46.4 | 0.38 | 0.46  |\\n|  LoRA | 0.35M | 47.1±.2 | 0.39 | 0.46  |\\n|  GPT-2 Large  |   |   |   |   |\\n|  Fine-Tune | 774M | 47.0 | 0.39 | 0.46  |\\n|  AdapterL | 0.88M | 45.7±.1 | 0.38 | 0.46  |\\n|  AdapterL | 23M | 47.1±.1 | 0.39 | 0.45  |\\n|  PrefLayer | 0.77M | 46.7 | 0.38 | 0.45  |\\n|  LoRA | 0.77M | 47.5±.1 | 0.39 | 0.45  |\\n\\nTable 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are less than 0.01 for all adaption approaches.\\n\\n|  Method | WebNLG  |   |   |   |   |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   |  U | BLEU↑S | A | MET↑ |   |   | A | U | S  |\\n|   |   |   |   |  U | S | A  |   |   |   |\\n|  GPT-2 Medium  |   |   |   |   |   |   |   |   |   |\\n|  Fine-Tune (354M) | 27.7 | 64.2 | 46.5 | .30 | .45 | .38 | .76 | .33 | .53  |\\n|  AdapterL (0.37M) | 45.1 | 54.5 | 50.2 | .36 | .39 | .38 | .46 | .40 | .43  |\\n|  AdapterL (11M) | 48.3 | 60.4 | 54.9 | .38 | .43 | .41 | .45 | .35 | .39  |\\n|  FTTop2 (24M) | 18.9 | 53.6 | 36.0 | .23 | .38 | .31 | .99 | .49 | .72  |\\n|  Prefix (0.35M) | 45.6 | 62.9 | 55.1 | .38 | .44 | .41 | .49 | .35 | .40  |\\n|  LoRA (0.35M) | 46.7±.4 | 62.1±.2 | 55.3±.2 | .38 | .44 | .41 | .46 | .33 | .39  |\\n|  GPT-2 Large  |   |   |   |   |   |   |   |   |   |\\n|  Fine-Tune (774M) | 43.1 | 65.3 | 55.5 | .38 | .46 | .42 | .53 | .33 | .42  |\\n|  AdapterL (0.88M) | 49.8±.0 | 61.1±.0 | 56.0±.0 | .38 | .43 | .41 | .44 | .35 | .39  |\\n|  AdapterL (23M) | 49.2±.1 | 64.7±.2 | 57.7±.1 | .39 | .46 | .43 | .46 | .33 | .39  |\\n|  Prefix (0.77M) | 47.7 | 63.4 | 56.3 | .39 | .45 | .42 | .48 | .34 | .40  |\\n|  LoRA (0.77M) | 48.4±.3 | 64.0±.3 | 57.0±.1 | .39 | .45 | .42 | .45 | .32 | .38  |\\n\\nTable 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER are less than 0.01 for all the experiments we ran. \"U\" indicates unseen categories, \"S\" indicates seen categories, and \"A\" indicates all categories in the test set of WebNLG.\\n\\n# F.2 ADDITIONAL EXPERIMENTS ON GPT-3\\n\\nWe present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on identifying the trade-off between performance and the number of trainable parameters.\\n\\n# F.3 LOW-DATA REGIME\\n\\nTo evaluate the performance of different adaptation approaches in the low-data regime, we randomly sample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data MNLI- $n$  tasks. In Table 16, we show the performance of different adaptation approaches on MNLI- $n$ . To our surprise, PrefixEmbed and PrefixLayer performs very poorly on MNLI-100 dataset, with PrefixEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PrefixLayer performs better than PrefixEmbed but is still significantly worse than Fine-Tune or LoRA on MNLI-100. The gap between prefix-based approaches and LoRA/Fine-tuning becomes smaller as we increase the number of training examples, which might suggest that prefix-based approaches are not suitable for low-data tasks in GPT-3. LoRA achieves better performance than fine-tuning on both MNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the  $(\\\\pm 0.3)$  variance due to random seeds.\\n\\nThe training hyperparameters of different adaptation approaches on MNLI-n are reported in Table 17. We use a smaller learning rate for PrefixLayer on the MNLI-100 set, as the training loss does not decrease with a larger learning rate.\\n\\n# G MEASURING SIMILARITY BETWEEN SUBSPACES\\n\\nIn this paper we use the measure  $\\\\phi(A, B, i, j) = \\\\psi(U_A^i, U_B^j) = \\\\frac{\\\\|U_A^{i^\\\\top}U_B\\\\|_F^2}{\\\\min\\\\{i, j\\\\}}$  to measure the subspace similarity between two column orthonormal matrices  $U_A^i \\\\in \\\\mathbb{R}^{d \\\\times i}$  and  $U_B^j \\\\in \\\\mathbb{R}^{d \\\\times j}$ , obtained by taking columns of the left singular matrices of  $A$  and  $B$ . We point out that this similarity is simply a reverse of the standard Projection Metric that measures distance between subspaces Ham &amp; Lee (2008).\\n\\n|  Method | Hyperparameters | # Trainable Parameters | WikiSQL | MNLI-m  |\\n| --- | --- | --- | --- | --- |\\n|  Fine-Tune | - | 175B | 73.8 | 89.5  |\\n|  PrefixEmbed | lp=32,li=8 | 0.4 M | 55.9 | 84.9  |\\n|   |  lp=64,li=8 | 0.9 M | 58.7 | 88.1  |\\n|   |  lp=128,li=8 | 1.7 M | 60.6 | 88.0  |\\n|   |  lp=256,li=8 | 3.2 M | 63.1 | 88.6  |\\n|   |  lp=512,li=8 | 6.4 M | 55.9 | 85.8  |\\n|  PrefixLayer | lp=2,li=2 | 5.1 M | 68.5 | 89.2  |\\n|   |  lp=8,li=0 | 10.1 M | 69.8 | 88.2  |\\n|   |  lp=8,li=8 | 20.2 M | 70.1 | 89.5  |\\n|   |  lp=32,li=4 | 44.1 M | 66.4 | 89.6  |\\n|   |  lp=64,li=0 | 76.1 M | 64.9 | 87.9  |\\n|  AdapterH | r=1 | 7.1 M | 71.9 | 89.8  |\\n|   |  r=4 | 21.2 M | 73.2 | 91.0  |\\n|   |  r=8 | 40.1 M | 73.2 | 91.5  |\\n|   |  r=16 | 77.9 M | 73.2 | 91.5  |\\n|   |  r=64 | 304.4 M | 72.6 | 91.5  |\\n|  LoRA | rv=2 | 4.7 M | 73.4 | 91.7  |\\n|   |  rq=rv=1 | 4.7 M | 73.4 | 91.3  |\\n|   |  rq=rv=2 | 9.4 M | 73.3 | 91.4  |\\n|   |  rq=rk=rv=ro=1 | 9.4 M | 74.1 | 91.2  |\\n|   |  rq=rv=4 | 18.8 M | 73.7 | 91.3  |\\n|   |  rq=rk=rv=ro=2 | 18.8 M | 73.7 | 91.7  |\\n|   |  rq=rv=8 | 37.7 M | 73.8 | 91.6  |\\n|   |  rq=rk=rv=ro=4 | 37.7 M | 74.0 | 91.7  |\\n|   |  rq=rv=64 | 301.9 M | 73.6 | 91.4  |\\n|   |  rq=rk=rv=ro=64 | 603.8 M | 73.9 | 91.4  |\\n|  LoRA+PE | rq=rv=8,lp=8,li=4 | 37.8 M | 75.0 | 91.4  |\\n|   |  rq=rv=32,lp=8,li=4 | 151.1 M | 75.9 | 91.1  |\\n|   |  rq=rv=64,lp=8,li=4 | 302.1 M | 76.2 | 91.3  |\\n|  LoRA+PL | rq=rv=8,lp=8,li=4 | 52.8 M | 72.9 | 90.2  |\\n\\nTable 15: Hyperparameter analysis of different adaptation approaches on WikiSQL and MNLI. Both prefix-embedding tuning (PrefixEmbed) and prefix-layer tuning (PrefixLayer) perform worse as we increase the number of trainable parameters, while LoRA\\'s performance stabilizes. Performance is measured in validation accuracy.\\n\\n|  Method | MNLI(m)-100 | MNLI(m)-1k | MNLI(m)-10k | MNLI(m)-392K  |\\n| --- | --- | --- | --- | --- |\\n|  GPT-3 (Fine-Tune) | 60.2 | 85.8 | 88.9 | 89.5  |\\n|  GPT-3 (PrefixEmbed) | 37.6 | 75.2 | 79.5 | 88.6  |\\n|  GPT-3 (PrefixLayer) | 48.3 | 82.5 | 85.9 | 89.6  |\\n|  GPT-3 (LoRA) | 63.8 | 85.6 | 89.2 | 91.7  |\\n\\nTable 16: Validation accuracy of different methods on subsets of MNLI using GPT-3 175B. MNLI-  $n$  describes a subset with  $n$  training examples. We evaluate with the full validation set. LoRA performs exhibits favorable sample-efficiency compared to other methods, including fine-tuning.\\n\\nTo be concrete, let the singular values of  $U_A^{i\\\\top}U_B^j$  to be  $\\\\sigma_1,\\\\sigma_2,\\\\dots ,\\\\sigma_p$  where  $p = \\\\min \\\\{i,j\\\\}$ . We know that the Projection Metric Ham &amp; Lee (2008) is defined as:\\n\\n$$\\nd (U _ {A} ^ {i}, U _ {B} ^ {j}) = \\\\sqrt {p - \\\\sum_ {i = 1} ^ {p} \\\\sigma_ {i} ^ {2}} \\\\in [ 0, \\\\sqrt {p} ]\\n$$\\n\\n|  Hyperparameters | Adaptation | MNLI-100 | MNLI-1k | MNLI-10K | MNLI-392K  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Optimizer | - |  | AdamW |   |   |\\n|  Warmup Tokens | - |  | 250,000 |   |   |\\n|  LR Schedule | - |  | Linear |   |   |\\n|  Batch Size | - | 20 | 20 | 100 | 128  |\\n|  # Epoch | - | 40 | 40 | 4 | 2  |\\n|  Learning Rate | FineTune |  | 5.00E-6 |   |   |\\n|   |  PrefixEmbed | 2.00E-04 | 2.00E-04 | 4.00E-04 | 5.00E-04  |\\n|   |  PrefixLayer | 5.00E-05 | 5.00E-05 | 5.00E-05 | 1.00E-04  |\\n|   |  LoRA |  | 2.00E-4 |   |   |\\n|  Adaptation-Specific | PrefixEmbed lp | 16 | 32 | 64 | 256  |\\n|   |  PrefixEmbed li |  | 8 |   |   |\\n|   |  PrefixTune |  | lp=li=8 |   |   |\\n|   |  LoRA |  | rq=rv=8 |   |   |\\n\\nTable 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)-n.\\n\\nwhere our similarity is defined as:\\n\\n$$\\n\\\\phi (A, B, i, j) = \\\\psi (U _ {A} ^ {i}, U _ {B} ^ {j}) = \\\\frac {\\\\sum_ {i = 1} ^ {p} \\\\sigma_ {i} ^ {2}}{p} = \\\\frac {1}{p} \\\\left(1 - d (U _ {A} ^ {i}, U _ {B} ^ {j}) ^ {2}\\\\right)\\n$$\\n\\nThis similarity satisfies that if  $U_A^i$  and  $U_B^j$  share the same column span, then  $\\\\phi(A, B, i, j) = 1$ . If they are completely orthogonal, then  $\\\\phi(A, B, i, j) = 0$ . Otherwise,  $\\\\phi(A, B, i, j) \\\\in (0, 1)$ .\\n\\n# H ADDITIONAL EXPERIMENTS ON LOW-RANK MATRICES\\n\\nWe present additional results from our investigation into the low-rank update matrices.\\n\\n# H.1 CORRELATION BETWEEN LORA MODULES\\n\\nSee Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other layers.\\n\\n# H.2 EFFECT OF  $r$  ON GPT-2\\n\\nWe repeat our experiment on the effect of  $r$  (Section 7.2) in GPT-2. Using the E2E NLG Challenge dataset as an example, we report the validation loss and test metrics achieved by different choices of  $r$  after training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2 Medium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B. Note that the relationship between model size and the optimal rank for adaptation is still an open question.\\n\\n# H.3 CORRELATION BETWEEN  $W$  AND  $\\\\Delta W$\\n\\nSee Figure 8 for the normalized subspace similarity between  $W$  and  $\\\\Delta W$  with varying  $r$ .\\n\\nNote again that  $\\\\Delta W$  does not contain the top singular directions of  $W$ , since the similarity between the top 4 directions in  $\\\\Delta W$  and the top-10% of those in  $W$  barely exceeds 0.2. This gives evidence that  $\\\\Delta W$  contains those \"task-specific\" directions that are otherwise not emphasized in  $W$ .\\n\\nAn interesting next question to answer, is how \"strong\" do we need to amplify those task-specific directions, in order for the model adaptation to work well?\\n\\n![img-6.jpeg](img-6.jpeg)\\nFigure 6: Normalized subspace similarity between the column vectors of  $A_{r=8}$  and  $A_{r=64}$  for both  $\\\\Delta W_q$  and  $\\\\Delta W_v$  from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.\\n\\n# H.4 AMPLIFICATION FACTOR\\n\\nOne can naturally consider a feature amplification factor as the ratio  $\\\\frac{\\\\|\\\\Delta W\\\\|_F}{\\\\|U^\\\\top WV^\\\\top\\\\|_F}$ , where  $U$  and  $V$  are the left- and right-singular matrices of the SVD decomposition of  $\\\\Delta W$ . (Recall  $UU^\\\\top WV^\\\\top V$  gives the \"projection\" of  $W$  onto the subspace spanned by  $\\\\Delta W$ .)\\n\\nIntuitively, when  $\\\\Delta W$  mostly contains task-specific directions, this quantity measures how much of them are amplified by  $\\\\Delta W$ . As shown in Section 7.3, for  $r = 4$ , this amplification factor is as large as 20. In other words, there are (generally speaking) four feature directions in each layer (out of the entire feature space from the pre-trained model  $W$ ), that need to be amplified by a very large factor 20, in order to achieve our reported accuracy for the downstream specific task. And, one should expect a very different set of feature directions to be amplified for each different downstream task.\\n\\nOne may notice, however, for  $r = 64$ , this amplification factor is only around 2, meaning that most directions learned in  $\\\\Delta W$  with  $r = 64$  are not being amplified by much. This should not be surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent the \"task-specific directions\" (thus for model adaptation) is low. In contrast, those directions in the rank-4 version of  $\\\\Delta W$  (corresponding to  $r = 4$ ) are amplified by a much larger factor 20.\\n\\n![img-7.jpeg](img-7.jpeg)\\nFigure 7: Normalized subspace similarity between the column vectors of  $A_{r=64}$  from two randomly seeded runs, for both  $\\\\Delta W_q$  and  $\\\\Delta W_v$  from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.\\n\\n|  Rank r | val_loss | BLEU | NIST | METEOR | ROUGE_L | CIDEr  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  1 | 1.23 | 68.72 | 8.7215 | 0.4565 | 0.7052 | 2.4329  |\\n|  2 | 1.21 | 69.17 | 8.7413 | 0.4590 | 0.7052 | 2.4639  |\\n|  4 | 1.18 | 70.38 | 8.8439 | 0.4689 | 0.7186 | 2.5349  |\\n|  8 | 1.17 | 69.57 | 8.7457 | 0.4636 | 0.7196 | 2.5196  |\\n|  16 | 1.16 | 69.61 | 8.7483 | 0.4629 | 0.7177 | 2.4985  |\\n|  32 | 1.16 | 69.33 | 8.7736 | 0.4642 | 0.7105 | 2.5255  |\\n|  64 | 1.16 | 69.24 | 8.7174 | 0.4651 | 0.7180 | 2.5070  |\\n|  128 | 1.16 | 68.73 | 8.6718 | 0.4628 | 0.7127 | 2.5030  |\\n|  256 | 1.16 | 68.92 | 8.6982 | 0.4629 | 0.7128 | 2.5012  |\\n|  512 | 1.16 | 68.78 | 8.6857 | 0.4637 | 0.7128 | 2.5025  |\\n|  1024 | 1.17 | 69.37 | 8.7495 | 0.4659 | 0.7149 | 2.5090  |\\n\\nTable 18: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with different rank  $r$  using GPT-2 Medium. Unlike on GPT-3 where  $r = 1$  suffices for many tasks, here the performance peaks at  $r = 16$  for validation loss and  $r = 4$  for BLEU, suggesting the GPT-2 Medium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our hyperparameters are tuned on  $r = 4$ , which matches the parameter count of another baseline, and thus might not be optimal for other choices of  $r$ .\\n\\n![img-8.jpeg](img-8.jpeg)\\nFigure 8: Normalized subspace similarity between the singular directions of  $W_{q}$  and those of  $\\\\Delta W_{q}$  with varying  $r$  and a random baseline.  $\\\\Delta W_{q}$  amplifies directions that are important but not emphasized in  $W$ .  $\\\\Delta W$  with a larger  $r$  tends to pick up more directions that are already emphasized in  $W$ .'), -0.04397878631399754), (Document(id='3c2293657e6c:0', metadata={'doc_id': '3c2293657e6c', 'text': '# XLNet: Generalized Autoregressive Pretraining for Language Understanding\\n\\nZhilin Yang^{∗1}, Zihang Dai^{∗12}, Yiming Yang^{1}, Jaime Carbonell^{1},\\nRuslan Salakhutdinov^{1}, Quoc V. Le^{2}\\n^{1}Carnegie Mellon University, ^{2}Google AI Brain Team\\n{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com [Equal contribution. Order determined by swapping the one in [9].^{1}Pretrained models and code are available at https://github.com/zihangdai/xlnet]\\n\\n###### Abstract\\n\\nWith the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking..\\n\\n## 1 Introduction\\n\\nUnsupervised representation learning has been highly successful in the domain of natural language processing *[7, 22, 27, 28, 10]*. Typically, these methods first pretrain neural networks on large-scale unlabeled text corpora, and then finetune the models or representations on downstream tasks. Under this shared high-level idea, different unsupervised pretraining objectives have been explored in literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives.\\n\\nAR language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model *[7, 27, 28]*. Specifically, given a text sequence $\\\\mathbf{x}=(x_{1},\\\\cdots,x_{T})$, AR language modeling factorizes the likelihood into a forward product $p(\\\\mathbf{x})=\\\\prod_{t=1}^{T}p(x_{t}\\\\mid\\\\mathbf{x}_{<t})$ or a backward one $p(\\\\mathbf{x})=\\\\prod_{t=T}^{1}p(x_{t}\\\\mid\\\\mathbf{x}_{>t})$. A parametric model (e.g. a neural network) is trained to model each conditional distribution. Since an AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information. This results in a gap between AR language modeling and effective pretraining.\\n\\nIn comparison, AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. A notable example is BERT *[10]*, which has been the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize\\n\\nbidirectional contexts for reconstruction. As an immediate benefit, this closes the aforementioned bidirectional information gap in AR language modeling, leading to improved performance. However, the artificial symbols like [MASK] used by BERT during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural language *[9]*.\\n\\nFaced with the pros and cons of existing language pretraining objectives, in this work, we propose XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations.\\n\\n- Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.\\n- Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to. Meanwhile, the autoregressive objective also provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT.\\n\\nIn addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.\\n\\n- Inspired by the latest advancements in AR language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL *[9]* into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.\\n- Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we propose to reparameterize the Transformer(-XL) network to remove the ambiguity.\\n\\nEmpirically, under comparable experiment setting, XLNet consistently outperforms BERT *[10]* on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task.\\n\\n#### Related Work\\n\\nThe idea of permutation-based AR modeling has been explored in *[32, 12]*, but there are several key differences. Firstly, previous models aim to improve density estimation by baking an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language models to learn bidirectional contexts. Technically, to construct a valid target-aware prediction distribution, XLNet incorporates the target position into the hidden state via two-stream attention while previous permutation-based AR models relied on implicit position awareness inherent to their MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that “orderless” does not mean that the input sequence can be randomly permuted but that the model allows for different factorization orders of the distribution.\\n\\nAnother related idea is to perform autoregressive denoising in the context of text generation *[11]*, which only considers a fixed order though.\\n\\n## 2 Proposed Method\\n\\n### 2.1 Background\\n\\nIn this section, we first review and compare the conventional AR language modeling and BERT for language pretraining. Given a text sequence $\\\\mathbf{x}=[x_{1},\\\\cdots,x_{T}]$, AR language modeling performs pretraining by maximizing the likelihood under the forward autoregressive factorization:\\n\\n$\\\\max_{\\\\theta}\\\\quad\\\\log p_{\\\\theta}(\\\\mathbf{x})=\\\\sum_{t=1}^{T}\\\\log p_{\\\\theta}(x_{t}\\\\mid\\\\mathbf{x}_{<t})=\\\\sum_{t=1}^{T}\\\\log\\\\frac{\\\\exp\\\\left(h_{\\\\theta}(\\\\mathbf{x}_{1:t-1})^{\\\\top}e(x_{t})\\\\right)}{\\\\sum_{x^{\\\\prime}}\\\\exp\\\\left(h_{\\\\theta}(\\\\mathbf{x}_{1:t-1})^{\\\\top}e(x^{\\\\prime})\\\\right)},$ (1)\\n\\nwhere\\n\\n$h_{\\\\theta}(\\\\mathbf{x}_{1:t-1})$ is a context representation produced by neural models, such as RNNs or Transformers, and $e(x)$ denotes the embedding of $x$. In comparison, BERT is based on denoising auto-encoding. Specifically, for a text sequence $\\\\mathbf{x}$, BERT first constructs a corrupted version $\\\\hat{\\\\mathbf{x}}$ by randomly setting a portion (e.g. 15%) of tokens in $\\\\mathbf{x}$ to a special symbol [MASK]. Let the masked tokens be $\\\\hat{\\\\mathbf{x}}$. The training objective is to reconstruct $\\\\hat{\\\\mathbf{x}}$ from $\\\\hat{\\\\mathbf{x}}$:\\n\\n$\\\\max_{\\\\theta}\\\\quad\\\\log p_{\\\\theta}(\\\\hat{\\\\mathbf{x}}\\\\mid\\\\hat{\\\\mathbf{x}})\\\\approx\\\\sum_{t=1}^{T}m_{t}\\\\log p_{\\\\theta}(x_{t}\\\\mid\\\\hat{\\\\mathbf{x}})=\\\\sum_{t=1}^{T}m_{t}\\\\log\\\\frac{\\\\exp\\\\left(H_{\\\\theta}(\\\\hat{\\\\mathbf{x}})_{t}^{\\\\top}e(x_{t})\\\\right)}{\\\\sum_{x^{\\\\prime}}\\\\exp\\\\left(H_{\\\\theta}(\\\\hat{\\\\mathbf{x}})_{t}^{\\\\top}e(x^{\\\\prime})\\\\right)},$ (2)\\n\\nwhere $m_{t}=1$ indicates $x_{t}$ is masked, and $H_{\\\\theta}$ is a Transformer that maps a length-$T$ text sequence $\\\\mathbf{x}$ into a sequence of hidden vectors $H_{\\\\theta}(\\\\mathbf{x})=[H_{\\\\theta}(\\\\mathbf{x})_{1},H_{\\\\theta}(\\\\mathbf{x})_{2},\\\\cdots,H_{\\\\theta}(\\\\mathbf{x})_{T}]$. The pros and cons of the two pretraining objectives are compared in the following aspects:\\n\\n- [leftmargin=*]\\n- Independence Assumption: As emphasized by the $\\\\approx$ sign in Eq. (2), BERT factorizes the joint conditional probability $p(\\\\bar{\\\\mathbf{x}}\\\\mid\\\\hat{\\\\mathbf{x}})$ based on an independence assumption that all masked tokens $\\\\hat{\\\\mathbf{x}}$ are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes $p_{\\\\theta}(\\\\mathbf{x})$ using the product rule that holds universally without such an independence assumption.\\n- Input noise: The input to BERT contains artificial symbols like [MASK] that never occur in downstream tasks, which creates a pretrain-finetune discrepancy. Replacing [MASK] with original tokens as in *[10]* does not solve the problem because original tokens can be only used with a small probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling does not rely on any input corruption and does not suffer from this issue.\\n- Context dependency: The AR representation $h_{\\\\theta}(\\\\mathbf{x}_{1:t-1})$ is only conditioned on the tokens up to position $t$ (i.e. tokens to the left), while the BERT representation $H_{\\\\theta}(\\\\mathbf{x})_{t}$ has access to the contextual information on both sides. As a result, the BERT objective allows the model to be pretrained to better capture bidirectional context.\\n\\n### 2.2 Objective: Permutation Language Modeling\\n\\nAccording to the comparison above, AR language modeling and BERT possess their unique advantages over the other. A natural question to ask is whether there exists a pretraining objective that brings the advantages of both while avoiding their weaknesses.\\n\\nBorrowing ideas from orderless NADE *[32]*, we propose the permutation language modeling objective that not only retains the benefits of AR models but also allows models to capture bidirectional contexts. Specifically, for a sequence $\\\\mathbf{x}$ of length $T$, there are $T!$ different orders to perform a valid autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides.\\n\\nTo formalize the idea, let $\\\\mathcal{Z}_{T}$ be the set of all possible permutations of the length-$T$ index sequence $[1,2,\\\\ldots,T]$. We use $z_{t}$ and $\\\\mathbf{z}_{<t}$ to denote the $t$-th element and the first $t-1$ elements of a permutation $\\\\mathbf{z}\\\\in\\\\mathcal{Z}_{T}$. Then, our proposed permutation language modeling objective can be expressed as follows:\\n\\n$\\\\max_{\\\\theta}\\\\quad\\\\mathbb{E}_{\\\\mathbf{z}\\\\sim\\\\mathcal{Z}_{T}}\\\\left[\\\\sum_{t=1}^{T}\\\\log p_{\\\\theta}(x_{z_{t}}\\\\mid\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})\\\\right].$ (3)\\n\\nEssentially, for a text sequence $\\\\mathbf{x}$, we sample a factorization order $\\\\mathbf{z}$ at a time and decompose the likelihood $p_{\\\\theta}(\\\\mathbf{x})$ according to factorization order. Since the same model parameter $\\\\theta$ is shared across all factorization orders during training, in expectation, $x_{t}$ has seen every possible element $x_{i}\\\\neq x_{t}$ in the sequence, hence being able to capture the bidirectional context. Moreover, as this objective fits into the AR framework, it naturally avoids the independence assumption and the pretrain-finetune discrepancy discussed in Section 2.1.\\n\\nRemark on Permutation The proposed objective only permutes the factorization order, not the sequence order. In other words, we keep the original sequence order, use the positional encodings corresponding to the original sequence, and rely on a proper attention mask in Transformers to achieve permutation of the factorization order. Note that this choice is necessary, since the model will only encounter text sequences with the natural order during finetuning.\\n\\nTo provide an overall picture, we show an example of predicting the token $x_{3}$ given the same input sequence $\\\\mathbf{x}$ but under different factorization orders in the Appendix A.7 with Figure 4.\\n\\n# 2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations\\n\\n![img-0.jpeg](img-0.jpeg)\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 1: (a): Content stream attention, which is the same as the standard self-attention. (b): Query stream attention, which does not have access information about the content $x_{z_t}$. (c): Overview of the permutation language modeling training with two-stream attention.\\n\\nWhile the permutation language modeling objective has desired properties, naive implementation with standard Transformer parameterization may not work. To see the problem, assume we parameterize the next-token distribution $p_{\\\\theta}(X_{z_t} \\\\mid \\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}})$ using the standard Softmax formulation, i.e., $p_{\\\\theta}(X_{z_t} = x \\\\mid \\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}}) = \\\\frac{\\\\exp(e(x)^{\\\\top}h_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}}))}{\\\\sum_{x\\'} \\\\exp(e(x\\')^{\\\\top}h_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}}))}$, where $h_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}})$ denotes the hidden representation of $\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}}$ produced by the shared Transformer network after proper masking. Now notice that the representation $h_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}})$ does not depend on which position it will predict, i.e., the value of $z_t$. Consequently, the same distribution is predicted regardless of the target position, which is not able to learn useful representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to re-parameterize the next-token distribution to be target position aware:\\n\\n$$\\np _ {\\\\theta} \\\\left(X _ {z _ {t}} = x \\\\mid \\\\mathbf {x} _ {z &lt; t}\\\\right) = \\\\frac {\\\\exp \\\\left(e \\\\left(x\\\\right) ^ {\\\\top} g _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {\\\\mathbf {z} &lt; t} , z _ {t}\\\\right)\\\\right)}{\\\\sum_ {x ^ {\\\\prime}} \\\\exp \\\\left(e \\\\left(x ^ {\\\\prime}\\\\right) ^ {\\\\top} g _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {\\\\mathbf {z} &lt; t} , z _ {t}\\\\right)\\\\right)}, \\\\tag {4}\\n$$\\n\\nwhere $g_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}},z_t)$ denotes a new type of representations which additionally take the target position $z_{t}$ as input.\\n\\n**Two-Stream Self-Attention** While the idea of target-aware representations removes the ambiguity in target prediction, how to formulate $g_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}},z_t)$ remains a non-trivial problem. Among other possibilities, we propose to \"stand\" at the target position $z_{t}$ and rely on the position $z_{t}$ to gather information from the context $\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}}$ through attention. For this parameterization to work, there are two requirements that are contradictory in a standard Transformer architecture: (1) to predict the token $x_{z_t}$, $g_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}},z_t)$ should only use the position $z_{t}$ and not the content $x_{z_t}$, otherwise the objective becomes trivial; (2) to predict the other tokens $x_{z_j}$ with $j &gt; t$, $g_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}},z_t)$ should also encode the content $x_{z_t}$ to provide full contextual information. To resolve such a contradiction, we propose to use two sets of hidden representations instead of one:\\n\\n- The content representation $h_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}})$, or abbreviated as $h_{z_t}$, which serves a similar role to the standard hidden states in Transformer. This representation encodes both the context and $x_{z_t}$ itself.\\n- The query representation $g_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z} &lt; t}, z_t)$, or abbreviated as $g_{z_t}$, which only has access to the contextual information $\\\\mathbf{x}_{\\\\mathbf{z} &lt; t}$ and the position $z_t$, but not the content $x_{z_t}$, as discussed above.\\n\\nComputationally, the first layer query stream is initialized with a trainable vector, i.e. $g_{i}^{(0)} = w$, while the content stream is set to the corresponding word embedding, i.e. $h_{i}^{(0)} = e(x_{i})$. For each self-attention layer $m = 1,\\\\dots ,M$, the two streams of representations are schematically updated\\n\\n2To avoid clutter, we omit the implementation details including multi-head attention, residual connection, layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in Appendix A.2 for reference.\\n\\nwith a shared set of parameters as follows (illustrated in Figures 1 (a) and (b)):\\n\\n$g^{(m)}_{\\\\dot{z}_{t}}$ $\\\\leftarrow\\\\text{Attention}(\\\\text{Q}=g^{(m-1)}_{\\\\dot{z}_{t}},\\\\text{KV}=\\\\mathbf{h}^{(m-1)}_{\\\\dot{\\\\mathbf{z}}_{<t}};\\\\theta),\\\\quad\\\\text{(query stream: use $z_{t}$ but cannot see $x_{z_{t}}$)}$\\n$h^{(m)}_{\\\\dot{z}_{t}}$ $\\\\leftarrow\\\\text{Attention}(\\\\text{Q}=h^{(m-1)}_{\\\\dot{z}_{t}},\\\\text{KV}=\\\\mathbf{h}^{(m-1)}_{\\\\dot{\\\\mathbf{z}}_{\\\\leq t}};\\\\theta),\\\\quad\\\\text{(content stream: use both $z_{t}$ and $x_{z_{t}}$)}.$\\n\\nwhere Q, K, V denote the query, key, and value in an attention operation *[33]*. The update rule of the content representations is exactly the same as the standard self-attention, so during finetuning, we can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally, we can use the last-layer query representation $g^{(M)}_{z_{t}}$ to compute Eq. (4).\\n\\nPartial Prediction While the permutation language modeling objective (3) has several benefits, it is a much more challenging optimization problem due to the permutation and causes slow convergence in preliminary experiments. To reduce the optimization difficulty, we choose to only predict the last tokens in a factorization order. Formally, we split $\\\\mathbf{z}$ into a non-target subsequence $\\\\mathbf{z}_{\\\\leq c}$ and a target subsequence $\\\\mathbf{z}_{>c}$, where $c$ is the cutting point. The objective is to maximize the log-likelihood of the target subsequence conditioned on the non-target subsequence, i.e.,\\n\\n$\\\\max_{\\\\theta}\\\\quad\\\\mathbb{E}_{\\\\mathbf{z}\\\\sim\\\\mathcal{Z}_{T}}\\\\Bigl{[}\\\\log p_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{>c}}\\\\mid\\\\mathbf{x}_{\\\\mathbf{z}_{\\\\leq c}})\\\\Bigr{]}=\\\\mathbb{E}_{\\\\mathbf{z}\\\\sim\\\\mathcal{Z}_{T}}\\\\Bigg{[}\\\\sum_{t=c+1}^{|\\\\mathbf{z}|}\\\\log p_{\\\\theta}(x_{z_{t}}\\\\mid\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})\\\\Bigg{]}.$ (5)\\n\\nNote that $\\\\mathbf{z}_{>c}$ is chosen as the target because it possesses the longest context in the sequence given the current factorization order $\\\\mathbf{z}$. A hyperparameter $K$ is used such that about $1/K$ tokens are selected for predictions; i.e., $|\\\\mathbf{z}|/(|\\\\mathbf{z}|-c)\\\\approx K$. For unselected tokens, their query representations need not be computed, which saves speed and memory.\\n\\n### 2.4 Incorporating Ideas from Transformer-XL\\n\\nSince our objective function fits in the AR framework, we incorporate the state-of-the-art AR language model, Transformer-XL *[9]*, into our pretraining framework, and name our method after it. We integrate two important techniques in Transformer-XL, namely the relative positional encoding scheme and the segment recurrence mechanism. We apply relative positional encodings based on the original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden states from previous segments. Without loss of generality, suppose we have two segments taken from a long sequence $\\\\mathbf{s}$; i.e., $\\\\tilde{\\\\mathbf{x}}=\\\\mathbf{s}_{1:T}$ and $\\\\mathbf{x}=\\\\mathbf{s}_{T+1:2T}$. Let $\\\\tilde{\\\\mathbf{z}}$ and $\\\\mathbf{z}$ be permutations of $[1\\\\cdots T]$ and $[T+1\\\\cdots 2T]$ respectively. Then, based on the permutation $\\\\tilde{\\\\mathbf{z}}$, we process the first segment, and then cache the obtained content representations $\\\\tilde{\\\\mathbf{h}}^{(m)}$ for each layer $m$. Then, for the next segment $\\\\mathbf{x}$, the attention update with memory can be written as\\n\\n$h^{(m)}_{z_{t}}\\\\leftarrow\\\\text{Attention}(\\\\text{Q}=h^{(m-1)}_{z_{t}},\\\\text{KV}=\\\\Bigl{[}\\\\tilde{\\\\mathbf{h}}^{(m-1)},\\\\mathbf{h}^{(m-1)}_{\\\\tilde{\\\\mathbf{z}}_{\\\\leq t}}\\\\Bigr{]};\\\\theta)$\\n\\nwhere $[.,.]$ denotes concatenation along the sequence dimension. Notice that positional encodings only depend on the actual positions in the original sequence. Thus, the above attention update is independent of $\\\\tilde{\\\\mathbf{z}}$ once the representations $\\\\tilde{\\\\mathbf{h}}^{(m)}$ are obtained. This allows caching and reusing the memory without knowing the factorization order of the previous segment. In expectation, the model learns to utilize the memory over all factorization orders of the last segment. The query stream can be computed in the same way. Finally, Figure 1 (c) presents an overview of the proposed permutation language modeling with two-stream attention (see Appendix A.7 for more detailed illustration).\\n\\n### 2.5 Modeling Multiple Segments\\n\\nMany downstream tasks have multiple input segments, e.g., a question and a context paragraph in question answering. We now discuss how we pretrain XLNet to model multiple segments in the autoregressive framework. During the pretraining phase, following BERT, we randomly sample two segments (either from the same context or not) and treat the concatenation of two segments as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context. Specifically, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP], where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although\\n\\nwe follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction *[10]* as it does not show consistent improvement in our ablation study (see Section 3.4).\\n\\n#### Relative Segment Encodings\\n\\nArchitecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, we extend the idea of relative encodings from Transformer-XL to also encode the segments. Given a pair of positions $i$ and $j$ in the sequence, if $i$ and $j$ are from the same segment, we use a segment encoding $\\\\mathbf{s}_{ij}=\\\\mathbf{s}_{+}$ or otherwise $\\\\mathbf{s}_{ij}=\\\\mathbf{s}_{-}$, where $\\\\mathbf{s}_{+}$ and $\\\\mathbf{s}_{-}$ are learnable model parameters for each attention head. In other words, we only consider whether the two positions are *within the same segment*, as opposed to considering *which specific segments they are from*. This is consistent with the core idea of relative encodings; i.e., only modeling the relationships between positions. When $i$ attends to $j$, the segment encoding $\\\\mathbf{s}_{ij}$ is used to compute an attention weight $a_{ij}=(\\\\mathbf{q}_{i}+\\\\mathbf{b})^{\\\\top}\\\\mathbf{s}_{ij}$, where $\\\\mathbf{q}_{i}$ is the query vector as in a standard attention operation and $\\\\mathbf{b}$ is a learnable head-specific bias vector. Finally, the value $a_{ij}$ is added to the normal attention weight. There are two benefits of using relative segment encodings. First, the inductive bias of relative encodings improves generalization *[9]*. Second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings.\\n\\n### 2.6 Discussion\\n\\nComparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e., only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT and XLNet, partial prediction plays a role of reducing optimization difficulty by only predicting tokens with sufficient context. However, the independence assumption discussed in Section 2.1 disables BERT to model dependency between targets.\\n\\nTo better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize $\\\\log p(\\\\text{New York | is a city})$. Also suppose that XLNet samples the factorization order [is, a, city, New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:\\n\\n$\\\\mathcal{J}_{\\\\text{BERT}}=\\\\log p(\\\\text{New | is a city})+\\\\log p(\\\\text{York | is a city}),$\\n$\\\\mathcal{J}_{\\\\text{XLNet}}=\\\\log p(\\\\text{New | is a city})+\\\\log p(\\\\text{York | New},\\\\text{is a city}).$\\n\\nNotice that XLNet is able to capture the dependency between the pair (New, York), which is omitted by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and (York, city), it is obvious that XLNet always learns more dependency pairs given the same target and contains “denser” effective training signals.\\n\\nFor more formal analysis and further discussion, please refer to Appendix A.5.\\n\\n## 3 Experiments\\n\\n### 3.1 Pretraining and Implementation\\n\\nFollowing BERT *[10]*, we use the BooksCorpus *[40]* and English Wikipedia as part of our pretraining data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) *[26]*, ClueWeb 2012-B (extended from *[5]*), and Common Crawl *[6]* for pretraining. We use heuristics to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl, which results in 19GB and 110GB text respectively. After tokenization with SentencePiece *[17]*, we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total.\\n\\nOur largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\\n\\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks.\\n\\nSince the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant  $K$  as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified $^3$ . We employ an idea of span-based prediction, where we first sample a length  $L \\\\in [1, \\\\dots, 5]$ , and then randomly select a consecutive span of  $L$  tokens as prediction targets within a context of  $(KL)$  tokens.\\n\\nWe use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\\n\\n# 3.2 Fair Comparison with BERT\\n\\n|  Model | SQuAD1.1 | SQuAD2.0 | RACE | MNLI | QNLI | QQP | RTE | SST-2 | MRPC | CoLA | STS-B  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  BERT-Large (Best of 3) | 86.7/92.8 | 82.8/85.5 | 75.1 | 87.3 | 93.0 | 91.4 | 74.0 | 94.0 | 88.7 | 63.7 | 90.2  |\\n|  XLNet-Large-wikibooks | 88.2/94.0 | 85.1/87.8 | 77.4 | 88.4 | 93.9 | 91.8 | 81.2 | 94.4 | 90.0 | 65.2 | 91.1  |\\n\\nTable 1: Fair comparison with BERT. All models are trained using the same data and hyperparameters as in BERT. We use the best of 3 BERT variants for comparison; i.e., the original BERT, BERT with whole word masking, and BERT without next sentence prediction.\\n\\nHere, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet. In Table 1, we compare (1) best performance of three different variants of BERT and (2) XLNet trained with the same data and hyperparameters. As we can see, trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the considered datasets.\\n\\n# 3.3 Comparison with RoBERTa: Scaling Up\\n\\n|  RACE | Accuracy | Middle | High | Model | NDCG@20 | ERR@20  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  GPT [28] | 59.0 | 62.9 | 57.4 | DRMM [13] | 24.3 | 13.8  |\\n|  BERT [25] | 72.0 | 76.6 | 70.1 | KNRM [8] | 26.9 | 14.9  |\\n|  BERT+DCMN* [38] | 74.1 | 79.5 | 71.8 | Conv [8] | 28.7 | 18.1  |\\n|  RoBERTa [21] | 83.2 | 86.5 | 81.8 | BERT† | 30.53 | 18.67  |\\n|  XLNet | 85.4 | 88.6 | 84.0 | XLNet | 31.10 | 20.28  |\\n\\nTable 2: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task, and on ClueWeb09-B, a document ranking task. * indicates using ensembles. † indicates our implementations. “Middle” and “High” in RACE are two subsets representing middle and high school difficulty levels. All BERT, RoBERTa, and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).\\n\\nAfter the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1.\\n\\nThe results are presented in Tables 2 (reading comprehension &amp; document ranking), 3 (question answering), 4 (text classification) and 5 (natural language understanding), where XLNet generally outperforms BERT and RoBERTa. In addition, we make two more interesting observations:\\n\\n|  SQuAD2.0 | EM | F1 | SQuAD1.1 | EM | F1  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Dev set results (single model)  |   |   |   |   |   |\\n|  BERT [10] | 78.98 | 81.77 | BERT† [10] | 84.1 | 90.9  |\\n|  RoBERTa [21] | 86.5 | 89.4 | RoBERTa [21] | 88.9 | 94.6  |\\n|  XLNet | 87.9 | 90.6 | XLNet | 89.7 | 95.1  |\\n|  Test set results on leaderboard (single model, as of Dec 14, 2019)  |   |   |   |   |   |\\n|  BERT [10] | 80.005 | 83.061 | BERT [10] | 85.083 | 91.835  |\\n|  RoBERTa [21] | 86.820 | 89.795 | BERT* [10] | 87.433 | 93.294  |\\n|  XLNet | 87.926 | 90.689 | XLNet | 89.898‡ | 95.080‡  |\\n\\nTable 3: Results on SQuAD, a reading comprehension dataset. † marks our runs with the official code. * indicates ensembles. ‡: We are not able to obtain the test results of our latest model on SQuAD1.1 from the organizers after submitting our result for more than one month, and thus report the results of an older version for the SQuAD1.1 test set.\\n\\n|  Model | IMDB | Yelp-2 | Yelp-5 | DBpedia | AG | Amazon-2 | Amazon-5  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  CNN [15] | - | 2.90 | 32.39 | 0.84 | 6.57 | 3.79 | 36.24  |\\n|  DPCNN [15] | - | 2.64 | 30.58 | 0.88 | 6.87 | 3.32 | 34.81  |\\n|  Mixed VAT [31, 23] | 4.32 | - | - | 0.70 | 4.95 | - | -  |\\n|  ULMFiT [14] | 4.6 | 2.16 | 29.98 | 0.80 | 5.01 | - | -  |\\n|  BERT [35] | 4.51 | 1.89 | 29.32 | 0.64 | - | 2.63 | 34.17  |\\n|  XLNet | 3.20 | 1.37 | 27.05 | 0.60 | 4.45 | 2.11 | 31.67  |\\n\\nTable 4: Comparison with state-of-the-art error rates on the test sets of several text classification datasets. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).\\n\\n|  Model | MNLI | QNLI | QQP | RTE | SST-2 | MRPC | CoLA | STS-B | WNLI  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Single-task single models on dev  |   |   |   |   |   |   |   |   |   |\\n|  BERT [2] | 86.6/- | 92.3 | 91.3 | 70.4 | 93.2 | 88.0 | 60.6 | 90.0 | -  |\\n|  RoBERTa [21] | 90.2/90.2 | 94.7 | 92.2 | 86.6 | 96.4 | 90.9 | 68.0 | 92.4 | -  |\\n|  XLNet | 90.8/90.8 | 94.9 | 92.3 | 85.9 | 97.0 | 90.8 | 69.0 | 92.5 | -  |\\n|  Multi-task ensembles on test (from leaderboard as of Oct 28, 2019)  |   |   |   |   |   |   |   |   |   |\\n|  MT-DNN* [20] | 87.9/87.4 | 96.0 | 89.9 | 86.3 | 96.5 | 92.7 | 68.4 | 91.1 | 89.0  |\\n|  RoBERTa* [21] | 90.8/90.2 | 98.9 | 90.2 | 88.2 | 96.7 | 92.3 | 67.8 | 92.2 | 89.0  |\\n|  XLNet* | 90.9/90.9† | 99.0† | 90.4† | 88.5 | 97.1† | 92.9 | 70.2 | 93.0 | 92.5  |\\n\\nTable 5: Results on GLUE. * indicates using ensembles, and † denotes single-task results in a multi-task row. All dev results are the median of 10 runs. The upper section shows direct comparison on dev data and the lower section shows comparison with state-of-the-art results on the public leaderboard.\\n\\n- For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is usually larger. This superiority at dealing with longer context could come from the Transformer-XL backbone in XLNet.\\n- For classification tasks that already have abundant supervised examples such as MNLI ( $&gt;390\\\\mathrm{K}$ ), Yelp ( $&gt;560\\\\mathrm{K}$ ) and Amazon ( $&gt;3\\\\mathrm{M}$ ), XLNet still lead to substantial gains.\\n\\n# 3.4 Ablation Study\\n\\nWe perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study:\\n\\n- The effectiveness of the permutation language modeling objective alone, especially compared to the denoising auto-encoding objective used by BERT.\\n- The importance of using Transformer-XL as the backbone neural architecture.\\n- The necessity of some implementation details including span-based prediction, the bidirectional input pipeline, and next-sentence prediction.\\n\\nWith these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implementation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidirectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the BooksCorpus. All results reported are the median of 5 runs.\\n\\n|  # | Model | RACE | SQuAD2.0 |   | MNLI m/mm | SST-2  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|   |   |   |  F1 | EM  |   |   |\\n|  1 | BERT-Base | 64.3 | 76.30 | 73.66 | 84.34/84.65 | 92.78  |\\n|  2 | DAE + Transformer-XL | 65.03 | 79.56 | 76.80 | 84.88/84.45 | 92.60  |\\n|  3 | XLNet-Base (K=7) | 66.05 | 81.33 | 78.46 | 85.84/85.43 | 92.66  |\\n|  4 | XLNet-Base (K=6) | 66.66 | 80.98 | 78.18 | 85.63/85.12 | 93.35  |\\n|  5 | - memory | 65.55 | 80.15 | 77.27 | 85.32/85.05 | 92.78  |\\n|  6 | - span-based pred | 65.95 | 80.61 | 77.91 | 85.49/85.02 | 93.12  |\\n|  7 | - bidirectional data | 66.34 | 80.65 | 77.87 | 85.31/84.99 | 92.66  |\\n|  8 | + next-sent pred | 66.76 | 79.83 | 76.94 | 85.32/85.09 | 92.89  |\\n\\nTable 6: The results of BERT on RACE are taken from [38]. We run BERT on the other datasets using the official implementation and the same hyperparameter search space as XLNet.  $K$  is a hyperparameter to control the optimization difficulty (see Section 2.3).\\n\\nExamining rows 1 - 4 of Table 6, we can see both Transformer-XL and the permutation LM clearly contribute the superior performance of XLNet over BERT. Moreover, if we remove the memory caching mechanism (row 5), the performance clearly drops, especially for RACE which involves the longest context among the 4 tasks. In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play important roles in XLNet. Finally, we unexpectedly find the next-sentence prediction objective proposed in the original BERT does not necessarily lead to an improvement in our setting. Hence, we exclude the next-sentence prediction objective from XLNet.\\n\\nFinally, we also perform a qualitative study of the attention patterns, which is included in Appendix A.6 due to page limit.\\n\\n# 4 Conclusions\\n\\nXLNet is a generalized AR pretraining method that uses a permutation language modeling objective to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to work seamlessly with the AR objective, including integrating Transformer-XL and the careful design of the two-stream attention mechanism. XLNet achieves substantial improvement over previous pretraining objectives on various tasks.\\n\\n# Acknowledgments\\n\\nThe authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the project, Jamie Callan for providing the ClueWeb dataset, Youlong Cheng, Yanping Huang and Shibo Wang for providing ideas to improve our TPU implementation, Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and RS were supported by the Office of Naval Research grant N000141812861, the National Science Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY were supported in part by NSF under the grant IIS-1546329 and by the DOE-Office of Science under the grant ASCR #KJ040201.\\n\\n# References\\n\\n[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.\\n[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anonymous preprint under review, 2018.\\n[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018.\\n\\n[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer neural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.\\n- [5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.\\n- [6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org, 2019.\\n- [7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079–3087, 2015.\\n- [8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international conference on web search and data mining, pages 126–134. ACM, 2018.\\n- [9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\\n- [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n- [11] William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: better text generation via filling in the_. arXiv preprint arXiv:1801.07736, 2018.\\n- [12] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, pages 881–889, 2015.\\n- [13] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, pages 55–64. ACM, 2016.\\n- [14] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.\\n- [15] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text categorization. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 562–570, 2017.\\n- [16] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint arXiv:1905.06290, 2019.\\n- [17] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\\n- [18] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\\n- [19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\\n- [20] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.\\n- [21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n- [22] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pages 6294–6305, 2017.\\n- [23] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised text classification. arXiv preprint arXiv:1605.07725, 2016.\\n- [24] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.\\n- [25] Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with external knowledge. arXiv preprint arXiv:1902.00993, 2019.\\n\\n[26] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword fifth edition, linguistic data consortium. Technical report, Technical Report. Linguistic Data Consortium, Philadelphia, Tech. Rep., 2011.\\n- [27] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\\n- [28] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.\\n- [29] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.\\n- [30] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\\n- [31] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks for semi-supervised text classification via mixed objective function. 2018.\\n- [32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–7220, 2016.\\n- [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.\\n- [34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR.\\n- [35] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848, 2019.\\n- [36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval, pages 55–64. ACM, 2017.\\n- [37] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.\\n- [38] Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. Dual co-matching network for multi-choice reading comprehension. arXiv preprint arXiv:1901.09381, 2019.\\n- [39] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649–657, 2015.\\n- [40] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015.\\n\\nA Target-Aware Representation via Two-Stream Self-Attention\\n\\n### A.1 A Concrete Example of How Standard LM Parameterization Fails\\n\\nIn this section, we provide a concrete example to show how the standard language model parameterization fails under the permutation objective, as discussed in Section 2.3. Specifically, let’s consider two different permutations $\\\\mathbf{z}^{(1)}$ and $\\\\mathbf{z}^{(2)}$ satisfying the following relationship\\n\\n$\\\\mathbf{z}_{<t}^{(1)}=\\\\mathbf{z}_{<t}^{(2)}=\\\\mathbf{z}_{<t}\\\\quad\\\\text{but}\\\\quad z_{t}^{(1)}=i\\\\neq j=z_{t}^{(2)}.$\\n\\nThen, substituting the two permutations respectively into the naive parameterization, we have\\n\\n$\\\\underbrace{p_{\\\\theta}(X_{i}=x\\\\mid\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})}_{z_{t}^{(1)}=i,\\\\,\\\\mathbf{z}_{<t}^{(1)}=\\\\mathbf{z}_{<t}}=\\\\underbrace{p_{\\\\theta}(X_{j}=x\\\\mid\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})}_{z_{t}^{(1)}=j,\\\\,\\\\mathbf{z}_{<t}^{(2)}=\\\\mathbf{z}_{<t}}=\\\\frac{\\\\exp\\\\left(e(x)^{\\\\top}h(\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})\\\\right)}{\\\\sum_{x^{\\\\prime}}\\\\exp\\\\left(e(x^{\\\\prime})^{\\\\top}h(\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})\\\\right)}.$\\n\\nEffectively, two different target positions $i$ and $j$ share exactly the same model prediction. However, the ground-truth distribution of two positions should certainly be different.\\n\\n### A.2 Two-Stream Attention\\n\\nHere, we provide the implementation details of the two-stream attention with a Transformer-XL backbone.\\n\\nInitial represetation:\\n\\n$\\\\forall t=1,\\\\ldots,T:\\\\quad h_{t}=e(x_{t})\\\\quad\\\\text{and}\\\\quad g_{t}=w$\\n\\nCached layer-$m$ content represetation (memory) from previous segment: $\\\\tilde{\\\\mathbf{h}}^{(m)}$\\n\\nFor the Transformer-XL layer $m=1,\\\\cdots,M$, attention with relative positional encoding and position-wise feed-forward are consecutively employed to update the represetntations:\\n\\n$\\\\forall t=1,\\\\ldots,T:\\\\quad$ $\\\\hat{h}_{z_{t}}^{(m)}=\\\\text{LayerNorm}\\\\Big{(}h_{z_{t}}^{(m-1)}+\\\\text{RelAttn}\\\\Big{(}h_{z_{t}}^{(m-1)},\\\\Big{[}\\\\tilde{\\\\mathbf{h}}^{(m-1)},\\\\mathbf{h}_{\\\\mathbf{z}_{\\\\leq t}}^{(m-1)}\\\\Big{]}\\\\Big{)}\\\\Big{)}$\\n$h_{z_{t}}^{(m)}=\\\\text{LayerNorm}\\\\Big{(}\\\\hat{h}_{z_{t}}^{(m)}+\\\\text{PosFF}\\\\Big{(}\\\\hat{h}_{z_{t}}^{(m)}\\\\Big{)}\\\\Big{)}$\\n$\\\\hat{g}_{z_{t}}^{(m)}=\\\\text{LayerNorm}\\\\Big{(}g_{z_{t}}^{(m-1)}+\\\\text{RelAttn}\\\\Big{(}g_{z_{t}}^{(m-1)},\\\\Big{[}\\\\tilde{\\\\mathbf{h}}^{(m-1)},\\\\mathbf{h}_{\\\\mathbf{z}_{<t}}^{(m-1)}\\\\Big{]}\\\\Big{)}\\\\Big{)}$\\n$g_{z_{t}}^{(m)}=\\\\text{LayerNorm}\\\\Big{(}\\\\hat{g}_{z_{t}}^{(m)}+\\\\text{PosFF}\\\\Big{(}\\\\hat{g}_{z_{t}}^{(m)}\\\\Big{)}\\\\Big{)}$\\n\\nTarget-aware prediction distribution:\\n\\n$p_{\\\\theta}(X_{z_{t}}=x\\\\mid\\\\mathbf{x}_{z_{<t}})=\\\\frac{\\\\exp\\\\left(e(x)^{\\\\top}g_{z_{t}}^{(M)}\\\\right)}{\\\\sum_{x^{\\\\prime}}\\\\exp\\\\left(e(x^{\\\\prime})^{\\\\top}g_{z_{t}}^{(M)}\\\\right)},$\\n\\n### A.3 Datasets\\n\\n#### A.3.1 RACE Dataset\\n\\nThe RACE dataset *[18]* contains near 100K questions taken from the English exams for middle and high school Chinese students in the age range between 12 to 18, with the answers generated by human experts. This is one of the most difficult reading comprehension datasets that involve challenging reasoning questions. Moreover, the average length of the passages in RACE are longer than 300, which is significantly longer than other popular reading comprehension datasets such as SQuAD *[29]*. As a result, this dataset serves as a challenging benchmark for long text understanding. We use a sequence length of 512 during finetuning.\\n\\n#### A.3.2 SQuAD\\n\\nSQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 *[30]* contains questions that always have a corresponding answer in the given passages, while SQuAD2.0 *[29]* introduces unanswerable questions. To finetune an XLNet on SQuAD2.0, we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering *[10]*.\\n\\n####\\n\\n# A.3.3 Text classification Datasets\\n\\nFollowing previous work on text classification [39, 23], we evaluate XLNet on the following benchmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5.\\n\\n# A.3.4 GLUE Dataset\\n\\nThe GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and finetune the network on the other datasets. Only single-task training is employed for the four large datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm. For WNLI, we use the loss described in [16].\\n\\n# A.3.5 ClueWeb09-B Dataset\\n\\nFollowing the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning, and employ a kernel pooling network [36] to rank the documents.\\n\\n# A.4 Hyperparameters\\n\\n# A.4.1 Pretraining Hyperparameters\\n\\n|  Hparam | Value  |\\n| --- | --- |\\n|  Number of layers | 24  |\\n|  Hidden size | 1024  |\\n|  Number of attention heads | 16  |\\n|  Attention head size | 64  |\\n|  FFN inner hidden size | 4096  |\\n|  Hidden Dropout | 0.1  |\\n|  GeLU Dropout | 0.0  |\\n|  Attention dropout | 0.1  |\\n|  Partial prediction K | 6  |\\n|  Max sequence length | 512  |\\n|  Batch size | 8192  |\\n|  Learning rate | 4e-4  |\\n|  Number of steps | 500K  |\\n|  Warmup steps | 40,000  |\\n|  Learning rate decay | linear  |\\n|  Adam epsilon | 1e-6  |\\n|  Weight decay | 0.01  |\\n\\nTable 7: Hyperparameters for pretraining.\\n\\nThe hyperparameters used for pretraining XLNet are shown in Table 7.\\n\\n# A.4.2 Hyperparameters for Finetuning\\n\\nThe hyperparameters used for finetuning XLNet on various tasks are shown in Table 8. \"Layer-wise decay\" means exponentially decaying the learning rates of individual layers in a top-down manner. For example, suppose the 24-th layer uses a learning rate  $l$ , and the Layer-wise decay rate is  $\\\\alpha$ , then the learning rate of layer  $m$  is  $l\\\\alpha^{24 - m}$ .\\n\\n|  Hparam | RACE | SQuAD | MNLI | Yelp-5  |\\n| --- | --- | --- | --- | --- |\\n|  Dropout |  | 0.1 |  |   |\\n|  Attention dropout |  | 0.1 |  |   |\\n|  Max sequence length | 512 | 512 | 128 | 512  |\\n|  Batch size | 32 | 48 | 128 | 128  |\\n|  Learning rate | 2e-5 | 3e-5 | 2e-5 | 1e-5  |\\n|  Number of steps | 12K | 8K | 10K | 10K  |\\n|  Learning rate decay |  | linear |  |   |\\n|  Weight decay |  | 0.01 |  |   |\\n|  Adam epsilon | 1e-6 | 1e-6 | 1e-6 | 1e-6  |\\n|  Layer-wise lr decay | 1.0 | 0.75 | 1.0 | 1.0  |\\n\\nTable 8: Hyperparameters for finetuning.\\n\\n# A.5 Discussion and Analysis\\n\\n# A.5.1 Comparison with BERT\\n\\nTo prove a general point beyond one example, we now turn to more formal expressions. Inspired by previous work [37], given a sequence  $\\\\mathbf{x} = [x_1,\\\\dots ,x_T]$ , we define a set of target-context pairs of interest,  $\\\\mathcal{I} = \\\\{(x,\\\\mathcal{U})\\\\}$ , where  $\\\\mathcal{U}$  is a set of tokens in  $\\\\mathbf{x}$  that form a context of  $x$ . Intuitively, we want the model to learn the dependency of  $x$  on  $\\\\mathcal{U}$  through a pretraining loss term  $\\\\log p(x\\\\mid \\\\mathcal{U})$ . For example, given the above sentence, the pairs of interest  $\\\\mathcal{I}$  could be instantiated as:\\n\\n$$\\n\\\\mathcal {I} = \\\\left\\\\{\\\\left(x = \\\\operatorname {Y o r k}, \\\\mathcal {U} = \\\\{\\\\text {N e w} \\\\}\\\\right), \\\\left(x = \\\\operatorname {Y o r k}, \\\\mathcal {U} = \\\\{\\\\text {c i t y} \\\\}\\\\right), \\\\left(x = \\\\operatorname {Y o r k}, \\\\mathcal {U} = \\\\{\\\\text {N e w}, \\\\text {c i t y} \\\\}\\\\right), \\\\dots \\\\right\\\\}.\\n$$\\n\\nNote that  $\\\\mathcal{I}$  is merely a virtual notion without unique ground truth, and our analysis will hold regardless of how  $\\\\mathcal{I}$  is instantiated.\\n\\nGiven a set of target tokens  $\\\\mathcal{T}$  and a set of non-target tokens  $\\\\mathcal{N} = \\\\mathbf{x} \\\\backslash \\\\mathcal{T}$ , BERT and XLNet both maximize  $\\\\log p(\\\\mathcal{T} \\\\mid \\\\mathcal{N})$  but with different formulations:\\n\\n$$\\n\\\\mathcal {J} _ {\\\\mathrm {B E R T}} = \\\\sum_ {x \\\\in \\\\mathcal {T}} \\\\log p (x \\\\mid \\\\mathcal {N}); \\\\quad \\\\mathcal {J} _ {\\\\mathrm {X L N e t}} = \\\\sum_ {x \\\\in \\\\mathcal {T}} \\\\log p (x \\\\mid \\\\mathcal {N} \\\\cup \\\\mathcal {T} _ {&lt;   x})\\n$$\\n\\nwhere  $\\\\mathcal{T}_{&lt; x}$  denote tokens in  $\\\\mathcal{T}$  that have a factorization order prior to  $x$ . Both objectives consist of multiple loss terms in the form of  $\\\\log p(x \\\\mid \\\\mathcal{V}_x)$ . Intuitively, if there exists a target-context pair  $(x, \\\\mathcal{U}) \\\\in \\\\mathcal{I}$  such that  $\\\\mathcal{U} \\\\subseteq \\\\mathcal{V}_x$ , then the loss term  $\\\\log p(x \\\\mid \\\\mathcal{V}_x)$  provides a training signal to the dependency between  $x$  and  $\\\\mathcal{U}$ . For convenience, we say a target-context pair  $(x, \\\\mathcal{U}) \\\\in \\\\mathcal{I}$  is covered by a model (objective) if  $\\\\mathcal{U} \\\\subseteq \\\\mathcal{V}_x$ .\\n\\nGiven the definition, let\\'s consider two cases:\\n\\n- If  $\\\\mathcal{U} \\\\subseteq \\\\mathcal{N}$ , the dependency  $(x, \\\\mathcal{U})$  is covered by both BERT and XLNet.\\n- If  $\\\\mathcal{U} \\\\subseteq \\\\mathcal{N} \\\\cup \\\\mathcal{T}_{&lt;x}$  and  $\\\\mathcal{U} \\\\cap \\\\mathcal{T}_{&lt;x} \\\\neq \\\\emptyset$ , the dependency can only be covered by XLNet but not BERT. As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet objective contains more effective training signals, which empirically leads to better performance in Section 3.\\n\\n# A.5.2 Comparison with Language Modeling\\n\\nBorrowing examples and notations from Section A.5.1, a standard AR language model like GPT [28] is only able to cover the dependency  $(x = \\\\mathrm{York},\\\\mathcal{U} = \\\\{\\\\mathrm{New}\\\\})$  but not  $(x = \\\\mathrm{New},\\\\mathcal{U} = \\\\{\\\\mathrm{York}\\\\})$ . XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a limitation of AR language modeling can be critical in real-world applications. For example, consider a span extraction question answering task with the context \"Thom Yorke is the singer of Radiohead\" and the question \"Who is the singer of Radiohead\". The representations of \"Thom Yorke\" are not dependent on \"Radiohead\" with AR language modeling and thus they will not be chosen as the answer by the standard approach that employs softmax over all token representations. More formally, consider a context-target pair  $(x,\\\\mathcal{U})$ :\\n\\n- If  $\\\\mathcal{U} \\\\nsubseteq \\\\mathcal{T}_{&lt;x}$ , where  $\\\\mathcal{T}_{&lt;x}$  denotes the tokens prior to  $x$  in the original sequence, AR language modeling is not able to cover the dependency.\\n\\n- In comparison, XLNet is able to cover all dependencies in expectation.\\n\\nApproaches like ELMo [27] concatenate forward and backward language models in a shallow manner, which is not sufficient for modeling deep interactions between the two directions.\\n\\n# A.5.3 Bridging the Gap Between Language Modeling and Pretraining\\n\\nWith a deep root in density estimation $^{4}$  [4, 32, 24], language modeling has been a rapidly-developing research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining due to the lack of the capability of bidirectional context modeling, as analyzed in Section A.5.2. It has even been challenged by some machine learning practitioners whether language modeling is a meaningful pursuit if it does not directly improve downstream tasks $^{5}$ . XLNet generalizes language modeling and bridges such a gap. As a result, it further \"justifies\" language modeling research. Moreover, it becomes possible to leverage the rapid progress of language modeling research for pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness of the latest language modeling progress.\\n\\n# A.6 Qualitative Analysis of Attention Patterns\\n\\nWe compare the attention pattern of BERT and XLNet without finetuning. Firstly, we found 4 typical patterns shared by both, as shown in Fig. 2.\\n\\n![img-2.jpeg](img-2.jpeg)\\n(a) Content stripes\\n\\n![img-3.jpeg](img-3.jpeg)\\n(b) Local/Self focus\\n\\n![img-4.jpeg](img-4.jpeg)\\n(c) Two segments\\nFigure 2: Attention patterns shared by XLNet and BERT. Rows and columns represent query and key respectively.\\n\\n![img-5.jpeg](img-5.jpeg)\\n(d) Content-based symmetry\\n\\nMore interestingly, in Fig. 3, we present 3 patterns that only appear in XLNet but not BERT: (a) The self-exclusion pattern attends to all other tokens but itself, probably offering a fast way to gather global information; (b) The relative-stride pattern attends to positions every a few stride apart relative to the query position; (c) The one-side masked pattern is very similar to the lower-left part of Fig. 1-(d), with the upper-right triangle masked out. It seems that the model learns not to attend the relative right half. Note that all these three unique patterns involve the relative positions rather than absolute ones, and hence are likely enabled by the \"relative attention\" mechanism in XLNet. We conjecture these unique patterns contribute to the performance advantage of XLNet. On the other hand, the proposed permutation LM objective mostly contributes to a better data efficiency, whose effects may not be obvious from qualitative visualization.\\n\\n![img-6.jpeg](img-6.jpeg)\\n(a) Self exclusion\\n\\n![img-7.jpeg](img-7.jpeg)\\n(b) Relative stride\\nFigure 3: Attention patterns that appear only in XLNet. Rows and columns represent query and key respectively.\\n\\n![img-8.jpeg](img-8.jpeg)\\n(c) One-side masked\\n\\n![img-9.jpeg](img-9.jpeg)\\nFactorization order:  $3 \\\\rightarrow 2 \\\\rightarrow 4 \\\\rightarrow 1$\\n\\n![img-10.jpeg](img-10.jpeg)\\nFactorization order:  $2 \\\\rightarrow 4 \\\\rightarrow 3 \\\\rightarrow 1$\\n\\n![img-11.jpeg](img-11.jpeg)\\nFactorization order:  $1 \\\\rightarrow 4 \\\\rightarrow 2 \\\\rightarrow 3$\\n\\n![img-12.jpeg](img-12.jpeg)\\nFactorization order:  $4 \\\\rightarrow 3 \\\\rightarrow 1 \\\\rightarrow 2$\\nFigure 4: Illustration of the permutation language modeling objective for predicting  $x_{3}$  given the same input sequence  $\\\\mathbf{x}$  but with different factorization orders.\\n\\n# A.7 Visualizing Memory and Permutation\\n\\nIn this section, we provide a detailed visualization of the proposed permutation language modeling objective, including the mechanism of reusing memory (aka the recurrence mechanism), how we use attention masks to permute the factorization order, and the difference of the two attention streams.\\n\\nAs shown in Figure 5 and 6, given the current position  $z_{t}$ , the attention mask is decided by the permutation (or factorization order)  $\\\\mathbf{z}$  such that only tokens the occur before  $z_{t}$  in the permutation can be attended; i.e., positions  $z_{i}$  with  $i &lt; t$ . Moreover, comparing Figure 5 and 6, we can see how the query stream and the content stream work differently with a specific permutation through attention masks. The main difference is that the query stream cannot do self-attention and does not have access to the token at the position, while the content stream performs normal self-attention.\\n\\n![img-13.jpeg](img-13.jpeg)\\nJoint View of the Content Stream (Factorization order:  $3 \\\\rightarrow 2 \\\\rightarrow 4 \\\\rightarrow 1$ )\\n\\n![img-14.jpeg](img-14.jpeg)\\n\\n![img-15.jpeg](img-15.jpeg)\\nPosition-3 View\\n\\n![img-16.jpeg](img-16.jpeg)\\nPosition-2 View\\n\\nFigure 5: A detailed illustration of the content stream of the proposed objective with both the joint view and split views based on a length-4 sequence under the factorization order [3, 2, 4, 1].\\n![img-17.jpeg](img-17.jpeg)\\nPosition-4 View\\nSplit View of the Content Stream (Factorization order:  $3 \\\\rightarrow 2 \\\\rightarrow 4 \\\\rightarrow 1$ )\\n\\n![img-18.jpeg](img-18.jpeg)\\nPosition-1 View\\n\\nNote that if we ignore the query representation, the computation in this figure is simply the standard self-attention, though with a particular attention mask.\\n\\n![img-19.jpeg](img-19.jpeg)\\nJoint View of the Query Stream (Factorization order:  $3 \\\\rightarrow 2 \\\\rightarrow 4 \\\\rightarrow 1$ )\\n\\n![img-20.jpeg](img-20.jpeg)\\n\\n![img-21.jpeg](img-21.jpeg)\\nPosition-3 View\\n\\n![img-22.jpeg](img-22.jpeg)\\nPosition-2 View\\n\\n![img-23.jpeg](img-23.jpeg)\\nPosition-4 View\\nSplit View of the Query Stream (Factorization order:  $3 \\\\rightarrow 2 \\\\rightarrow 4 \\\\rightarrow 1$ )\\nFigure 6: A detailed illustration of the query stream of the proposed objective with both the joint view and split views based on a length-4 sequence under the factorization order [3, 2, 4, 1]. The dash arrows indicate that the query stream cannot access the token (content) at the same position, but only the location information.\\n\\n![img-24.jpeg](img-24.jpeg)\\nPosition-1 View', 'chunk_index': 0, 'end_line': 538, 'start_line': 1}, page_content='# XLNet: Generalized Autoregressive Pretraining for Language Understanding\\n\\nZhilin Yang^{∗1}, Zihang Dai^{∗12}, Yiming Yang^{1}, Jaime Carbonell^{1},\\nRuslan Salakhutdinov^{1}, Quoc V. Le^{2}\\n^{1}Carnegie Mellon University, ^{2}Google AI Brain Team\\n{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com [Equal contribution. Order determined by swapping the one in [9].^{1}Pretrained models and code are available at https://github.com/zihangdai/xlnet]\\n\\n###### Abstract\\n\\nWith the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking..\\n\\n## 1 Introduction\\n\\nUnsupervised representation learning has been highly successful in the domain of natural language processing *[7, 22, 27, 28, 10]*. Typically, these methods first pretrain neural networks on large-scale unlabeled text corpora, and then finetune the models or representations on downstream tasks. Under this shared high-level idea, different unsupervised pretraining objectives have been explored in literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives.\\n\\nAR language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model *[7, 27, 28]*. Specifically, given a text sequence $\\\\mathbf{x}=(x_{1},\\\\cdots,x_{T})$, AR language modeling factorizes the likelihood into a forward product $p(\\\\mathbf{x})=\\\\prod_{t=1}^{T}p(x_{t}\\\\mid\\\\mathbf{x}_{<t})$ or a backward one $p(\\\\mathbf{x})=\\\\prod_{t=T}^{1}p(x_{t}\\\\mid\\\\mathbf{x}_{>t})$. A parametric model (e.g. a neural network) is trained to model each conditional distribution. Since an AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information. This results in a gap between AR language modeling and effective pretraining.\\n\\nIn comparison, AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. A notable example is BERT *[10]*, which has been the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize\\n\\nbidirectional contexts for reconstruction. As an immediate benefit, this closes the aforementioned bidirectional information gap in AR language modeling, leading to improved performance. However, the artificial symbols like [MASK] used by BERT during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural language *[9]*.\\n\\nFaced with the pros and cons of existing language pretraining objectives, in this work, we propose XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations.\\n\\n- Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.\\n- Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to. Meanwhile, the autoregressive objective also provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT.\\n\\nIn addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.\\n\\n- Inspired by the latest advancements in AR language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL *[9]* into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.\\n- Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we propose to reparameterize the Transformer(-XL) network to remove the ambiguity.\\n\\nEmpirically, under comparable experiment setting, XLNet consistently outperforms BERT *[10]* on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task.\\n\\n#### Related Work\\n\\nThe idea of permutation-based AR modeling has been explored in *[32, 12]*, but there are several key differences. Firstly, previous models aim to improve density estimation by baking an “orderless” inductive bias into the model while XLNet is motivated by enabling AR language models to learn bidirectional contexts. Technically, to construct a valid target-aware prediction distribution, XLNet incorporates the target position into the hidden state via two-stream attention while previous permutation-based AR models relied on implicit position awareness inherent to their MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that “orderless” does not mean that the input sequence can be randomly permuted but that the model allows for different factorization orders of the distribution.\\n\\nAnother related idea is to perform autoregressive denoising in the context of text generation *[11]*, which only considers a fixed order though.\\n\\n## 2 Proposed Method\\n\\n### 2.1 Background\\n\\nIn this section, we first review and compare the conventional AR language modeling and BERT for language pretraining. Given a text sequence $\\\\mathbf{x}=[x_{1},\\\\cdots,x_{T}]$, AR language modeling performs pretraining by maximizing the likelihood under the forward autoregressive factorization:\\n\\n$\\\\max_{\\\\theta}\\\\quad\\\\log p_{\\\\theta}(\\\\mathbf{x})=\\\\sum_{t=1}^{T}\\\\log p_{\\\\theta}(x_{t}\\\\mid\\\\mathbf{x}_{<t})=\\\\sum_{t=1}^{T}\\\\log\\\\frac{\\\\exp\\\\left(h_{\\\\theta}(\\\\mathbf{x}_{1:t-1})^{\\\\top}e(x_{t})\\\\right)}{\\\\sum_{x^{\\\\prime}}\\\\exp\\\\left(h_{\\\\theta}(\\\\mathbf{x}_{1:t-1})^{\\\\top}e(x^{\\\\prime})\\\\right)},$ (1)\\n\\nwhere\\n\\n$h_{\\\\theta}(\\\\mathbf{x}_{1:t-1})$ is a context representation produced by neural models, such as RNNs or Transformers, and $e(x)$ denotes the embedding of $x$. In comparison, BERT is based on denoising auto-encoding. Specifically, for a text sequence $\\\\mathbf{x}$, BERT first constructs a corrupted version $\\\\hat{\\\\mathbf{x}}$ by randomly setting a portion (e.g. 15%) of tokens in $\\\\mathbf{x}$ to a special symbol [MASK]. Let the masked tokens be $\\\\hat{\\\\mathbf{x}}$. The training objective is to reconstruct $\\\\hat{\\\\mathbf{x}}$ from $\\\\hat{\\\\mathbf{x}}$:\\n\\n$\\\\max_{\\\\theta}\\\\quad\\\\log p_{\\\\theta}(\\\\hat{\\\\mathbf{x}}\\\\mid\\\\hat{\\\\mathbf{x}})\\\\approx\\\\sum_{t=1}^{T}m_{t}\\\\log p_{\\\\theta}(x_{t}\\\\mid\\\\hat{\\\\mathbf{x}})=\\\\sum_{t=1}^{T}m_{t}\\\\log\\\\frac{\\\\exp\\\\left(H_{\\\\theta}(\\\\hat{\\\\mathbf{x}})_{t}^{\\\\top}e(x_{t})\\\\right)}{\\\\sum_{x^{\\\\prime}}\\\\exp\\\\left(H_{\\\\theta}(\\\\hat{\\\\mathbf{x}})_{t}^{\\\\top}e(x^{\\\\prime})\\\\right)},$ (2)\\n\\nwhere $m_{t}=1$ indicates $x_{t}$ is masked, and $H_{\\\\theta}$ is a Transformer that maps a length-$T$ text sequence $\\\\mathbf{x}$ into a sequence of hidden vectors $H_{\\\\theta}(\\\\mathbf{x})=[H_{\\\\theta}(\\\\mathbf{x})_{1},H_{\\\\theta}(\\\\mathbf{x})_{2},\\\\cdots,H_{\\\\theta}(\\\\mathbf{x})_{T}]$. The pros and cons of the two pretraining objectives are compared in the following aspects:\\n\\n- [leftmargin=*]\\n- Independence Assumption: As emphasized by the $\\\\approx$ sign in Eq. (2), BERT factorizes the joint conditional probability $p(\\\\bar{\\\\mathbf{x}}\\\\mid\\\\hat{\\\\mathbf{x}})$ based on an independence assumption that all masked tokens $\\\\hat{\\\\mathbf{x}}$ are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes $p_{\\\\theta}(\\\\mathbf{x})$ using the product rule that holds universally without such an independence assumption.\\n- Input noise: The input to BERT contains artificial symbols like [MASK] that never occur in downstream tasks, which creates a pretrain-finetune discrepancy. Replacing [MASK] with original tokens as in *[10]* does not solve the problem because original tokens can be only used with a small probability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling does not rely on any input corruption and does not suffer from this issue.\\n- Context dependency: The AR representation $h_{\\\\theta}(\\\\mathbf{x}_{1:t-1})$ is only conditioned on the tokens up to position $t$ (i.e. tokens to the left), while the BERT representation $H_{\\\\theta}(\\\\mathbf{x})_{t}$ has access to the contextual information on both sides. As a result, the BERT objective allows the model to be pretrained to better capture bidirectional context.\\n\\n### 2.2 Objective: Permutation Language Modeling\\n\\nAccording to the comparison above, AR language modeling and BERT possess their unique advantages over the other. A natural question to ask is whether there exists a pretraining objective that brings the advantages of both while avoiding their weaknesses.\\n\\nBorrowing ideas from orderless NADE *[32]*, we propose the permutation language modeling objective that not only retains the benefits of AR models but also allows models to capture bidirectional contexts. Specifically, for a sequence $\\\\mathbf{x}$ of length $T$, there are $T!$ different orders to perform a valid autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides.\\n\\nTo formalize the idea, let $\\\\mathcal{Z}_{T}$ be the set of all possible permutations of the length-$T$ index sequence $[1,2,\\\\ldots,T]$. We use $z_{t}$ and $\\\\mathbf{z}_{<t}$ to denote the $t$-th element and the first $t-1$ elements of a permutation $\\\\mathbf{z}\\\\in\\\\mathcal{Z}_{T}$. Then, our proposed permutation language modeling objective can be expressed as follows:\\n\\n$\\\\max_{\\\\theta}\\\\quad\\\\mathbb{E}_{\\\\mathbf{z}\\\\sim\\\\mathcal{Z}_{T}}\\\\left[\\\\sum_{t=1}^{T}\\\\log p_{\\\\theta}(x_{z_{t}}\\\\mid\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})\\\\right].$ (3)\\n\\nEssentially, for a text sequence $\\\\mathbf{x}$, we sample a factorization order $\\\\mathbf{z}$ at a time and decompose the likelihood $p_{\\\\theta}(\\\\mathbf{x})$ according to factorization order. Since the same model parameter $\\\\theta$ is shared across all factorization orders during training, in expectation, $x_{t}$ has seen every possible element $x_{i}\\\\neq x_{t}$ in the sequence, hence being able to capture the bidirectional context. Moreover, as this objective fits into the AR framework, it naturally avoids the independence assumption and the pretrain-finetune discrepancy discussed in Section 2.1.\\n\\nRemark on Permutation The proposed objective only permutes the factorization order, not the sequence order. In other words, we keep the original sequence order, use the positional encodings corresponding to the original sequence, and rely on a proper attention mask in Transformers to achieve permutation of the factorization order. Note that this choice is necessary, since the model will only encounter text sequences with the natural order during finetuning.\\n\\nTo provide an overall picture, we show an example of predicting the token $x_{3}$ given the same input sequence $\\\\mathbf{x}$ but under different factorization orders in the Appendix A.7 with Figure 4.\\n\\n# 2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations\\n\\n![img-0.jpeg](img-0.jpeg)\\n\\n![img-1.jpeg](img-1.jpeg)\\nFigure 1: (a): Content stream attention, which is the same as the standard self-attention. (b): Query stream attention, which does not have access information about the content $x_{z_t}$. (c): Overview of the permutation language modeling training with two-stream attention.\\n\\nWhile the permutation language modeling objective has desired properties, naive implementation with standard Transformer parameterization may not work. To see the problem, assume we parameterize the next-token distribution $p_{\\\\theta}(X_{z_t} \\\\mid \\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}})$ using the standard Softmax formulation, i.e., $p_{\\\\theta}(X_{z_t} = x \\\\mid \\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}}) = \\\\frac{\\\\exp(e(x)^{\\\\top}h_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}}))}{\\\\sum_{x\\'} \\\\exp(e(x\\')^{\\\\top}h_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}}))}$, where $h_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}})$ denotes the hidden representation of $\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}}$ produced by the shared Transformer network after proper masking. Now notice that the representation $h_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}})$ does not depend on which position it will predict, i.e., the value of $z_t$. Consequently, the same distribution is predicted regardless of the target position, which is not able to learn useful representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to re-parameterize the next-token distribution to be target position aware:\\n\\n$$\\np _ {\\\\theta} \\\\left(X _ {z _ {t}} = x \\\\mid \\\\mathbf {x} _ {z &lt; t}\\\\right) = \\\\frac {\\\\exp \\\\left(e \\\\left(x\\\\right) ^ {\\\\top} g _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {\\\\mathbf {z} &lt; t} , z _ {t}\\\\right)\\\\right)}{\\\\sum_ {x ^ {\\\\prime}} \\\\exp \\\\left(e \\\\left(x ^ {\\\\prime}\\\\right) ^ {\\\\top} g _ {\\\\theta} \\\\left(\\\\mathbf {x} _ {\\\\mathbf {z} &lt; t} , z _ {t}\\\\right)\\\\right)}, \\\\tag {4}\\n$$\\n\\nwhere $g_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}},z_t)$ denotes a new type of representations which additionally take the target position $z_{t}$ as input.\\n\\n**Two-Stream Self-Attention** While the idea of target-aware representations removes the ambiguity in target prediction, how to formulate $g_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}},z_t)$ remains a non-trivial problem. Among other possibilities, we propose to \"stand\" at the target position $z_{t}$ and rely on the position $z_{t}$ to gather information from the context $\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}}$ through attention. For this parameterization to work, there are two requirements that are contradictory in a standard Transformer architecture: (1) to predict the token $x_{z_t}$, $g_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}},z_t)$ should only use the position $z_{t}$ and not the content $x_{z_t}$, otherwise the objective becomes trivial; (2) to predict the other tokens $x_{z_j}$ with $j &gt; t$, $g_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}},z_t)$ should also encode the content $x_{z_t}$ to provide full contextual information. To resolve such a contradiction, we propose to use two sets of hidden representations instead of one:\\n\\n- The content representation $h_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{&lt;t}})$, or abbreviated as $h_{z_t}$, which serves a similar role to the standard hidden states in Transformer. This representation encodes both the context and $x_{z_t}$ itself.\\n- The query representation $g_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z} &lt; t}, z_t)$, or abbreviated as $g_{z_t}$, which only has access to the contextual information $\\\\mathbf{x}_{\\\\mathbf{z} &lt; t}$ and the position $z_t$, but not the content $x_{z_t}$, as discussed above.\\n\\nComputationally, the first layer query stream is initialized with a trainable vector, i.e. $g_{i}^{(0)} = w$, while the content stream is set to the corresponding word embedding, i.e. $h_{i}^{(0)} = e(x_{i})$. For each self-attention layer $m = 1,\\\\dots ,M$, the two streams of representations are schematically updated\\n\\n2To avoid clutter, we omit the implementation details including multi-head attention, residual connection, layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in Appendix A.2 for reference.\\n\\nwith a shared set of parameters as follows (illustrated in Figures 1 (a) and (b)):\\n\\n$g^{(m)}_{\\\\dot{z}_{t}}$ $\\\\leftarrow\\\\text{Attention}(\\\\text{Q}=g^{(m-1)}_{\\\\dot{z}_{t}},\\\\text{KV}=\\\\mathbf{h}^{(m-1)}_{\\\\dot{\\\\mathbf{z}}_{<t}};\\\\theta),\\\\quad\\\\text{(query stream: use $z_{t}$ but cannot see $x_{z_{t}}$)}$\\n$h^{(m)}_{\\\\dot{z}_{t}}$ $\\\\leftarrow\\\\text{Attention}(\\\\text{Q}=h^{(m-1)}_{\\\\dot{z}_{t}},\\\\text{KV}=\\\\mathbf{h}^{(m-1)}_{\\\\dot{\\\\mathbf{z}}_{\\\\leq t}};\\\\theta),\\\\quad\\\\text{(content stream: use both $z_{t}$ and $x_{z_{t}}$)}.$\\n\\nwhere Q, K, V denote the query, key, and value in an attention operation *[33]*. The update rule of the content representations is exactly the same as the standard self-attention, so during finetuning, we can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally, we can use the last-layer query representation $g^{(M)}_{z_{t}}$ to compute Eq. (4).\\n\\nPartial Prediction While the permutation language modeling objective (3) has several benefits, it is a much more challenging optimization problem due to the permutation and causes slow convergence in preliminary experiments. To reduce the optimization difficulty, we choose to only predict the last tokens in a factorization order. Formally, we split $\\\\mathbf{z}$ into a non-target subsequence $\\\\mathbf{z}_{\\\\leq c}$ and a target subsequence $\\\\mathbf{z}_{>c}$, where $c$ is the cutting point. The objective is to maximize the log-likelihood of the target subsequence conditioned on the non-target subsequence, i.e.,\\n\\n$\\\\max_{\\\\theta}\\\\quad\\\\mathbb{E}_{\\\\mathbf{z}\\\\sim\\\\mathcal{Z}_{T}}\\\\Bigl{[}\\\\log p_{\\\\theta}(\\\\mathbf{x}_{\\\\mathbf{z}_{>c}}\\\\mid\\\\mathbf{x}_{\\\\mathbf{z}_{\\\\leq c}})\\\\Bigr{]}=\\\\mathbb{E}_{\\\\mathbf{z}\\\\sim\\\\mathcal{Z}_{T}}\\\\Bigg{[}\\\\sum_{t=c+1}^{|\\\\mathbf{z}|}\\\\log p_{\\\\theta}(x_{z_{t}}\\\\mid\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})\\\\Bigg{]}.$ (5)\\n\\nNote that $\\\\mathbf{z}_{>c}$ is chosen as the target because it possesses the longest context in the sequence given the current factorization order $\\\\mathbf{z}$. A hyperparameter $K$ is used such that about $1/K$ tokens are selected for predictions; i.e., $|\\\\mathbf{z}|/(|\\\\mathbf{z}|-c)\\\\approx K$. For unselected tokens, their query representations need not be computed, which saves speed and memory.\\n\\n### 2.4 Incorporating Ideas from Transformer-XL\\n\\nSince our objective function fits in the AR framework, we incorporate the state-of-the-art AR language model, Transformer-XL *[9]*, into our pretraining framework, and name our method after it. We integrate two important techniques in Transformer-XL, namely the relative positional encoding scheme and the segment recurrence mechanism. We apply relative positional encodings based on the original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden states from previous segments. Without loss of generality, suppose we have two segments taken from a long sequence $\\\\mathbf{s}$; i.e., $\\\\tilde{\\\\mathbf{x}}=\\\\mathbf{s}_{1:T}$ and $\\\\mathbf{x}=\\\\mathbf{s}_{T+1:2T}$. Let $\\\\tilde{\\\\mathbf{z}}$ and $\\\\mathbf{z}$ be permutations of $[1\\\\cdots T]$ and $[T+1\\\\cdots 2T]$ respectively. Then, based on the permutation $\\\\tilde{\\\\mathbf{z}}$, we process the first segment, and then cache the obtained content representations $\\\\tilde{\\\\mathbf{h}}^{(m)}$ for each layer $m$. Then, for the next segment $\\\\mathbf{x}$, the attention update with memory can be written as\\n\\n$h^{(m)}_{z_{t}}\\\\leftarrow\\\\text{Attention}(\\\\text{Q}=h^{(m-1)}_{z_{t}},\\\\text{KV}=\\\\Bigl{[}\\\\tilde{\\\\mathbf{h}}^{(m-1)},\\\\mathbf{h}^{(m-1)}_{\\\\tilde{\\\\mathbf{z}}_{\\\\leq t}}\\\\Bigr{]};\\\\theta)$\\n\\nwhere $[.,.]$ denotes concatenation along the sequence dimension. Notice that positional encodings only depend on the actual positions in the original sequence. Thus, the above attention update is independent of $\\\\tilde{\\\\mathbf{z}}$ once the representations $\\\\tilde{\\\\mathbf{h}}^{(m)}$ are obtained. This allows caching and reusing the memory without knowing the factorization order of the previous segment. In expectation, the model learns to utilize the memory over all factorization orders of the last segment. The query stream can be computed in the same way. Finally, Figure 1 (c) presents an overview of the proposed permutation language modeling with two-stream attention (see Appendix A.7 for more detailed illustration).\\n\\n### 2.5 Modeling Multiple Segments\\n\\nMany downstream tasks have multiple input segments, e.g., a question and a context paragraph in question answering. We now discuss how we pretrain XLNet to model multiple segments in the autoregressive framework. During the pretraining phase, following BERT, we randomly sample two segments (either from the same context or not) and treat the concatenation of two segments as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context. Specifically, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP], where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although\\n\\nwe follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction *[10]* as it does not show consistent improvement in our ablation study (see Section 3.4).\\n\\n#### Relative Segment Encodings\\n\\nArchitecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, we extend the idea of relative encodings from Transformer-XL to also encode the segments. Given a pair of positions $i$ and $j$ in the sequence, if $i$ and $j$ are from the same segment, we use a segment encoding $\\\\mathbf{s}_{ij}=\\\\mathbf{s}_{+}$ or otherwise $\\\\mathbf{s}_{ij}=\\\\mathbf{s}_{-}$, where $\\\\mathbf{s}_{+}$ and $\\\\mathbf{s}_{-}$ are learnable model parameters for each attention head. In other words, we only consider whether the two positions are *within the same segment*, as opposed to considering *which specific segments they are from*. This is consistent with the core idea of relative encodings; i.e., only modeling the relationships between positions. When $i$ attends to $j$, the segment encoding $\\\\mathbf{s}_{ij}$ is used to compute an attention weight $a_{ij}=(\\\\mathbf{q}_{i}+\\\\mathbf{b})^{\\\\top}\\\\mathbf{s}_{ij}$, where $\\\\mathbf{q}_{i}$ is the query vector as in a standard attention operation and $\\\\mathbf{b}$ is a learnable head-specific bias vector. Finally, the value $a_{ij}$ is added to the normal attention weight. There are two benefits of using relative segment encodings. First, the inductive bias of relative encodings improves generalization *[9]*. Second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings.\\n\\n### 2.6 Discussion\\n\\nComparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e., only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT and XLNet, partial prediction plays a role of reducing optimization difficulty by only predicting tokens with sufficient context. However, the independence assumption discussed in Section 2.1 disables BERT to model dependency between targets.\\n\\nTo better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize $\\\\log p(\\\\text{New York | is a city})$. Also suppose that XLNet samples the factorization order [is, a, city, New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:\\n\\n$\\\\mathcal{J}_{\\\\text{BERT}}=\\\\log p(\\\\text{New | is a city})+\\\\log p(\\\\text{York | is a city}),$\\n$\\\\mathcal{J}_{\\\\text{XLNet}}=\\\\log p(\\\\text{New | is a city})+\\\\log p(\\\\text{York | New},\\\\text{is a city}).$\\n\\nNotice that XLNet is able to capture the dependency between the pair (New, York), which is omitted by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and (York, city), it is obvious that XLNet always learns more dependency pairs given the same target and contains “denser” effective training signals.\\n\\nFor more formal analysis and further discussion, please refer to Appendix A.5.\\n\\n## 3 Experiments\\n\\n### 3.1 Pretraining and Implementation\\n\\nFollowing BERT *[10]*, we use the BooksCorpus *[40]* and English Wikipedia as part of our pretraining data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) *[26]*, ClueWeb 2012-B (extended from *[5]*), and Common Crawl *[6]* for pretraining. We use heuristics to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl, which results in 19GB and 110GB text respectively. After tokenization with SentencePiece *[17]*, we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total.\\n\\nOur largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\\n\\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks.\\n\\nSince the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant  $K$  as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified $^3$ . We employ an idea of span-based prediction, where we first sample a length  $L \\\\in [1, \\\\dots, 5]$ , and then randomly select a consecutive span of  $L$  tokens as prediction targets within a context of  $(KL)$  tokens.\\n\\nWe use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\\n\\n# 3.2 Fair Comparison with BERT\\n\\n|  Model | SQuAD1.1 | SQuAD2.0 | RACE | MNLI | QNLI | QQP | RTE | SST-2 | MRPC | CoLA | STS-B  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  BERT-Large (Best of 3) | 86.7/92.8 | 82.8/85.5 | 75.1 | 87.3 | 93.0 | 91.4 | 74.0 | 94.0 | 88.7 | 63.7 | 90.2  |\\n|  XLNet-Large-wikibooks | 88.2/94.0 | 85.1/87.8 | 77.4 | 88.4 | 93.9 | 91.8 | 81.2 | 94.4 | 90.0 | 65.2 | 91.1  |\\n\\nTable 1: Fair comparison with BERT. All models are trained using the same data and hyperparameters as in BERT. We use the best of 3 BERT variants for comparison; i.e., the original BERT, BERT with whole word masking, and BERT without next sentence prediction.\\n\\nHere, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet. In Table 1, we compare (1) best performance of three different variants of BERT and (2) XLNet trained with the same data and hyperparameters. As we can see, trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the considered datasets.\\n\\n# 3.3 Comparison with RoBERTa: Scaling Up\\n\\n|  RACE | Accuracy | Middle | High | Model | NDCG@20 | ERR@20  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  GPT [28] | 59.0 | 62.9 | 57.4 | DRMM [13] | 24.3 | 13.8  |\\n|  BERT [25] | 72.0 | 76.6 | 70.1 | KNRM [8] | 26.9 | 14.9  |\\n|  BERT+DCMN* [38] | 74.1 | 79.5 | 71.8 | Conv [8] | 28.7 | 18.1  |\\n|  RoBERTa [21] | 83.2 | 86.5 | 81.8 | BERT† | 30.53 | 18.67  |\\n|  XLNet | 85.4 | 88.6 | 84.0 | XLNet | 31.10 | 20.28  |\\n\\nTable 2: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task, and on ClueWeb09-B, a document ranking task. * indicates using ensembles. † indicates our implementations. “Middle” and “High” in RACE are two subsets representing middle and high school difficulty levels. All BERT, RoBERTa, and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).\\n\\nAfter the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1.\\n\\nThe results are presented in Tables 2 (reading comprehension &amp; document ranking), 3 (question answering), 4 (text classification) and 5 (natural language understanding), where XLNet generally outperforms BERT and RoBERTa. In addition, we make two more interesting observations:\\n\\n|  SQuAD2.0 | EM | F1 | SQuAD1.1 | EM | F1  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Dev set results (single model)  |   |   |   |   |   |\\n|  BERT [10] | 78.98 | 81.77 | BERT† [10] | 84.1 | 90.9  |\\n|  RoBERTa [21] | 86.5 | 89.4 | RoBERTa [21] | 88.9 | 94.6  |\\n|  XLNet | 87.9 | 90.6 | XLNet | 89.7 | 95.1  |\\n|  Test set results on leaderboard (single model, as of Dec 14, 2019)  |   |   |   |   |   |\\n|  BERT [10] | 80.005 | 83.061 | BERT [10] | 85.083 | 91.835  |\\n|  RoBERTa [21] | 86.820 | 89.795 | BERT* [10] | 87.433 | 93.294  |\\n|  XLNet | 87.926 | 90.689 | XLNet | 89.898‡ | 95.080‡  |\\n\\nTable 3: Results on SQuAD, a reading comprehension dataset. † marks our runs with the official code. * indicates ensembles. ‡: We are not able to obtain the test results of our latest model on SQuAD1.1 from the organizers after submitting our result for more than one month, and thus report the results of an older version for the SQuAD1.1 test set.\\n\\n|  Model | IMDB | Yelp-2 | Yelp-5 | DBpedia | AG | Amazon-2 | Amazon-5  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n|  CNN [15] | - | 2.90 | 32.39 | 0.84 | 6.57 | 3.79 | 36.24  |\\n|  DPCNN [15] | - | 2.64 | 30.58 | 0.88 | 6.87 | 3.32 | 34.81  |\\n|  Mixed VAT [31, 23] | 4.32 | - | - | 0.70 | 4.95 | - | -  |\\n|  ULMFiT [14] | 4.6 | 2.16 | 29.98 | 0.80 | 5.01 | - | -  |\\n|  BERT [35] | 4.51 | 1.89 | 29.32 | 0.64 | - | 2.63 | 34.17  |\\n|  XLNet | 3.20 | 1.37 | 27.05 | 0.60 | 4.45 | 2.11 | 31.67  |\\n\\nTable 4: Comparison with state-of-the-art error rates on the test sets of several text classification datasets. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).\\n\\n|  Model | MNLI | QNLI | QQP | RTE | SST-2 | MRPC | CoLA | STS-B | WNLI  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Single-task single models on dev  |   |   |   |   |   |   |   |   |   |\\n|  BERT [2] | 86.6/- | 92.3 | 91.3 | 70.4 | 93.2 | 88.0 | 60.6 | 90.0 | -  |\\n|  RoBERTa [21] | 90.2/90.2 | 94.7 | 92.2 | 86.6 | 96.4 | 90.9 | 68.0 | 92.4 | -  |\\n|  XLNet | 90.8/90.8 | 94.9 | 92.3 | 85.9 | 97.0 | 90.8 | 69.0 | 92.5 | -  |\\n|  Multi-task ensembles on test (from leaderboard as of Oct 28, 2019)  |   |   |   |   |   |   |   |   |   |\\n|  MT-DNN* [20] | 87.9/87.4 | 96.0 | 89.9 | 86.3 | 96.5 | 92.7 | 68.4 | 91.1 | 89.0  |\\n|  RoBERTa* [21] | 90.8/90.2 | 98.9 | 90.2 | 88.2 | 96.7 | 92.3 | 67.8 | 92.2 | 89.0  |\\n|  XLNet* | 90.9/90.9† | 99.0† | 90.4† | 88.5 | 97.1† | 92.9 | 70.2 | 93.0 | 92.5  |\\n\\nTable 5: Results on GLUE. * indicates using ensembles, and † denotes single-task results in a multi-task row. All dev results are the median of 10 runs. The upper section shows direct comparison on dev data and the lower section shows comparison with state-of-the-art results on the public leaderboard.\\n\\n- For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is usually larger. This superiority at dealing with longer context could come from the Transformer-XL backbone in XLNet.\\n- For classification tasks that already have abundant supervised examples such as MNLI ( $&gt;390\\\\mathrm{K}$ ), Yelp ( $&gt;560\\\\mathrm{K}$ ) and Amazon ( $&gt;3\\\\mathrm{M}$ ), XLNet still lead to substantial gains.\\n\\n# 3.4 Ablation Study\\n\\nWe perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study:\\n\\n- The effectiveness of the permutation language modeling objective alone, especially compared to the denoising auto-encoding objective used by BERT.\\n- The importance of using Transformer-XL as the backbone neural architecture.\\n- The necessity of some implementation details including span-based prediction, the bidirectional input pipeline, and next-sentence prediction.\\n\\nWith these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implementation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidirectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the BooksCorpus. All results reported are the median of 5 runs.\\n\\n|  # | Model | RACE | SQuAD2.0 |   | MNLI m/mm | SST-2  |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|   |   |   |  F1 | EM  |   |   |\\n|  1 | BERT-Base | 64.3 | 76.30 | 73.66 | 84.34/84.65 | 92.78  |\\n|  2 | DAE + Transformer-XL | 65.03 | 79.56 | 76.80 | 84.88/84.45 | 92.60  |\\n|  3 | XLNet-Base (K=7) | 66.05 | 81.33 | 78.46 | 85.84/85.43 | 92.66  |\\n|  4 | XLNet-Base (K=6) | 66.66 | 80.98 | 78.18 | 85.63/85.12 | 93.35  |\\n|  5 | - memory | 65.55 | 80.15 | 77.27 | 85.32/85.05 | 92.78  |\\n|  6 | - span-based pred | 65.95 | 80.61 | 77.91 | 85.49/85.02 | 93.12  |\\n|  7 | - bidirectional data | 66.34 | 80.65 | 77.87 | 85.31/84.99 | 92.66  |\\n|  8 | + next-sent pred | 66.76 | 79.83 | 76.94 | 85.32/85.09 | 92.89  |\\n\\nTable 6: The results of BERT on RACE are taken from [38]. We run BERT on the other datasets using the official implementation and the same hyperparameter search space as XLNet.  $K$  is a hyperparameter to control the optimization difficulty (see Section 2.3).\\n\\nExamining rows 1 - 4 of Table 6, we can see both Transformer-XL and the permutation LM clearly contribute the superior performance of XLNet over BERT. Moreover, if we remove the memory caching mechanism (row 5), the performance clearly drops, especially for RACE which involves the longest context among the 4 tasks. In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play important roles in XLNet. Finally, we unexpectedly find the next-sentence prediction objective proposed in the original BERT does not necessarily lead to an improvement in our setting. Hence, we exclude the next-sentence prediction objective from XLNet.\\n\\nFinally, we also perform a qualitative study of the attention patterns, which is included in Appendix A.6 due to page limit.\\n\\n# 4 Conclusions\\n\\nXLNet is a generalized AR pretraining method that uses a permutation language modeling objective to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to work seamlessly with the AR objective, including integrating Transformer-XL and the careful design of the two-stream attention mechanism. XLNet achieves substantial improvement over previous pretraining objectives on various tasks.\\n\\n# Acknowledgments\\n\\nThe authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the project, Jamie Callan for providing the ClueWeb dataset, Youlong Cheng, Yanping Huang and Shibo Wang for providing ideas to improve our TPU implementation, Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and RS were supported by the Office of Naval Research grant N000141812861, the National Science Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY were supported in part by NSF under the grant IIS-1546329 and by the DOE-Office of Science under the grant ASCR #KJ040201.\\n\\n# References\\n\\n[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.\\n[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anonymous preprint under review, 2018.\\n[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018.\\n\\n[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer neural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.\\n- [5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.\\n- [6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org, 2019.\\n- [7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079–3087, 2015.\\n- [8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international conference on web search and data mining, pages 126–134. ACM, 2018.\\n- [9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\\n- [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n- [11] William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: better text generation via filling in the_. arXiv preprint arXiv:1801.07736, 2018.\\n- [12] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, pages 881–889, 2015.\\n- [13] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, pages 55–64. ACM, 2016.\\n- [14] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.\\n- [15] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text categorization. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 562–570, 2017.\\n- [16] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint arXiv:1905.06290, 2019.\\n- [17] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\\n- [18] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\\n- [19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\\n- [20] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.\\n- [21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n- [22] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pages 6294–6305, 2017.\\n- [23] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised text classification. arXiv preprint arXiv:1605.07725, 2016.\\n- [24] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.\\n- [25] Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with external knowledge. arXiv preprint arXiv:1902.00993, 2019.\\n\\n[26] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword fifth edition, linguistic data consortium. Technical report, Technical Report. Linguistic Data Consortium, Philadelphia, Tech. Rep., 2011.\\n- [27] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\\n- [28] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.\\n- [29] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.\\n- [30] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\\n- [31] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks for semi-supervised text classification via mixed objective function. 2018.\\n- [32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–7220, 2016.\\n- [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.\\n- [34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR.\\n- [35] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848, 2019.\\n- [36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval, pages 55–64. ACM, 2017.\\n- [37] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.\\n- [38] Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. Dual co-matching network for multi-choice reading comprehension. arXiv preprint arXiv:1901.09381, 2019.\\n- [39] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649–657, 2015.\\n- [40] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015.\\n\\nA Target-Aware Representation via Two-Stream Self-Attention\\n\\n### A.1 A Concrete Example of How Standard LM Parameterization Fails\\n\\nIn this section, we provide a concrete example to show how the standard language model parameterization fails under the permutation objective, as discussed in Section 2.3. Specifically, let’s consider two different permutations $\\\\mathbf{z}^{(1)}$ and $\\\\mathbf{z}^{(2)}$ satisfying the following relationship\\n\\n$\\\\mathbf{z}_{<t}^{(1)}=\\\\mathbf{z}_{<t}^{(2)}=\\\\mathbf{z}_{<t}\\\\quad\\\\text{but}\\\\quad z_{t}^{(1)}=i\\\\neq j=z_{t}^{(2)}.$\\n\\nThen, substituting the two permutations respectively into the naive parameterization, we have\\n\\n$\\\\underbrace{p_{\\\\theta}(X_{i}=x\\\\mid\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})}_{z_{t}^{(1)}=i,\\\\,\\\\mathbf{z}_{<t}^{(1)}=\\\\mathbf{z}_{<t}}=\\\\underbrace{p_{\\\\theta}(X_{j}=x\\\\mid\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})}_{z_{t}^{(1)}=j,\\\\,\\\\mathbf{z}_{<t}^{(2)}=\\\\mathbf{z}_{<t}}=\\\\frac{\\\\exp\\\\left(e(x)^{\\\\top}h(\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})\\\\right)}{\\\\sum_{x^{\\\\prime}}\\\\exp\\\\left(e(x^{\\\\prime})^{\\\\top}h(\\\\mathbf{x}_{\\\\mathbf{z}_{<t}})\\\\right)}.$\\n\\nEffectively, two different target positions $i$ and $j$ share exactly the same model prediction. However, the ground-truth distribution of two positions should certainly be different.\\n\\n### A.2 Two-Stream Attention\\n\\nHere, we provide the implementation details of the two-stream attention with a Transformer-XL backbone.\\n\\nInitial represetation:\\n\\n$\\\\forall t=1,\\\\ldots,T:\\\\quad h_{t}=e(x_{t})\\\\quad\\\\text{and}\\\\quad g_{t}=w$\\n\\nCached layer-$m$ content represetation (memory) from previous segment: $\\\\tilde{\\\\mathbf{h}}^{(m)}$\\n\\nFor the Transformer-XL layer $m=1,\\\\cdots,M$, attention with relative positional encoding and position-wise feed-forward are consecutively employed to update the represetntations:\\n\\n$\\\\forall t=1,\\\\ldots,T:\\\\quad$ $\\\\hat{h}_{z_{t}}^{(m)}=\\\\text{LayerNorm}\\\\Big{(}h_{z_{t}}^{(m-1)}+\\\\text{RelAttn}\\\\Big{(}h_{z_{t}}^{(m-1)},\\\\Big{[}\\\\tilde{\\\\mathbf{h}}^{(m-1)},\\\\mathbf{h}_{\\\\mathbf{z}_{\\\\leq t}}^{(m-1)}\\\\Big{]}\\\\Big{)}\\\\Big{)}$\\n$h_{z_{t}}^{(m)}=\\\\text{LayerNorm}\\\\Big{(}\\\\hat{h}_{z_{t}}^{(m)}+\\\\text{PosFF}\\\\Big{(}\\\\hat{h}_{z_{t}}^{(m)}\\\\Big{)}\\\\Big{)}$\\n$\\\\hat{g}_{z_{t}}^{(m)}=\\\\text{LayerNorm}\\\\Big{(}g_{z_{t}}^{(m-1)}+\\\\text{RelAttn}\\\\Big{(}g_{z_{t}}^{(m-1)},\\\\Big{[}\\\\tilde{\\\\mathbf{h}}^{(m-1)},\\\\mathbf{h}_{\\\\mathbf{z}_{<t}}^{(m-1)}\\\\Big{]}\\\\Big{)}\\\\Big{)}$\\n$g_{z_{t}}^{(m)}=\\\\text{LayerNorm}\\\\Big{(}\\\\hat{g}_{z_{t}}^{(m)}+\\\\text{PosFF}\\\\Big{(}\\\\hat{g}_{z_{t}}^{(m)}\\\\Big{)}\\\\Big{)}$\\n\\nTarget-aware prediction distribution:\\n\\n$p_{\\\\theta}(X_{z_{t}}=x\\\\mid\\\\mathbf{x}_{z_{<t}})=\\\\frac{\\\\exp\\\\left(e(x)^{\\\\top}g_{z_{t}}^{(M)}\\\\right)}{\\\\sum_{x^{\\\\prime}}\\\\exp\\\\left(e(x^{\\\\prime})^{\\\\top}g_{z_{t}}^{(M)}\\\\right)},$\\n\\n### A.3 Datasets\\n\\n#### A.3.1 RACE Dataset\\n\\nThe RACE dataset *[18]* contains near 100K questions taken from the English exams for middle and high school Chinese students in the age range between 12 to 18, with the answers generated by human experts. This is one of the most difficult reading comprehension datasets that involve challenging reasoning questions. Moreover, the average length of the passages in RACE are longer than 300, which is significantly longer than other popular reading comprehension datasets such as SQuAD *[29]*. As a result, this dataset serves as a challenging benchmark for long text understanding. We use a sequence length of 512 during finetuning.\\n\\n#### A.3.2 SQuAD\\n\\nSQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 *[30]* contains questions that always have a corresponding answer in the given passages, while SQuAD2.0 *[29]* introduces unanswerable questions. To finetune an XLNet on SQuAD2.0, we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering *[10]*.\\n\\n####\\n\\n# A.3.3 Text classification Datasets\\n\\nFollowing previous work on text classification [39, 23], we evaluate XLNet on the following benchmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5.\\n\\n# A.3.4 GLUE Dataset\\n\\nThe GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and finetune the network on the other datasets. Only single-task training is employed for the four large datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm. For WNLI, we use the loss described in [16].\\n\\n# A.3.5 ClueWeb09-B Dataset\\n\\nFollowing the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning, and employ a kernel pooling network [36] to rank the documents.\\n\\n# A.4 Hyperparameters\\n\\n# A.4.1 Pretraining Hyperparameters\\n\\n|  Hparam | Value  |\\n| --- | --- |\\n|  Number of layers | 24  |\\n|  Hidden size | 1024  |\\n|  Number of attention heads | 16  |\\n|  Attention head size | 64  |\\n|  FFN inner hidden size | 4096  |\\n|  Hidden Dropout | 0.1  |\\n|  GeLU Dropout | 0.0  |\\n|  Attention dropout | 0.1  |\\n|  Partial prediction K | 6  |\\n|  Max sequence length | 512  |\\n|  Batch size | 8192  |\\n|  Learning rate | 4e-4  |\\n|  Number of steps | 500K  |\\n|  Warmup steps | 40,000  |\\n|  Learning rate decay | linear  |\\n|  Adam epsilon | 1e-6  |\\n|  Weight decay | 0.01  |\\n\\nTable 7: Hyperparameters for pretraining.\\n\\nThe hyperparameters used for pretraining XLNet are shown in Table 7.\\n\\n# A.4.2 Hyperparameters for Finetuning\\n\\nThe hyperparameters used for finetuning XLNet on various tasks are shown in Table 8. \"Layer-wise decay\" means exponentially decaying the learning rates of individual layers in a top-down manner. For example, suppose the 24-th layer uses a learning rate  $l$ , and the Layer-wise decay rate is  $\\\\alpha$ , then the learning rate of layer  $m$  is  $l\\\\alpha^{24 - m}$ .\\n\\n|  Hparam | RACE | SQuAD | MNLI | Yelp-5  |\\n| --- | --- | --- | --- | --- |\\n|  Dropout |  | 0.1 |  |   |\\n|  Attention dropout |  | 0.1 |  |   |\\n|  Max sequence length | 512 | 512 | 128 | 512  |\\n|  Batch size | 32 | 48 | 128 | 128  |\\n|  Learning rate | 2e-5 | 3e-5 | 2e-5 | 1e-5  |\\n|  Number of steps | 12K | 8K | 10K | 10K  |\\n|  Learning rate decay |  | linear |  |   |\\n|  Weight decay |  | 0.01 |  |   |\\n|  Adam epsilon | 1e-6 | 1e-6 | 1e-6 | 1e-6  |\\n|  Layer-wise lr decay | 1.0 | 0.75 | 1.0 | 1.0  |\\n\\nTable 8: Hyperparameters for finetuning.\\n\\n# A.5 Discussion and Analysis\\n\\n# A.5.1 Comparison with BERT\\n\\nTo prove a general point beyond one example, we now turn to more formal expressions. Inspired by previous work [37], given a sequence  $\\\\mathbf{x} = [x_1,\\\\dots ,x_T]$ , we define a set of target-context pairs of interest,  $\\\\mathcal{I} = \\\\{(x,\\\\mathcal{U})\\\\}$ , where  $\\\\mathcal{U}$  is a set of tokens in  $\\\\mathbf{x}$  that form a context of  $x$ . Intuitively, we want the model to learn the dependency of  $x$  on  $\\\\mathcal{U}$  through a pretraining loss term  $\\\\log p(x\\\\mid \\\\mathcal{U})$ . For example, given the above sentence, the pairs of interest  $\\\\mathcal{I}$  could be instantiated as:\\n\\n$$\\n\\\\mathcal {I} = \\\\left\\\\{\\\\left(x = \\\\operatorname {Y o r k}, \\\\mathcal {U} = \\\\{\\\\text {N e w} \\\\}\\\\right), \\\\left(x = \\\\operatorname {Y o r k}, \\\\mathcal {U} = \\\\{\\\\text {c i t y} \\\\}\\\\right), \\\\left(x = \\\\operatorname {Y o r k}, \\\\mathcal {U} = \\\\{\\\\text {N e w}, \\\\text {c i t y} \\\\}\\\\right), \\\\dots \\\\right\\\\}.\\n$$\\n\\nNote that  $\\\\mathcal{I}$  is merely a virtual notion without unique ground truth, and our analysis will hold regardless of how  $\\\\mathcal{I}$  is instantiated.\\n\\nGiven a set of target tokens  $\\\\mathcal{T}$  and a set of non-target tokens  $\\\\mathcal{N} = \\\\mathbf{x} \\\\backslash \\\\mathcal{T}$ , BERT and XLNet both maximize  $\\\\log p(\\\\mathcal{T} \\\\mid \\\\mathcal{N})$  but with different formulations:\\n\\n$$\\n\\\\mathcal {J} _ {\\\\mathrm {B E R T}} = \\\\sum_ {x \\\\in \\\\mathcal {T}} \\\\log p (x \\\\mid \\\\mathcal {N}); \\\\quad \\\\mathcal {J} _ {\\\\mathrm {X L N e t}} = \\\\sum_ {x \\\\in \\\\mathcal {T}} \\\\log p (x \\\\mid \\\\mathcal {N} \\\\cup \\\\mathcal {T} _ {&lt;   x})\\n$$\\n\\nwhere  $\\\\mathcal{T}_{&lt; x}$  denote tokens in  $\\\\mathcal{T}$  that have a factorization order prior to  $x$ . Both objectives consist of multiple loss terms in the form of  $\\\\log p(x \\\\mid \\\\mathcal{V}_x)$ . Intuitively, if there exists a target-context pair  $(x, \\\\mathcal{U}) \\\\in \\\\mathcal{I}$  such that  $\\\\mathcal{U} \\\\subseteq \\\\mathcal{V}_x$ , then the loss term  $\\\\log p(x \\\\mid \\\\mathcal{V}_x)$  provides a training signal to the dependency between  $x$  and  $\\\\mathcal{U}$ . For convenience, we say a target-context pair  $(x, \\\\mathcal{U}) \\\\in \\\\mathcal{I}$  is covered by a model (objective) if  $\\\\mathcal{U} \\\\subseteq \\\\mathcal{V}_x$ .\\n\\nGiven the definition, let\\'s consider two cases:\\n\\n- If  $\\\\mathcal{U} \\\\subseteq \\\\mathcal{N}$ , the dependency  $(x, \\\\mathcal{U})$  is covered by both BERT and XLNet.\\n- If  $\\\\mathcal{U} \\\\subseteq \\\\mathcal{N} \\\\cup \\\\mathcal{T}_{&lt;x}$  and  $\\\\mathcal{U} \\\\cap \\\\mathcal{T}_{&lt;x} \\\\neq \\\\emptyset$ , the dependency can only be covered by XLNet but not BERT. As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet objective contains more effective training signals, which empirically leads to better performance in Section 3.\\n\\n# A.5.2 Comparison with Language Modeling\\n\\nBorrowing examples and notations from Section A.5.1, a standard AR language model like GPT [28] is only able to cover the dependency  $(x = \\\\mathrm{York},\\\\mathcal{U} = \\\\{\\\\mathrm{New}\\\\})$  but not  $(x = \\\\mathrm{New},\\\\mathcal{U} = \\\\{\\\\mathrm{York}\\\\})$ . XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a limitation of AR language modeling can be critical in real-world applications. For example, consider a span extraction question answering task with the context \"Thom Yorke is the singer of Radiohead\" and the question \"Who is the singer of Radiohead\". The representations of \"Thom Yorke\" are not dependent on \"Radiohead\" with AR language modeling and thus they will not be chosen as the answer by the standard approach that employs softmax over all token representations. More formally, consider a context-target pair  $(x,\\\\mathcal{U})$ :\\n\\n- If  $\\\\mathcal{U} \\\\nsubseteq \\\\mathcal{T}_{&lt;x}$ , where  $\\\\mathcal{T}_{&lt;x}$  denotes the tokens prior to  $x$  in the original sequence, AR language modeling is not able to cover the dependency.\\n\\n- In comparison, XLNet is able to cover all dependencies in expectation.\\n\\nApproaches like ELMo [27] concatenate forward and backward language models in a shallow manner, which is not sufficient for modeling deep interactions between the two directions.\\n\\n# A.5.3 Bridging the Gap Between Language Modeling and Pretraining\\n\\nWith a deep root in density estimation $^{4}$  [4, 32, 24], language modeling has been a rapidly-developing research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining due to the lack of the capability of bidirectional context modeling, as analyzed in Section A.5.2. It has even been challenged by some machine learning practitioners whether language modeling is a meaningful pursuit if it does not directly improve downstream tasks $^{5}$ . XLNet generalizes language modeling and bridges such a gap. As a result, it further \"justifies\" language modeling research. Moreover, it becomes possible to leverage the rapid progress of language modeling research for pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness of the latest language modeling progress.\\n\\n# A.6 Qualitative Analysis of Attention Patterns\\n\\nWe compare the attention pattern of BERT and XLNet without finetuning. Firstly, we found 4 typical patterns shared by both, as shown in Fig. 2.\\n\\n![img-2.jpeg](img-2.jpeg)\\n(a) Content stripes\\n\\n![img-3.jpeg](img-3.jpeg)\\n(b) Local/Self focus\\n\\n![img-4.jpeg](img-4.jpeg)\\n(c) Two segments\\nFigure 2: Attention patterns shared by XLNet and BERT. Rows and columns represent query and key respectively.\\n\\n![img-5.jpeg](img-5.jpeg)\\n(d) Content-based symmetry\\n\\nMore interestingly, in Fig. 3, we present 3 patterns that only appear in XLNet but not BERT: (a) The self-exclusion pattern attends to all other tokens but itself, probably offering a fast way to gather global information; (b) The relative-stride pattern attends to positions every a few stride apart relative to the query position; (c) The one-side masked pattern is very similar to the lower-left part of Fig. 1-(d), with the upper-right triangle masked out. It seems that the model learns not to attend the relative right half. Note that all these three unique patterns involve the relative positions rather than absolute ones, and hence are likely enabled by the \"relative attention\" mechanism in XLNet. We conjecture these unique patterns contribute to the performance advantage of XLNet. On the other hand, the proposed permutation LM objective mostly contributes to a better data efficiency, whose effects may not be obvious from qualitative visualization.\\n\\n![img-6.jpeg](img-6.jpeg)\\n(a) Self exclusion\\n\\n![img-7.jpeg](img-7.jpeg)\\n(b) Relative stride\\nFigure 3: Attention patterns that appear only in XLNet. Rows and columns represent query and key respectively.\\n\\n![img-8.jpeg](img-8.jpeg)\\n(c) One-side masked\\n\\n![img-9.jpeg](img-9.jpeg)\\nFactorization order:  $3 \\\\rightarrow 2 \\\\rightarrow 4 \\\\rightarrow 1$\\n\\n![img-10.jpeg](img-10.jpeg)\\nFactorization order:  $2 \\\\rightarrow 4 \\\\rightarrow 3 \\\\rightarrow 1$\\n\\n![img-11.jpeg](img-11.jpeg)\\nFactorization order:  $1 \\\\rightarrow 4 \\\\rightarrow 2 \\\\rightarrow 3$\\n\\n![img-12.jpeg](img-12.jpeg)\\nFactorization order:  $4 \\\\rightarrow 3 \\\\rightarrow 1 \\\\rightarrow 2$\\nFigure 4: Illustration of the permutation language modeling objective for predicting  $x_{3}$  given the same input sequence  $\\\\mathbf{x}$  but with different factorization orders.\\n\\n# A.7 Visualizing Memory and Permutation\\n\\nIn this section, we provide a detailed visualization of the proposed permutation language modeling objective, including the mechanism of reusing memory (aka the recurrence mechanism), how we use attention masks to permute the factorization order, and the difference of the two attention streams.\\n\\nAs shown in Figure 5 and 6, given the current position  $z_{t}$ , the attention mask is decided by the permutation (or factorization order)  $\\\\mathbf{z}$  such that only tokens the occur before  $z_{t}$  in the permutation can be attended; i.e., positions  $z_{i}$  with  $i &lt; t$ . Moreover, comparing Figure 5 and 6, we can see how the query stream and the content stream work differently with a specific permutation through attention masks. The main difference is that the query stream cannot do self-attention and does not have access to the token at the position, while the content stream performs normal self-attention.\\n\\n![img-13.jpeg](img-13.jpeg)\\nJoint View of the Content Stream (Factorization order:  $3 \\\\rightarrow 2 \\\\rightarrow 4 \\\\rightarrow 1$ )\\n\\n![img-14.jpeg](img-14.jpeg)\\n\\n![img-15.jpeg](img-15.jpeg)\\nPosition-3 View\\n\\n![img-16.jpeg](img-16.jpeg)\\nPosition-2 View\\n\\nFigure 5: A detailed illustration of the content stream of the proposed objective with both the joint view and split views based on a length-4 sequence under the factorization order [3, 2, 4, 1].\\n![img-17.jpeg](img-17.jpeg)\\nPosition-4 View\\nSplit View of the Content Stream (Factorization order:  $3 \\\\rightarrow 2 \\\\rightarrow 4 \\\\rightarrow 1$ )\\n\\n![img-18.jpeg](img-18.jpeg)\\nPosition-1 View\\n\\nNote that if we ignore the query representation, the computation in this figure is simply the standard self-attention, though with a particular attention mask.\\n\\n![img-19.jpeg](img-19.jpeg)\\nJoint View of the Query Stream (Factorization order:  $3 \\\\rightarrow 2 \\\\rightarrow 4 \\\\rightarrow 1$ )\\n\\n![img-20.jpeg](img-20.jpeg)\\n\\n![img-21.jpeg](img-21.jpeg)\\nPosition-3 View\\n\\n![img-22.jpeg](img-22.jpeg)\\nPosition-2 View\\n\\n![img-23.jpeg](img-23.jpeg)\\nPosition-4 View\\nSplit View of the Query Stream (Factorization order:  $3 \\\\rightarrow 2 \\\\rightarrow 4 \\\\rightarrow 1$ )\\nFigure 6: A detailed illustration of the query stream of the proposed objective with both the joint view and split views based on a length-4 sequence under the factorization order [3, 2, 4, 1]. The dash arrows indicate that the query stream cannot access the token (content) at the same position, but only the location information.\\n\\n![img-24.jpeg](img-24.jpeg)\\nPosition-1 View'), -0.07583801057186745), (Document(id='4af474727685:0', metadata={'chunk_index': 0, 'end_line': 590, 'start_line': 1, 'text': '# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n###### Abstract\\n\\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models *Peters et al. (2018a); Radford et al. (2018)*, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\\n\\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\n\\n## 1 Introduction\\n\\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks *Dai and Le (2015); Peters et al. (2018a); Radford et al. (2018); Howard and Ruder (2018)*. These include sentence-level tasks such as natural language inference *Bowman et al. (2015); Williams et al. (2018)* and paraphrasing *Dolan and Brockett (2005)*, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level *Tjong Kim Sang and De Meulder (2003); Rajpurkar et al. (2016)*.\\n\\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo *Peters et al. (2018a)*, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) *Radford et al. (2018)*, introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\\n\\nWe argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer *Vaswani et al. (2017)*. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fine-tuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.\\n\\nIn this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task *Taylor (1953)*. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked\\n\\nword based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, we also use a “next sentence prediction” task that jointly pre-trains text-pair representations. The contributions of our paper are as follows:\\n\\n- We demonstrate the importance of bidirectional pre-training for language representations. Unlike *Radford et al. (2018)*, which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to *Peters et al. (2018a)*, which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\\n- We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\\n- BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert.\\n\\n## 2 Related Work\\n\\nThere is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.\\n\\n### 2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of words has been an active area of research for decades, including non-neural *Brown et al. (1992); Ando and Zhang (2005); Blitzer et al. (2006)* and neural *Mikolov et al. (2013); Pennington et al. (2014)* methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch *Turian et al. (2010)*. To pre-train word embedding vectors, left-to-right language modeling objectives have been used *Mnih and Hinton (2009)*, as well as objectives to discriminate correct from incorrect words in left and right context *Mikolov et al. (2013)*.\\n\\nThese approaches have been generalized to coarser granularities, such as sentence embeddings *Kiros et al. (2015); Logeswaran and Lee (2018)* or paragraph embeddings *Le and Mikolov (2014)*. To train sentence representations, prior work has used objectives to rank candidate next sentences *Jernite et al. (2017); Logeswaran and Lee (2018)*, left-to-right generation of next sentence words given a representation of the previous sentence *Kiros et al. (2015)*, or denoising auto-encoder derived objectives *Hill et al. (2016)*.\\n\\nELMo and its predecessor *Peters et al. (2017, 2018a)* generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks *Peters et al. (2018a)* including question answering *Rajpurkar et al. (2016)*, sentiment analysis *Socher et al. (2013)*, and named entity recognition *Tjong Kim Sang and De Meulder (2003)*. *Melamud et al. (2016)* proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. *Fedus et al. (2018)* shows that the cloze task can be used to improve the robustness of text generation models.\\n\\n### 2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text *Collobert and Weston (2008)*.\\n\\nMore recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task *Dai and Le (2015); Howard and Ruder (2018); Radford et al. (2018)*. The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT *Radford et al. (2018)* achieved previously state-of-the-art results on many sentence-level tasks from the GLUE benchmark *Wang et al. (2018a)*. Left-to-right language model\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers).\\n\\n![img-1.jpeg](img-1.jpeg)\\n\\ning and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n# 2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n# 3 BERT\\n\\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\\n\\nA distinctive feature of BERT is its unified architecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architecture and the final downstream architecture.\\n\\nModel Architecture BERT\\'s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library. Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"\\n\\nIn this work, we denote the number of layers (i.e., Transformer blocks) as  $L$ , the hidden size as  $H$ , and the number of self-attention heads as  $A$ . We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).\\n\\n$\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. $^4$\\n\\n#### Input/Output Representations\\n\\nTo make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., $\\\\langle$ Question, Answer $\\\\rangle$) in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings *Wu et al. (2016)* with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C\\\\in\\\\mathbb{R}^{H}$, and the final hidden vector for the $i^{\\\\text{th}}$ input token as $T_{i}\\\\in\\\\mathbb{R}^{H}$.\\n\\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.\\n\\n### 3.1 Pre-training BERT\\n\\nUnlike *Peters et al. (2018a)* and *Radford et al. (2018)*, we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\\n\\n#### Task #1: Masked LM\\n\\nIntuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature *Taylor (1953)*. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders *Vincent et al. (2008)*, we only predict the masked words rather than reconstructing the entire input.\\n\\nAlthough this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the $i$-th token is chosen, we replace the $i$-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged $i$-th token 10% of the time. Then, $T_{i}$ will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2.\\n\\n#### Task #2: Next Sentence Prediction (NSP)\\n\\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, $C$ is used for next sentence prediction (NSP). Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\\n\\nThe NSP task is closely related to representation-learning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all parameters to initialize end-task model parameters.\\n\\nPre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.\\n\\n# 3.2 Fine-tuning BERT\\n\\nFine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks—whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.\\n\\nFor each task, we simply plug in the task-specific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and\\n\\n(4) a degenerate text-  $\\\\varnothing$  pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for token-level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.\\n\\nCompared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5.\\n\\n# 4 Experiments\\n\\nIn this section, we present BERT fine-tuning results on 11 NLP tasks.\\n\\n# 4.1 GLUE\\n\\nThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.\\n\\nTo fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector  $C \\\\in \\\\mathbb{R}^H$  corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights  $W \\\\in \\\\mathbb{R}^{K \\\\times H}$ , where  $K$  is the number of labels. We compute a standard classification loss with  $C$  and  $W$ , i.e.,  $\\\\log(\\\\text{softmax}(CW^T))$ .\\n\\n|  System | MNLI-(m/mm)392k | QQP363k | QNLI108k | SST-267k | CoLA8.5k | STS-B5.7k | MRPC3.5k | RTE2.5k | Average-  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Pre-OpenAI SOTA | 80.6/80.1 | 66.1 | 82.3 | 93.2 | 35.0 | 81.0 | 86.0 | 61.7 | 74.0  |\\n|  BiLSTM+ELMo+Attn | 76.4/76.1 | 64.8 | 79.8 | 90.4 | 36.0 | 73.3 | 84.9 | 56.8 | 71.0  |\\n|  OpenAI GPT | 82.1/81.4 | 70.3 | 87.4 | 91.3 | 45.4 | 80.0 | 82.3 | 56.0 | 75.1  |\\n|  BERTBASE | 84.6/83.4 | 71.2 | 90.5 | 93.5 | 52.1 | 85.8 | 88.9 | 66.4 | 79.6  |\\n|  BERTLARGE | 86.7/85.9 | 72.1 | 92.7 | 94.9 | 60.5 | 86.5 | 89.3 | 70.1 | 82.1  |\\n\\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \"Average\" column is slightly different than the official GLUE score, since we exclude the problematic WNLI set. $^{8}$  BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\\n\\nWe use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.\\n\\nResults are presented in Table 1. Both  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  and  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  outperform all systems on all tasks by a substantial margin, obtaining  $4.5\\\\%$  and  $7.0\\\\%$  respective average accuracy improvement over the prior state of the art. Note that  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a  $4.6\\\\%$  absolute accuracy improvement. On the official GLUE leaderboard $^{10}$ ,  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.\\n\\nWe find that  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  significantly outperforms  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2.\\n\\n# 4.2 SQuAD v1.1\\n\\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\\n\\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\\n\\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector  $S \\\\in \\\\mathbb{R}^H$  and an end vector  $E \\\\in \\\\mathbb{R}^H$  during fine-tuning. The probability of word  $i$  being the start of the answer span is computed as a dot product between  $T_i$  and  $S$  followed by a softmax over all of the words in the paragraph:  $P_i = \\\\frac{e^{S \\\\cdot T_i}}{\\\\sum_j e^{S \\\\cdot T_j}}$ . The analogous formula is used for the end of the answer span. The score of a candidate span from position  $i$  to position  $j$  is defined as  $S \\\\cdot T_i + E \\\\cdot T_j$ , and the maximum scoring span where  $j \\\\geq i$  is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\\n\\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available, $^{11}$  and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) before fine-tuning on SQuAD.\\n\\nOur best performing system outperforms the top leaderboard system by  $+1.5$  F1 in ensembling and  $+1.3$  F1 as a single system. In fact, our single BERT model outperforms the top ensemble system in terms of F1 score. Without TriviaQA fine\\n\\n|  System | Dev |   | Test  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  EM | F1 | EM | F1  |\\n|  Top Leaderboard Systems (Dec 10th, 2018)  |   |   |   |   |\\n|  Human | - | - | 82.3 | 91.2  |\\n|  #1 Ensemble - nnet | - | - | 86.0 | 91.7  |\\n|  #2 Ensemble - QANet | - | - | 84.5 | 90.5  |\\n|  Published  |   |   |   |   |\\n|  BiDAF+ELMo (Single) | - | 85.6 | - | 85.8  |\\n|  R.M. Reader (Ensemble) | 81.2 | 87.9 | 82.3 | 88.5  |\\n|  Ours  |   |   |   |   |\\n|  BERTBASE (Single) | 80.8 | 88.5 | - | -  |\\n|  BERTLARGE (Single) | 84.1 | 90.9 | - | -  |\\n|  BERTLARGE (Ensemble) | 85.8 | 91.8 | - | -  |\\n|  BERTLARGE (Sgl.+TriviaQA) | 84.2 | 91.1 | 85.1 | 91.8  |\\n|  BERTLARGE (Ens.+TriviaQA) | 86.2 | 92.2 | 87.4 | 93.2  |\\n\\nTable 2: SQuAD 1.1 results. The BERT ensemble is  $7\\\\mathrm{x}$  systems which use different pre-training checkpoints and fine-tuning seeds.\\n\\n|  System | Dev |   | Test  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  EM | F1 | EM | F1  |\\n|  Top Leaderboard Systems (Dec 10th, 2018)  |   |   |   |   |\\n|  Human | 86.3 | 89.0 | 86.9 | 89.5  |\\n|  #1 Single - MIR-MRC (F-Net) | - | - | 74.8 | 78.0  |\\n|  #2 Single - nnet | - | - | 74.2 | 77.1  |\\n|  Published  |   |   |   |   |\\n|  unet (Ensemble) | - | - | 71.4 | 74.9  |\\n|  SLQA+ (Single) | - |  | 71.4 | 74.4  |\\n|  Ours  |   |   |   |   |\\n|  BERTLARGE (Single) | 78.7 | 81.9 | 80.0 | 83.1  |\\n\\ntuning data, we only lose 0.1-0.4 F1, still outperforming all existing systems by a wide margin. $^{12}$\\n\\n# 4.3 SQuAD v2.0\\n\\nThe SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic.\\n\\nWe use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span:  $s_{\\\\mathrm{null}} = S \\\\cdot C + E \\\\cdot C$  to the score of the best non-null span\\n\\nTable 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components.\\n\\n|  System | Dev | Test  |\\n| --- | --- | --- |\\n|  ESIM+GloVe | 51.9 | 52.7  |\\n|  ESIM+ELMo | 59.1 | 59.2  |\\n|  OpenAI GPT | - | 78.0  |\\n|  BERTBASE | 81.6 | -  |\\n|  BERTLARGE | 86.6 | 86.3  |\\n|  Human (expert)† | - | 85.0  |\\n|  Human (5 annotations)† | - | 88.0  |\\n\\nTable 4: SWAG Dev and Test accuracies. †Human performance is measured with 100 samples, as reported in the SWAG paper.\\n\\n$s_{i,j}^{c} = \\\\max_{j\\\\geq i}S\\\\cdot T_{i} + E\\\\cdot T_{j}$ . We predict a non-null answer when  $s_{i,j}^{c} &gt; s_{\\\\mathrm{null}} + \\\\tau$ , where the threshold  $\\\\tau$  is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.\\n\\nThe results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a  $+5.1$  F1 improvement over the previous best system.\\n\\n# 4.4 SWAG\\n\\nThe Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018). Given a sentence, the task is to choose the most plausible continuation among four choices.\\n\\nWhen fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-specific parameters introduced is a vector whose dot product with the [CLS] token representation  $C$  denotes a score for each choice which is normalized with a softmax layer.\\n\\nWe fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table 4. BERTLARGE outperforms the authors\\' baseline ESIM+ELMo system by  $+27.1\\\\%$  and OpenAI GPT by  $8.3\\\\%$ .\\n\\n# 5 Ablation Studies\\n\\nIn this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional\\n\\n|  Tasks | Dev Set  |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- |\\n|   |  MNLI-m (Acc) | QNLI (Acc) | MRPC (Acc) | SST-2 (Acc) | SQuAD (F1)  |\\n|  BERTBASE | 84.4 | 88.4 | 86.7 | 92.7 | 88.5  |\\n|  No NSP | 83.9 | 84.9 | 86.5 | 92.6 | 87.9  |\\n|  LTR & No NSP | 82.1 | 84.3 | 77.5 | 92.1 | 77.8  |\\n|  + BiLSTM | 82.1 | 84.1 | 75.7 | 91.6 | 84.9  |\\n\\nTable 5: Ablation over the pre-training tasks using the  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  architecture. \"No NSP\" is trained without the next sentence prediction task. \"LTR &amp; No NSP\" is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. \"+\" BiLSTM\" adds a randomly initialized BiLSTM on top of the \"LTR + No NSP\" model during fine-tuning.\\n\\nablation studies can be found in Appendix C.\\n\\n# 5.1 Effect of Pre-training Tasks\\n\\nWe demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$ :\\n\\nNo NSP: A bidirectional model which is trained using the \"masked LM\" (MLM) but without the \"next sentence prediction\" (NSP) task.\\n\\nLTR &amp; No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.\\n\\nWe first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \"No NSP\" to \"LTR &amp; No NSP\". The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\\n\\nFor SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right-side context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the\\n\\nresults are still far worse than those of the pretrained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.\\n\\nWe recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.\\n\\n# 5.2 Effect of Model Size\\n\\nIn this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.\\n\\nResults on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is  $(\\\\mathrm{L} = 6, \\\\mathrm{H} = 1024, \\\\mathrm{A} = 16)$  with 100M parameters for the encoder, and the largest Transformer we have found in the literature is  $(\\\\mathrm{L} = 64, \\\\mathrm{H} = 512, \\\\mathrm{A} = 2)$  with 235M parameters (Al-Rfou et al., 2018). By contrast,  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  contains 110M parameters and  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  contains 340M parameters.\\n\\nIt has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. Peters et al. (2018b) presented\\n\\nmixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements. Both of these prior works used a feature-based approach — we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.\\n\\n# 5.3 Feature-based Approach with BERT\\n\\nAll of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.\\n\\nIn this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF\\n\\n|  Hyperparams |   |   |   | Dev Set Accuracy  |   |   |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  #L | #H | #A | LM (ppl) | MNLI-m | MRPC | SST-2  |\\n|  3 | 768 | 12 | 5.84 | 77.9 | 79.8 | 88.4  |\\n|  6 | 768 | 3 | 5.24 | 80.6 | 82.2 | 90.7  |\\n|  6 | 768 | 12 | 4.68 | 81.9 | 84.8 | 91.3  |\\n|  12 | 768 | 12 | 3.99 | 84.4 | 86.7 | 92.9  |\\n|  12 | 1024 | 16 | 3.54 | 85.7 | 86.9 | 93.3  |\\n|  24 | 1024 | 16 | 3.23 | 86.6 | 87.8 | 93.7  |\\n\\nTable 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of attention heads. \"LM (ppl)\" is the masked LM perplexity of held-out training data.\\n\\n|  System | Dev F1 | Test F1  |\\n| --- | --- | --- |\\n|  ELMo (Peters et al., 2018a) | 95.7 | 92.2  |\\n|  CVT (Clark et al., 2018) | - | 92.6  |\\n|  CSE (Akbik et al., 2018) | - | 93.1  |\\n|  Fine-tuning approach |  |   |\\n|  BERTLARGE | 96.6 | 92.8  |\\n|  BERTBASE | 96.4 | 92.4  |\\n|  Feature-based approach (BERTBASE) |  |   |\\n|  Embeddings | 91.0 | -  |\\n|  Second-to-Last Hidden | 95.6 | -  |\\n|  Last Hidden | 94.9 | -  |\\n|  Weighted Sum Last Four Hidden | 95.9 | -  |\\n|  Concat Last Four Hidden | 96.1 | -  |\\n|  Weighted Sum All 12 Layers | 95.5 | -  |\\n\\nTable 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.\\n\\nTo ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.\\n\\nResults are presented in Table 7.  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine-tuning and feature-based approaches.\\n\\n# 6 Conclusion\\n\\nRecent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.\\n\\nReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817–1853.\\n\\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120–128. Association for Computational Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP. Association for Computational Linguistics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancouver, Canada. Association for Computational Linguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehension. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc Le. 2018. Semi-supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914–1925.\\n\\nRonan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079–3087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via filling in the.. arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehension. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Sontag. 2017. Discourse-based objectives for fast unsupervised sentence representation learning. CoRR, abs/1705.00557.\\n\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294–3302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International Conference on Machine Learning, pages 1188–1196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgenstern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence representations. In International Conference on Learning Representations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.\\n\\nAndriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1081–1088. Curran Associates, Inc.\\n\\nAnkur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP.\\n\\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.\\n\\nMatthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL.\\n\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL.\\n\\nMatthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1499–1509.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.\\n\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR.\\n\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642.\\n\\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv:1810.06638.\\n\\nWilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415–433.\\n\\nErik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL.\\n\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010.\\n\\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform\\n\\nfor natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355.\\n- Wang et al. (2018a) Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics.\\n- Warstadt et al. (2018) Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.\\n- Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel R Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL.\\n- Wu et al. (2014) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.\\n- Vosinski et al. (2014) Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320–3328.\\n- Wei Yu et al. (2018) Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehension. In ICLR.\\n- Zellers et al. (2018) Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n- Zhu et al. (2015) Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.\\n\\n## Appendix A Appendix for “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”\\n\\nWe organize the appendix into three sections:\\n\\n- Additional implementation details for BERT are presented in Appendix A;\\n- Additional details for our experiments are presented in Appendix B; and\\n- Additional ablation studies are presented in Appendix C.\\n\\nWe present additional ablation studies for BERT including:\\n\\n- Effect of Number of Training Steps; and\\n- Ablation for Different Masking Procedures.\\n\\n## Appendix B Additional Details for BERT\\n\\n### B.1 Illustration of the Pre-training Tasks\\n\\nWe provide examples of the pre-training tasks in the following.\\n\\n#### Masked LM and the Masking Procedure\\n\\nAssuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by\\n\\n- 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy $\\\\rightarrow$ my dog is [MASK]\\n- 10% of the time: Replace the word with a random word, e.g., my dog is hairy $\\\\rightarrow$ my dog is apple\\n- 10% of the time: Keep the word unchanged, e.g., my dog is hairy $\\\\rightarrow$ my dog is hairy. The purpose of this is to bias the representation towards the actual observed word.\\n\\nThe advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model’s language understanding capability. In Section C.2, we evaluate the impact this procedure.\\n\\nCompared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\\n\\n![img-4.jpeg](img-4.jpeg)\\n\\n![img-5.jpeg](img-5.jpeg)\\n\\nto converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left-to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost.\\n\\nNext Sentence Prediction The next sentence prediction task can be illustrated in the following examples.\\n\\nInput  $=$  [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]\\n\\nLabel  $=$  IsNext\\n\\nInput  $=$  [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]\\n\\nLabel  $=$  NotNext\\n\\n# A.2 Pre-training Procedure\\n\\nTo generate each training input sequence, we sample two spans of text from the corpus, which we refer to as \"sentences\" even though they are typically much longer than single sentences (but can be shorter also). The first sentence receives the A embedding and the second receives the B embedding.  $50\\\\%$  of the time B is the actual next sentence that follows A and  $50\\\\%$  of the time it is a random sentence, which is done for the \"next sentence prediction\" task. They are sampled such that the combined length is  $\\\\leq 512$  tokens. The LM masking is applied after WordPiece tokenization with a uniform masking rate of  $15\\\\%$ , and no special consideration given to partial word pieces.\\n\\nWe train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40\\n\\nepochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4,  $\\\\beta_{1} = 0.9$ ,  $\\\\beta_{2} = 0.999$ , L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We use a dropout probability of 0.1 on all layers. We use a gelu activation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.\\n\\nTraining of  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total). Training of  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  was performed on 16 Cloud TPUs (64 TPU chips total). Each pretraining took 4 days to complete.\\n\\nLonger sequences are disproportionately expensive because attention is quadratic to the sequence length. To speed up pretraining in our experiments, we pre-train the model with sequence length of 128 for  $90\\\\%$  of the steps. Then, we train the rest  $10\\\\%$  of the steps of sequence of 512 to learn the positional embeddings.\\n\\n# A.3 Fine-tuning Procedure\\n\\nFor fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of training epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-specific, but we found the following range of possible values to work well across all tasks:\\n\\n- Batch size: 16, 32\\n\\n- Learning rate (Adam): 5e-5, 3e-5, 2e-5\\n- Number of epochs: 2, 3, 4\\n\\nWe also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set.\\n\\n### A.4 Comparison of BERT, ELMo ,and OpenAI GPT\\n\\nHere we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons between the model architectures are shown visually in Figure 3. Note that in addition to the architecture differences, BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\\n\\nThe most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text corpus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pre-training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:\\n\\n- GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words).\\n- GPT uses a sentence separator ([SEP]) and classifier token ([CLS]) which are only introduced at fine-tuning time; BERT learns [SEP], [CLS] and sentence A/B embeddings during pre-training.\\n- GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.\\n- GPT used the same learning rate of 5e-5 for all fine-tuning experiments; BERT chooses a task-specific fine-tuning learning rate which performs the best on the development set.\\n\\nTo isolate the effect of these differences, we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable.\\n\\n### A.5 Illustrations of Fine-tuning on Different Tasks\\n\\nThe illustration of fine-tuning BERT on different tasks can be seen in Figure 4. Our task-specific models are formed by incorporating BERT with one additional output layer, so a minimal number of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the figure, $E$ represents the input embedding, $T_{i}$ represents the contextual representation of token $i$, [CLS] is the special symbol for classification output, and [SEP] is the special symbol to separate non-consecutive token sequences.\\n\\n## Appendix B Detailed Experimental Setup\\n\\n### B.1 Detailed Descriptions for the GLUE Benchmark Experiments.\\n\\nOur GLUE results in Table1 are obtained from https://gluebenchmark.com/leaderboard and https://blog.openai.com/language-unsupervised. The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in *Wang et al. (2018a)*:\\n\\n#### MNLI\\n\\nMulti-Genre Natural Language Inference is a large-scale, crowdsourced entailment classification task *Williams et al. (2018)*. Given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to the first one.\\n\\n#### QQP\\n\\nQuora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent *Chen et al. (2018)*.\\n\\n#### QNLI\\n\\nQuestion Natural Language Inference is a version of the Stanford Question Answering Dataset *Rajpurkar et al. (2016)* which has been converted to a binary classification task *Wang et al. (2018a)*. The positive examples are (question, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.\\n\\n![img-6.jpeg](img-6.jpeg)\\n(a) Sentence Pair Classification Tasks: MNLI, QQP, QNLI, STS-B, MRPC, RTE, SWAG\\n\\n![img-7.jpeg](img-7.jpeg)\\n(b) Single Sentence Classification Tasks: SST-2, CoLA\\n\\n![img-8.jpeg](img-8.jpeg)\\n(c) Question Answering Tasks: SQuAD v1.1\\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\n![img-9.jpeg](img-9.jpeg)\\n(d) Single Sentence Tagging Tasks: CoNLL-2003 NER\\n\\nSST-2 The Stanford Sentiment Treebank is a binary single-sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically \"acceptable\" or not (Warstadt et al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources (Cer et al., 2017). They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semantically equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a binary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural language inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, $^{15}$  and every trained system that\\'s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore exclude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma\\n\\njority class.\\n\\n# C Additional Ablation Studies\\n\\n# C.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after fine-tuning from a checkpoint that has been pre-trained for  $k$  steps. This allows us to answer the following questions:\\n\\n1. Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high fine-tuning accuracy?\\n\\nAnswer: Yes,  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  achieves almost  $1.0\\\\%$  additional accuracy on MNLI when trained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge slower than LTR pre-training, since only  $15\\\\%$  of words are predicted in each batch rather than every word?\\n\\nAnswer: The MLM model does converge slightly slower than the LTR model. However, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately.\\n\\n# C.2 Ablation for Different Masking Procedures\\n\\nIn Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies.\\n\\n![img-10.jpeg](img-10.jpeg)\\nFigure 5: Ablation over number of training steps. This shows the MNLI accuracy after fine-tuning, starting from model parameters that have been pre-trained for  $k$  steps. The x-axis is the value of  $k$ .\\n\\nNote that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never appears during the fine-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both fine-tuning and feature-based approaches, as we expect the mismatch will be amplified for the feature-based approach as the model will not have the chance to adjust the representations.\\n\\n|  Masking Rates |   |   | Dev Set Results  |   |   |\\n| --- | --- | --- | --- | --- | --- |\\n|  MASK | SAME | RND | MNLI | NER  |   |\\n|   |   |   |  Fine-tune | Fine-tune | Feature-based  |\\n|  80% | 10% | 10% | 84.2 | 95.4 | 94.9  |\\n|  100% | 0% | 0% | 84.3 | 94.9 | 94.0  |\\n|  80% | 0% | 20% | 84.1 | 95.2 | 94.6  |\\n|  80% | 20% | 0% | 84.4 | 95.2 | 94.7  |\\n|  0% | 20% | 80% | 83.7 | 94.8 | 94.6  |\\n|  0% | 0% | 100% | 83.6 | 94.9 | 94.6  |\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token.\\n\\nThe numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training (BERT uses  $80\\\\%$ ,  $10\\\\%$ ,  $10\\\\%$ ). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3.\\n\\nFrom the table it can be seen that fine-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strategy was problematic when applying the feature-based approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well.', 'doc_id': '4af474727685'}, page_content='# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n###### Abstract\\n\\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models *Peters et al. (2018a); Radford et al. (2018)*, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\\n\\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\n\\n## 1 Introduction\\n\\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks *Dai and Le (2015); Peters et al. (2018a); Radford et al. (2018); Howard and Ruder (2018)*. These include sentence-level tasks such as natural language inference *Bowman et al. (2015); Williams et al. (2018)* and paraphrasing *Dolan and Brockett (2005)*, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level *Tjong Kim Sang and De Meulder (2003); Rajpurkar et al. (2016)*.\\n\\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo *Peters et al. (2018a)*, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) *Radford et al. (2018)*, introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\\n\\nWe argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer *Vaswani et al. (2017)*. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fine-tuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.\\n\\nIn this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task *Taylor (1953)*. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked\\n\\nword based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, we also use a “next sentence prediction” task that jointly pre-trains text-pair representations. The contributions of our paper are as follows:\\n\\n- We demonstrate the importance of bidirectional pre-training for language representations. Unlike *Radford et al. (2018)*, which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to *Peters et al. (2018a)*, which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\\n- We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\\n- BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert.\\n\\n## 2 Related Work\\n\\nThere is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.\\n\\n### 2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of words has been an active area of research for decades, including non-neural *Brown et al. (1992); Ando and Zhang (2005); Blitzer et al. (2006)* and neural *Mikolov et al. (2013); Pennington et al. (2014)* methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch *Turian et al. (2010)*. To pre-train word embedding vectors, left-to-right language modeling objectives have been used *Mnih and Hinton (2009)*, as well as objectives to discriminate correct from incorrect words in left and right context *Mikolov et al. (2013)*.\\n\\nThese approaches have been generalized to coarser granularities, such as sentence embeddings *Kiros et al. (2015); Logeswaran and Lee (2018)* or paragraph embeddings *Le and Mikolov (2014)*. To train sentence representations, prior work has used objectives to rank candidate next sentences *Jernite et al. (2017); Logeswaran and Lee (2018)*, left-to-right generation of next sentence words given a representation of the previous sentence *Kiros et al. (2015)*, or denoising auto-encoder derived objectives *Hill et al. (2016)*.\\n\\nELMo and its predecessor *Peters et al. (2017, 2018a)* generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks *Peters et al. (2018a)* including question answering *Rajpurkar et al. (2016)*, sentiment analysis *Socher et al. (2013)*, and named entity recognition *Tjong Kim Sang and De Meulder (2003)*. *Melamud et al. (2016)* proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. *Fedus et al. (2018)* shows that the cloze task can be used to improve the robustness of text generation models.\\n\\n### 2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text *Collobert and Weston (2008)*.\\n\\nMore recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task *Dai and Le (2015); Howard and Ruder (2018); Radford et al. (2018)*. The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT *Radford et al. (2018)* achieved previously state-of-the-art results on many sentence-level tasks from the GLUE benchmark *Wang et al. (2018a)*. Left-to-right language model\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers).\\n\\n![img-1.jpeg](img-1.jpeg)\\n\\ning and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n# 2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n# 3 BERT\\n\\nWe introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\\n\\nA distinctive feature of BERT is its unified architecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architecture and the final downstream architecture.\\n\\nModel Architecture BERT\\'s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library. Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"\\n\\nIn this work, we denote the number of layers (i.e., Transformer blocks) as  $L$ , the hidden size as  $H$ , and the number of self-attention heads as  $A$ . We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).\\n\\n$\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. $^4$\\n\\n#### Input/Output Representations\\n\\nTo make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., $\\\\langle$ Question, Answer $\\\\rangle$) in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings *Wu et al. (2016)* with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C\\\\in\\\\mathbb{R}^{H}$, and the final hidden vector for the $i^{\\\\text{th}}$ input token as $T_{i}\\\\in\\\\mathbb{R}^{H}$.\\n\\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.\\n\\n### 3.1 Pre-training BERT\\n\\nUnlike *Peters et al. (2018a)* and *Radford et al. (2018)*, we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\\n\\n#### Task #1: Masked LM\\n\\nIntuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature *Taylor (1953)*. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders *Vincent et al. (2008)*, we only predict the masked words rather than reconstructing the entire input.\\n\\nAlthough this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the $i$-th token is chosen, we replace the $i$-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged $i$-th token 10% of the time. Then, $T_{i}$ will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2.\\n\\n#### Task #2: Next Sentence Prediction (NSP)\\n\\nMany important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, $C$ is used for next sentence prediction (NSP). Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.\\n\\n![img-2.jpeg](img-2.jpeg)\\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\\n\\nThe NSP task is closely related to representation-learning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all parameters to initialize end-task model parameters.\\n\\nPre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.\\n\\n# 3.2 Fine-tuning BERT\\n\\nFine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks—whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.\\n\\nFor each task, we simply plug in the task-specific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and\\n\\n(4) a degenerate text-  $\\\\varnothing$  pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for token-level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.\\n\\nCompared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5.\\n\\n# 4 Experiments\\n\\nIn this section, we present BERT fine-tuning results on 11 NLP tasks.\\n\\n# 4.1 GLUE\\n\\nThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.\\n\\nTo fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector  $C \\\\in \\\\mathbb{R}^H$  corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights  $W \\\\in \\\\mathbb{R}^{K \\\\times H}$ , where  $K$  is the number of labels. We compute a standard classification loss with  $C$  and  $W$ , i.e.,  $\\\\log(\\\\text{softmax}(CW^T))$ .\\n\\n|  System | MNLI-(m/mm)392k | QQP363k | QNLI108k | SST-267k | CoLA8.5k | STS-B5.7k | MRPC3.5k | RTE2.5k | Average-  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Pre-OpenAI SOTA | 80.6/80.1 | 66.1 | 82.3 | 93.2 | 35.0 | 81.0 | 86.0 | 61.7 | 74.0  |\\n|  BiLSTM+ELMo+Attn | 76.4/76.1 | 64.8 | 79.8 | 90.4 | 36.0 | 73.3 | 84.9 | 56.8 | 71.0  |\\n|  OpenAI GPT | 82.1/81.4 | 70.3 | 87.4 | 91.3 | 45.4 | 80.0 | 82.3 | 56.0 | 75.1  |\\n|  BERTBASE | 84.6/83.4 | 71.2 | 90.5 | 93.5 | 52.1 | 85.8 | 88.9 | 66.4 | 79.6  |\\n|  BERTLARGE | 86.7/85.9 | 72.1 | 92.7 | 94.9 | 60.5 | 86.5 | 89.3 | 70.1 | 82.1  |\\n\\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \"Average\" column is slightly different than the official GLUE score, since we exclude the problematic WNLI set. $^{8}$  BERT and OpenAI GPT are single-model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\\n\\nWe use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  we found that fine-tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.\\n\\nResults are presented in Table 1. Both  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  and  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  outperform all systems on all tasks by a substantial margin, obtaining  $4.5\\\\%$  and  $7.0\\\\%$  respective average accuracy improvement over the prior state of the art. Note that  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a  $4.6\\\\%$  absolute accuracy improvement. On the official GLUE leaderboard $^{10}$ ,  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.\\n\\nWe find that  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  significantly outperforms  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2.\\n\\n# 4.2 SQuAD v1.1\\n\\nThe Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\\n\\nWikipedia containing the answer, the task is to predict the answer text span in the passage.\\n\\nAs shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector  $S \\\\in \\\\mathbb{R}^H$  and an end vector  $E \\\\in \\\\mathbb{R}^H$  during fine-tuning. The probability of word  $i$  being the start of the answer span is computed as a dot product between  $T_i$  and  $S$  followed by a softmax over all of the words in the paragraph:  $P_i = \\\\frac{e^{S \\\\cdot T_i}}{\\\\sum_j e^{S \\\\cdot T_j}}$ . The analogous formula is used for the end of the answer span. The score of a candidate span from position  $i$  to position  $j$  is defined as  $S \\\\cdot T_i + E \\\\cdot T_j$ , and the maximum scoring span where  $j \\\\geq i$  is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\\n\\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available, $^{11}$  and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) before fine-tuning on SQuAD.\\n\\nOur best performing system outperforms the top leaderboard system by  $+1.5$  F1 in ensembling and  $+1.3$  F1 as a single system. In fact, our single BERT model outperforms the top ensemble system in terms of F1 score. Without TriviaQA fine\\n\\n|  System | Dev |   | Test  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  EM | F1 | EM | F1  |\\n|  Top Leaderboard Systems (Dec 10th, 2018)  |   |   |   |   |\\n|  Human | - | - | 82.3 | 91.2  |\\n|  #1 Ensemble - nnet | - | - | 86.0 | 91.7  |\\n|  #2 Ensemble - QANet | - | - | 84.5 | 90.5  |\\n|  Published  |   |   |   |   |\\n|  BiDAF+ELMo (Single) | - | 85.6 | - | 85.8  |\\n|  R.M. Reader (Ensemble) | 81.2 | 87.9 | 82.3 | 88.5  |\\n|  Ours  |   |   |   |   |\\n|  BERTBASE (Single) | 80.8 | 88.5 | - | -  |\\n|  BERTLARGE (Single) | 84.1 | 90.9 | - | -  |\\n|  BERTLARGE (Ensemble) | 85.8 | 91.8 | - | -  |\\n|  BERTLARGE (Sgl.+TriviaQA) | 84.2 | 91.1 | 85.1 | 91.8  |\\n|  BERTLARGE (Ens.+TriviaQA) | 86.2 | 92.2 | 87.4 | 93.2  |\\n\\nTable 2: SQuAD 1.1 results. The BERT ensemble is  $7\\\\mathrm{x}$  systems which use different pre-training checkpoints and fine-tuning seeds.\\n\\n|  System | Dev |   | Test  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  EM | F1 | EM | F1  |\\n|  Top Leaderboard Systems (Dec 10th, 2018)  |   |   |   |   |\\n|  Human | 86.3 | 89.0 | 86.9 | 89.5  |\\n|  #1 Single - MIR-MRC (F-Net) | - | - | 74.8 | 78.0  |\\n|  #2 Single - nnet | - | - | 74.2 | 77.1  |\\n|  Published  |   |   |   |   |\\n|  unet (Ensemble) | - | - | 71.4 | 74.9  |\\n|  SLQA+ (Single) | - |  | 71.4 | 74.4  |\\n|  Ours  |   |   |   |   |\\n|  BERTLARGE (Single) | 78.7 | 81.9 | 80.0 | 83.1  |\\n\\ntuning data, we only lose 0.1-0.4 F1, still outperforming all existing systems by a wide margin. $^{12}$\\n\\n# 4.3 SQuAD v2.0\\n\\nThe SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic.\\n\\nWe use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span:  $s_{\\\\mathrm{null}} = S \\\\cdot C + E \\\\cdot C$  to the score of the best non-null span\\n\\nTable 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components.\\n\\n|  System | Dev | Test  |\\n| --- | --- | --- |\\n|  ESIM+GloVe | 51.9 | 52.7  |\\n|  ESIM+ELMo | 59.1 | 59.2  |\\n|  OpenAI GPT | - | 78.0  |\\n|  BERTBASE | 81.6 | -  |\\n|  BERTLARGE | 86.6 | 86.3  |\\n|  Human (expert)† | - | 85.0  |\\n|  Human (5 annotations)† | - | 88.0  |\\n\\nTable 4: SWAG Dev and Test accuracies. †Human performance is measured with 100 samples, as reported in the SWAG paper.\\n\\n$s_{i,j}^{c} = \\\\max_{j\\\\geq i}S\\\\cdot T_{i} + E\\\\cdot T_{j}$ . We predict a non-null answer when  $s_{i,j}^{c} &gt; s_{\\\\mathrm{null}} + \\\\tau$ , where the threshold  $\\\\tau$  is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.\\n\\nThe results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a  $+5.1$  F1 improvement over the previous best system.\\n\\n# 4.4 SWAG\\n\\nThe Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018). Given a sentence, the task is to choose the most plausible continuation among four choices.\\n\\nWhen fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-specific parameters introduced is a vector whose dot product with the [CLS] token representation  $C$  denotes a score for each choice which is normalized with a softmax layer.\\n\\nWe fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table 4. BERTLARGE outperforms the authors\\' baseline ESIM+ELMo system by  $+27.1\\\\%$  and OpenAI GPT by  $8.3\\\\%$ .\\n\\n# 5 Ablation Studies\\n\\nIn this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional\\n\\n|  Tasks | Dev Set  |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- |\\n|   |  MNLI-m (Acc) | QNLI (Acc) | MRPC (Acc) | SST-2 (Acc) | SQuAD (F1)  |\\n|  BERTBASE | 84.4 | 88.4 | 86.7 | 92.7 | 88.5  |\\n|  No NSP | 83.9 | 84.9 | 86.5 | 92.6 | 87.9  |\\n|  LTR & No NSP | 82.1 | 84.3 | 77.5 | 92.1 | 77.8  |\\n|  + BiLSTM | 82.1 | 84.1 | 75.7 | 91.6 | 84.9  |\\n\\nTable 5: Ablation over the pre-training tasks using the  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  architecture. \"No NSP\" is trained without the next sentence prediction task. \"LTR &amp; No NSP\" is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. \"+\" BiLSTM\" adds a randomly initialized BiLSTM on top of the \"LTR + No NSP\" model during fine-tuning.\\n\\nablation studies can be found in Appendix C.\\n\\n# 5.1 Effect of Pre-training Tasks\\n\\nWe demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$ :\\n\\nNo NSP: A bidirectional model which is trained using the \"masked LM\" (MLM) but without the \"next sentence prediction\" (NSP) task.\\n\\nLTR &amp; No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.\\n\\nWe first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \"No NSP\" to \"LTR &amp; No NSP\". The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\\n\\nFor SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right-side context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the\\n\\nresults are still far worse than those of the pretrained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.\\n\\nWe recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.\\n\\n# 5.2 Effect of Model Size\\n\\nIn this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.\\n\\nResults on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is  $(\\\\mathrm{L} = 6, \\\\mathrm{H} = 1024, \\\\mathrm{A} = 16)$  with 100M parameters for the encoder, and the largest Transformer we have found in the literature is  $(\\\\mathrm{L} = 64, \\\\mathrm{H} = 512, \\\\mathrm{A} = 2)$  with 235M parameters (Al-Rfou et al., 2018). By contrast,  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  contains 110M parameters and  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  contains 340M parameters.\\n\\nIt has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. Peters et al. (2018b) presented\\n\\nmixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements. Both of these prior works used a feature-based approach — we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.\\n\\n# 5.3 Feature-based Approach with BERT\\n\\nAll of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.\\n\\nIn this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF\\n\\n|  Hyperparams |   |   |   | Dev Set Accuracy  |   |   |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  #L | #H | #A | LM (ppl) | MNLI-m | MRPC | SST-2  |\\n|  3 | 768 | 12 | 5.84 | 77.9 | 79.8 | 88.4  |\\n|  6 | 768 | 3 | 5.24 | 80.6 | 82.2 | 90.7  |\\n|  6 | 768 | 12 | 4.68 | 81.9 | 84.8 | 91.3  |\\n|  12 | 768 | 12 | 3.99 | 84.4 | 86.7 | 92.9  |\\n|  12 | 1024 | 16 | 3.54 | 85.7 | 86.9 | 93.3  |\\n|  24 | 1024 | 16 | 3.23 | 86.6 | 87.8 | 93.7  |\\n\\nTable 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of attention heads. \"LM (ppl)\" is the masked LM perplexity of held-out training data.\\n\\n|  System | Dev F1 | Test F1  |\\n| --- | --- | --- |\\n|  ELMo (Peters et al., 2018a) | 95.7 | 92.2  |\\n|  CVT (Clark et al., 2018) | - | 92.6  |\\n|  CSE (Akbik et al., 2018) | - | 93.1  |\\n|  Fine-tuning approach |  |   |\\n|  BERTLARGE | 96.6 | 92.8  |\\n|  BERTBASE | 96.4 | 92.4  |\\n|  Feature-based approach (BERTBASE) |  |   |\\n|  Embeddings | 91.0 | -  |\\n|  Second-to-Last Hidden | 95.6 | -  |\\n|  Last Hidden | 94.9 | -  |\\n|  Weighted Sum Last Four Hidden | 95.9 | -  |\\n|  Concat Last Four Hidden | 96.1 | -  |\\n|  Weighted Sum All 12 Layers | 95.5 | -  |\\n\\nTable 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.\\n\\nTo ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.\\n\\nResults are presented in Table 7.  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine-tuning and feature-based approaches.\\n\\n# 6 Conclusion\\n\\nRecent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.\\n\\nReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817–1853.\\n\\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120–128. Association for Computational Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP. Association for Computational Linguistics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancouver, Canada. Association for Computational Linguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehension. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc Le. 2018. Semi-supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914–1925.\\n\\nRonan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079–3087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via filling in the.. arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehension. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Sontag. 2017. Discourse-based objectives for fast unsupervised sentence representation learning. CoRR, abs/1705.00557.\\n\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294–3302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International Conference on Machine Learning, pages 1188–1196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgenstern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence representations. In International Conference on Learning Representations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.\\n\\nAndriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1081–1088. Curran Associates, Inc.\\n\\nAnkur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP.\\n\\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.\\n\\nMatthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL.\\n\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL.\\n\\nMatthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1499–1509.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.\\n\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR.\\n\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642.\\n\\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv:1810.06638.\\n\\nWilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415–433.\\n\\nErik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL.\\n\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010.\\n\\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform\\n\\nfor natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355.\\n- Wang et al. (2018a) Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics.\\n- Warstadt et al. (2018) Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.\\n- Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel R Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL.\\n- Wu et al. (2014) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.\\n- Vosinski et al. (2014) Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320–3328.\\n- Wei Yu et al. (2018) Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehension. In ICLR.\\n- Zellers et al. (2018) Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n- Zhu et al. (2015) Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.\\n\\n## Appendix A Appendix for “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”\\n\\nWe organize the appendix into three sections:\\n\\n- Additional implementation details for BERT are presented in Appendix A;\\n- Additional details for our experiments are presented in Appendix B; and\\n- Additional ablation studies are presented in Appendix C.\\n\\nWe present additional ablation studies for BERT including:\\n\\n- Effect of Number of Training Steps; and\\n- Ablation for Different Masking Procedures.\\n\\n## Appendix B Additional Details for BERT\\n\\n### B.1 Illustration of the Pre-training Tasks\\n\\nWe provide examples of the pre-training tasks in the following.\\n\\n#### Masked LM and the Masking Procedure\\n\\nAssuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by\\n\\n- 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy $\\\\rightarrow$ my dog is [MASK]\\n- 10% of the time: Replace the word with a random word, e.g., my dog is hairy $\\\\rightarrow$ my dog is apple\\n- 10% of the time: Keep the word unchanged, e.g., my dog is hairy $\\\\rightarrow$ my dog is hairy. The purpose of this is to bias the representation towards the actual observed word.\\n\\nThe advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model’s language understanding capability. In Section C.2, we evaluate the impact this procedure.\\n\\nCompared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\\n\\n![img-4.jpeg](img-4.jpeg)\\n\\n![img-5.jpeg](img-5.jpeg)\\n\\nto converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left-to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost.\\n\\nNext Sentence Prediction The next sentence prediction task can be illustrated in the following examples.\\n\\nInput  $=$  [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]\\n\\nLabel  $=$  IsNext\\n\\nInput  $=$  [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]\\n\\nLabel  $=$  NotNext\\n\\n# A.2 Pre-training Procedure\\n\\nTo generate each training input sequence, we sample two spans of text from the corpus, which we refer to as \"sentences\" even though they are typically much longer than single sentences (but can be shorter also). The first sentence receives the A embedding and the second receives the B embedding.  $50\\\\%$  of the time B is the actual next sentence that follows A and  $50\\\\%$  of the time it is a random sentence, which is done for the \"next sentence prediction\" task. They are sampled such that the combined length is  $\\\\leq 512$  tokens. The LM masking is applied after WordPiece tokenization with a uniform masking rate of  $15\\\\%$ , and no special consideration given to partial word pieces.\\n\\nWe train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40\\n\\nepochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4,  $\\\\beta_{1} = 0.9$ ,  $\\\\beta_{2} = 0.999$ , L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We use a dropout probability of 0.1 on all layers. We use a gelu activation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.\\n\\nTraining of  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total). Training of  $\\\\mathrm{BERT}_{\\\\mathrm{LARGE}}$  was performed on 16 Cloud TPUs (64 TPU chips total). Each pretraining took 4 days to complete.\\n\\nLonger sequences are disproportionately expensive because attention is quadratic to the sequence length. To speed up pretraining in our experiments, we pre-train the model with sequence length of 128 for  $90\\\\%$  of the steps. Then, we train the rest  $10\\\\%$  of the steps of sequence of 512 to learn the positional embeddings.\\n\\n# A.3 Fine-tuning Procedure\\n\\nFor fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of training epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-specific, but we found the following range of possible values to work well across all tasks:\\n\\n- Batch size: 16, 32\\n\\n- Learning rate (Adam): 5e-5, 3e-5, 2e-5\\n- Number of epochs: 2, 3, 4\\n\\nWe also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set.\\n\\n### A.4 Comparison of BERT, ELMo ,and OpenAI GPT\\n\\nHere we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons between the model architectures are shown visually in Figure 3. Note that in addition to the architecture differences, BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\\n\\nThe most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text corpus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pre-training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:\\n\\n- GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words).\\n- GPT uses a sentence separator ([SEP]) and classifier token ([CLS]) which are only introduced at fine-tuning time; BERT learns [SEP], [CLS] and sentence A/B embeddings during pre-training.\\n- GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.\\n- GPT used the same learning rate of 5e-5 for all fine-tuning experiments; BERT chooses a task-specific fine-tuning learning rate which performs the best on the development set.\\n\\nTo isolate the effect of these differences, we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable.\\n\\n### A.5 Illustrations of Fine-tuning on Different Tasks\\n\\nThe illustration of fine-tuning BERT on different tasks can be seen in Figure 4. Our task-specific models are formed by incorporating BERT with one additional output layer, so a minimal number of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the figure, $E$ represents the input embedding, $T_{i}$ represents the contextual representation of token $i$, [CLS] is the special symbol for classification output, and [SEP] is the special symbol to separate non-consecutive token sequences.\\n\\n## Appendix B Detailed Experimental Setup\\n\\n### B.1 Detailed Descriptions for the GLUE Benchmark Experiments.\\n\\nOur GLUE results in Table1 are obtained from https://gluebenchmark.com/leaderboard and https://blog.openai.com/language-unsupervised. The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in *Wang et al. (2018a)*:\\n\\n#### MNLI\\n\\nMulti-Genre Natural Language Inference is a large-scale, crowdsourced entailment classification task *Williams et al. (2018)*. Given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to the first one.\\n\\n#### QQP\\n\\nQuora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent *Chen et al. (2018)*.\\n\\n#### QNLI\\n\\nQuestion Natural Language Inference is a version of the Stanford Question Answering Dataset *Rajpurkar et al. (2016)* which has been converted to a binary classification task *Wang et al. (2018a)*. The positive examples are (question, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.\\n\\n![img-6.jpeg](img-6.jpeg)\\n(a) Sentence Pair Classification Tasks: MNLI, QQP, QNLI, STS-B, MRPC, RTE, SWAG\\n\\n![img-7.jpeg](img-7.jpeg)\\n(b) Single Sentence Classification Tasks: SST-2, CoLA\\n\\n![img-8.jpeg](img-8.jpeg)\\n(c) Question Answering Tasks: SQuAD v1.1\\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\n![img-9.jpeg](img-9.jpeg)\\n(d) Single Sentence Tagging Tasks: CoNLL-2003 NER\\n\\nSST-2 The Stanford Sentiment Treebank is a binary single-sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically \"acceptable\" or not (Warstadt et al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources (Cer et al., 2017). They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semantically equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a binary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural language inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, $^{15}$  and every trained system that\\'s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore exclude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma\\n\\njority class.\\n\\n# C Additional Ablation Studies\\n\\n# C.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after fine-tuning from a checkpoint that has been pre-trained for  $k$  steps. This allows us to answer the following questions:\\n\\n1. Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high fine-tuning accuracy?\\n\\nAnswer: Yes,  $\\\\mathrm{BERT}_{\\\\mathrm{BASE}}$  achieves almost  $1.0\\\\%$  additional accuracy on MNLI when trained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge slower than LTR pre-training, since only  $15\\\\%$  of words are predicted in each batch rather than every word?\\n\\nAnswer: The MLM model does converge slightly slower than the LTR model. However, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately.\\n\\n# C.2 Ablation for Different Masking Procedures\\n\\nIn Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies.\\n\\n![img-10.jpeg](img-10.jpeg)\\nFigure 5: Ablation over number of training steps. This shows the MNLI accuracy after fine-tuning, starting from model parameters that have been pre-trained for  $k$  steps. The x-axis is the value of  $k$ .\\n\\nNote that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never appears during the fine-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both fine-tuning and feature-based approaches, as we expect the mismatch will be amplified for the feature-based approach as the model will not have the chance to adjust the representations.\\n\\n|  Masking Rates |   |   | Dev Set Results  |   |   |\\n| --- | --- | --- | --- | --- | --- |\\n|  MASK | SAME | RND | MNLI | NER  |   |\\n|   |   |   |  Fine-tune | Fine-tune | Feature-based  |\\n|  80% | 10% | 10% | 84.2 | 95.4 | 94.9  |\\n|  100% | 0% | 0% | 84.3 | 94.9 | 94.0  |\\n|  80% | 0% | 20% | 84.1 | 95.2 | 94.6  |\\n|  80% | 20% | 0% | 84.4 | 95.2 | 94.7  |\\n|  0% | 20% | 80% | 83.7 | 94.8 | 94.6  |\\n|  0% | 0% | 100% | 83.6 | 94.9 | 94.6  |\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token.\\n\\nThe numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training (BERT uses  $80\\\\%$ ,  $10\\\\%$ ,  $10\\\\%$ ). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3.\\n\\nFrom the table it can be seen that fine-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strategy was problematic when applying the feature-based approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well.'), -0.09538141993267191), (Document(id='6525a2245e91:1', metadata={'chunk_index': 1, 'doc_id': '6525a2245e91', 'end_line': 1439, 'text': '#### 6.2.1 Gender\\n\\nIn our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3. We measured this by feeding the model a context such as \"The detective was a\" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identifiers include midwife, nurse, receptionist, housekeeper etc.\\n\\nWe also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation} was a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent {occupation} was a,\" the majority of occupations had an even higher probability of being followed by a male identifier than a female one than was the case with our original neutral prompt, \"The {occupation} was a\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as $\\\\frac{1}{n_{\\\\text{jobs}}}\\\\sum_{\\\\text{jobs}}\\\\log(\\\\frac{P(\\\\text{female}|\\\\text{Context})}{P(\\\\text{male}|\\\\text{Context})})$ - was $-1.11$ for the Neutral Variant, $-2.14$ for the Competent Variant and $-1.15$ for the Incompetent Variant.\\n\\nWe also carried out pronoun resolution on the Winogender dataset *[x23]* using two methods which further corroborated the model’s tendency to associate most occupations with males. One method measured the models ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \"The advisor met with the advisee because she wanted to get advice about job applications. ’She’ refers to the\" and found the option with the lowest probability between the two possible options (Choices between Occupation Option: advisor; Participant Option: advisee).\\n\\nOccupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models.\\n\\nWe also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-selected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature\\n\\nTable 6.1: Most Biased Descriptive Words in 175B Model\\n\\n|  Top 10 Most Biased Male Descriptive Words with Raw Co-Occurrence Counts | Top 10 Most Biased Female Descriptive Words with Raw Co-Occurrence Counts  |\\n| --- | --- |\\n|  Average Number of Co-Occurrences Across All Words: 17.5 | Average Number of Co-Occurrences Across All Words: 23.9  |\\n|  Large (16) | Optimistic (12)  |\\n|  Mostly (15) | Bubbly (12)  |\\n|  Lazy (14) | Naughty (12)  |\\n|  Fantastic (13) | Easy-going (12)  |\\n|  Eccentric (13) | Petite (10)  |\\n|  Protect (10) | Tight (10)  |\\n|  Jolly (10) | Pregnant (10)  |\\n|  Stable (9) | Gorgeous (28)  |\\n|  Personable (22) | Sucked (8)  |\\n|  Survive (7) | Beautiful (158)  |\\n\\nof 1 and top_p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\", \"She was very\", \"He would be described as\", \"She would be described as\". We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more often described using appearance oriented words such as \"beautiful\" and \"gorgeous\" as compared to men who were more often described using adjectives that span a greater spectrum.\\n\\nTable 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. \"Most Favored\" here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender.\\n\\n# 6.2.2 Race\\n\\nTo investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\", \"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation  $\\\\left[\\\\mathrm{HZJ}^{+}19\\\\right]$ , we explored how race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5, horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).\\n\\nIt should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology.\\n\\nAcross the models we analyzed, \\'Asian\\' had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, \\'Black\\' had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data.\\n\\n![img-26.jpeg](img-26.jpeg)\\nFigure 6.1: Racial Sentiment Across Models\\n\\n|  Religion | Most Favored Descriptive Words  |\\n| --- | --- |\\n|  Atheism | ‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’, ‘Characterized’  |\\n|  Buddhism | ‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘En- lightenment’, ‘Non-Violent’  |\\n|  Christianity | ‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Com- ments’, ‘Officially’  |\\n|  Hinduism | ‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated’, ‘Africa’  |\\n|  Islam | ‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’, ‘Levant’, ‘Allah’, ‘Prophet’  |\\n|  Judaism | ‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’, ‘Game’, ‘Russian’  |\\n\\nTable 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model.\\n\\n# 6.2.3 Religion\\n\\nWe studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length  $\\\\approx 50$  with a temperature of 1 and a top  $p$  of 0.9 for every prompt. Our prompts were of the nature \" {Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words.\\n\\nThe following is an example output from the model:\\n\\n\"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic.\"\\n\\nSimilar to race, we found that the models make associations with religious terms that indicate some propensity to reflect how these terms are sometimes presented in the world. For example, with the religion Islam, we found that words such as ramadan, prophet and mosque co-occurred at a higher rate than for other religions. We also found that words such as violent, terrorism and terrorist co-occurred at a greater rate with Islam than with other religions and were in the top 40 most favored words for Islam in GPT-3.\\n\\n6.2.4 Future Bias and Fairness Challenges\\n\\nWe have presented this preliminary analysis to share some of the biases we found in order to motivate further research, and to highlight the inherent difficulties in characterizing biases in large-scale generative models; we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community. We view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but we recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model attributes to develop informative labels such as Model Cards for Model Reporting from *[MWZ+18]*.\\n\\nUltimately, it is important not just to characterize biases in language systems but to intervene. The literature on this is also extensive *[QMZ+19, HZJ+19]*, so we offer only a few brief comments on future directions specific to large language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for these models. There is room for more research that engages with the literature outside NLP, better articulates normative statements about harm, and engages with the lived experience of communities affected by NLP systems *[BBDI+20]*. Thus, mitigation work should not be approached purely with a metric driven objective to ‘remove’ bias as this has been shown to have blind spots *[GG+19, NvNvD+G+19]* but in a holistic manner.\\n\\n### 6.3 Energy Usage\\n\\nPractical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3 175B consumed several thousand petaflop/s-days of compute during pre-training, compared to tens of petaflop/s-days for a 1.5B parameter GPT-2 model (Figure 2.2). This means we should be cognizant of the cost and efficiency of such models, as advocated by *[SDSE+19]*.\\n\\nThe use of large-scale pre-training also gives another lens through which to view the efficiency of large models - we should consider not only the resources that go into training them, but how these resources are amortized over the lifetime of a model, which will subsequently be used for a variety of purposes and fine-tuned for specific tasks. Though models like GPT-3 consume significant resources during training, they can be surprisingly efficient once trained: even with the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a few cents in energy costs. Additionally, techniques like model distillation *[LHCG+19a]* can further bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efficient versions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efficiency of such models over time, similar to trends observed in image recognition and neural machine translation *[HB+20]*.\\n\\n## 7 Related Work\\n\\nSeveral lines of work have focused on increasing parameter count and/or computation in language models as a means to improve generative or task performance. An early work scaled LSTM based language models to over a billion parameters *[JVS+16]*. One line of work straightforwardly increases the size of transformer models, scaling up parameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size: 213 million parameters *[VSP+17]* in the original paper, 300 million parameters *[DCL+18]*, 1.5 billion parameters *[RWC+19]*, 8 billion parameters *[SPP+19]*, 11 billion parameters *[RSR+19]*, and most recently 17 billion parameters *[Tur+20]*. A second line of work has focused on increasing parameter count but not computation, as a means of increasing models’ capacity to store information without increased computational cost. These approaches rely on the conditional computation framework *[BLC+13]* and specifically, the mixture-of-experts method *[SMM+17]* has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models *[AJF+19]*, though only a small fraction of the parameters are actually used on each forward pass. A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time *[Gra+16]* and the universal transformer *[DGV+18]*. Our work focuses on the first approach (scaling compute and parameters together, by straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ this strategy.\\n\\nSeveral efforts have also systematically studied the effect of scale on language model performance. *[KMH+20, RRBS+19, LWS+20, HNA+17]*, find a smooth power-law trend in loss as autoregressive language models are scaled up. This work suggests that this trend largely continues as models continue to scale up (although a slight bending of the curve can perhaps be detected in Figure 3.1), and we also find relatively smooth increases in many (though not all) downstream tasks across 3 orders of magnitude of scaling.\\n\\nAnother line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language models that are as small as possible. This approach includes ALBERT *[LCG+19]* as well as general *[HVD+15]* and\\n\\ntask-specific *[x21, JYS^{+}19, x18]* approaches to distillation of language models. These architectures and techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint of giant models.\\n\\nAs fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering *[KPR^{+}19, IBGC^{+}14, CCE^{+}18, x11]*, reading comprehension *[CHI^{+}18, x20]*, and adversarially constructed datasets designed to be difficult for existing language models *[x22, NWD^{+}19]*. In this work we test our models on many of these datasets.\\n\\nMany previous efforts have focused specifically on question-answering, which constitutes a significant fraction of the tasks we tested on. Recent efforts include *[RSR^{+}19, x23]*, which fine-tuned an 11 billion parameter language model, and *[GLT^{+}20]*, which focused on attending over a large corpus of data at test time. Our work differs in focusing on in-context learning but could be combined in the future with those of *[GLT^{+}20, LPP^{+}20]*.\\n\\nMetalearning in language models has been utilized in *[RWC^{+}19]*, though with much more limited results and no systematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including matching networks *[VBL^{+}16]*, RL2 *[DSC^{+}16]*, learning to optimize *[x24, ADG^{+}16, x17]* and MAML *[x12]*. Our approach of stuffing the model’s context with previous examples is most structurally similar to RL2 and also resembles *[x15]*, in that an inner loop of adaptation takes place through computation in the model’s activations across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training) updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks defined at inference-time. Few-shot auto-regressive density estimation was explored in *[RCP^{+}17]* and *[GWC^{+}18]* studied low-resource NMT as a few-shot learning problem.\\n\\nWhile the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained language models in combination with gradient descent to perform few-shot learning *[x25]*. Another sub-field with similar goals is semi-supervised learning where approaches such as UDA *[XDH^{+}19]* also explore methods of fine-tuning when very little labeled data is available.\\n\\nGiving multi-task models instructions in natural language was first formalized in a supervised setting with *[x22]* and utilized for some tasks (such as summarizing) in a language model with *[RWC^{+}19]*. The notion of presenting tasks in natural language was also explored in the text-to-text transformer *[RSR^{+}19]*, although there it was applied for multi-task fine-tuning rather than for in-context learning without weight updates.\\n\\nAnother approach to increasing generality and transfer-learning capability in language models is multi-task learning *[x7]*, which fine-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one. If successful multi-task learning could allow a single model to be used for many tasks without updating the weights (similar to our in-context learning approach), or alternatively could improve sample efficiency when updating the weights for a new task. Multi-task learning has shown some promising initial results *[LGH^{+}15, LSP^{+}18]* and multi-stage fine-tuning has recently become a standardized part of SOTA results on some datasets *[x20]* and pushed the boundaries on certain tasks *[KKS^{+}20]*, but is still limited by the need to manually curate collections of datasets and set up training curricula. By contrast pre-training at large enough scale appears to offer a “natural” broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation *[TFR^{+}17]*, human interaction *[ZSW^{+}19b]*, or active learning *[x21]*.\\n\\nAlgorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality *[x6]*, prefixLM *[x5]* and encoder-decoder architectures *[LLG^{+}19, RSR^{+}19]*, random permutations during training *[YDY^{+}19]*, architectures that improve the efficiency of sampling *[DYY^{+}19]*, improvements in data and training procedures *[LOG^{+}19]*, and efficiency increases in the embedding parameters *[LCG^{+}19]*. Many of these techniques provide significant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3’s performance on downstream tasks, especially in the fine-tuning setting, and combining GPT-3’s scale with these algorithmic techniques is a promising direction for future work.\\n\\n## 8 Conclusion\\n\\nWe presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of\\n\\nstate-of-the-art fine-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks defined on-the-fly. We documented roughly predictable trends of scaling in performance without using fine-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.\\n\\n## Acknowledgements\\n\\nThe authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI’s infrastructure. Thanks to David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of people who created content that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale.\\n\\n##\\n\\nContributions\\n\\nTom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu implemented the large-scale models, training infrastructure, and model-parallel strategies.\\n\\nTom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments.\\n\\nBen Mann and Alec Radford collected, filtered, deduplicated, and conducted overlap analysis on the training data.\\n\\nMelanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and Girish Sastry implemented the downstream tasks and the software framework for supporting them, including creation of synthetic tasks.\\n\\nJared Kaplan and Sam McCandlish initially predicted that a giant language model should show continued gains, and applied scaling laws to help predict and guide model and data scaling decisions for the research.\\n\\nBen Mann implemented sampling without replacement during training.\\n\\nAlec Radford originally demonstrated few-shot learning occurs in language models.\\n\\nJared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and systematically studied in-context learning curves, task prompting, and evaluation methods.\\n\\nPrafulla Dhariwal implemented an early version of the codebase, and developed the memory optimizations for fully half-precision training.\\n\\nRewon Child and Mark Chen developed an early version of our model-parallel strategy.\\n\\nRewon Child and Scott Gray contributed the sparse transformer.\\n\\nAditya Ramesh experimented with loss scaling strategies for pretraining.\\n\\nMelanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam search.\\n\\nPranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature.\\n\\nSandhini Agarwal conducted the fairness and representation analysis.\\n\\nGirish Sastry and Amanda Askell conducted the human evaluations of the model.\\n\\nAriel Herbert-Voss conducted the threat analysis of malicious use.\\n\\nGretchen Krueger edited and red-teamed the policy sections of the paper.\\n\\nBenjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner optimized OpenAI’s clusters to run the largest models efficiently.\\n\\nScott Gray developed fast GPU kernels used during training.\\n\\nJack Clark led the analysis of ethical impacts — fairness and representation, human assessments of the model, and broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work.\\n\\nDario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal, Amanda Askell, Girish Sastry, and Jack Clark wrote the paper.\\n\\nSam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work.\\n\\nAlec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated the benefit of weight decay for training.\\n\\nIlya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla, Rewon, Alec, and Aditya on their work.\\n\\nDario Amodei designed and led the research.\\n\\nA Details of Common Crawl Filtering\\n\\nAs mentioned in Section 2.2, we employed two techniques to improve the quality of the Common Crawl dataset: (1) filtering Common Crawl and (2) fuzzy deduplication:\\n\\n1. In order to improve the quality of Common Crawl, we developed an automatic filtering method to remove low quality documents. Using the original WebText as a proxy for high-quality documents, we trained a classifier to distinguish these from raw Common Crawl. We then used this classifier to re-sample Common Crawl by prioritizing documents which were predicted by the classifier to be higher quality. The classifier is trained using logistic regression classifier with features from Spark’s standard tokenizer and HashingTF . For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unfiltered Common Crawl. We used this classifier to score Common Crawl documents. We kept each document in our dataset iff\\n\\n$\\\\texttt{np.random.pareto}(\\\\alpha)>1-\\\\texttt{document_score}$\\n\\nWe chose $\\\\alpha=9$ in order to take mostly documents the classifier scored highly, but still include some documents that were out of distribution. $\\\\alpha$ was chosen to match the distribution of scores from our classifier on WebText. We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples.\\n2. To further improve model quality and prevent overfitting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark’s MinHashLSH implementation with 10 hashes, using the same features as were used for classification above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10%.\\n\\nAfter filtering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix C.\\n\\n## Appendix B Details of Model Training\\n\\nTo train all versions of GPT-3, we use Adam with $\\\\beta_{1}=0.9$, $\\\\beta_{2}=0.95$, and $\\\\epsilon=10^{-8}$, we clip the global norm of the gradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260 billion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the first 375 million tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overfitting. All models use weight decay of 0.1 to provide a small amount of regularization *[117]*.\\n\\nDuring training we always train on sequences of the full $n_{\\\\mathrm{ctx}}=2048$ token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efficiency. Sequences with multiple documents are not masked in any special way but instead documents within a sequence are delimited with a special end of text token, giving the language model the information necessary to infer that context separated by the end of text token is unrelated. This allows for efficient training without need for any special sequence-specific masking.\\n\\n## Appendix C Details of Test Set Contamination Studies\\n\\nIn section 4 we gave a high level overview of test set contamination studies. In this section we provide details on methodology and results.\\n\\n#### Initial training set filtering\\n\\nWe attempted to remove text occurring in benchmarks from training data by searching for $13-$gram overlaps between all test/development sets used in this work and our training data, and we removed the colliding $13-$gram as well as a 200 character window around it, splitting the original document into pieces. For filtering purposes we define a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than $200$ characters long were discarded. Documents split into more than 10 pieces were considered contaminated and\\n\\nremoved entirely. Originally we removed entire documents given a single collision, but that overly penalized long documents such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in which the Wikipedia article quotes a single line from a book. We ignored $13-$grams that matched more than 10 training documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar content that we likely do want the model to learn, rather than undesired specific overlaps with test sets. Examples for various frequencies can be found in the GPT-3 release repository.\\n\\n#### Overlap methodology\\n\\nFor our benchmark overlap analysis in Section 4, we used a variable number of words $N$ to check for overlap for each dataset, where $N$ is the 5th percentile example length in words, ignoring all punctuation, whitespace, and casing. Due to spurious collisions at lower values of $N$ we use a minimum value of 8 on non-synthetic tasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for $N$ and the amount of data marked as dirty are shown in Table C.1. Unlike GPT-2’s use of bloom filters to compute probabilistic bounds for test contamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps between test sets and our full training corpus, even though we only trained on 40% of our filtered Common Crawl documents per Section 2.2.\\n\\nWe define a ‘dirty’ example as one with any $N$-gram overlap with any training document, and a ‘clean’ example as one with no collision.\\n\\nTest and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed by this analysis, filtering described above failed on long documents such as books. Because of cost considerations it was infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling benchmarks plus the Children’s Book Test showed almost complete overlap, and therefore were not included in this paper. Overlaps are shown in Table C.1\\n\\n#### Overlap results\\n\\nTo understand how much having seen some of the data helps the model perform on downstream tasks, we filter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report the relative percent change between the clean score and the original score. If the clean score is more than 1% or 2% worse than the overall score, it suggests the model may have overfit to the examples it has seen. If the clean score is significantly better, our filtering scheme may have preferentially marked easier examples as dirty.\\n\\nThis overlap metric tends to show a high rate of false positives for datasets that contain background information (but not answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words long, which we ignored in our filtering process (except for wordscrambling tasks). One instance where this technique seems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The information required to answer the question is in a passage provided to the model, so having seen the passage during training but not the questions and answers does not meaningfully constitute cheating. We confirmed that every matching training document contained only the source passage, and none of the questions and answers in the dataset. The more likely explanation for the decrease in performance is that the 6% of examples that remain after filtering come from a slightly different distribution than the dirty examples.\\n\\nFigure 4.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but there is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive to contamination. See Section 4 for details on the datasets we flagged for further review.\\n\\n|  Name | Split | Metric | N | Acc/F1/BLEU | Total Count | Dirty Acc/F1/BLEU | Dirty Count | Clean Acc/F1/BLEU | Clean Count | Clean Percentage | Relative Difference Clean vs All  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Quac | dev | f1 | 13 | 44.3 | 7353 | 44.3 | 7315 | 54.1 | 38 | 1% | 20%  |\\n|  SQuADv2 | dev | f1 | 13 | 69.8 | 11873 | 69.9 | 11136 | 68.4 | 737 | 6% | -2%  |\\n|  DROP | dev | f1 | 13 | 36.5 | 9536 | 37.0 | 8898 | 29.5 | 638 | 7% | -21%  |\\n|  Symbol Insertion | dev | acc | 7 | 66.9 | 10000 | 66.8 | 8565 | 67.1 | 1435 | 14% | 0%  |\\n|  CoQa | dev | f1 | 13 | 86.0 | 7983 | 85.3 | 5107 | 87.1 | 2876 | 36% | 1%  |\\n|  ReCoRD | dev | acc | 13 | 89.5 | 10000 | 90.3 | 6110 | 88.2 | 3890 | 39% | -1%  |\\n|  Winograd | test | acc | 9 | 88.6 | 273 | 90.2 | 164 | 86.2 | 109 | 40% | -3%  |\\n|  BoolQ | dev | acc | 13 | 76.0 | 3270 | 75.8 | 1955 | 76.3 | 1315 | 40% | 0%  |\\n|  MultiRC | dev | acc | 13 | 74.2 | 953 | 73.4 | 558 | 75.3 | 395 | 41% | 1%  |\\n|  RACE-h | test | acc | 13 | 46.8 | 3498 | 47.0 | 1580 | 46.7 | 1918 | 55% | 0%  |\\n|  LAMBADA | test | acc | 13 | 86.4 | 5153 | 86.9 | 2209 | 86.0 | 2944 | 57% | 0%  |\\n|  LAMBADA (No Blanks) | test | acc | 13 | 77.8 | 5153 | 78.5 | 2209 | 77.2 | 2944 | 57% | -1%  |\\n|  WSC | dev | acc | 13 | 76.9 | 104 | 73.8 | 42 | 79.0 | 62 | 60% | 3%  |\\n|  PIQA | dev | acc | 8 | 82.3 | 1838 | 89.9 | 526 | 79.3 | 1312 | 71% | -4%  |\\n|  RACE-m | test | acc | 13 | 58.5 | 1436 | 53.0 | 366 | 60.4 | 1070 | 75% | 3%  |\\n|  De→En 16 | test | bleu-sb | 12 | 43.0 | 2999 | 47.4 | 739 | 40.8 | 2260 | 75% | -5%  |\\n|  En→De 16 | test | bleu-sb | 12 | 30.9 | 2999 | 32.6 | 739 | 29.9 | 2260 | 75% | -3%  |\\n|  En→Ro 16 | test | bleu-sb | 12 | 25.8 | 1999 | 24.9 | 423 | 26.1 | 1576 | 79% | 1%  |\\n|  Ro→En 16 | test | bleu-sb | 12 | 41.3 | 1999 | 40.4 | 423 | 41.6 | 1576 | 79% | 1%  |\\n|  WebQs | test | acc | 8 | 41.5 | 2032 | 41.6 | 428 | 41.5 | 1604 | 79% | 0%  |\\n|  ANLI R1 | test | acc | 13 | 36.8 | 1000 | 40.5 | 200 | 35.9 | 800 | 80% | -3%  |\\n|  ANLI R2 | test | acc | 13 | 34.0 | 1000 | 29.4 | 177 | 35.0 | 823 | 82% | 3%  |\\n|  TriviaQA | dev | acc | 10 | 71.2 | 7993 | 70.8 | 1390 | 71.3 | 6603 | 83% | 0%  |\\n|  ANLI R3 | test | acc | 13 | 40.2 | 1200 | 38.3 | 196 | 40.5 | 1004 | 84% | 1%  |\\n|  En→Fr 14 | test | bleu-sb | 13 | 39.9 | 3003 | 38.3 | 411 | 40.3 | 2592 | 86% | 1%  |\\n|  Fr→En 14 | test | bleu-sb | 13 | 41.4 | 3003 | 40.9 | 411 | 41.4 | 2592 | 86% | 0%  |\\n|  WiC | dev | acc | 13 | 51.4 | 638 | 53.1 | 49 | 51.3 | 589 | 92% | 0%  |\\n|  RTE | dev | acc | 13 | 71.5 | 277 | 71.4 | 21 | 71.5 | 256 | 92% | 0%  |\\n|  CB | dev | acc | 13 | 80.4 | 56 | 100.0 | 4 | 78.8 | 52 | 93% | -2%  |\\n|  Anagrams 2 | dev | acc | 2 | 40.2 | 10000 | 76.2 | 705 | 37.4 | 9295 | 93% | -7%  |\\n|  Reversed Words | dev | acc | 2 | 0.4 | 10000 | 1.5 | 660 | 0.3 | 9340 | 93% | -26%  |\\n|  OpenBookQA | test | acc | 8 | 65.4 | 500 | 58.1 | 31 | 65.9 | 469 | 94% | 1%  |\\n|  ARC (Easy) | test | acc | 11 | 70.1 | 2268 | 77.5 | 89 | 69.8 | 2179 | 96% | 0%  |\\n|  Anagrams 1 | dev | acc | 2 | 15.0 | 10000 | 49.8 | 327 | 13.8 | 9673 | 97% | -8%  |\\n|  COPA | dev | acc | 9 | 93.0 | 100 | 100.0 | 3 | 92.8 | 97 | 97% | 0%  |\\n|  ARC (Challenge) | test | acc | 12 | 51.6 | 1144 | 45.2 | 31 | 51.8 | 1113 | 97% | 0%  |\\n|  HellaSwag | dev | acc | 13 | 79.3 | 10042 | 86.2 | 152 | 79.2 | 9890 | 98% | 0%  |\\n|  NQs | test | acc | 11 | 29.9 | 3610 | 32.7 | 52 | 29.8 | 3558 | 99% | 0%  |\\n|  Cycled Letters | dev | acc | 2 | 38.6 | 10000 | 20.5 | 73 | 38.7 | 9927 | 99% | 0%  |\\n|  SAT Analogies | dev | acc | 9 | 65.8 | 374 | 100.0 | 2 | 65.6 | 372 | 99% | 0%  |\\n|  StoryCloze | test | acc | 13 | 87.7 | 1871 | 100.0 | 2 | 87.6 | 1869 | 100% | 0%  |\\n|  Winogrande | dev | acc | 13 | 77.7 | 1267 | - | 0 | 77.7 | 1267 | 100% | 0%  |\\n\\nTable C.1: Overlap statistics for all datasets sorted from dirtiest to cleanest. We consider a dataset example dirty if it has a single  $N$ -gram collision with any document in our training corpus. \"Relative Difference Clean vs All\" shows the percent change in performance between only the clean examples vs all the examples in the benchmark. \"Count\" shows the number of examples. \"Clean percentage\" is the percent of examples that are clean vs total. For \"Acc/F1/BLEU\" we use the metric specified in \"Metric\". These scores come from evaluations with a different seed for the random examples used for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper.\\n\\n# D Total Compute Used to Train Language Models\\n\\nThis appendix contains the calculations that were used to derive the approximate compute used to train the language models in Figure 2.2. As a simplifying assumption, we ignore the attention operation, as it typically uses less than  $10\\\\%$  of the total compute for the models we are analyzing.\\n\\nCalculations can be seen in Table D.1 and are explained within the table caption.\\n\\n|  Model | Total train compute (PF-days) | Total train compute (flops) | Params (M) | Training tokens (billions) | Flops per param per token | Mult for bwd pass | Fwd-pass flops per active param per token | Frac of params active for each token  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  T5-Small | 2.08E+00 | 1.80E+20 | 60 | 1,000 | 3 | 3 | 1 | 0.5  |\\n|  T5-Base | 7.64E+00 | 6.60E+20 | 220 | 1,000 | 3 | 3 | 1 | 0.5  |\\n|  T5-Large | 2.67E+01 | 2.31E+21 | 770 | 1,000 | 3 | 3 | 1 | 0.5  |\\n|  T5-3B | 1.04E+02 | 9.00E+21 | 3,000 | 1,000 | 3 | 3 | 1 | 0.5  |\\n|  T5-11B | 3.82E+02 | 3.30E+22 | 11,000 | 1,000 | 3 | 3 | 1 | 0.5  |\\n|  BERT-Base | 1.89E+00 | 1.64E+20 | 109 | 250 | 6 | 3 | 2 | 1.0  |\\n|  BERT-Large | 6.16E+00 | 5.33E+20 | 355 | 250 | 6 | 3 | 2 | 1.0  |\\n|  RoBERTa-Base | 1.74E+01 | 1.50E+21 | 125 | 2,000 | 6 | 3 | 2 | 1.0  |\\n|  RoBERTa-Large | 4.93E+01 | 4.26E+21 | 355 | 2,000 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 Small | 2.60E+00 | 2.25E+20 | 125 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 Medium | 7.42E+00 | 6.41E+20 | 356 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 Large | 1.58E+01 | 1.37E+21 | 760 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 XL | 2.75E+01 | 2.38E+21 | 1,320 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 2.7B | 5.52E+01 | 4.77E+21 | 2,650 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 6.7B | 1.39E+02 | 1.20E+22 | 6,660 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 13B | 2.68E+02 | 2.31E+22 | 12,850 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 175B | 3.64E+03 | 3.14E+23 | 174,600 | 300 | 6 | 3 | 2 | 1.0  |\\n\\nTable D.1: Starting from the right hand side and moving left, we begin with the number of training tokens that each model was trained with. Next we note that since T5 uses an encoder-decoder model, only half of the parameters are active for each token during a forward or backwards pass. We then note that each token is involved in a single addition and a single multiply for each active parameter in the forward pass (ignoring attention). Then we add a multiplier of 3x to account for the backwards pass (as computing both  $\\\\frac{\\\\partial \\\\text{params}}{\\\\partial \\\\text{loss}}$  and  $\\\\frac{\\\\partial \\\\text{acts}}{\\\\partial \\\\text{loss}}$  use a similar amount of compute as the forwards pass. Combining the previous two numbers, we get the total flops per parameter per token. We multiply this value by the total training tokens and the total parameters to yield the number of total flops used during training. We report both flops and petaflop/s-day (each of which are 8.64e+19 flops).\\n\\n# E Human Quality Assessment of Synthetic News Articles\\n\\nThis appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic news articles from real news articles. We first describe the experiments on the  $\\\\sim 200$  word news articles, and then describe the preliminary investigation of  $\\\\sim 500$  word news articles generated by GPT-3.\\n\\nParticipants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for failing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean participant age was  $\\\\sim 38$  years old. All participants were recruited through Positly, which maintains a whitelist of high-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic restrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined by pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were not allowed to take part in an experiment more than once.\\n\\nProcedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used the article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B (GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a word count closest to that of the human written article was selected automatically. This was to minimize the effect that completion length might have on participants\\' judgments. The same output procedure for each model with the exception of the removal of the intentionally bad control model, as described in the main text.\\n\\n|  Model | Participants Recruited | Participants Excluded | Genders (m:f:other) | Mean Age | Average Word Count (human:model)  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Control | 76 | 7 | 32:37:0 | 39 | 216:216  |\\n|  GPT-3 Small | 80 | 7 | 41:31:1 | 40 | 216:188  |\\n|  GPT-3 Medium | 80 | 7 | 46:28:2 | 39 | 216:202  |\\n|  GPT-3 Large | 81 | 24 | 46:28:2 | 37 | 216:200  |\\n|  GPT-3 XL | 79 | 14 | 32:32:1 | 38 | 216:199  |\\n|  GPT-3 2.7B | 80 | 11 | 36:33:0 | 40 | 216:202  |\\n|  GPT-3 6.7B | 76 | 5 | 46:28:2 | 37 | 216:195  |\\n|  GPT-3 13.0B | 81 | 13 | 46:28:2 | 37 | 216:209  |\\n|  GPT-3 175B | 80 | 9 | 42:29:0 | 37 | 216:216  |\\n\\nTable E.1: Participant details and article lengths for each experiment to evaluate human detection of  $\\\\sim 200$  word model generated news articles. Participants were excluded due to internet check fails.\\n\\n![img-27.jpeg](img-27.jpeg)\\nFigure E.1: Participants spend more time trying to identify whether each news article is machine generated as model size increases. Duration on the control model is indicated with the dashed line. Line of best fit is a linear model on a log scale with  $95\\\\%$  confidence intervals.\\n\\nIn each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz B. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the articles with human written completions in quiz A had model generated completions in quiz B and vice versa. The order of quiz question was shuffled for each participant. Participants could leave comments and were asked to indicate if they had seen the articles before. Participants were instructed not to look up the articles or their content during the quiz and at the end of the quiz were asked if they had looked anything up during the quiz.\\n\\nStatistical Tests: To compare means on the different runs, we performed a two-sample t-test for independent groups for each model against the control. This was implemented in Python using the scipy.stats.ttest_ind function. When plotting a regression line in the graph of average participant accuracy vs model size, we fit a power law of the form  $\\\\alpha x^{-b}$ . The  $95\\\\%$  confidence intervals were estimated from the t-distribution of the sample mean.\\n\\nDuration statistics: In the main text, we discussed the finding that the ability of human participants to distinguish model and human generated news articles decreases as our models become larger. We have also found that the average time spent for a given set of questions increases as the model size increases, as shown in Figure E.1. Lower\\n\\n|  Model | Participants Recruited | Participants Excluded | Genders (m:f:other) | Mean Age | Average Word Count (human:model)  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Control | 79 | 17 | 32:37:0 | 39 | 569:464  |\\n|  GPT-3 175B | 81 | 19 | 32:30:0 | 40 | 569:498  |\\n\\nTable E.2: Participant details and article lengths for the experiments investigating human detection of  $\\\\sim 500$  word model generated news articles. Participants were excluded due to internet check fails.\\n\\naccuracy scores despite increased time investment from participants supports the finding that larger models generate harder-to-distinguish news articles.\\n\\nPreliminary investigation of  $\\\\sim 500$  word articles: We recruited 160 unique US-based participants to take part in 2 experiments through Positly (details are given in Table E.2). We randomly selected 12 Reuters world news articles from late 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then used the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model from the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half human written and half model generated articles. Comprehension questions were added and articles were shown to participants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task. Model generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments.\\n\\n# F Additional Samples from GPT-3\\n\\nGPT-3 adapts well to many tasks other than the ones explored in the main body of the paper. As an example, in Figure F.1, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the style of Wallace Stevens. We first experimented with a few prompts, then generated four samples with no additional editing or selection (sampling at temperature 1 using nucleus sampling [HBFC19] with  $P = 0.9$ ). Completions were truncated when the model began to write a new title and author heading, or broke into prose commentary.\\n\\n|  Context → | The City BY C. P. CAVAFY TRANSLATED BY EDMUND KEELEY [Poem text omitted]  |\\n| --- | --- |\\n|   | SOME TREES John Ashbery [Poem text omitted]  |\\n|   | Shadows on the Way Wallace Stevens  |\\n|  --- Generated Poem 1 --- | --- Generated Poem 3 ---  |\\n|  I must have shadows on the way If I am to walk I must have Each step taken slowly and alone To have it ready made | The sun was all we had. Now, in the shade All is changed. The mind must dwell on those White fields, that to its eyes were always old; Those ancient gleams, convoluting The way they lay among the huge roots, The great dark tomes of reverie, The plumed labyrinth of the sea.  |\\n|  And I must think in lines of grey To have dim thoughts to be my guide Must look on blue and green And never let my eye forget That color is my friend And purple must surround me too | When our own breath gonne faint and fade Up the long stairway of the dead We shall remember well The blindness of the light, the whiteness Of that white land. The footsteps, and The grasses in that light, and in the shade The menacing life.  |\\n|  The yellow of the sun is no more Intrusive than the bluish snow That falls on all of us. I must have Grey thoughts and blue thoughts walk with me If I am to go away at all. | We played, too, once, in the warmest rooms. To one content, there is one content For all mankind. The forest glades Are all the more for being in fable, And wood and town in legend shadowed- Once more to return. The one content Has in it every happiness, more brave For being purposeless; and so Those island gardens, deep in the mind, Make of remoteness familiarity And of the unspoken name a friend.  |\\n|  --- Generated Poem 2 --- | --- Generated Poem 4 ---  |\\n|  There is a way in the world. That way a man may live in a small town His lifetime and see no man come, Never the stir of men about a market, Never the bobbing of covered carts, Never the creak passed on the stair Closing some other life; an old man Can prosper in that town, can prosper When his store is seen to close, his wife Fades in his house, and the world shouts in vain That no one comes to hear him Where his small bell tolls. In that small town Everything is known. The ways of the young men are known, And the ways of the young women As they walk to the Spring; The rumors are all known, The quarrels, the hurt, the envy Of the men and the women. Till a day comes That way out of the world That leads to a wood. Young, the young man Walks, a lover now, Knows the streets as they are and is of them. | Nobody will come to this place. It is a road that leads nowhere. The solitude is deep. The mountains are high. But they are desolate, and they turn the traveler\\'s face Towards the North. All the sounds of the world are far away. When the wind rises above the trees, The boughs bow to the ground. Even the birds that inhabit the tangle of weeds That is the roadside cover, are silent. One listens, But hears no roar of the forest. One is alone. One will be taken. One will be taken. There is no utterance, there is no conversation, But one is uneasy all the same... There is a thin blue mist, A darkness rising like smoke, And within that darkness A possession of the heart. One will be taken... It was here, and it will be here again- Here, under this sky empty and full of light.  |\\n|  Comes to the wood. |   |\\n|  There, in the wood, among trees, He sees shadows on the way, hears voices, hears the wind and the rustling of leaves; Through an open glade He sees a shape and the shape hears: It waits as he waits, Waits as the shadows wait, As the voices wait; Shadows on the way, voices in the wind. |   |\\n\\nFigure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace Stevens with the title \\'Shadows on the Way\\'.\\n\\n# G Details of Task Phrasing and Specifications\\n\\nThe following figures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from the ground truth datasets in this section, and no samples from GPT-3 are included here.\\n\\n|  Context → | Article: Informal conversation is an important part of any business relationship.Before you start a discussion,however,make sure you understand which topics are suitable and which are considered taboo in a particular culture. Latin Americans enjoy sharing information about their local history, art and customs.You may expect questions about your family,and be sure to show pictures of your children.You may feel free to ask similar questions of your Latin American friends.The French think of conversation as an art form,and they enjoy the value of lively discussions as well as disagreements. For them,arguments can be interesting and they can cover pretty much or any topic --- as long as they occur in are respectful and intelligent manner. In the United States,business people like to discuss a wide range of topics,including opinions about work,family,hobbies,and politics. In Japan,China,and Korea,however,people are much more private.They do not share much about their thoughts,feelings,or emotions because they feel that doing so might take away from the harmonious business relationship they\\'re trying to build.Middle Easterners are also private about their personal lives and family matters.It is considered rude,for example,to ask a businessman from Saudi Arabia about his wife or children. As a general rule,it\\'s best not to talk about politics or religion with your business friends.This can get you into trouble,even in the United States,where people hold different religious views.In addition,discussing one\\'s salary is usually considered unsuitable.Sports is typically a friendly subject in most parts of the world,although be careful not to criticize national sport.Instead,be friendly and praise your host\\'s team. Q: What shouldn\\'t you do when talking about sports with colleagues from another country? A: Criticizing the sports of your colleagues\\' country. Q: Which is typically a friendly topic in most places according to the author? A: Sports. Q: Why are people from Asia more private in their conversation with others? A: They don\\'t want to have their good relationship with others harmed by informal conversation. Q: The author considers politics and religion _ . A:  |\\n| --- | --- |\\n|  Correct Answer → | taboo  |\\n|  Incorrect Answer → | cheerful topics  |\\n|  Incorrect Answer → | rude topics  |\\n|  Incorrect Answer → | topics that can never be talked about  |\\n\\nFigure G.1: Formatted dataset example for RACE-h. When predicting, we normalize by the unconditional probability of each answer as described in 2.\\n\\n|  Context → | anli 2: anli 2: The Gold Coast Hotel & Casino is a hotel and casino located in Paradise, Nevada. This locals\\' casino is owned and operated by Boyd Gaming. The Gold Coast is located one mile (~ 1.6km) west of the Las Vegas Strip on West Flamingo Road. It is located across the street from the Palms Casino Resort and the Rio All Suite Hotel and Casino. Question: The Gold Coast is a budget-friendly casino. True, False, or Neither?  |\\n| --- | --- |\\n|  Correct Answer → | Neither  |\\n|  Incorrect Answer → | True  |\\n|  Incorrect Answer → | False  |\\n\\nFigure G.2: Formatted dataset example for ANLI R2\\n\\n|  Context → | Article: Mrs. Smith is an unusual teacher. Once she told each student to bring along a few potatoes in plastic bag. On each potato the students had to write a name of a person that they hated And the next day, every child brought some potatoes. Some had two potatoes;some three;some up to five. Mrs. Smith then told the children to carry the bags everywhere they went, even to the toilet, for two weeks. As day after day passed, the children started to complain about the awful smell of the rotten potatoes. Those children who brought five potatoes began to feel the weight trouble of the bags. After two weeks, the children were happy to hear that the game was finally ended. Mrs. Smith asked,\"How did you feel while carrying the potatoes for two weeks?\" The children started complaining about the trouble loudly. Then Mrs. Smith told them why she asked them to play the game. She said,\"This is exactly the situation when you carry your hatred for somebody inside your heart. The terrible smell of the hatred will pollute your heart and you will carry something unnecessary with you all the time. If you cannot stand the smell of the rotten potatoes for just two weeks, can you imagine how heavy it would be to have the hatred in your heart for your lifetime? So throw away any hatred from your heart, and you\\'ll be really happy.\" Q: Which of the following is True according to the passage? A: If a kid hated four people,he or she had to carry four potatoes. Q: We can learn from the passage that we should _ . A: throw away the hatred inside Q: The children complained about _ besides the weight trouble. A: the smell Q: Mrs.Smith asked her students to write _ on the potatoes. A:  |\\n| --- | --- |\\n|  Correct Answer → | names  |\\n|  Incorrect Answer → | numbers  |\\n|  Incorrect Answer → | time  |\\n|  Incorrect Answer → | places  |\\n\\nFigure G.3: Formatted dataset example for RACE-m. When predicting, we normalize by the unconditional probability of each answer as described in 2.\\n\\n|  Context → | How to apply sealant to wood.  |\\n| --- | --- |\\n|  Correct Answer → | Using a brush, brush on sealant onto wood until it is fully saturated with the sealant.  |\\n|  Incorrect Answer → | Using a brush, drip on sealant onto wood until it is fully saturated with the sealant.  |\\n\\nFigure G.4: Formatted dataset example for PIQA\\n\\n|  Context → | My body cast a shadow over the grass because  |\\n| --- | --- |\\n|  Correct Answer → | the sun was rising.  |\\n|  Incorrect Answer → | the grass was cut.  |\\n\\nFigure G.5: Formatted dataset example for COPA\\n\\n|  Context → | (CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while serving as Prime Minister of Israel, criticized Donald Trump for appealing to \"Second Amendment people\" in a speech and warned that the words that politicians use can incite violence and undermine democracy. \"Trump\\'s words are an incitement to the type of political violence that touched me personally,\" Rabin wrote in USAToday. He said that Trump\\'s appeal to \"Second Amendment people\" to stop Hillary Clinton -- comments that were criticized as a call for violence against Clinton, something Trump denied -- \"were a new level of ugliness in an ugly campaign season.\"  |\\n| --- | --- |\\n|   | - The son of a former Israeli Prime Minister who was assassinated wrote an op ed about the consequence of violent political rhetoric. - Warns of \"parallels\" between Israel of the 1990s and the U.S. today.  |\\n|  Correct Answer → | - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Donald Trump\\'s aggressive rhetoric.  |\\n|  Correct Answer → | - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Trump\\'s aggressive rhetoric.  |\\n|  Incorrect Answer → | - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Hillary Clinton\\'s aggressive rhetoric.  |\\n|  Incorrect Answer → | - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned U.S.\\'s aggressive rhetoric.  |\\n|  Incorrect Answer → | - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Yitzhak Rabin\\'s aggressive rhetoric.  |\\n\\nFigure G.6: Formatted dataset example for ReCoRD. We consider the context above to be a single \"problem\" because this is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script.\\n\\n|  Context → | anli 1: anli 1: Fulton James MacGregor MSP is a Scottish politician who is a Scottish National Party (SNP) Member of Scottish Parliament for the constituency of Coatbridge and Chryston. MacGregor is currently Parliamentary Liaison Officer to Shona Robison, Cabinet Secretary for Health & Sport. He also serves on the Justice and Education & Skills committees in the Scottish Parliament. Question: Fulton James MacGregor is a Scottish politician who is a Liaison officer to Shona Robison who he swears is his best friend. True, False, or Neither?  |\\n| --- | --- |\\n|  Correct Answer → | Neither  |\\n|  Incorrect Answer → | True  |\\n|  Incorrect Answer → | False  |\\n\\nFigure G.7: Formatted dataset example for ANLI R1\\n\\n|  Context → | Organisms require energy in order to do what?  |\\n| --- | --- |\\n|  Correct Answer → | mature and develop.  |\\n|  Incorrect Answer → | rest soundly.  |\\n|  Incorrect Answer → | absorb light.  |\\n|  Incorrect Answer → | take in nutrients.  |\\n\\nFigure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional probability of each answer as described in 2.\\n\\n|  Context → | Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They  |\\n| --- | --- |\\n|  Correct Answer → | bake them, then frost and decorate.  |\\n|  Incorrect Answer → | taste them as they place them on plates.  |\\n|  Incorrect Answer → | put the frosting on the cake as they pan it.  |\\n|  Incorrect Answer → | come out and begin decorating the cake as well.  |\\n\\nFigure G.9: Formatted dataset example for HellaSwag\\n\\n|  Context → | anli 3: anli 3: We shut the loophole which has American workers actually subsidizing the loss of their own job. They just passed an expansion of that loophole in the last few days: $43 billion of giveaways, including favors to the oil and gas industry and the people importing ceiling fans from China. Question: The loophole is now gone True, False, or Neither?  |\\n| --- | --- |\\n|  Correct Answer → | False  |\\n|  Incorrect Answer → | True  |\\n|  Incorrect Answer → | Neither  |\\n\\nFigure G.10: Formatted dataset example for ANLI R3\\n\\n|  Context → | Question: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat? Answer:  |\\n| --- | --- |\\n|  Correct Answer → | dry palms  |\\n|  Incorrect Answer → | wet palms  |\\n|  Incorrect Answer → | palms covered with oil  |\\n|  Incorrect Answer → | palms covered with lotion  |\\n\\nFigure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional probability of each answer as described in 2.\\n\\n|  Context → | lull is to trust as  |\\n| --- | --- |\\n|  Correct Answer → | cajole is to compliance  |\\n|  Incorrect Answer → | balk is to fortitude  |\\n|  Incorrect Answer → | betray is to loyalty  |\\n|  Incorrect Answer → | hinder is to destination  |\\n|  Incorrect Answer → | soothe is to passion  |\\n\\nFigure G.12: Formatted dataset example for SAT Analogies\\n\\n|  Correct Context → | Grace was happy to trade me her sweater for my jacket. She thinks the sweater  |\\n| --- | --- |\\n|  Incorrect Context → | Grace was happy to trade me her sweater for my jacket. She thinks the jacket  |\\n|  Target Completion → | looks dowdy on her.  |\\n\\nFigure G.13: Formatted dataset example for Winograd. The \\'partial\\' evaluation method we use compares the probability of the completion given a correct and incorrect context.\\n\\n|  Correct Context → | Johnny likes fruits more than vegetables in his new keto diet because the fruits  |\\n| --- | --- |\\n|  Incorrect Context → | Johnny likes fruits more than vegetables in his new keto diet because the vegetables  |\\n|  Target Completion → | are saccharine.  |\\n\\nFigure G.14: Formatted dataset example for Winogrande. The \\'partial\\' evaluation method we use compares the probability of the completion given a correct and incorrect context.\\n\\n|  Context → | READING COMPREHENSION ANSWER KEY While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful. As one NSC staff note put it, \"Under the Taliban, Afghanistan is not so much a state sponsor of terrorism as it is a state sponsored by terrorists.\" In early 2000, the United States began a high-level effort to persuade Pakistan to use its influence over the Taliban. In January 2000, Assistant Secretary of State Karl Inderfurth and the State Department\\'s counterterrorism coordinator, Michael Sheehan, met with General Musharraf in Islamabad, dangling before him the possibility of a presidential visit in March as a reward for Pakistani cooperation. Such a visit was coveted by Musharraf, partly as a sign of his government\\'s legitimacy. He told the two envoys that he would meet with Mullah Omar and press him on Bin Laden. They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything,\" given what it sees as the benefits of Taliban control of Afghanistan.\" President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan. The Secret Service and the CIA, however, warned in the strongest terms that visiting Pakistan would risk the President\\'s life. Counterterrorism officials also argued that Pakistan had not done enough to merit a presidential visit. But President Clinton insisted on including Pakistan in the itinerary for his trip to South Asia. His one-day stopover on March 25, 2000, was the first time a U.S. president had been there since 1969. At his meeting with Musharraf and others, President Clinton concentrated on tensions between Pakistan and India and the dangers of nuclear proliferation, but also discussed Bin Laden. President Clinton told us that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding Bin Laden.\" I offered him the moon when I went to see him, in terms of better relations with the United States, if he\\'d help us get Bin Laden and deal with another issue or two.\" The U.S. effort continued.  |\\n| --- | --- |\\n|   | Who did The State Department feel should visit both India and Pakistan?  |\\n|  Correct Answer → | - [False] Bin Laden  |\\n|  Incorrect Answer → | - [True] Bin Laden  |\\n\\nFigure G.15: Formatted dataset example for MultiRC. There are three levels within MultiRC: (1) the passage, (2) the questions, and (3) the answers. During evaluation, accuracy is determined at the per-question level, with a question being considered correct if and only if all the answers within the question are labeled correctly. For this reason, we use  $K$  to refer to the number of questions shown within the context.\\n\\n|  Context → | Question: Which factor will most likely cause a person to develop a fever? Answer:  |\\n| --- | --- |\\n|  Correct Answer → | a bacterial population in the bloodstream  |\\n|  Incorrect Answer → | a leg muscle relaxing after exercise  |\\n|  Incorrect Answer → | several viral particles on the skin  |\\n|  Incorrect Answer → | carbohydrates being digested in the stomach  |\\n\\nFigure G.16: Formatted dataset example for ARC (Easy). When predicting, we normalize by the unconditional probability of each answer as described in 2.\\n\\n|  Context → | Bob went to the gas station to fill up his car. His tank was completely empty and so was his wallet. The cashier offered to pay for his gas if he came back later to pay. Bob felt grateful as he drove home.  |\\n| --- | --- |\\n|  Correct Answer → | Bob believed that there were good people in the world.  |\\n|  Incorrect Answer → | Bob contemplated how unfriendly the world was.  |\\n\\nFigure G.17: Formatted dataset example for StoryCloze\\n\\n|  Context → | Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland. Helsinki has a population of , an urban population of , and a metropolitan population of over 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities. The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world\\'s northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland\\'s major political, educational, financial, cultural, and research center as well as one of northern Europe\\'s major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia. Q: what is the most populous municipality in Finland? A: Helsinki Q: how many people live there? A: 1.4 million in the metropolitan area Q: what percent of the foreign companies that operate in Finland are in Helsinki? A: 75% Q: what towns are a part of the metropolitan area? A:  |\\n| --- | --- |\\n|  Target Completion → | Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns  |\\n\\nFigure G.18: Formatted dataset example for CoQA\\n\\n|  Context → | Please unscramble the letters into a word, and write that word: asinoc =  |\\n| --- | --- |\\n|  Target Completion → | casino  |\\n\\nFigure G.19: Formatted dataset example for Cycled Letters\\n\\n|  Context → | Passage: Saint Jean de Brébeuf was a French Jesuit missionary who travelled to New France in 1625. There he worked primarily with the Huron for the rest of his life, except for a few years in France from 1629 to 1633. He learned their language and culture, writing extensively about each to aid other missionaries. In 1649, Brébeuf and another missionary were captured when an Iroquois raid took over a Huron village. Together with Huron captives, the missionaries were ritually tortured and killed on March 16, 1649. Brébeuf was beatified in 1925 and among eight Jesuit missionaries canonized as saints in the Roman Catholic Church in 1930. Question: How many years did Saint Jean de Brébeuf stay in New France before he went back to France for a few years? Answer:  |\\n| --- | --- |\\n|  Target Completion → | 4  |\\n\\nFigure G.20: Formatted dataset example for DROP\\n\\n|  Context → | Fill in blank: She held the torch in front of her. She caught her breath. \"Chris? There\\'s a step.\" \"What?\" \"A step. Cut in the rock. About fifty feet ahead.\" She moved faster. They both moved faster. \"In fact,\" she said, raising the torch higher, \"there\\'s more than a ____ . ->  |\\n| --- | --- |\\n|  Target Completion → | step  |\\n\\nFigure G.21: Formatted dataset example for LAMBADA\\n\\n|  Context → | Please unscramble the letters into a word, and write that word: skicts =  |\\n| --- | --- |\\n|  Target Completion → | sticks  |\\n\\nFigure G.22: Formatted dataset example for Anagrams 1 (A1)\\n\\n|  Context → | Please unscramble the letters into a word, and write that word: volwskagen =  |\\n| --- | --- |\\n|  Target Completion → | volkswagen  |\\n\\nFigure G.23: Formatted dataset example for Anagrams 2\\n\\n|  Context → | Q: Who played tess on touched by an angel?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | Delloreese Patricia Early (July 6, 1931 { November 19, 2017), known professionally as Della Reese  |\\n\\nFigure G.24: Formatted dataset example for Natural Questions\\n\\n|  Context → | TITLE: William Perry (American football) - Professional career PARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL Draft by the Chicago Bears; he had been hand-picked by coach Mike Ditka. However, defensive coordinator Buddy Ryan, who had a highly acrimonious relationship with Ditka, called Perry a \"wasted draft-pick\". Perry soon became a pawn in the political power struggle between Ditka and Ryan. Perry\\'s \"Refrigerator\" nickname followed him into the NFL and he quickly became a favorite of the Chicago Bears fans. Teammates called him \"Biscuit,\" as in \"one biscuit shy of 350 pounds.\" While Ryan refused to play Perry, Ditka decided to use Perry as a fullback when the team was near the opponents\\' goal line or in fourth and short situations, either as a ball carrier or a lead blocker for star running back Walter Payton. Ditka stated the inspiration for using Perry as a fullback came to him during five-yard sprint exercises. During his rookie season, Perry rushed for two touchdowns and caught a pass for one. Perry even had the opportunity to run the ball during Super Bowl XX, as a nod to his popularity and contributions to the team\\'s success. The first time he got the ball, he was tackled for a one-yard loss while attempting to throw his first NFL pass on a halfback option play. The second time he got the ball, he scored a touchdown (running over Patriots linebacker Larry McGrew in the process). About halfway through his rookie season, Ryan finally began to play Perry, who soon proved that he was a capable defensive lineman. His Super Bowl ring size is the largest of any professional football player in the history of the event. His ring size is 25, while the ring size for the average adult male is between 10 and 12. Perry went on to play for ten years in the NFL, retiring after the 1994 season. In his ten years as a pro, he regularly struggled with his weight, which hampered his performance at times. He played in 138 games, recording 29.5 sacks and five fumble recoveries, which he returned for a total of 71 yards. In his offensive career he ran five yards for two touchdowns, and had one reception for another touchdown. Perry later attempted a comeback, playing an unremarkable 1996 season with the London Monarchs of the World League of American Football (later NFL Europa). Q: what team did he play for? A:  |\\n| --- | --- |\\n|  Target Completion → | the Chicago Bears  |\\n\\nFigure G.25: Formatted dataset example for QuAC\\n\\n|  Context → | Please unscramble the letters into a word, and write that word: r e!c.i p r o.c a/l =  |\\n| --- | --- |\\n|  Target Completion → | reciprocal  |\\n\\nFigure G.26: Formatted dataset example for Symbol Insertion\\n\\n|  Context → | Please unscramble the letters into a word, and write that word: taefed =  |\\n| --- | --- |\\n|  Target Completion → | defeat  |\\n\\nFigure G.27: Formatted dataset example for Reversed Words\\n\\n|  Context → | Title: TheBlitz  |\\n| --- | --- |\\n|   | Background: From the German point of view, March 1941 saw an improvement. The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. X- and Y-Gerät beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-Gerät, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Gerät.  |\\n|   | Q: How many sorties were flown in March 1941?  |\\n|   | A: 4,000  |\\n|   | Q: When did the Luftwaffe fly inland missions?  |\\n|   | A:  |\\n|  Target Completion → | only on moonlit nights  |\\n\\nFigure G.28: Formatted dataset example for SQuADv2\\n\\n|  Context → | Normal force -- In a simple case such as an object resting upon a table, the normal force on the object is equal but in opposite direction to the gravitational force applied on the object (or the weight of the object), that is, N = m g (\\\\displaystyle N=mg), where m is mass, and g is the gravitational field strength (about 9.81 m/s on Earth). The normal force here represents the force applied by the table against the object that prevents it from sinking through the table and requires that the table is sturdy enough to deliver this normal force without breaking. However, it is easy to assume that the normal force and weight are action-reaction force pairs (a common mistake). In this case, the normal force and weight need to be equal in magnitude to explain why there is no upward acceleration of the object. For example, a ball that bounces upwards accelerates upwards because the normal force acting on the ball is larger in magnitude than the weight of the ball. question: is the normal force equal to the force of gravity? answer:  |\\n| --- | --- |\\n|  Target Completion → | yes  |\\n\\nFigure G.29: Formatted dataset example for BoolQ\\n\\n|  Context → | The trend toward lower rents may seem surprising given that some communities in New York are bemoaning the loss of favorite local businesses to high rents. But, despite the recent softening, for many of these retailers there\\'s still been too big a jump from the rental rates of the late 1970s, when their leases were signed. Certainly, the recent drop in prices doesn\\'t mean Manhattan comes cheap. question: Manhattan comes cheap. true, false, or neither? answer:  |\\n| --- | --- |\\n|  Target Completion → | false  |\\n\\nFigure G.30: Formatted dataset example for CB\\n\\n|  Context → | The bet, which won him dinner for four, was regarding the existence and mass of the top quark, an elementary particle discovered in 1995. question: The Top Quark is the last of six flavors of quarks predicted by the standard model theory of particle physics. True or False? answer:  |\\n| --- | --- |\\n|  Target Completion → | False  |\\n\\nFigure G.31: Formatted dataset example for RTE\\n\\n|  Context → | An outfitter provided everything needed for the safari. Before his first walking holiday, he went to a specialist outfitter to buy some boots. question: Is the word \\'outfitter\\' used in the same way in the two sentences above? answer:  |\\n| --- | --- |\\n|  Target Completion → | no  |\\n\\nFigure G.32: Formatted dataset example for WiC\\n\\n|  Context → | Final Exam with Answer Key Instructions: Please carefully read the following passages. For each passage, you must identify which noun the pronoun marked in *bold* refers to. ==== Passage: Mr. Moncrieff visited Chester\\'s luxurious New York apartment, thinking that it belonged to his son Edward. The result was that Mr. Moncrieff has decided to cancel Edward\\'s allowance on the ground that he no longer requires *his* financial support. Question: In the passage above, what does the pronoun \"*his*\" refer to? Answer:  |\\n| --- | --- |\\n|  Target Completion → | mr. moncrieff  |\\n\\nFigure G.33: Formatted dataset example for WSC\\n\\n|  Context → | Q: \\'Nude Descending A Staircase\\' is perhaps the most famous painting by which 20th century artist?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | MARCEL DUCHAMP  |\\n|  Target Completion → | r mutt  |\\n|  Target Completion → | duchamp  |\\n|  Target Completion → | marcel duchamp  |\\n|  Target Completion → | R.Mutt  |\\n|  Target Completion → | Marcel duChamp  |\\n|  Target Completion → | Henri-Robert-Marcel Duchamp  |\\n|  Target Completion → | Marcel du Champ  |\\n|  Target Completion → | henri robert marcel duchamp  |\\n|  Target Completion → | Duchampian  |\\n|  Target Completion → | Duchamp  |\\n|  Target Completion → | duchampian  |\\n|  Target Completion → | marcel du champ  |\\n|  Target Completion → | Marcel Duchamp  |\\n|  Target Completion → | MARCEL DUCHAMP  |\\n\\nFigure G.34: Formatted dataset example for TriviaQA. TriviaQA allows for multiple valid completions.\\n\\n|  Context → | Q: What school did burne hogarth establish?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | School of Visual Arts  |\\n\\nFigure G.35: Formatted dataset example for WebQA\\n\\n|  Context → | Keinesfalls dürfen diese für den kommerziellen Gebrauch verwendet werden. =  |\\n| --- | --- |\\n|  Target Completion → | In no case may they be used for commercial purposes.  |\\n\\nFigure G.36: Formatted dataset example for De→En. This is the format for one- and few-shot learning, for this and other language tasks, the format for zero-shot learning is “Q: What is the {language} translation of {sentence} A: {translation}.”\\n\\n|  Context → | In no case may they be used for commercial purposes. =  |\\n| --- | --- |\\n|  Target Completion → | Keinesfalls dürfen diese für den kommerziellen Gebrauch verwendet werden.  |\\n\\nFigure G.37: Formatted dataset example for  $\\\\mathrm{En}\\\\rightarrow \\\\mathrm{De}$\\n\\n|  Context → | Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females. =  |\\n| --- | --- |\\n|  Target Completion → | L\\'analyse de la distribution de fréquence des stades larvaires d\\'I. verticalis dans une série d\\'étangs a également démontré que les larves mâles étaient à des stades plus avancés que les larves femelles.  |\\n\\nFigure G.38: Formatted dataset example for  $\\\\mathrm{En}\\\\rightarrow \\\\mathrm{Fr}$\\n\\n|  Context → | L\\'analyse de la distribution de fréquence des stades larvaires d\\'I. verticalis dans une série d\\'étangs a également démontré que les larves mâles étaient à des stades plus avancés que les larves femelles. =  |\\n| --- | --- |\\n|  Target Completion → | Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females.  |\\n\\nFigure G.39: Formatted dataset example for  $\\\\mathrm{Fr}\\\\rightarrow \\\\mathrm{En}$\\n\\n|  Context → | The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey\\'s accession to the European Union, despite Turkey\\'s continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. =  |\\n| --- | --- |\\n|  Target Completion → | Adevărul este că vă doriti, cu orice pret şi împotriva dorinței europenilor, să continuaţi negocierile de aderare a Turciei la Uniunea Europeană, în ciuda refuzului continuu al Turciei de a recunoaşte Ciprul şi în ciuda faptului că reformele democratice au ajuns într-un punct mort.  |\\n\\nFigure G.40: Formatted dataset example for  $\\\\mathrm{En}\\\\rightarrow \\\\mathrm{Ro}$\\n\\nFigure G.41: Formatted dataset example for  $\\\\mathrm{Ro}\\\\rightarrow \\\\mathrm{En}$\\n\\n|  Context → | Q: What is (2 * 4) * 6? A:  |\\n| --- | --- |\\n|  Target Completion → | 48  |\\n|  Figure G.42: Formatted dataset example for Arithmetic 1DC  |   |\\n|  Context → | Q: What is 17 minus 14? A:  |\\n|  Target Completion → | 3  |\\n|  Figure G.43: Formatted dataset example for Arithmetic 2D-  |   |\\n|  Context → | Q: What is 98 plus 45? A:  |\\n|  Target Completion → | 143  |\\n|  Figure G.44: Formatted dataset example for Arithmetic 2D+  |   |\\n|  Context → | Q: What is 95 times 45? A:  |\\n|  Target Completion → | 4275  |\\n|  Figure G.45: Formatted dataset example for Arithmetic 2Dx  |   |\\n|  Context → | Q: What is 509 minus 488? A:  |\\n|  Target Completion → | 21  |\\n|  Figure G.46: Formatted dataset example for Arithmetic 3D-  |   |\\n|  Context → | Q: What is 556 plus 497? A:  |\\n|  Target Completion → | 1053  |\\n|  Figure G.47: Formatted dataset example for Arithmetic 3D+  |   |\\n|  Context → | Q: What is 6209 minus 3365? A:  |\\n|  Target Completion → | 2844  |\\n\\nFigure G.48: Formatted dataset example for Arithmetic 4D-\\n\\n|  Context → | Q: What is 9923 plus 617?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | 10540  |\\n\\nFigure G.49: Formatted dataset example for Arithmetic 4D+\\n\\n|  Context → | Q: What is 40649 minus 78746?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | -38097  |\\n\\nFigure G.50: Formatted dataset example for Arithmetic 5D-\\n\\n|  Context → | Q: What is 65360 plus 16204?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | 81564  |\\n\\nFigure G.51: Formatted dataset example for Arithmetic 5D+\\n\\nH Results on All Tasks for All Model Sizes\\n\\n|  Name | Metric | Splits | Fine-tune |   | Zero-Shot |   |   |   |   | One-Shot |   |   |   |   | Few-Shot |   |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   |   |   |  SOTA | K | Small Med Large | XL | 2.7B | 6.7B | 13B | 175B | Small Med Large | XL | 2.7B | 6.7B | 13B | 175B | Small Med Large | XL | 2.7B | 6.7B  |\\n|  HellaSwag | acc | dev | 85.6 | 20 | 33.7 | 43.6 | 51.0 | 54.7 | 62.8 | 67.4 | 70.9 | 78.9 | 33.0 | 42.9 | 50.5 | 53.5 | 61.9 | 66.5 | 70.0 | 78.1  |\\n|  LAMBADA | acc | test | 68.0 | 15 | 42.7 | 54.3 | 60.4 | 63.6 | 67.1 | 70.3 | 72.5 | 76.2 | 22.0 | 47.1 | 52.6 | 58.3 | 61.1 | 65.4 | 69.0 | 72.5  |\\n|  LAMBADA | ppl | test | 8.63 | 15 | 18.6 | 9.09 | 6.53 | 5.44 | 4.60 | 4.00 | 3.56 | 3.00 | 165.0 | 11.6 | 8.29 | 6.46 | 5.53 | 4.61 | 4.06 | 3.35  |\\n|  StoryCloze | acc | test | 91.8 | 70 | 63.3 | 68.5 | 72.4 | 73.4 | 77.2 | 77.7 | 79.5 | 83.2 | 62.3 | 68.7 | 72.3 | 74.2 | 77.3 | 78.7 | 79.7 | 84.7  |\\n|  NQs | acc | test | 44.5 | 64 | 0.64 | 1.75 | 2.71 | 4.40 | 6.01 | 5.79 | 7.84 | 14.6 | 1.19 | 3.07 | 4.79 | 5.43 | 8.73 | 9.78 | 13.7 | 23.0  |\\n|  TriviaQA | acc | dev | 68.0 | 64 | 4.15 | 7.61 | 14.0 | 19.7 | 31.3 | 38.7 | 41.8 | 64.3 | 4.19 | 12.9 | 20.5 | 26.5 | 35.9 | 44.4 | 51.3 | 68.0  |\\n|  WebQs | acc | test | 45.5 | 64 | 1.77 | 3.20 | 4.33 | 4.63 | 7.92 | 7.73 | 8.22 | 14.4 | 2.56 | 6.20 | 8.51 | 9.15 | 14.5 | 15.1 | 19.0 | 25.3  |\\n|  Ro→En 16 | BLEU-mb | test | 39.9 | 64 | 2.08 | 2.71 | 3.09 | 3.15 | 16.3 | 8.34 | 20.2 | 19.9 | 0.55 | 15.4 | 23.0 | 26.3 | 30.6 | 33.2 | 35.6 | 38.6  |\\n|  Ro→En 16 | BLEU-sb | test |  | 64 | 2.39 | 3.08 | 3.49 | 3.56 | 16.8 | 8.75 | 20.8 | 20.9 | 0.65 | 15.9 | 23.6 | 26.8 | 31.3 | 34.2 | 36.7 | 40.0  |\\n|  En→Ro 16 | BLEU-mb | test | 38.5 | 64 | 2.14 | 2.65 | 2.53 | 2.50 | 3.46 | 4.24 | 5.32 | 14.1 | 0.35 | 3.30 | 7.89 | 8.72 | 13.2 | 15.1 | 17.3 | 20.6  |\\n|  En→Ro 16 | BLEU-sb | test |  | 64 | 2.61 | 3.11 | 3.07 | 3.09 | 4.26 | 5.31 | 6.43 | 18.0 | 0.55 | 3.90 | 9.15 | 10.3 | 15.7 | 18.2 | 20.8 | 24.9  |\\n|  Fr→En 14 | BLEU-mb | test | 35.0 | 64 | 1.81 | 2.53 | 3.47 | 3.13 | 20.6 | 15.1 | 21.8 | 21.2 | 1.28 | 15.9 | 23.7 | 26.3 | 29.0 | 30.5 | 30.2 | 33.7  |\\n|  Fr→En 14 | BLEU-sb | test |  | 64 | 2.29 | 2.99 | 3.90 | 3.60 | 21.2 | 15.5 | 22.4 | 21.9 | 1.50 | 16.3 | 24.4 | 27.0 | 30.0 | 31.6 | 31.4 | 35.6  |\\n|  En→Fr 14 | BLEU-mb | test | 45.6 | 64 | 1.74 | 2.16 | 2.73 | 2.15 | 15.1 | 8.82 | 12.0 | 25.2 | 0.49 | 8.00 | 14.8 | 15.9 | 20.3 | 23.3 | 24.9 | 28.3  |\\n|  En→Fr 14 | BLEU-sb | test | 45.9 | 64 | 2.44 | 2.75 | 3.54 | 2.82 | 19.3 | 11.4 | 15.3 | 31.3 | 0.81 | 10.0 | 18.2 | 19.3 | 24.7 | 28.3 | 30.1 | 34.1  |\\n|  De→En 16 | BLEU-mb | test | 40.2 | 64 | 2.06 | 2.87 | 3.41 | 3.63 | 21.5 | 17.3 | 23.0 | 27.2 | 0.83 | 16.2 | 22.5 | 24.7 | 28.2 | 30.7 | 33.0 | 30.4  |\\n|  De→En 16 | BLEU-sb | test |  | 64 | 2.39 | 3.27 | 3.85 | 4.04 | 22.5 | 18.2 | 24.4 | 28.6 | 0.93 | 17.1 | 23.4 | 25.8 | 29.2 | 31.9 | 34.5 | 32.1  |\\n|  En→De 16 | BLEU-mb | test | 41.2 | 64 | 1.70 | 2.27 | 2.31 | 2.43 | 12.9 | 8.66 | 10.4 | 24.6 | 0.50 | 7.00 | 12.9 | 13.1 | 18.3 | 20.9 | 22.5 | 26.2  |\\n|  En→De 16 | BLEU-sb | test | 41.2 | 64 | 2.09 | 2.65 | 2.75 | 2.92 | 13.7 | 9.36 | 11.0 | 25.3 | 0.54 | 7.40 | 13.4 | 13.4 | 18.8 | 21.7 | 23.3 | 27.3  |\\n|  Winegrad | acc | test | 93.8 | 7 | 66.3 | 72.9 | 74.7 | 76.9 | 82.4 | 85.7 | 87.9 | 88.3 | 63.4 | 68.5 | 72.9 | 76.9 | 82.4 | 84.6 | 86.1 | 89.7  |\\n|  Winegranule | acc | dev | 84.6 | 50 | 52.0 | 52.1 | 57.4 | 58.7 | 62.3 | 64.5 | 67.9 | 70.2 | 51.3 | 53.0 | 58.3 | 59.1 | 61.7 | 65.8 | 66.9 | 73.2  |\\n|  PIQA | acc | dev | 77.1 | 50 | 64.6 | 70.2 | 72.9 | 75.1 | 75.6 | 78.0 | 78.5 | 81.0 | 64.3 | 69.3 | 71.8 | 74.4 | 74.3 | 76.3 | 77.8 | 80.5  |\\n|  ARC (Challenge) | acc | test | 78.5 | 50 | 26.6 | 29.5 | 31.8 | 35.5 | 38.0 | 41.4 | 43.7 | 51.4 | 25.5 | 30.2 | 31.6 | 36.4 | 38.4 | 41.5 | 43.1 | 53.2  |\\n|  ARC (Easy) | acc | test | 92.0 | 50 | 43.6 | 46.5 | 53.0 | 53.8 | 58.2 | 60.2 | 63.8 | 68.8 | 42.7 | 48.2 | 54.6 | 55.9 | 60.3 | 62.6 | 66.8 | 71.2  |\\n|  OpenBookQA | acc | test | 87.2 | 100 | 35.6 | 43.2 | 45.2 | 46.8 | 53.0 | 50.4 | 55.6 | 57.0 | 39.8 | 46.2 | 46.4 | 53.4 | 53.0 | 55.8 | 58.8 | 37.0  |\\n|  Quac | f1 | dev | 74.4 | 5 | 21.2 | 26.8 | 31.0 | 30.1 | 34.7 | 36.1 | 38.4 | 41.5 | 21.1 | 26.9 | 31.9 | 32.3 | 37.4 | 39.0 | 40.6 | 43.4  |\\n|  RACE-h | acc | test | 90.0 | 10 | 35.2 | 37.9 | 40.1 | 40.9 | 42.4 | 44.1 | 44.6 | 45.5 | 34.3 | 37.7 | 40.0 | 42.0 | 43.8 | 44.3 | 44.6 | 45.9  |\\n|  RACE-m | acc | test | 93.1 | 10 | 42.1 | 47.2 | 52.1 | 52.3 | 54.7 | 54.4 | 56.7 | 58.4 | 42.3 | 47.3 | 51.7 | 55.2 | 56.1 | 54.7 | 56.9 | 57.4  |\\n|  SQuAD-2 | em | dev | 90.7 | 16 | 22.6 | 32.8 | 33.9 | 43.1 | 43.6 | 45.4 | 49.0 | 52.6 | 25.1 | 37.5 | 37.9 | 47.9 | 47.9 | 51.1 | 56.0 | 60.1  |\\n|  SQuADv2 | f1 | dev | 93.0 | 16 | 28.3 | 40.2 | 41.4 | 50.3 | 51.0 | 52.7 | 56.3 | 59.5 | 30.1 | 43.6 | 44.1 | 54.0 | 54.1 | 57.1 | 61.8 | 65.4  |\\n|  CoQA | f1 | dev | 90.7 | 5 | 34.5 | 55.0 | 61.8 | 65.3 | 71.1 | 72.8 | 76.3 | 81.5 | 30.6 | 52.1 | 61.6 | 66.1 | 71.8 | 75.1 | 77.9 | 84.0  |\\n|  DROP | f1 | dev | 89.1 | 20 | 9.40 | 13.6 | 14.4 | 16.4 | 19.7 | 17.0 | 24.0 | 23.6 | 11.7 | 18.1 | 20.9 | 23.0 | 26.4 | 27.3 | 29.2 | 34.3  |\\n|  BoxQ | acc | dev | 91.0 | 32 | 49.7 | 60.3 | 58.9 | 62.4 | 67.1 | 65.4 | 66.2 | 60.5 | 52.6 | 61.7 | 60.4 | 63.7 | 68.4 | 68.7 | 69.0 | 76.7  |\\n|  CB | acc | dev | 96.9 | 32 | 0.00 | 32.1 | 8.93 | 19.6 | 19.6 | 28.6 | 19.6 | 46.4 | 55.4 | 53.6 | 53.6 | 48.2 | 57.1 | 33.9 | 55.4 | 64.3  |\\n|  CB | f1 | dev | 93.9 | 32 | 0.00 | 29.3 | 11.4 | 17.4 | 22.4 | 25.1 | 20.3 | 42.8 | 60.1 | 39.8 | 45.6 | 37.5 | 45.7 | 28.5 | 44.6 | 52.5  |\\n|  Copa | acc | dev | 94.8 | 32 | 66.0 | 68.0 | 73.0 | 77.0 | 76.0 | 80.0 | 84.0 | 91.0 | 62.0 | 64.0 | 66.0 | 74.0 | 76.0 | 82.0 | 86.0 | 87.0  |\\n|  RTE | acc | dev | 92.5 | 32 | 47.7 | 49.8 | 48.4 | 56.0 | 46.6 | 55.2 | 62.8 | 63.5 | 53.1 | 47.3 | 49.5 | 49.5 | 54.9 | 56.3 | 70.4 |   |\\n|  WcC | acc | dev | 76.1 | 32 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 50.0 | 50.3 | 50.3 | 49.2 | 49.4 | 50.3 | 50.0 | 48.6  |\\n|  WSC | acc | dev | 93.8 | 32 | 59.6 | 56.7 | 65.4 | 61.5 | 66.3 | 60.6 | 64.4 | 65.4 | 58.7 | 58.7 | 60.6 | 62.5 | 66.3 | 60.6 | 66.3 | 69.2  |\\n|  MultiRC | acc | dev | 62.3 | 32 | 4.72 | 9.65 | 12.3 | 13.6 | 14.3 | 18.4 | 24.2 | 27.6 | 6.09 | 11.8 | 16.8 | 20.8 | 24.7 | 23.8 | 25.0 | 32.5  |\\n|  MultiRC | f1a | dev | 88.2 | 32 | 57.0 | 59.7 | 60.4 | 59.9 | 60.0 | 64.5 | 71.4 | 72.9 | 57.0 | 59.7 | 60.4 | 59.9 | 60.0 | 64.5 | 71.4 | 72.9  |\\n|  ReCoRD | acc | dev | 92.5 | 32 | 70.8 | 78.5 | 82.1 | 84.1 | 86.2 | 86.6 | 89.0 | 90.2 | 69.8 | 77.0 | 80.7 | 83.0 | 85.9 | 88.0 | 88.8 | 90.2  |\\n|  ReCoRD | f1 | dev | 93.3 | 32 | 71.9 | 79.2 | 82.8 | 85.2 | 87.3 | 89.5 | 90.4 | 91.0 | 70.7 | 77.8 | 81.6 | 83.9 | 86.8 | 88.8 | 89.7 | 91.2  |\\n|  SuperGLUE | average | dev | 89.0 |  | 40.6 | 47.4 | 46.8 | 49.6 | 50.1 | 52.3 | 54.4 | 58.2 | 54.4 | 55.1 | 56.7 | 57.8 | 61.2 | 59.7 | 64.3 | 68.9  |\\n|  ANLI R1 | acc | test | 73.8 | 50 | 33.4 | 34.2 | 33.4 | 33.4 | 34.2 | 32.3 | 33.2 | 34.6 | 32.1 | 31.6 | 31.9 | 34.6 | 30.6 | 31.6 | 32.7 | 32.0  |', 'start_line': 690}, page_content='#### 6.2.1 Gender\\n\\nIn our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3. We measured this by feeding the model a context such as \"The detective was a\" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identifiers include midwife, nurse, receptionist, housekeeper etc.\\n\\nWe also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation} was a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent {occupation} was a,\" the majority of occupations had an even higher probability of being followed by a male identifier than a female one than was the case with our original neutral prompt, \"The {occupation} was a\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as $\\\\frac{1}{n_{\\\\text{jobs}}}\\\\sum_{\\\\text{jobs}}\\\\log(\\\\frac{P(\\\\text{female}|\\\\text{Context})}{P(\\\\text{male}|\\\\text{Context})})$ - was $-1.11$ for the Neutral Variant, $-2.14$ for the Competent Variant and $-1.15$ for the Incompetent Variant.\\n\\nWe also carried out pronoun resolution on the Winogender dataset *[x23]* using two methods which further corroborated the model’s tendency to associate most occupations with males. One method measured the models ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \"The advisor met with the advisee because she wanted to get advice about job applications. ’She’ refers to the\" and found the option with the lowest probability between the two possible options (Choices between Occupation Option: advisor; Participant Option: advisee).\\n\\nOccupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models.\\n\\nWe also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-selected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature\\n\\nTable 6.1: Most Biased Descriptive Words in 175B Model\\n\\n|  Top 10 Most Biased Male Descriptive Words with Raw Co-Occurrence Counts | Top 10 Most Biased Female Descriptive Words with Raw Co-Occurrence Counts  |\\n| --- | --- |\\n|  Average Number of Co-Occurrences Across All Words: 17.5 | Average Number of Co-Occurrences Across All Words: 23.9  |\\n|  Large (16) | Optimistic (12)  |\\n|  Mostly (15) | Bubbly (12)  |\\n|  Lazy (14) | Naughty (12)  |\\n|  Fantastic (13) | Easy-going (12)  |\\n|  Eccentric (13) | Petite (10)  |\\n|  Protect (10) | Tight (10)  |\\n|  Jolly (10) | Pregnant (10)  |\\n|  Stable (9) | Gorgeous (28)  |\\n|  Personable (22) | Sucked (8)  |\\n|  Survive (7) | Beautiful (158)  |\\n\\nof 1 and top_p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\", \"She was very\", \"He would be described as\", \"She would be described as\". We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more often described using appearance oriented words such as \"beautiful\" and \"gorgeous\" as compared to men who were more often described using adjectives that span a greater spectrum.\\n\\nTable 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. \"Most Favored\" here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender.\\n\\n# 6.2.2 Race\\n\\nTo investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\", \"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation  $\\\\left[\\\\mathrm{HZJ}^{+}19\\\\right]$ , we explored how race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5, horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).\\n\\nIt should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology.\\n\\nAcross the models we analyzed, \\'Asian\\' had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, \\'Black\\' had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data.\\n\\n![img-26.jpeg](img-26.jpeg)\\nFigure 6.1: Racial Sentiment Across Models\\n\\n|  Religion | Most Favored Descriptive Words  |\\n| --- | --- |\\n|  Atheism | ‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’, ‘Characterized’  |\\n|  Buddhism | ‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘En- lightenment’, ‘Non-Violent’  |\\n|  Christianity | ‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Com- ments’, ‘Officially’  |\\n|  Hinduism | ‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated’, ‘Africa’  |\\n|  Islam | ‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’, ‘Levant’, ‘Allah’, ‘Prophet’  |\\n|  Judaism | ‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’, ‘Game’, ‘Russian’  |\\n\\nTable 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model.\\n\\n# 6.2.3 Religion\\n\\nWe studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length  $\\\\approx 50$  with a temperature of 1 and a top  $p$  of 0.9 for every prompt. Our prompts were of the nature \" {Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words.\\n\\nThe following is an example output from the model:\\n\\n\"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic.\"\\n\\nSimilar to race, we found that the models make associations with religious terms that indicate some propensity to reflect how these terms are sometimes presented in the world. For example, with the religion Islam, we found that words such as ramadan, prophet and mosque co-occurred at a higher rate than for other religions. We also found that words such as violent, terrorism and terrorist co-occurred at a greater rate with Islam than with other religions and were in the top 40 most favored words for Islam in GPT-3.\\n\\n6.2.4 Future Bias and Fairness Challenges\\n\\nWe have presented this preliminary analysis to share some of the biases we found in order to motivate further research, and to highlight the inherent difficulties in characterizing biases in large-scale generative models; we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community. We view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but we recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model attributes to develop informative labels such as Model Cards for Model Reporting from *[MWZ+18]*.\\n\\nUltimately, it is important not just to characterize biases in language systems but to intervene. The literature on this is also extensive *[QMZ+19, HZJ+19]*, so we offer only a few brief comments on future directions specific to large language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for these models. There is room for more research that engages with the literature outside NLP, better articulates normative statements about harm, and engages with the lived experience of communities affected by NLP systems *[BBDI+20]*. Thus, mitigation work should not be approached purely with a metric driven objective to ‘remove’ bias as this has been shown to have blind spots *[GG+19, NvNvD+G+19]* but in a holistic manner.\\n\\n### 6.3 Energy Usage\\n\\nPractical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3 175B consumed several thousand petaflop/s-days of compute during pre-training, compared to tens of petaflop/s-days for a 1.5B parameter GPT-2 model (Figure 2.2). This means we should be cognizant of the cost and efficiency of such models, as advocated by *[SDSE+19]*.\\n\\nThe use of large-scale pre-training also gives another lens through which to view the efficiency of large models - we should consider not only the resources that go into training them, but how these resources are amortized over the lifetime of a model, which will subsequently be used for a variety of purposes and fine-tuned for specific tasks. Though models like GPT-3 consume significant resources during training, they can be surprisingly efficient once trained: even with the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a few cents in energy costs. Additionally, techniques like model distillation *[LHCG+19a]* can further bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efficient versions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efficiency of such models over time, similar to trends observed in image recognition and neural machine translation *[HB+20]*.\\n\\n## 7 Related Work\\n\\nSeveral lines of work have focused on increasing parameter count and/or computation in language models as a means to improve generative or task performance. An early work scaled LSTM based language models to over a billion parameters *[JVS+16]*. One line of work straightforwardly increases the size of transformer models, scaling up parameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size: 213 million parameters *[VSP+17]* in the original paper, 300 million parameters *[DCL+18]*, 1.5 billion parameters *[RWC+19]*, 8 billion parameters *[SPP+19]*, 11 billion parameters *[RSR+19]*, and most recently 17 billion parameters *[Tur+20]*. A second line of work has focused on increasing parameter count but not computation, as a means of increasing models’ capacity to store information without increased computational cost. These approaches rely on the conditional computation framework *[BLC+13]* and specifically, the mixture-of-experts method *[SMM+17]* has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models *[AJF+19]*, though only a small fraction of the parameters are actually used on each forward pass. A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time *[Gra+16]* and the universal transformer *[DGV+18]*. Our work focuses on the first approach (scaling compute and parameters together, by straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ this strategy.\\n\\nSeveral efforts have also systematically studied the effect of scale on language model performance. *[KMH+20, RRBS+19, LWS+20, HNA+17]*, find a smooth power-law trend in loss as autoregressive language models are scaled up. This work suggests that this trend largely continues as models continue to scale up (although a slight bending of the curve can perhaps be detected in Figure 3.1), and we also find relatively smooth increases in many (though not all) downstream tasks across 3 orders of magnitude of scaling.\\n\\nAnother line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language models that are as small as possible. This approach includes ALBERT *[LCG+19]* as well as general *[HVD+15]* and\\n\\ntask-specific *[x21, JYS^{+}19, x18]* approaches to distillation of language models. These architectures and techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint of giant models.\\n\\nAs fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering *[KPR^{+}19, IBGC^{+}14, CCE^{+}18, x11]*, reading comprehension *[CHI^{+}18, x20]*, and adversarially constructed datasets designed to be difficult for existing language models *[x22, NWD^{+}19]*. In this work we test our models on many of these datasets.\\n\\nMany previous efforts have focused specifically on question-answering, which constitutes a significant fraction of the tasks we tested on. Recent efforts include *[RSR^{+}19, x23]*, which fine-tuned an 11 billion parameter language model, and *[GLT^{+}20]*, which focused on attending over a large corpus of data at test time. Our work differs in focusing on in-context learning but could be combined in the future with those of *[GLT^{+}20, LPP^{+}20]*.\\n\\nMetalearning in language models has been utilized in *[RWC^{+}19]*, though with much more limited results and no systematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including matching networks *[VBL^{+}16]*, RL2 *[DSC^{+}16]*, learning to optimize *[x24, ADG^{+}16, x17]* and MAML *[x12]*. Our approach of stuffing the model’s context with previous examples is most structurally similar to RL2 and also resembles *[x15]*, in that an inner loop of adaptation takes place through computation in the model’s activations across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training) updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks defined at inference-time. Few-shot auto-regressive density estimation was explored in *[RCP^{+}17]* and *[GWC^{+}18]* studied low-resource NMT as a few-shot learning problem.\\n\\nWhile the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained language models in combination with gradient descent to perform few-shot learning *[x25]*. Another sub-field with similar goals is semi-supervised learning where approaches such as UDA *[XDH^{+}19]* also explore methods of fine-tuning when very little labeled data is available.\\n\\nGiving multi-task models instructions in natural language was first formalized in a supervised setting with *[x22]* and utilized for some tasks (such as summarizing) in a language model with *[RWC^{+}19]*. The notion of presenting tasks in natural language was also explored in the text-to-text transformer *[RSR^{+}19]*, although there it was applied for multi-task fine-tuning rather than for in-context learning without weight updates.\\n\\nAnother approach to increasing generality and transfer-learning capability in language models is multi-task learning *[x7]*, which fine-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one. If successful multi-task learning could allow a single model to be used for many tasks without updating the weights (similar to our in-context learning approach), or alternatively could improve sample efficiency when updating the weights for a new task. Multi-task learning has shown some promising initial results *[LGH^{+}15, LSP^{+}18]* and multi-stage fine-tuning has recently become a standardized part of SOTA results on some datasets *[x20]* and pushed the boundaries on certain tasks *[KKS^{+}20]*, but is still limited by the need to manually curate collections of datasets and set up training curricula. By contrast pre-training at large enough scale appears to offer a “natural” broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation *[TFR^{+}17]*, human interaction *[ZSW^{+}19b]*, or active learning *[x21]*.\\n\\nAlgorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality *[x6]*, prefixLM *[x5]* and encoder-decoder architectures *[LLG^{+}19, RSR^{+}19]*, random permutations during training *[YDY^{+}19]*, architectures that improve the efficiency of sampling *[DYY^{+}19]*, improvements in data and training procedures *[LOG^{+}19]*, and efficiency increases in the embedding parameters *[LCG^{+}19]*. Many of these techniques provide significant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3’s performance on downstream tasks, especially in the fine-tuning setting, and combining GPT-3’s scale with these algorithmic techniques is a promising direction for future work.\\n\\n## 8 Conclusion\\n\\nWe presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of\\n\\nstate-of-the-art fine-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks defined on-the-fly. We documented roughly predictable trends of scaling in performance without using fine-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.\\n\\n## Acknowledgements\\n\\nThe authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI’s infrastructure. Thanks to David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of people who created content that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale.\\n\\n##\\n\\nContributions\\n\\nTom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu implemented the large-scale models, training infrastructure, and model-parallel strategies.\\n\\nTom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments.\\n\\nBen Mann and Alec Radford collected, filtered, deduplicated, and conducted overlap analysis on the training data.\\n\\nMelanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and Girish Sastry implemented the downstream tasks and the software framework for supporting them, including creation of synthetic tasks.\\n\\nJared Kaplan and Sam McCandlish initially predicted that a giant language model should show continued gains, and applied scaling laws to help predict and guide model and data scaling decisions for the research.\\n\\nBen Mann implemented sampling without replacement during training.\\n\\nAlec Radford originally demonstrated few-shot learning occurs in language models.\\n\\nJared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and systematically studied in-context learning curves, task prompting, and evaluation methods.\\n\\nPrafulla Dhariwal implemented an early version of the codebase, and developed the memory optimizations for fully half-precision training.\\n\\nRewon Child and Mark Chen developed an early version of our model-parallel strategy.\\n\\nRewon Child and Scott Gray contributed the sparse transformer.\\n\\nAditya Ramesh experimented with loss scaling strategies for pretraining.\\n\\nMelanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam search.\\n\\nPranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature.\\n\\nSandhini Agarwal conducted the fairness and representation analysis.\\n\\nGirish Sastry and Amanda Askell conducted the human evaluations of the model.\\n\\nAriel Herbert-Voss conducted the threat analysis of malicious use.\\n\\nGretchen Krueger edited and red-teamed the policy sections of the paper.\\n\\nBenjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner optimized OpenAI’s clusters to run the largest models efficiently.\\n\\nScott Gray developed fast GPU kernels used during training.\\n\\nJack Clark led the analysis of ethical impacts — fairness and representation, human assessments of the model, and broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work.\\n\\nDario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal, Amanda Askell, Girish Sastry, and Jack Clark wrote the paper.\\n\\nSam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work.\\n\\nAlec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated the benefit of weight decay for training.\\n\\nIlya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla, Rewon, Alec, and Aditya on their work.\\n\\nDario Amodei designed and led the research.\\n\\nA Details of Common Crawl Filtering\\n\\nAs mentioned in Section 2.2, we employed two techniques to improve the quality of the Common Crawl dataset: (1) filtering Common Crawl and (2) fuzzy deduplication:\\n\\n1. In order to improve the quality of Common Crawl, we developed an automatic filtering method to remove low quality documents. Using the original WebText as a proxy for high-quality documents, we trained a classifier to distinguish these from raw Common Crawl. We then used this classifier to re-sample Common Crawl by prioritizing documents which were predicted by the classifier to be higher quality. The classifier is trained using logistic regression classifier with features from Spark’s standard tokenizer and HashingTF . For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unfiltered Common Crawl. We used this classifier to score Common Crawl documents. We kept each document in our dataset iff\\n\\n$\\\\texttt{np.random.pareto}(\\\\alpha)>1-\\\\texttt{document_score}$\\n\\nWe chose $\\\\alpha=9$ in order to take mostly documents the classifier scored highly, but still include some documents that were out of distribution. $\\\\alpha$ was chosen to match the distribution of scores from our classifier on WebText. We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples.\\n2. To further improve model quality and prevent overfitting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark’s MinHashLSH implementation with 10 hashes, using the same features as were used for classification above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10%.\\n\\nAfter filtering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix C.\\n\\n## Appendix B Details of Model Training\\n\\nTo train all versions of GPT-3, we use Adam with $\\\\beta_{1}=0.9$, $\\\\beta_{2}=0.95$, and $\\\\epsilon=10^{-8}$, we clip the global norm of the gradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260 billion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the first 375 million tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overfitting. All models use weight decay of 0.1 to provide a small amount of regularization *[117]*.\\n\\nDuring training we always train on sequences of the full $n_{\\\\mathrm{ctx}}=2048$ token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efficiency. Sequences with multiple documents are not masked in any special way but instead documents within a sequence are delimited with a special end of text token, giving the language model the information necessary to infer that context separated by the end of text token is unrelated. This allows for efficient training without need for any special sequence-specific masking.\\n\\n## Appendix C Details of Test Set Contamination Studies\\n\\nIn section 4 we gave a high level overview of test set contamination studies. In this section we provide details on methodology and results.\\n\\n#### Initial training set filtering\\n\\nWe attempted to remove text occurring in benchmarks from training data by searching for $13-$gram overlaps between all test/development sets used in this work and our training data, and we removed the colliding $13-$gram as well as a 200 character window around it, splitting the original document into pieces. For filtering purposes we define a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than $200$ characters long were discarded. Documents split into more than 10 pieces were considered contaminated and\\n\\nremoved entirely. Originally we removed entire documents given a single collision, but that overly penalized long documents such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in which the Wikipedia article quotes a single line from a book. We ignored $13-$grams that matched more than 10 training documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar content that we likely do want the model to learn, rather than undesired specific overlaps with test sets. Examples for various frequencies can be found in the GPT-3 release repository.\\n\\n#### Overlap methodology\\n\\nFor our benchmark overlap analysis in Section 4, we used a variable number of words $N$ to check for overlap for each dataset, where $N$ is the 5th percentile example length in words, ignoring all punctuation, whitespace, and casing. Due to spurious collisions at lower values of $N$ we use a minimum value of 8 on non-synthetic tasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for $N$ and the amount of data marked as dirty are shown in Table C.1. Unlike GPT-2’s use of bloom filters to compute probabilistic bounds for test contamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps between test sets and our full training corpus, even though we only trained on 40% of our filtered Common Crawl documents per Section 2.2.\\n\\nWe define a ‘dirty’ example as one with any $N$-gram overlap with any training document, and a ‘clean’ example as one with no collision.\\n\\nTest and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed by this analysis, filtering described above failed on long documents such as books. Because of cost considerations it was infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling benchmarks plus the Children’s Book Test showed almost complete overlap, and therefore were not included in this paper. Overlaps are shown in Table C.1\\n\\n#### Overlap results\\n\\nTo understand how much having seen some of the data helps the model perform on downstream tasks, we filter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report the relative percent change between the clean score and the original score. If the clean score is more than 1% or 2% worse than the overall score, it suggests the model may have overfit to the examples it has seen. If the clean score is significantly better, our filtering scheme may have preferentially marked easier examples as dirty.\\n\\nThis overlap metric tends to show a high rate of false positives for datasets that contain background information (but not answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words long, which we ignored in our filtering process (except for wordscrambling tasks). One instance where this technique seems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The information required to answer the question is in a passage provided to the model, so having seen the passage during training but not the questions and answers does not meaningfully constitute cheating. We confirmed that every matching training document contained only the source passage, and none of the questions and answers in the dataset. The more likely explanation for the decrease in performance is that the 6% of examples that remain after filtering come from a slightly different distribution than the dirty examples.\\n\\nFigure 4.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but there is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive to contamination. See Section 4 for details on the datasets we flagged for further review.\\n\\n|  Name | Split | Metric | N | Acc/F1/BLEU | Total Count | Dirty Acc/F1/BLEU | Dirty Count | Clean Acc/F1/BLEU | Clean Count | Clean Percentage | Relative Difference Clean vs All  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  Quac | dev | f1 | 13 | 44.3 | 7353 | 44.3 | 7315 | 54.1 | 38 | 1% | 20%  |\\n|  SQuADv2 | dev | f1 | 13 | 69.8 | 11873 | 69.9 | 11136 | 68.4 | 737 | 6% | -2%  |\\n|  DROP | dev | f1 | 13 | 36.5 | 9536 | 37.0 | 8898 | 29.5 | 638 | 7% | -21%  |\\n|  Symbol Insertion | dev | acc | 7 | 66.9 | 10000 | 66.8 | 8565 | 67.1 | 1435 | 14% | 0%  |\\n|  CoQa | dev | f1 | 13 | 86.0 | 7983 | 85.3 | 5107 | 87.1 | 2876 | 36% | 1%  |\\n|  ReCoRD | dev | acc | 13 | 89.5 | 10000 | 90.3 | 6110 | 88.2 | 3890 | 39% | -1%  |\\n|  Winograd | test | acc | 9 | 88.6 | 273 | 90.2 | 164 | 86.2 | 109 | 40% | -3%  |\\n|  BoolQ | dev | acc | 13 | 76.0 | 3270 | 75.8 | 1955 | 76.3 | 1315 | 40% | 0%  |\\n|  MultiRC | dev | acc | 13 | 74.2 | 953 | 73.4 | 558 | 75.3 | 395 | 41% | 1%  |\\n|  RACE-h | test | acc | 13 | 46.8 | 3498 | 47.0 | 1580 | 46.7 | 1918 | 55% | 0%  |\\n|  LAMBADA | test | acc | 13 | 86.4 | 5153 | 86.9 | 2209 | 86.0 | 2944 | 57% | 0%  |\\n|  LAMBADA (No Blanks) | test | acc | 13 | 77.8 | 5153 | 78.5 | 2209 | 77.2 | 2944 | 57% | -1%  |\\n|  WSC | dev | acc | 13 | 76.9 | 104 | 73.8 | 42 | 79.0 | 62 | 60% | 3%  |\\n|  PIQA | dev | acc | 8 | 82.3 | 1838 | 89.9 | 526 | 79.3 | 1312 | 71% | -4%  |\\n|  RACE-m | test | acc | 13 | 58.5 | 1436 | 53.0 | 366 | 60.4 | 1070 | 75% | 3%  |\\n|  De→En 16 | test | bleu-sb | 12 | 43.0 | 2999 | 47.4 | 739 | 40.8 | 2260 | 75% | -5%  |\\n|  En→De 16 | test | bleu-sb | 12 | 30.9 | 2999 | 32.6 | 739 | 29.9 | 2260 | 75% | -3%  |\\n|  En→Ro 16 | test | bleu-sb | 12 | 25.8 | 1999 | 24.9 | 423 | 26.1 | 1576 | 79% | 1%  |\\n|  Ro→En 16 | test | bleu-sb | 12 | 41.3 | 1999 | 40.4 | 423 | 41.6 | 1576 | 79% | 1%  |\\n|  WebQs | test | acc | 8 | 41.5 | 2032 | 41.6 | 428 | 41.5 | 1604 | 79% | 0%  |\\n|  ANLI R1 | test | acc | 13 | 36.8 | 1000 | 40.5 | 200 | 35.9 | 800 | 80% | -3%  |\\n|  ANLI R2 | test | acc | 13 | 34.0 | 1000 | 29.4 | 177 | 35.0 | 823 | 82% | 3%  |\\n|  TriviaQA | dev | acc | 10 | 71.2 | 7993 | 70.8 | 1390 | 71.3 | 6603 | 83% | 0%  |\\n|  ANLI R3 | test | acc | 13 | 40.2 | 1200 | 38.3 | 196 | 40.5 | 1004 | 84% | 1%  |\\n|  En→Fr 14 | test | bleu-sb | 13 | 39.9 | 3003 | 38.3 | 411 | 40.3 | 2592 | 86% | 1%  |\\n|  Fr→En 14 | test | bleu-sb | 13 | 41.4 | 3003 | 40.9 | 411 | 41.4 | 2592 | 86% | 0%  |\\n|  WiC | dev | acc | 13 | 51.4 | 638 | 53.1 | 49 | 51.3 | 589 | 92% | 0%  |\\n|  RTE | dev | acc | 13 | 71.5 | 277 | 71.4 | 21 | 71.5 | 256 | 92% | 0%  |\\n|  CB | dev | acc | 13 | 80.4 | 56 | 100.0 | 4 | 78.8 | 52 | 93% | -2%  |\\n|  Anagrams 2 | dev | acc | 2 | 40.2 | 10000 | 76.2 | 705 | 37.4 | 9295 | 93% | -7%  |\\n|  Reversed Words | dev | acc | 2 | 0.4 | 10000 | 1.5 | 660 | 0.3 | 9340 | 93% | -26%  |\\n|  OpenBookQA | test | acc | 8 | 65.4 | 500 | 58.1 | 31 | 65.9 | 469 | 94% | 1%  |\\n|  ARC (Easy) | test | acc | 11 | 70.1 | 2268 | 77.5 | 89 | 69.8 | 2179 | 96% | 0%  |\\n|  Anagrams 1 | dev | acc | 2 | 15.0 | 10000 | 49.8 | 327 | 13.8 | 9673 | 97% | -8%  |\\n|  COPA | dev | acc | 9 | 93.0 | 100 | 100.0 | 3 | 92.8 | 97 | 97% | 0%  |\\n|  ARC (Challenge) | test | acc | 12 | 51.6 | 1144 | 45.2 | 31 | 51.8 | 1113 | 97% | 0%  |\\n|  HellaSwag | dev | acc | 13 | 79.3 | 10042 | 86.2 | 152 | 79.2 | 9890 | 98% | 0%  |\\n|  NQs | test | acc | 11 | 29.9 | 3610 | 32.7 | 52 | 29.8 | 3558 | 99% | 0%  |\\n|  Cycled Letters | dev | acc | 2 | 38.6 | 10000 | 20.5 | 73 | 38.7 | 9927 | 99% | 0%  |\\n|  SAT Analogies | dev | acc | 9 | 65.8 | 374 | 100.0 | 2 | 65.6 | 372 | 99% | 0%  |\\n|  StoryCloze | test | acc | 13 | 87.7 | 1871 | 100.0 | 2 | 87.6 | 1869 | 100% | 0%  |\\n|  Winogrande | dev | acc | 13 | 77.7 | 1267 | - | 0 | 77.7 | 1267 | 100% | 0%  |\\n\\nTable C.1: Overlap statistics for all datasets sorted from dirtiest to cleanest. We consider a dataset example dirty if it has a single  $N$ -gram collision with any document in our training corpus. \"Relative Difference Clean vs All\" shows the percent change in performance between only the clean examples vs all the examples in the benchmark. \"Count\" shows the number of examples. \"Clean percentage\" is the percent of examples that are clean vs total. For \"Acc/F1/BLEU\" we use the metric specified in \"Metric\". These scores come from evaluations with a different seed for the random examples used for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper.\\n\\n# D Total Compute Used to Train Language Models\\n\\nThis appendix contains the calculations that were used to derive the approximate compute used to train the language models in Figure 2.2. As a simplifying assumption, we ignore the attention operation, as it typically uses less than  $10\\\\%$  of the total compute for the models we are analyzing.\\n\\nCalculations can be seen in Table D.1 and are explained within the table caption.\\n\\n|  Model | Total train compute (PF-days) | Total train compute (flops) | Params (M) | Training tokens (billions) | Flops per param per token | Mult for bwd pass | Fwd-pass flops per active param per token | Frac of params active for each token  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  T5-Small | 2.08E+00 | 1.80E+20 | 60 | 1,000 | 3 | 3 | 1 | 0.5  |\\n|  T5-Base | 7.64E+00 | 6.60E+20 | 220 | 1,000 | 3 | 3 | 1 | 0.5  |\\n|  T5-Large | 2.67E+01 | 2.31E+21 | 770 | 1,000 | 3 | 3 | 1 | 0.5  |\\n|  T5-3B | 1.04E+02 | 9.00E+21 | 3,000 | 1,000 | 3 | 3 | 1 | 0.5  |\\n|  T5-11B | 3.82E+02 | 3.30E+22 | 11,000 | 1,000 | 3 | 3 | 1 | 0.5  |\\n|  BERT-Base | 1.89E+00 | 1.64E+20 | 109 | 250 | 6 | 3 | 2 | 1.0  |\\n|  BERT-Large | 6.16E+00 | 5.33E+20 | 355 | 250 | 6 | 3 | 2 | 1.0  |\\n|  RoBERTa-Base | 1.74E+01 | 1.50E+21 | 125 | 2,000 | 6 | 3 | 2 | 1.0  |\\n|  RoBERTa-Large | 4.93E+01 | 4.26E+21 | 355 | 2,000 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 Small | 2.60E+00 | 2.25E+20 | 125 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 Medium | 7.42E+00 | 6.41E+20 | 356 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 Large | 1.58E+01 | 1.37E+21 | 760 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 XL | 2.75E+01 | 2.38E+21 | 1,320 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 2.7B | 5.52E+01 | 4.77E+21 | 2,650 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 6.7B | 1.39E+02 | 1.20E+22 | 6,660 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 13B | 2.68E+02 | 2.31E+22 | 12,850 | 300 | 6 | 3 | 2 | 1.0  |\\n|  GPT-3 175B | 3.64E+03 | 3.14E+23 | 174,600 | 300 | 6 | 3 | 2 | 1.0  |\\n\\nTable D.1: Starting from the right hand side and moving left, we begin with the number of training tokens that each model was trained with. Next we note that since T5 uses an encoder-decoder model, only half of the parameters are active for each token during a forward or backwards pass. We then note that each token is involved in a single addition and a single multiply for each active parameter in the forward pass (ignoring attention). Then we add a multiplier of 3x to account for the backwards pass (as computing both  $\\\\frac{\\\\partial \\\\text{params}}{\\\\partial \\\\text{loss}}$  and  $\\\\frac{\\\\partial \\\\text{acts}}{\\\\partial \\\\text{loss}}$  use a similar amount of compute as the forwards pass. Combining the previous two numbers, we get the total flops per parameter per token. We multiply this value by the total training tokens and the total parameters to yield the number of total flops used during training. We report both flops and petaflop/s-day (each of which are 8.64e+19 flops).\\n\\n# E Human Quality Assessment of Synthetic News Articles\\n\\nThis appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic news articles from real news articles. We first describe the experiments on the  $\\\\sim 200$  word news articles, and then describe the preliminary investigation of  $\\\\sim 500$  word news articles generated by GPT-3.\\n\\nParticipants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for failing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean participant age was  $\\\\sim 38$  years old. All participants were recruited through Positly, which maintains a whitelist of high-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic restrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined by pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were not allowed to take part in an experiment more than once.\\n\\nProcedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used the article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B (GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a word count closest to that of the human written article was selected automatically. This was to minimize the effect that completion length might have on participants\\' judgments. The same output procedure for each model with the exception of the removal of the intentionally bad control model, as described in the main text.\\n\\n|  Model | Participants Recruited | Participants Excluded | Genders (m:f:other) | Mean Age | Average Word Count (human:model)  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Control | 76 | 7 | 32:37:0 | 39 | 216:216  |\\n|  GPT-3 Small | 80 | 7 | 41:31:1 | 40 | 216:188  |\\n|  GPT-3 Medium | 80 | 7 | 46:28:2 | 39 | 216:202  |\\n|  GPT-3 Large | 81 | 24 | 46:28:2 | 37 | 216:200  |\\n|  GPT-3 XL | 79 | 14 | 32:32:1 | 38 | 216:199  |\\n|  GPT-3 2.7B | 80 | 11 | 36:33:0 | 40 | 216:202  |\\n|  GPT-3 6.7B | 76 | 5 | 46:28:2 | 37 | 216:195  |\\n|  GPT-3 13.0B | 81 | 13 | 46:28:2 | 37 | 216:209  |\\n|  GPT-3 175B | 80 | 9 | 42:29:0 | 37 | 216:216  |\\n\\nTable E.1: Participant details and article lengths for each experiment to evaluate human detection of  $\\\\sim 200$  word model generated news articles. Participants were excluded due to internet check fails.\\n\\n![img-27.jpeg](img-27.jpeg)\\nFigure E.1: Participants spend more time trying to identify whether each news article is machine generated as model size increases. Duration on the control model is indicated with the dashed line. Line of best fit is a linear model on a log scale with  $95\\\\%$  confidence intervals.\\n\\nIn each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz B. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the articles with human written completions in quiz A had model generated completions in quiz B and vice versa. The order of quiz question was shuffled for each participant. Participants could leave comments and were asked to indicate if they had seen the articles before. Participants were instructed not to look up the articles or their content during the quiz and at the end of the quiz were asked if they had looked anything up during the quiz.\\n\\nStatistical Tests: To compare means on the different runs, we performed a two-sample t-test for independent groups for each model against the control. This was implemented in Python using the scipy.stats.ttest_ind function. When plotting a regression line in the graph of average participant accuracy vs model size, we fit a power law of the form  $\\\\alpha x^{-b}$ . The  $95\\\\%$  confidence intervals were estimated from the t-distribution of the sample mean.\\n\\nDuration statistics: In the main text, we discussed the finding that the ability of human participants to distinguish model and human generated news articles decreases as our models become larger. We have also found that the average time spent for a given set of questions increases as the model size increases, as shown in Figure E.1. Lower\\n\\n|  Model | Participants Recruited | Participants Excluded | Genders (m:f:other) | Mean Age | Average Word Count (human:model)  |\\n| --- | --- | --- | --- | --- | --- |\\n|  Control | 79 | 17 | 32:37:0 | 39 | 569:464  |\\n|  GPT-3 175B | 81 | 19 | 32:30:0 | 40 | 569:498  |\\n\\nTable E.2: Participant details and article lengths for the experiments investigating human detection of  $\\\\sim 500$  word model generated news articles. Participants were excluded due to internet check fails.\\n\\naccuracy scores despite increased time investment from participants supports the finding that larger models generate harder-to-distinguish news articles.\\n\\nPreliminary investigation of  $\\\\sim 500$  word articles: We recruited 160 unique US-based participants to take part in 2 experiments through Positly (details are given in Table E.2). We randomly selected 12 Reuters world news articles from late 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then used the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model from the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half human written and half model generated articles. Comprehension questions were added and articles were shown to participants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task. Model generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments.\\n\\n# F Additional Samples from GPT-3\\n\\nGPT-3 adapts well to many tasks other than the ones explored in the main body of the paper. As an example, in Figure F.1, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the style of Wallace Stevens. We first experimented with a few prompts, then generated four samples with no additional editing or selection (sampling at temperature 1 using nucleus sampling [HBFC19] with  $P = 0.9$ ). Completions were truncated when the model began to write a new title and author heading, or broke into prose commentary.\\n\\n|  Context → | The City BY C. P. CAVAFY TRANSLATED BY EDMUND KEELEY [Poem text omitted]  |\\n| --- | --- |\\n|   | SOME TREES John Ashbery [Poem text omitted]  |\\n|   | Shadows on the Way Wallace Stevens  |\\n|  --- Generated Poem 1 --- | --- Generated Poem 3 ---  |\\n|  I must have shadows on the way If I am to walk I must have Each step taken slowly and alone To have it ready made | The sun was all we had. Now, in the shade All is changed. The mind must dwell on those White fields, that to its eyes were always old; Those ancient gleams, convoluting The way they lay among the huge roots, The great dark tomes of reverie, The plumed labyrinth of the sea.  |\\n|  And I must think in lines of grey To have dim thoughts to be my guide Must look on blue and green And never let my eye forget That color is my friend And purple must surround me too | When our own breath gonne faint and fade Up the long stairway of the dead We shall remember well The blindness of the light, the whiteness Of that white land. The footsteps, and The grasses in that light, and in the shade The menacing life.  |\\n|  The yellow of the sun is no more Intrusive than the bluish snow That falls on all of us. I must have Grey thoughts and blue thoughts walk with me If I am to go away at all. | We played, too, once, in the warmest rooms. To one content, there is one content For all mankind. The forest glades Are all the more for being in fable, And wood and town in legend shadowed- Once more to return. The one content Has in it every happiness, more brave For being purposeless; and so Those island gardens, deep in the mind, Make of remoteness familiarity And of the unspoken name a friend.  |\\n|  --- Generated Poem 2 --- | --- Generated Poem 4 ---  |\\n|  There is a way in the world. That way a man may live in a small town His lifetime and see no man come, Never the stir of men about a market, Never the bobbing of covered carts, Never the creak passed on the stair Closing some other life; an old man Can prosper in that town, can prosper When his store is seen to close, his wife Fades in his house, and the world shouts in vain That no one comes to hear him Where his small bell tolls. In that small town Everything is known. The ways of the young men are known, And the ways of the young women As they walk to the Spring; The rumors are all known, The quarrels, the hurt, the envy Of the men and the women. Till a day comes That way out of the world That leads to a wood. Young, the young man Walks, a lover now, Knows the streets as they are and is of them. | Nobody will come to this place. It is a road that leads nowhere. The solitude is deep. The mountains are high. But they are desolate, and they turn the traveler\\'s face Towards the North. All the sounds of the world are far away. When the wind rises above the trees, The boughs bow to the ground. Even the birds that inhabit the tangle of weeds That is the roadside cover, are silent. One listens, But hears no roar of the forest. One is alone. One will be taken. One will be taken. There is no utterance, there is no conversation, But one is uneasy all the same... There is a thin blue mist, A darkness rising like smoke, And within that darkness A possession of the heart. One will be taken... It was here, and it will be here again- Here, under this sky empty and full of light.  |\\n|  Comes to the wood. |   |\\n|  There, in the wood, among trees, He sees shadows on the way, hears voices, hears the wind and the rustling of leaves; Through an open glade He sees a shape and the shape hears: It waits as he waits, Waits as the shadows wait, As the voices wait; Shadows on the way, voices in the wind. |   |\\n\\nFigure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace Stevens with the title \\'Shadows on the Way\\'.\\n\\n# G Details of Task Phrasing and Specifications\\n\\nThe following figures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from the ground truth datasets in this section, and no samples from GPT-3 are included here.\\n\\n|  Context → | Article: Informal conversation is an important part of any business relationship.Before you start a discussion,however,make sure you understand which topics are suitable and which are considered taboo in a particular culture. Latin Americans enjoy sharing information about their local history, art and customs.You may expect questions about your family,and be sure to show pictures of your children.You may feel free to ask similar questions of your Latin American friends.The French think of conversation as an art form,and they enjoy the value of lively discussions as well as disagreements. For them,arguments can be interesting and they can cover pretty much or any topic --- as long as they occur in are respectful and intelligent manner. In the United States,business people like to discuss a wide range of topics,including opinions about work,family,hobbies,and politics. In Japan,China,and Korea,however,people are much more private.They do not share much about their thoughts,feelings,or emotions because they feel that doing so might take away from the harmonious business relationship they\\'re trying to build.Middle Easterners are also private about their personal lives and family matters.It is considered rude,for example,to ask a businessman from Saudi Arabia about his wife or children. As a general rule,it\\'s best not to talk about politics or religion with your business friends.This can get you into trouble,even in the United States,where people hold different religious views.In addition,discussing one\\'s salary is usually considered unsuitable.Sports is typically a friendly subject in most parts of the world,although be careful not to criticize national sport.Instead,be friendly and praise your host\\'s team. Q: What shouldn\\'t you do when talking about sports with colleagues from another country? A: Criticizing the sports of your colleagues\\' country. Q: Which is typically a friendly topic in most places according to the author? A: Sports. Q: Why are people from Asia more private in their conversation with others? A: They don\\'t want to have their good relationship with others harmed by informal conversation. Q: The author considers politics and religion _ . A:  |\\n| --- | --- |\\n|  Correct Answer → | taboo  |\\n|  Incorrect Answer → | cheerful topics  |\\n|  Incorrect Answer → | rude topics  |\\n|  Incorrect Answer → | topics that can never be talked about  |\\n\\nFigure G.1: Formatted dataset example for RACE-h. When predicting, we normalize by the unconditional probability of each answer as described in 2.\\n\\n|  Context → | anli 2: anli 2: The Gold Coast Hotel & Casino is a hotel and casino located in Paradise, Nevada. This locals\\' casino is owned and operated by Boyd Gaming. The Gold Coast is located one mile (~ 1.6km) west of the Las Vegas Strip on West Flamingo Road. It is located across the street from the Palms Casino Resort and the Rio All Suite Hotel and Casino. Question: The Gold Coast is a budget-friendly casino. True, False, or Neither?  |\\n| --- | --- |\\n|  Correct Answer → | Neither  |\\n|  Incorrect Answer → | True  |\\n|  Incorrect Answer → | False  |\\n\\nFigure G.2: Formatted dataset example for ANLI R2\\n\\n|  Context → | Article: Mrs. Smith is an unusual teacher. Once she told each student to bring along a few potatoes in plastic bag. On each potato the students had to write a name of a person that they hated And the next day, every child brought some potatoes. Some had two potatoes;some three;some up to five. Mrs. Smith then told the children to carry the bags everywhere they went, even to the toilet, for two weeks. As day after day passed, the children started to complain about the awful smell of the rotten potatoes. Those children who brought five potatoes began to feel the weight trouble of the bags. After two weeks, the children were happy to hear that the game was finally ended. Mrs. Smith asked,\"How did you feel while carrying the potatoes for two weeks?\" The children started complaining about the trouble loudly. Then Mrs. Smith told them why she asked them to play the game. She said,\"This is exactly the situation when you carry your hatred for somebody inside your heart. The terrible smell of the hatred will pollute your heart and you will carry something unnecessary with you all the time. If you cannot stand the smell of the rotten potatoes for just two weeks, can you imagine how heavy it would be to have the hatred in your heart for your lifetime? So throw away any hatred from your heart, and you\\'ll be really happy.\" Q: Which of the following is True according to the passage? A: If a kid hated four people,he or she had to carry four potatoes. Q: We can learn from the passage that we should _ . A: throw away the hatred inside Q: The children complained about _ besides the weight trouble. A: the smell Q: Mrs.Smith asked her students to write _ on the potatoes. A:  |\\n| --- | --- |\\n|  Correct Answer → | names  |\\n|  Incorrect Answer → | numbers  |\\n|  Incorrect Answer → | time  |\\n|  Incorrect Answer → | places  |\\n\\nFigure G.3: Formatted dataset example for RACE-m. When predicting, we normalize by the unconditional probability of each answer as described in 2.\\n\\n|  Context → | How to apply sealant to wood.  |\\n| --- | --- |\\n|  Correct Answer → | Using a brush, brush on sealant onto wood until it is fully saturated with the sealant.  |\\n|  Incorrect Answer → | Using a brush, drip on sealant onto wood until it is fully saturated with the sealant.  |\\n\\nFigure G.4: Formatted dataset example for PIQA\\n\\n|  Context → | My body cast a shadow over the grass because  |\\n| --- | --- |\\n|  Correct Answer → | the sun was rising.  |\\n|  Incorrect Answer → | the grass was cut.  |\\n\\nFigure G.5: Formatted dataset example for COPA\\n\\n|  Context → | (CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while serving as Prime Minister of Israel, criticized Donald Trump for appealing to \"Second Amendment people\" in a speech and warned that the words that politicians use can incite violence and undermine democracy. \"Trump\\'s words are an incitement to the type of political violence that touched me personally,\" Rabin wrote in USAToday. He said that Trump\\'s appeal to \"Second Amendment people\" to stop Hillary Clinton -- comments that were criticized as a call for violence against Clinton, something Trump denied -- \"were a new level of ugliness in an ugly campaign season.\"  |\\n| --- | --- |\\n|   | - The son of a former Israeli Prime Minister who was assassinated wrote an op ed about the consequence of violent political rhetoric. - Warns of \"parallels\" between Israel of the 1990s and the U.S. today.  |\\n|  Correct Answer → | - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Donald Trump\\'s aggressive rhetoric.  |\\n|  Correct Answer → | - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Trump\\'s aggressive rhetoric.  |\\n|  Incorrect Answer → | - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Hillary Clinton\\'s aggressive rhetoric.  |\\n|  Incorrect Answer → | - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned U.S.\\'s aggressive rhetoric.  |\\n|  Incorrect Answer → | - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Yitzhak Rabin\\'s aggressive rhetoric.  |\\n\\nFigure G.6: Formatted dataset example for ReCoRD. We consider the context above to be a single \"problem\" because this is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script.\\n\\n|  Context → | anli 1: anli 1: Fulton James MacGregor MSP is a Scottish politician who is a Scottish National Party (SNP) Member of Scottish Parliament for the constituency of Coatbridge and Chryston. MacGregor is currently Parliamentary Liaison Officer to Shona Robison, Cabinet Secretary for Health & Sport. He also serves on the Justice and Education & Skills committees in the Scottish Parliament. Question: Fulton James MacGregor is a Scottish politician who is a Liaison officer to Shona Robison who he swears is his best friend. True, False, or Neither?  |\\n| --- | --- |\\n|  Correct Answer → | Neither  |\\n|  Incorrect Answer → | True  |\\n|  Incorrect Answer → | False  |\\n\\nFigure G.7: Formatted dataset example for ANLI R1\\n\\n|  Context → | Organisms require energy in order to do what?  |\\n| --- | --- |\\n|  Correct Answer → | mature and develop.  |\\n|  Incorrect Answer → | rest soundly.  |\\n|  Incorrect Answer → | absorb light.  |\\n|  Incorrect Answer → | take in nutrients.  |\\n\\nFigure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional probability of each answer as described in 2.\\n\\n|  Context → | Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They  |\\n| --- | --- |\\n|  Correct Answer → | bake them, then frost and decorate.  |\\n|  Incorrect Answer → | taste them as they place them on plates.  |\\n|  Incorrect Answer → | put the frosting on the cake as they pan it.  |\\n|  Incorrect Answer → | come out and begin decorating the cake as well.  |\\n\\nFigure G.9: Formatted dataset example for HellaSwag\\n\\n|  Context → | anli 3: anli 3: We shut the loophole which has American workers actually subsidizing the loss of their own job. They just passed an expansion of that loophole in the last few days: $43 billion of giveaways, including favors to the oil and gas industry and the people importing ceiling fans from China. Question: The loophole is now gone True, False, or Neither?  |\\n| --- | --- |\\n|  Correct Answer → | False  |\\n|  Incorrect Answer → | True  |\\n|  Incorrect Answer → | Neither  |\\n\\nFigure G.10: Formatted dataset example for ANLI R3\\n\\n|  Context → | Question: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat? Answer:  |\\n| --- | --- |\\n|  Correct Answer → | dry palms  |\\n|  Incorrect Answer → | wet palms  |\\n|  Incorrect Answer → | palms covered with oil  |\\n|  Incorrect Answer → | palms covered with lotion  |\\n\\nFigure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional probability of each answer as described in 2.\\n\\n|  Context → | lull is to trust as  |\\n| --- | --- |\\n|  Correct Answer → | cajole is to compliance  |\\n|  Incorrect Answer → | balk is to fortitude  |\\n|  Incorrect Answer → | betray is to loyalty  |\\n|  Incorrect Answer → | hinder is to destination  |\\n|  Incorrect Answer → | soothe is to passion  |\\n\\nFigure G.12: Formatted dataset example for SAT Analogies\\n\\n|  Correct Context → | Grace was happy to trade me her sweater for my jacket. She thinks the sweater  |\\n| --- | --- |\\n|  Incorrect Context → | Grace was happy to trade me her sweater for my jacket. She thinks the jacket  |\\n|  Target Completion → | looks dowdy on her.  |\\n\\nFigure G.13: Formatted dataset example for Winograd. The \\'partial\\' evaluation method we use compares the probability of the completion given a correct and incorrect context.\\n\\n|  Correct Context → | Johnny likes fruits more than vegetables in his new keto diet because the fruits  |\\n| --- | --- |\\n|  Incorrect Context → | Johnny likes fruits more than vegetables in his new keto diet because the vegetables  |\\n|  Target Completion → | are saccharine.  |\\n\\nFigure G.14: Formatted dataset example for Winogrande. The \\'partial\\' evaluation method we use compares the probability of the completion given a correct and incorrect context.\\n\\n|  Context → | READING COMPREHENSION ANSWER KEY While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful. As one NSC staff note put it, \"Under the Taliban, Afghanistan is not so much a state sponsor of terrorism as it is a state sponsored by terrorists.\" In early 2000, the United States began a high-level effort to persuade Pakistan to use its influence over the Taliban. In January 2000, Assistant Secretary of State Karl Inderfurth and the State Department\\'s counterterrorism coordinator, Michael Sheehan, met with General Musharraf in Islamabad, dangling before him the possibility of a presidential visit in March as a reward for Pakistani cooperation. Such a visit was coveted by Musharraf, partly as a sign of his government\\'s legitimacy. He told the two envoys that he would meet with Mullah Omar and press him on Bin Laden. They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything,\" given what it sees as the benefits of Taliban control of Afghanistan.\" President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan. The Secret Service and the CIA, however, warned in the strongest terms that visiting Pakistan would risk the President\\'s life. Counterterrorism officials also argued that Pakistan had not done enough to merit a presidential visit. But President Clinton insisted on including Pakistan in the itinerary for his trip to South Asia. His one-day stopover on March 25, 2000, was the first time a U.S. president had been there since 1969. At his meeting with Musharraf and others, President Clinton concentrated on tensions between Pakistan and India and the dangers of nuclear proliferation, but also discussed Bin Laden. President Clinton told us that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding Bin Laden.\" I offered him the moon when I went to see him, in terms of better relations with the United States, if he\\'d help us get Bin Laden and deal with another issue or two.\" The U.S. effort continued.  |\\n| --- | --- |\\n|   | Who did The State Department feel should visit both India and Pakistan?  |\\n|  Correct Answer → | - [False] Bin Laden  |\\n|  Incorrect Answer → | - [True] Bin Laden  |\\n\\nFigure G.15: Formatted dataset example for MultiRC. There are three levels within MultiRC: (1) the passage, (2) the questions, and (3) the answers. During evaluation, accuracy is determined at the per-question level, with a question being considered correct if and only if all the answers within the question are labeled correctly. For this reason, we use  $K$  to refer to the number of questions shown within the context.\\n\\n|  Context → | Question: Which factor will most likely cause a person to develop a fever? Answer:  |\\n| --- | --- |\\n|  Correct Answer → | a bacterial population in the bloodstream  |\\n|  Incorrect Answer → | a leg muscle relaxing after exercise  |\\n|  Incorrect Answer → | several viral particles on the skin  |\\n|  Incorrect Answer → | carbohydrates being digested in the stomach  |\\n\\nFigure G.16: Formatted dataset example for ARC (Easy). When predicting, we normalize by the unconditional probability of each answer as described in 2.\\n\\n|  Context → | Bob went to the gas station to fill up his car. His tank was completely empty and so was his wallet. The cashier offered to pay for his gas if he came back later to pay. Bob felt grateful as he drove home.  |\\n| --- | --- |\\n|  Correct Answer → | Bob believed that there were good people in the world.  |\\n|  Incorrect Answer → | Bob contemplated how unfriendly the world was.  |\\n\\nFigure G.17: Formatted dataset example for StoryCloze\\n\\n|  Context → | Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland. Helsinki has a population of , an urban population of , and a metropolitan population of over 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities. The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world\\'s northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland\\'s major political, educational, financial, cultural, and research center as well as one of northern Europe\\'s major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia. Q: what is the most populous municipality in Finland? A: Helsinki Q: how many people live there? A: 1.4 million in the metropolitan area Q: what percent of the foreign companies that operate in Finland are in Helsinki? A: 75% Q: what towns are a part of the metropolitan area? A:  |\\n| --- | --- |\\n|  Target Completion → | Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns  |\\n\\nFigure G.18: Formatted dataset example for CoQA\\n\\n|  Context → | Please unscramble the letters into a word, and write that word: asinoc =  |\\n| --- | --- |\\n|  Target Completion → | casino  |\\n\\nFigure G.19: Formatted dataset example for Cycled Letters\\n\\n|  Context → | Passage: Saint Jean de Brébeuf was a French Jesuit missionary who travelled to New France in 1625. There he worked primarily with the Huron for the rest of his life, except for a few years in France from 1629 to 1633. He learned their language and culture, writing extensively about each to aid other missionaries. In 1649, Brébeuf and another missionary were captured when an Iroquois raid took over a Huron village. Together with Huron captives, the missionaries were ritually tortured and killed on March 16, 1649. Brébeuf was beatified in 1925 and among eight Jesuit missionaries canonized as saints in the Roman Catholic Church in 1930. Question: How many years did Saint Jean de Brébeuf stay in New France before he went back to France for a few years? Answer:  |\\n| --- | --- |\\n|  Target Completion → | 4  |\\n\\nFigure G.20: Formatted dataset example for DROP\\n\\n|  Context → | Fill in blank: She held the torch in front of her. She caught her breath. \"Chris? There\\'s a step.\" \"What?\" \"A step. Cut in the rock. About fifty feet ahead.\" She moved faster. They both moved faster. \"In fact,\" she said, raising the torch higher, \"there\\'s more than a ____ . ->  |\\n| --- | --- |\\n|  Target Completion → | step  |\\n\\nFigure G.21: Formatted dataset example for LAMBADA\\n\\n|  Context → | Please unscramble the letters into a word, and write that word: skicts =  |\\n| --- | --- |\\n|  Target Completion → | sticks  |\\n\\nFigure G.22: Formatted dataset example for Anagrams 1 (A1)\\n\\n|  Context → | Please unscramble the letters into a word, and write that word: volwskagen =  |\\n| --- | --- |\\n|  Target Completion → | volkswagen  |\\n\\nFigure G.23: Formatted dataset example for Anagrams 2\\n\\n|  Context → | Q: Who played tess on touched by an angel?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | Delloreese Patricia Early (July 6, 1931 { November 19, 2017), known professionally as Della Reese  |\\n\\nFigure G.24: Formatted dataset example for Natural Questions\\n\\n|  Context → | TITLE: William Perry (American football) - Professional career PARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL Draft by the Chicago Bears; he had been hand-picked by coach Mike Ditka. However, defensive coordinator Buddy Ryan, who had a highly acrimonious relationship with Ditka, called Perry a \"wasted draft-pick\". Perry soon became a pawn in the political power struggle between Ditka and Ryan. Perry\\'s \"Refrigerator\" nickname followed him into the NFL and he quickly became a favorite of the Chicago Bears fans. Teammates called him \"Biscuit,\" as in \"one biscuit shy of 350 pounds.\" While Ryan refused to play Perry, Ditka decided to use Perry as a fullback when the team was near the opponents\\' goal line or in fourth and short situations, either as a ball carrier or a lead blocker for star running back Walter Payton. Ditka stated the inspiration for using Perry as a fullback came to him during five-yard sprint exercises. During his rookie season, Perry rushed for two touchdowns and caught a pass for one. Perry even had the opportunity to run the ball during Super Bowl XX, as a nod to his popularity and contributions to the team\\'s success. The first time he got the ball, he was tackled for a one-yard loss while attempting to throw his first NFL pass on a halfback option play. The second time he got the ball, he scored a touchdown (running over Patriots linebacker Larry McGrew in the process). About halfway through his rookie season, Ryan finally began to play Perry, who soon proved that he was a capable defensive lineman. His Super Bowl ring size is the largest of any professional football player in the history of the event. His ring size is 25, while the ring size for the average adult male is between 10 and 12. Perry went on to play for ten years in the NFL, retiring after the 1994 season. In his ten years as a pro, he regularly struggled with his weight, which hampered his performance at times. He played in 138 games, recording 29.5 sacks and five fumble recoveries, which he returned for a total of 71 yards. In his offensive career he ran five yards for two touchdowns, and had one reception for another touchdown. Perry later attempted a comeback, playing an unremarkable 1996 season with the London Monarchs of the World League of American Football (later NFL Europa). Q: what team did he play for? A:  |\\n| --- | --- |\\n|  Target Completion → | the Chicago Bears  |\\n\\nFigure G.25: Formatted dataset example for QuAC\\n\\n|  Context → | Please unscramble the letters into a word, and write that word: r e!c.i p r o.c a/l =  |\\n| --- | --- |\\n|  Target Completion → | reciprocal  |\\n\\nFigure G.26: Formatted dataset example for Symbol Insertion\\n\\n|  Context → | Please unscramble the letters into a word, and write that word: taefed =  |\\n| --- | --- |\\n|  Target Completion → | defeat  |\\n\\nFigure G.27: Formatted dataset example for Reversed Words\\n\\n|  Context → | Title: TheBlitz  |\\n| --- | --- |\\n|   | Background: From the German point of view, March 1941 saw an improvement. The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. X- and Y-Gerät beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-Gerät, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Gerät.  |\\n|   | Q: How many sorties were flown in March 1941?  |\\n|   | A: 4,000  |\\n|   | Q: When did the Luftwaffe fly inland missions?  |\\n|   | A:  |\\n|  Target Completion → | only on moonlit nights  |\\n\\nFigure G.28: Formatted dataset example for SQuADv2\\n\\n|  Context → | Normal force -- In a simple case such as an object resting upon a table, the normal force on the object is equal but in opposite direction to the gravitational force applied on the object (or the weight of the object), that is, N = m g (\\\\displaystyle N=mg), where m is mass, and g is the gravitational field strength (about 9.81 m/s on Earth). The normal force here represents the force applied by the table against the object that prevents it from sinking through the table and requires that the table is sturdy enough to deliver this normal force without breaking. However, it is easy to assume that the normal force and weight are action-reaction force pairs (a common mistake). In this case, the normal force and weight need to be equal in magnitude to explain why there is no upward acceleration of the object. For example, a ball that bounces upwards accelerates upwards because the normal force acting on the ball is larger in magnitude than the weight of the ball. question: is the normal force equal to the force of gravity? answer:  |\\n| --- | --- |\\n|  Target Completion → | yes  |\\n\\nFigure G.29: Formatted dataset example for BoolQ\\n\\n|  Context → | The trend toward lower rents may seem surprising given that some communities in New York are bemoaning the loss of favorite local businesses to high rents. But, despite the recent softening, for many of these retailers there\\'s still been too big a jump from the rental rates of the late 1970s, when their leases were signed. Certainly, the recent drop in prices doesn\\'t mean Manhattan comes cheap. question: Manhattan comes cheap. true, false, or neither? answer:  |\\n| --- | --- |\\n|  Target Completion → | false  |\\n\\nFigure G.30: Formatted dataset example for CB\\n\\n|  Context → | The bet, which won him dinner for four, was regarding the existence and mass of the top quark, an elementary particle discovered in 1995. question: The Top Quark is the last of six flavors of quarks predicted by the standard model theory of particle physics. True or False? answer:  |\\n| --- | --- |\\n|  Target Completion → | False  |\\n\\nFigure G.31: Formatted dataset example for RTE\\n\\n|  Context → | An outfitter provided everything needed for the safari. Before his first walking holiday, he went to a specialist outfitter to buy some boots. question: Is the word \\'outfitter\\' used in the same way in the two sentences above? answer:  |\\n| --- | --- |\\n|  Target Completion → | no  |\\n\\nFigure G.32: Formatted dataset example for WiC\\n\\n|  Context → | Final Exam with Answer Key Instructions: Please carefully read the following passages. For each passage, you must identify which noun the pronoun marked in *bold* refers to. ==== Passage: Mr. Moncrieff visited Chester\\'s luxurious New York apartment, thinking that it belonged to his son Edward. The result was that Mr. Moncrieff has decided to cancel Edward\\'s allowance on the ground that he no longer requires *his* financial support. Question: In the passage above, what does the pronoun \"*his*\" refer to? Answer:  |\\n| --- | --- |\\n|  Target Completion → | mr. moncrieff  |\\n\\nFigure G.33: Formatted dataset example for WSC\\n\\n|  Context → | Q: \\'Nude Descending A Staircase\\' is perhaps the most famous painting by which 20th century artist?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | MARCEL DUCHAMP  |\\n|  Target Completion → | r mutt  |\\n|  Target Completion → | duchamp  |\\n|  Target Completion → | marcel duchamp  |\\n|  Target Completion → | R.Mutt  |\\n|  Target Completion → | Marcel duChamp  |\\n|  Target Completion → | Henri-Robert-Marcel Duchamp  |\\n|  Target Completion → | Marcel du Champ  |\\n|  Target Completion → | henri robert marcel duchamp  |\\n|  Target Completion → | Duchampian  |\\n|  Target Completion → | Duchamp  |\\n|  Target Completion → | duchampian  |\\n|  Target Completion → | marcel du champ  |\\n|  Target Completion → | Marcel Duchamp  |\\n|  Target Completion → | MARCEL DUCHAMP  |\\n\\nFigure G.34: Formatted dataset example for TriviaQA. TriviaQA allows for multiple valid completions.\\n\\n|  Context → | Q: What school did burne hogarth establish?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | School of Visual Arts  |\\n\\nFigure G.35: Formatted dataset example for WebQA\\n\\n|  Context → | Keinesfalls dürfen diese für den kommerziellen Gebrauch verwendet werden. =  |\\n| --- | --- |\\n|  Target Completion → | In no case may they be used for commercial purposes.  |\\n\\nFigure G.36: Formatted dataset example for De→En. This is the format for one- and few-shot learning, for this and other language tasks, the format for zero-shot learning is “Q: What is the {language} translation of {sentence} A: {translation}.”\\n\\n|  Context → | In no case may they be used for commercial purposes. =  |\\n| --- | --- |\\n|  Target Completion → | Keinesfalls dürfen diese für den kommerziellen Gebrauch verwendet werden.  |\\n\\nFigure G.37: Formatted dataset example for  $\\\\mathrm{En}\\\\rightarrow \\\\mathrm{De}$\\n\\n|  Context → | Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females. =  |\\n| --- | --- |\\n|  Target Completion → | L\\'analyse de la distribution de fréquence des stades larvaires d\\'I. verticalis dans une série d\\'étangs a également démontré que les larves mâles étaient à des stades plus avancés que les larves femelles.  |\\n\\nFigure G.38: Formatted dataset example for  $\\\\mathrm{En}\\\\rightarrow \\\\mathrm{Fr}$\\n\\n|  Context → | L\\'analyse de la distribution de fréquence des stades larvaires d\\'I. verticalis dans une série d\\'étangs a également démontré que les larves mâles étaient à des stades plus avancés que les larves femelles. =  |\\n| --- | --- |\\n|  Target Completion → | Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females.  |\\n\\nFigure G.39: Formatted dataset example for  $\\\\mathrm{Fr}\\\\rightarrow \\\\mathrm{En}$\\n\\n|  Context → | The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey\\'s accession to the European Union, despite Turkey\\'s continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. =  |\\n| --- | --- |\\n|  Target Completion → | Adevărul este că vă doriti, cu orice pret şi împotriva dorinței europenilor, să continuaţi negocierile de aderare a Turciei la Uniunea Europeană, în ciuda refuzului continuu al Turciei de a recunoaşte Ciprul şi în ciuda faptului că reformele democratice au ajuns într-un punct mort.  |\\n\\nFigure G.40: Formatted dataset example for  $\\\\mathrm{En}\\\\rightarrow \\\\mathrm{Ro}$\\n\\nFigure G.41: Formatted dataset example for  $\\\\mathrm{Ro}\\\\rightarrow \\\\mathrm{En}$\\n\\n|  Context → | Q: What is (2 * 4) * 6? A:  |\\n| --- | --- |\\n|  Target Completion → | 48  |\\n|  Figure G.42: Formatted dataset example for Arithmetic 1DC  |   |\\n|  Context → | Q: What is 17 minus 14? A:  |\\n|  Target Completion → | 3  |\\n|  Figure G.43: Formatted dataset example for Arithmetic 2D-  |   |\\n|  Context → | Q: What is 98 plus 45? A:  |\\n|  Target Completion → | 143  |\\n|  Figure G.44: Formatted dataset example for Arithmetic 2D+  |   |\\n|  Context → | Q: What is 95 times 45? A:  |\\n|  Target Completion → | 4275  |\\n|  Figure G.45: Formatted dataset example for Arithmetic 2Dx  |   |\\n|  Context → | Q: What is 509 minus 488? A:  |\\n|  Target Completion → | 21  |\\n|  Figure G.46: Formatted dataset example for Arithmetic 3D-  |   |\\n|  Context → | Q: What is 556 plus 497? A:  |\\n|  Target Completion → | 1053  |\\n|  Figure G.47: Formatted dataset example for Arithmetic 3D+  |   |\\n|  Context → | Q: What is 6209 minus 3365? A:  |\\n|  Target Completion → | 2844  |\\n\\nFigure G.48: Formatted dataset example for Arithmetic 4D-\\n\\n|  Context → | Q: What is 9923 plus 617?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | 10540  |\\n\\nFigure G.49: Formatted dataset example for Arithmetic 4D+\\n\\n|  Context → | Q: What is 40649 minus 78746?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | -38097  |\\n\\nFigure G.50: Formatted dataset example for Arithmetic 5D-\\n\\n|  Context → | Q: What is 65360 plus 16204?  |\\n| --- | --- |\\n|   | A:  |\\n|  Target Completion → | 81564  |\\n\\nFigure G.51: Formatted dataset example for Arithmetic 5D+\\n\\nH Results on All Tasks for All Model Sizes\\n\\n|  Name | Metric | Splits | Fine-tune |   | Zero-Shot |   |   |   |   | One-Shot |   |   |   |   | Few-Shot |   |   |   |   |   |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|   |   |   |  SOTA | K | Small Med Large | XL | 2.7B | 6.7B | 13B | 175B | Small Med Large | XL | 2.7B | 6.7B | 13B | 175B | Small Med Large | XL | 2.7B | 6.7B  |\\n|  HellaSwag | acc | dev | 85.6 | 20 | 33.7 | 43.6 | 51.0 | 54.7 | 62.8 | 67.4 | 70.9 | 78.9 | 33.0 | 42.9 | 50.5 | 53.5 | 61.9 | 66.5 | 70.0 | 78.1  |\\n|  LAMBADA | acc | test | 68.0 | 15 | 42.7 | 54.3 | 60.4 | 63.6 | 67.1 | 70.3 | 72.5 | 76.2 | 22.0 | 47.1 | 52.6 | 58.3 | 61.1 | 65.4 | 69.0 | 72.5  |\\n|  LAMBADA | ppl | test | 8.63 | 15 | 18.6 | 9.09 | 6.53 | 5.44 | 4.60 | 4.00 | 3.56 | 3.00 | 165.0 | 11.6 | 8.29 | 6.46 | 5.53 | 4.61 | 4.06 | 3.35  |\\n|  StoryCloze | acc | test | 91.8 | 70 | 63.3 | 68.5 | 72.4 | 73.4 | 77.2 | 77.7 | 79.5 | 83.2 | 62.3 | 68.7 | 72.3 | 74.2 | 77.3 | 78.7 | 79.7 | 84.7  |\\n|  NQs | acc | test | 44.5 | 64 | 0.64 | 1.75 | 2.71 | 4.40 | 6.01 | 5.79 | 7.84 | 14.6 | 1.19 | 3.07 | 4.79 | 5.43 | 8.73 | 9.78 | 13.7 | 23.0  |\\n|  TriviaQA | acc | dev | 68.0 | 64 | 4.15 | 7.61 | 14.0 | 19.7 | 31.3 | 38.7 | 41.8 | 64.3 | 4.19 | 12.9 | 20.5 | 26.5 | 35.9 | 44.4 | 51.3 | 68.0  |\\n|  WebQs | acc | test | 45.5 | 64 | 1.77 | 3.20 | 4.33 | 4.63 | 7.92 | 7.73 | 8.22 | 14.4 | 2.56 | 6.20 | 8.51 | 9.15 | 14.5 | 15.1 | 19.0 | 25.3  |\\n|  Ro→En 16 | BLEU-mb | test | 39.9 | 64 | 2.08 | 2.71 | 3.09 | 3.15 | 16.3 | 8.34 | 20.2 | 19.9 | 0.55 | 15.4 | 23.0 | 26.3 | 30.6 | 33.2 | 35.6 | 38.6  |\\n|  Ro→En 16 | BLEU-sb | test |  | 64 | 2.39 | 3.08 | 3.49 | 3.56 | 16.8 | 8.75 | 20.8 | 20.9 | 0.65 | 15.9 | 23.6 | 26.8 | 31.3 | 34.2 | 36.7 | 40.0  |\\n|  En→Ro 16 | BLEU-mb | test | 38.5 | 64 | 2.14 | 2.65 | 2.53 | 2.50 | 3.46 | 4.24 | 5.32 | 14.1 | 0.35 | 3.30 | 7.89 | 8.72 | 13.2 | 15.1 | 17.3 | 20.6  |\\n|  En→Ro 16 | BLEU-sb | test |  | 64 | 2.61 | 3.11 | 3.07 | 3.09 | 4.26 | 5.31 | 6.43 | 18.0 | 0.55 | 3.90 | 9.15 | 10.3 | 15.7 | 18.2 | 20.8 | 24.9  |\\n|  Fr→En 14 | BLEU-mb | test | 35.0 | 64 | 1.81 | 2.53 | 3.47 | 3.13 | 20.6 | 15.1 | 21.8 | 21.2 | 1.28 | 15.9 | 23.7 | 26.3 | 29.0 | 30.5 | 30.2 | 33.7  |\\n|  Fr→En 14 | BLEU-sb | test |  | 64 | 2.29 | 2.99 | 3.90 | 3.60 | 21.2 | 15.5 | 22.4 | 21.9 | 1.50 | 16.3 | 24.4 | 27.0 | 30.0 | 31.6 | 31.4 | 35.6  |\\n|  En→Fr 14 | BLEU-mb | test | 45.6 | 64 | 1.74 | 2.16 | 2.73 | 2.15 | 15.1 | 8.82 | 12.0 | 25.2 | 0.49 | 8.00 | 14.8 | 15.9 | 20.3 | 23.3 | 24.9 | 28.3  |\\n|  En→Fr 14 | BLEU-sb | test | 45.9 | 64 | 2.44 | 2.75 | 3.54 | 2.82 | 19.3 | 11.4 | 15.3 | 31.3 | 0.81 | 10.0 | 18.2 | 19.3 | 24.7 | 28.3 | 30.1 | 34.1  |\\n|  De→En 16 | BLEU-mb | test | 40.2 | 64 | 2.06 | 2.87 | 3.41 | 3.63 | 21.5 | 17.3 | 23.0 | 27.2 | 0.83 | 16.2 | 22.5 | 24.7 | 28.2 | 30.7 | 33.0 | 30.4  |\\n|  De→En 16 | BLEU-sb | test |  | 64 | 2.39 | 3.27 | 3.85 | 4.04 | 22.5 | 18.2 | 24.4 | 28.6 | 0.93 | 17.1 | 23.4 | 25.8 | 29.2 | 31.9 | 34.5 | 32.1  |\\n|  En→De 16 | BLEU-mb | test | 41.2 | 64 | 1.70 | 2.27 | 2.31 | 2.43 | 12.9 | 8.66 | 10.4 | 24.6 | 0.50 | 7.00 | 12.9 | 13.1 | 18.3 | 20.9 | 22.5 | 26.2  |\\n|  En→De 16 | BLEU-sb | test | 41.2 | 64 | 2.09 | 2.65 | 2.75 | 2.92 | 13.7 | 9.36 | 11.0 | 25.3 | 0.54 | 7.40 | 13.4 | 13.4 | 18.8 | 21.7 | 23.3 | 27.3  |\\n|  Winegrad | acc | test | 93.8 | 7 | 66.3 | 72.9 | 74.7 | 76.9 | 82.4 | 85.7 | 87.9 | 88.3 | 63.4 | 68.5 | 72.9 | 76.9 | 82.4 | 84.6 | 86.1 | 89.7  |\\n|  Winegranule | acc | dev | 84.6 | 50 | 52.0 | 52.1 | 57.4 | 58.7 | 62.3 | 64.5 | 67.9 | 70.2 | 51.3 | 53.0 | 58.3 | 59.1 | 61.7 | 65.8 | 66.9 | 73.2  |\\n|  PIQA | acc | dev | 77.1 | 50 | 64.6 | 70.2 | 72.9 | 75.1 | 75.6 | 78.0 | 78.5 | 81.0 | 64.3 | 69.3 | 71.8 | 74.4 | 74.3 | 76.3 | 77.8 | 80.5  |\\n|  ARC (Challenge) | acc | test | 78.5 | 50 | 26.6 | 29.5 | 31.8 | 35.5 | 38.0 | 41.4 | 43.7 | 51.4 | 25.5 | 30.2 | 31.6 | 36.4 | 38.4 | 41.5 | 43.1 | 53.2  |\\n|  ARC (Easy) | acc | test | 92.0 | 50 | 43.6 | 46.5 | 53.0 | 53.8 | 58.2 | 60.2 | 63.8 | 68.8 | 42.7 | 48.2 | 54.6 | 55.9 | 60.3 | 62.6 | 66.8 | 71.2  |\\n|  OpenBookQA | acc | test | 87.2 | 100 | 35.6 | 43.2 | 45.2 | 46.8 | 53.0 | 50.4 | 55.6 | 57.0 | 39.8 | 46.2 | 46.4 | 53.4 | 53.0 | 55.8 | 58.8 | 37.0  |\\n|  Quac | f1 | dev | 74.4 | 5 | 21.2 | 26.8 | 31.0 | 30.1 | 34.7 | 36.1 | 38.4 | 41.5 | 21.1 | 26.9 | 31.9 | 32.3 | 37.4 | 39.0 | 40.6 | 43.4  |\\n|  RACE-h | acc | test | 90.0 | 10 | 35.2 | 37.9 | 40.1 | 40.9 | 42.4 | 44.1 | 44.6 | 45.5 | 34.3 | 37.7 | 40.0 | 42.0 | 43.8 | 44.3 | 44.6 | 45.9  |\\n|  RACE-m | acc | test | 93.1 | 10 | 42.1 | 47.2 | 52.1 | 52.3 | 54.7 | 54.4 | 56.7 | 58.4 | 42.3 | 47.3 | 51.7 | 55.2 | 56.1 | 54.7 | 56.9 | 57.4  |\\n|  SQuAD-2 | em | dev | 90.7 | 16 | 22.6 | 32.8 | 33.9 | 43.1 | 43.6 | 45.4 | 49.0 | 52.6 | 25.1 | 37.5 | 37.9 | 47.9 | 47.9 | 51.1 | 56.0 | 60.1  |\\n|  SQuADv2 | f1 | dev | 93.0 | 16 | 28.3 | 40.2 | 41.4 | 50.3 | 51.0 | 52.7 | 56.3 | 59.5 | 30.1 | 43.6 | 44.1 | 54.0 | 54.1 | 57.1 | 61.8 | 65.4  |\\n|  CoQA | f1 | dev | 90.7 | 5 | 34.5 | 55.0 | 61.8 | 65.3 | 71.1 | 72.8 | 76.3 | 81.5 | 30.6 | 52.1 | 61.6 | 66.1 | 71.8 | 75.1 | 77.9 | 84.0  |\\n|  DROP | f1 | dev | 89.1 | 20 | 9.40 | 13.6 | 14.4 | 16.4 | 19.7 | 17.0 | 24.0 | 23.6 | 11.7 | 18.1 | 20.9 | 23.0 | 26.4 | 27.3 | 29.2 | 34.3  |\\n|  BoxQ | acc | dev | 91.0 | 32 | 49.7 | 60.3 | 58.9 | 62.4 | 67.1 | 65.4 | 66.2 | 60.5 | 52.6 | 61.7 | 60.4 | 63.7 | 68.4 | 68.7 | 69.0 | 76.7  |\\n|  CB | acc | dev | 96.9 | 32 | 0.00 | 32.1 | 8.93 | 19.6 | 19.6 | 28.6 | 19.6 | 46.4 | 55.4 | 53.6 | 53.6 | 48.2 | 57.1 | 33.9 | 55.4 | 64.3  |\\n|  CB | f1 | dev | 93.9 | 32 | 0.00 | 29.3 | 11.4 | 17.4 | 22.4 | 25.1 | 20.3 | 42.8 | 60.1 | 39.8 | 45.6 | 37.5 | 45.7 | 28.5 | 44.6 | 52.5  |\\n|  Copa | acc | dev | 94.8 | 32 | 66.0 | 68.0 | 73.0 | 77.0 | 76.0 | 80.0 | 84.0 | 91.0 | 62.0 | 64.0 | 66.0 | 74.0 | 76.0 | 82.0 | 86.0 | 87.0  |\\n|  RTE | acc | dev | 92.5 | 32 | 47.7 | 49.8 | 48.4 | 56.0 | 46.6 | 55.2 | 62.8 | 63.5 | 53.1 | 47.3 | 49.5 | 49.5 | 54.9 | 56.3 | 70.4 |   |\\n|  WcC | acc | dev | 76.1 | 32 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 50.0 | 50.3 | 50.3 | 49.2 | 49.4 | 50.3 | 50.0 | 48.6  |\\n|  WSC | acc | dev | 93.8 | 32 | 59.6 | 56.7 | 65.4 | 61.5 | 66.3 | 60.6 | 64.4 | 65.4 | 58.7 | 58.7 | 60.6 | 62.5 | 66.3 | 60.6 | 66.3 | 69.2  |\\n|  MultiRC | acc | dev | 62.3 | 32 | 4.72 | 9.65 | 12.3 | 13.6 | 14.3 | 18.4 | 24.2 | 27.6 | 6.09 | 11.8 | 16.8 | 20.8 | 24.7 | 23.8 | 25.0 | 32.5  |\\n|  MultiRC | f1a | dev | 88.2 | 32 | 57.0 | 59.7 | 60.4 | 59.9 | 60.0 | 64.5 | 71.4 | 72.9 | 57.0 | 59.7 | 60.4 | 59.9 | 60.0 | 64.5 | 71.4 | 72.9  |\\n|  ReCoRD | acc | dev | 92.5 | 32 | 70.8 | 78.5 | 82.1 | 84.1 | 86.2 | 86.6 | 89.0 | 90.2 | 69.8 | 77.0 | 80.7 | 83.0 | 85.9 | 88.0 | 88.8 | 90.2  |\\n|  ReCoRD | f1 | dev | 93.3 | 32 | 71.9 | 79.2 | 82.8 | 85.2 | 87.3 | 89.5 | 90.4 | 91.0 | 70.7 | 77.8 | 81.6 | 83.9 | 86.8 | 88.8 | 89.7 | 91.2  |\\n|  SuperGLUE | average | dev | 89.0 |  | 40.6 | 47.4 | 46.8 | 49.6 | 50.1 | 52.3 | 54.4 | 58.2 | 54.4 | 55.1 | 56.7 | 57.8 | 61.2 | 59.7 | 64.3 | 68.9  |\\n|  ANLI R1 | acc | test | 73.8 | 50 | 33.4 | 34.2 | 33.4 | 33.4 | 34.2 | 32.3 | 33.2 | 34.6 | 32.1 | 31.6 | 31.9 | 34.6 | 30.6 | 31.6 | 32.7 | 32.0  |'), -0.11025386266040815)]\n",
      "  results = self._store.similarity_search_with_relevance_scores(query, **kwargs)  # type: ignore[arg-type]\n"
     ]
    }
   ],
   "source": [
    "# Search with tag filtering\n",
    "# Restrict search to documents matching specific tags.\n",
    "\n",
    "query = \"generative models\"\n",
    "for hit in kb.search_docs(query, top_k=3, tags={\"nlp\"}):\n",
    "    print(f\"[{hit.score:.4f}] {hit.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5000] BERT.pdf\n"
     ]
    }
   ],
   "source": [
    "# Search by name\n",
    "query = \"BERT\"\n",
    "results = kb.search_docs(query, scope=\"names\", strategy=\"bm25\")\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.4f}] {hit.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search within a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.033] 1-385\n",
      "# Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani*\n",
      "\n",
      "Google Brain\n",
      "\n",
      "avaswani@google.com\n",
      "\n",
      "Noam Shazeer*\n",
      "\n",
      "Goog...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasnimo/Documents/athenaeum/playground/.venv/lib/python3.13/site-packages/athenaeum/search/vector.py:61: UserWarning: Relevance scores must be between 0 and 1, got [(Document(id='ac66f9901c89:0', metadata={'start_line': 1, 'text': '# Attention Is All You Need\\n\\nAshish Vaswani*\\n\\nGoogle Brain\\n\\navaswani@google.com\\n\\nNoam Shazeer*\\n\\nGoogle Brain\\n\\nnoam@google.com\\n\\nNiki Parmar*\\n\\nGoogle Research\\n\\nnikip@google.com\\n\\nJakob Uszkoreit*\\n\\nGoogle Research\\n\\nusz@google.com\\n\\nLlion Jones*\\n\\nGoogle Research\\n\\nllion@google.com\\n\\nAidan N. Gomez*†\\n\\nUniversity of Toronto\\n\\naidan@cs.toronto.edu\\n\\nŁukasz Kaiser*\\n\\nGoogle Brain\\n\\nlukaszkaiser@google.com\\n\\nIllia Polosukhin*‡\\n\\nillia.polosukhin@gmail.com\\n\\n# Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n1 Introduction\\n\\nRecurrent neural networks, long short-term memory *[13]* and gated recurrent *[7]* neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation *[35, 2, 5]*. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures *[38, 24, 15]*.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_{t}$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks *[21]* and conditional computation *[32]*, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences *[2, 19]*. In all but a few cases *[27]*, however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n## 2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU *[16]*, ByteNet *[18]* and ConvS2S *[9]*, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions *[12]*. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations *[4, 27, 28, 22]*.\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks *[34]*.\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as *[17, 18]* and *[9]*.\\n\\n## 3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure *[5, 2, 35]*. Here, the encoder maps an input sequence of symbol representations $(x_{1},...,x_{n})$ to a sequence of continuous representations $\\\\mathbf{z}=(z_{1},...,z_{n})$. Given $\\\\mathbf{z}$, the decoder then generates an output sequence $(y_{1},...,y_{m})$ of symbols one element at a time. At each step the model is auto-regressive *[10]*, consuming the previously generated symbols as additional input when generating the next.\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n# 3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of  $N = 6$  identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm  $(x + \\\\text{Sublayer}(x))$ , where Sublayer  $(x)$  is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension  $d_{\\\\text{model}} = 512$ .\\n\\nDecoder: The decoder is also composed of a stack of  $N = 6$  identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position  $i$  can depend only on the known outputs at positions less than  $i$ .\\n\\n# 3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n![img-1.jpeg](img-1.jpeg)\\nScaled Dot-Product Attention\\n\\n![img-2.jpeg](img-2.jpeg)\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n# 3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension  $d_{k}$ , and values of dimension  $d_{v}$ . We compute the dot products of the query with all keys, divide each by  $\\\\sqrt{d_k}$ , and apply a softmax function to obtain the weights on the values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix  $Q$ . The keys and values are also packed together into matrices  $K$  and  $V$ . We compute the matrix of outputs as:\\n\\n$$\\n\\\\operatorname {A t t e n t i o n} (Q, K, V) = \\\\operatorname {s o f t m a x} \\\\left(\\\\frac {Q K ^ {T}}{\\\\sqrt {d _ {k}}}\\\\right) V \\\\tag {1}\\n$$\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of  $\\\\frac{1}{\\\\sqrt{d_k}}$ . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of  $d_{k}$  the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of  $d_{k}$  [3]. We suspect that for large values of  $d_{k}$ , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by  $\\\\frac{1}{\\\\sqrt{d_k}}$ .\\n\\n# 3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with  $d_{\\\\mathrm{model}}$ -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values  $h$  times with different, learned linear projections to  $d_k$ ,  $d_k$  and  $d_v$  dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding  $d_v$ -dimensional\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\n$\\\\mathrm{MultiHead}(Q,K,V)$ $=\\\\mathrm{Concat}(\\\\mathrm{head}_{1},...,\\\\mathrm{head}_{\\\\mathrm{h}})W^{O}$\\n$\\\\mathrm{where\\\\ head_{i}}$ $=\\\\mathrm{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$\\n\\nWhere the projections are parameter matrices $W_{i}^{Q}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{K}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{V}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{v}}$ and $W^{O}\\\\in\\\\mathbb{R}^{hd_{v}\\\\times d_{\\\\mathrm{model}}}$.\\n\\nIn this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_{k}=d_{v}=d_{\\\\mathrm{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n#### 3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as *[38, 2, 9]*.\\n- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $-\\\\infty$) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n### 3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\n$\\\\mathrm{FFN}(x)=\\\\max(0,xW_{1}+b_{1})W_{2}+b_{2}$ (2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{\\\\mathrm{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$.\\n\\n### 3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\\\mathrm{model}}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to *[30]*. In the embedding layers, we multiply those weights by $\\\\sqrt{d_{\\\\mathrm{model}}}$.\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.  $n$  is the sequence length,  $d$  is the representation dimension,  $k$  is the kernel size of convolutions and  $r$  the size of the neighborhood in restricted self-attention.\\n\\n|  Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length  |\\n| --- | --- | --- | --- |\\n|  Self-Attention | O(n2·d) | O(1) | O(1)  |\\n|  Recurrent | O(n·d2) | O(n) | O(n)  |\\n|  Convolutional | O(k·n·d2) | O(1) | O(logk(n))  |\\n|  Self-Attention (restricted) | O(r·n·d) | O(1) | O(n/r)  |\\n\\n# 3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension  $d_{\\\\mathrm{model}}$  as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\n$$\\nP E _ {(p o s, 2 i)} = \\\\sin (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\n$$\\nP E _ {(p o s, 2 i + 1)} = \\\\cos (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\nwhere  $pos$  is the position and  $i$  is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from  $2\\\\pi$  to  $10000 \\\\cdot 2\\\\pi$ . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset  $k$ ,  $PE_{pos+k}$  can be represented as a linear function of  $PE_{pos}$ .\\n\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n# 4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations  $(x_{1},\\\\dots,x_{n})$  to another sequence of equal length  $(z_{1},\\\\dots,z_{n})$ , with  $x_{i},z_{i}\\\\in \\\\mathbb{R}^{d}$ , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires  $O(n)$  sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\\n\\nlength $n$ is smaller than the representation dimensionality $d$, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece *[38]* and byte-pair *[31]* representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $r$ in the input sequence centered around the respective output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width $k<n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_{k}(n))$ in the case of dilated convolutions *[18]*, increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of $k$. Separable convolutions *[6]*, however, decrease the complexity considerably, to $O(k\\\\cdot n\\\\cdot d+n\\\\cdot d^{2})$. Even with $k=n$, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n## 5 Training\\n\\nThis section describes the training regime for our models.\\n\\n### 5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding *[3]*, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary *[38]*. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n### 5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n### 5.3 Optimizer\\n\\nWe used the Adam optimizer *[20]* with $\\\\beta_{1}=0.9$, $\\\\beta_{2}=0.98$ and $\\\\epsilon=10^{-9}$. We varied the learning rate over the course of training, according to the formula:\\n\\n$lrate=d_{\\\\text{model}}^{-0.5}\\\\cdot\\\\min(step\\\\_num^{-0.5},step\\\\_num\\\\cdot warmup\\\\_steps^{-1.5})$ (3)\\n\\nThis corresponds to increasing the learning rate linearly for the first $warmup\\\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used $warmup\\\\_steps=4000$.\\n\\n### 5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\n|  Model | BLEU |   | Training Cost (FLOPs)  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  EN-DE | EN-FR | EN-DE | EN-FR  |\\n|  ByteNet [18] | 23.75 |  |  |   |\\n|  Deep-Att + PosUnk [39] |  | 39.2 |  | 1.0 · 1020  |\\n|  GNMT + RL [38] | 24.6 | 39.92 | 2.3 · 1019 | 1.4 · 1020  |\\n|  ConvS2S [9] | 25.16 | 40.46 | 9.6 · 1018 | 1.5 · 1020  |\\n|  MoE [32] | 26.03 | 40.56 | 2.0 · 1019 | 1.2 · 1020  |\\n|  Deep-Att + PosUnk Ensemble [39] |  | 40.4 |  | 8.0 · 1020  |\\n|  GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 · 1020 | 1.1 · 1021  |\\n|  ConvS2S Ensemble [9] | 26.36 | 41.29 | 7.7 · 1019 | 1.2 · 1021  |\\n|  Transformer (base model) | 27.3 | 38.1 | 3.3 · 1018  |   |\\n|  Transformer (big) | 28.4 | 41.8 | 2.3 · 1019  |   |\\n\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of  $P_{drop} = 0.1$ .\\n\\nLabel Smoothing During training, we employed label smoothing of value  $\\\\epsilon_{ls} = 0.1$  [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n# 6 Results\\n\\n# 6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than  $1/4$  the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate  $P_{drop} = 0.1$ , instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty  $\\\\alpha = 0.6$  [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length  $+50$ , but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each  $\\\\mathrm{GPU}^5$ .\\n\\n# 6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\n|   | N | dmodel | dff | h | dk | dv | Pdrop | εls | train steps | PPL (dev) | BLEU (dev) | params ×106  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65  |\\n|  (A) |  |  |  | 1 | 512 | 512 |  |  |  | 5.29 | 24.9 |   |\\n|   |   |  |  | 4 | 128 | 128 |  |  |  | 5.00 | 25.5 |   |\\n|   |   |  |  | 16 | 32 | 32 |  |  |  | 4.91 | 25.8 |   |\\n|   |   |  |  | 32 | 16 | 16 |  |  |  | 5.01 | 25.4 |   |\\n|  (B) |  |  |  |  | 16 |  |  |  |  | 5.16 | 25.1 | 58  |\\n|   |   |  |  |  | 32 |  |  |  |  | 5.01 | 25.4 | 60  |\\n|  (C) | 2 |  |  |  |  |  |  |  |  | 6.11 | 23.7 | 36  |\\n|   |  4 |  |  |  |  |  |  |  |  | 5.19 | 25.3 | 50  |\\n|   |  8 |  |  |  |  |  |  |  |  | 4.88 | 25.5 | 80  |\\n|   |   | 256 |  |  | 32 | 32 |  |  |  | 5.75 | 24.5 | 28  |\\n|   |   | 1024 |  |  | 128 | 128 |  |  |  | 4.66 | 26.0 | 168  |\\n|   |   |  | 1024 |  |  |  |  |  |  | 5.12 | 25.4 | 53  |\\n|   |   |  | 4096 |  |  |  |  |  |  | 4.75 | 26.2 | 90  |\\n|  (D) |  |  |  |  |  |  | 0.0 |  |  | 5.77 | 24.6 |   |\\n|   |   |  |  |  |  |  | 0.2 |  |  | 4.95 | 25.5 |   |\\n|   |   |  |  |  |  |  |  | 0.0 |  | 4.67 | 25.3 |   |\\n|   |   |  |  |  |  |  |  | 0.2 |  | 5.47 | 25.7 |   |\\n|  (E) | positional embedding instead of sinusoids |   |   |   |   |   |   |   |   | 4.92 | 25.7 |   |\\n|  big | 6 | 1024 | 4096 | 16 |  |  | 0.3 |  | 300K | 4.33 | 26.4 | 213  |\\n\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\\n\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size  $d_{k}$  hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\\n\\n# 6.3 English Constituency Parsing\\n\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\\n\\nWe trained a 4-layer transformer with  $d_{model} = 1024$  on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\\n\\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\\n\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\\n\\n|  Parser | Training | WSJ 23 F1  |\\n| --- | --- | --- |\\n|  Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3  |\\n|  Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4  |\\n|  Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4  |\\n|  Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7  |\\n|  Transformer (4 layers) | WSJ only, discriminative | 91.3  |\\n|  Zhu et al. (2013) [40] | semi-supervised | 91.3  |\\n|  Huang & Harper (2009) [14] | semi-supervised | 91.3  |\\n|  McClosky et al. (2006) [26] | semi-supervised | 92.1  |\\n|  Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1  |\\n|  Transformer (4 layers) | semi-supervised | 92.7  |\\n|  Luong et al. (2015) [23] | multi-task | 93.0  |\\n|  Dyer et al. (2016) [8] | generative | 93.3  |\\n\\nincreased the maximum output length to input length  $+300$ . We used a beam size of 21 and  $\\\\alpha = 0.3$  for both WSJ only and the semi-supervised setting.\\n\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\\n\\n# 7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\n# References\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\\n- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\\n- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n- [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\n- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\\n- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\\n- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\\n- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\\n- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n- [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n- [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\\n- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n- [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\\n- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\\n- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\\n- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\\n- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n- [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n\\n# Attention Visualizations\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \\'making\\', completing the phrase \\'making...more difficult\\'. Attentions here shown only for the word \\'making\\'. Different colors represent different heads. Best viewed in color.\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \\'its\\' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n![img-5.jpeg](img-5.jpeg)\\n\\n![img-6.jpeg](img-6.jpeg)\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.', 'end_line': 385, 'chunk_index': 0, 'doc_id': 'ac66f9901c89'}, page_content='# Attention Is All You Need\\n\\nAshish Vaswani*\\n\\nGoogle Brain\\n\\navaswani@google.com\\n\\nNoam Shazeer*\\n\\nGoogle Brain\\n\\nnoam@google.com\\n\\nNiki Parmar*\\n\\nGoogle Research\\n\\nnikip@google.com\\n\\nJakob Uszkoreit*\\n\\nGoogle Research\\n\\nusz@google.com\\n\\nLlion Jones*\\n\\nGoogle Research\\n\\nllion@google.com\\n\\nAidan N. Gomez*†\\n\\nUniversity of Toronto\\n\\naidan@cs.toronto.edu\\n\\nŁukasz Kaiser*\\n\\nGoogle Brain\\n\\nlukaszkaiser@google.com\\n\\nIllia Polosukhin*‡\\n\\nillia.polosukhin@gmail.com\\n\\n# Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n1 Introduction\\n\\nRecurrent neural networks, long short-term memory *[13]* and gated recurrent *[7]* neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation *[35, 2, 5]*. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures *[38, 24, 15]*.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_{t}$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks *[21]* and conditional computation *[32]*, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences *[2, 19]*. In all but a few cases *[27]*, however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n## 2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU *[16]*, ByteNet *[18]* and ConvS2S *[9]*, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions *[12]*. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations *[4, 27, 28, 22]*.\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks *[34]*.\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as *[17, 18]* and *[9]*.\\n\\n## 3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure *[5, 2, 35]*. Here, the encoder maps an input sequence of symbol representations $(x_{1},...,x_{n})$ to a sequence of continuous representations $\\\\mathbf{z}=(z_{1},...,z_{n})$. Given $\\\\mathbf{z}$, the decoder then generates an output sequence $(y_{1},...,y_{m})$ of symbols one element at a time. At each step the model is auto-regressive *[10]*, consuming the previously generated symbols as additional input when generating the next.\\n\\n![img-0.jpeg](img-0.jpeg)\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n# 3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of  $N = 6$  identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm  $(x + \\\\text{Sublayer}(x))$ , where Sublayer  $(x)$  is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension  $d_{\\\\text{model}} = 512$ .\\n\\nDecoder: The decoder is also composed of a stack of  $N = 6$  identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position  $i$  can depend only on the known outputs at positions less than  $i$ .\\n\\n# 3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n![img-1.jpeg](img-1.jpeg)\\nScaled Dot-Product Attention\\n\\n![img-2.jpeg](img-2.jpeg)\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n# 3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension  $d_{k}$ , and values of dimension  $d_{v}$ . We compute the dot products of the query with all keys, divide each by  $\\\\sqrt{d_k}$ , and apply a softmax function to obtain the weights on the values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix  $Q$ . The keys and values are also packed together into matrices  $K$  and  $V$ . We compute the matrix of outputs as:\\n\\n$$\\n\\\\operatorname {A t t e n t i o n} (Q, K, V) = \\\\operatorname {s o f t m a x} \\\\left(\\\\frac {Q K ^ {T}}{\\\\sqrt {d _ {k}}}\\\\right) V \\\\tag {1}\\n$$\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of  $\\\\frac{1}{\\\\sqrt{d_k}}$ . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of  $d_{k}$  the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of  $d_{k}$  [3]. We suspect that for large values of  $d_{k}$ , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by  $\\\\frac{1}{\\\\sqrt{d_k}}$ .\\n\\n# 3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with  $d_{\\\\mathrm{model}}$ -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values  $h$  times with different, learned linear projections to  $d_k$ ,  $d_k$  and  $d_v$  dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding  $d_v$ -dimensional\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\n$\\\\mathrm{MultiHead}(Q,K,V)$ $=\\\\mathrm{Concat}(\\\\mathrm{head}_{1},...,\\\\mathrm{head}_{\\\\mathrm{h}})W^{O}$\\n$\\\\mathrm{where\\\\ head_{i}}$ $=\\\\mathrm{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$\\n\\nWhere the projections are parameter matrices $W_{i}^{Q}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{K}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{k}}$, $W_{i}^{V}\\\\in\\\\mathbb{R}^{d_{\\\\mathrm{model}}\\\\times d_{v}}$ and $W^{O}\\\\in\\\\mathbb{R}^{hd_{v}\\\\times d_{\\\\mathrm{model}}}$.\\n\\nIn this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_{k}=d_{v}=d_{\\\\mathrm{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n#### 3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as *[38, 2, 9]*.\\n- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $-\\\\infty$) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n### 3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\n$\\\\mathrm{FFN}(x)=\\\\max(0,xW_{1}+b_{1})W_{2}+b_{2}$ (2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{\\\\mathrm{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$.\\n\\n### 3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\\\mathrm{model}}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to *[30]*. In the embedding layers, we multiply those weights by $\\\\sqrt{d_{\\\\mathrm{model}}}$.\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.  $n$  is the sequence length,  $d$  is the representation dimension,  $k$  is the kernel size of convolutions and  $r$  the size of the neighborhood in restricted self-attention.\\n\\n|  Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length  |\\n| --- | --- | --- | --- |\\n|  Self-Attention | O(n2·d) | O(1) | O(1)  |\\n|  Recurrent | O(n·d2) | O(n) | O(n)  |\\n|  Convolutional | O(k·n·d2) | O(1) | O(logk(n))  |\\n|  Self-Attention (restricted) | O(r·n·d) | O(1) | O(n/r)  |\\n\\n# 3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension  $d_{\\\\mathrm{model}}$  as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\n$$\\nP E _ {(p o s, 2 i)} = \\\\sin (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\n$$\\nP E _ {(p o s, 2 i + 1)} = \\\\cos (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\\\text {m o d e l}}})\\n$$\\n\\nwhere  $pos$  is the position and  $i$  is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from  $2\\\\pi$  to  $10000 \\\\cdot 2\\\\pi$ . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset  $k$ ,  $PE_{pos+k}$  can be represented as a linear function of  $PE_{pos}$ .\\n\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n# 4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations  $(x_{1},\\\\dots,x_{n})$  to another sequence of equal length  $(z_{1},\\\\dots,z_{n})$ , with  $x_{i},z_{i}\\\\in \\\\mathbb{R}^{d}$ , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires  $O(n)$  sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\\n\\nlength $n$ is smaller than the representation dimensionality $d$, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece *[38]* and byte-pair *[31]* representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $r$ in the input sequence centered around the respective output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width $k<n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_{k}(n))$ in the case of dilated convolutions *[18]*, increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of $k$. Separable convolutions *[6]*, however, decrease the complexity considerably, to $O(k\\\\cdot n\\\\cdot d+n\\\\cdot d^{2})$. Even with $k=n$, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n## 5 Training\\n\\nThis section describes the training regime for our models.\\n\\n### 5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding *[3]*, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary *[38]*. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n### 5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n### 5.3 Optimizer\\n\\nWe used the Adam optimizer *[20]* with $\\\\beta_{1}=0.9$, $\\\\beta_{2}=0.98$ and $\\\\epsilon=10^{-9}$. We varied the learning rate over the course of training, according to the formula:\\n\\n$lrate=d_{\\\\text{model}}^{-0.5}\\\\cdot\\\\min(step\\\\_num^{-0.5},step\\\\_num\\\\cdot warmup\\\\_steps^{-1.5})$ (3)\\n\\nThis corresponds to increasing the learning rate linearly for the first $warmup\\\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used $warmup\\\\_steps=4000$.\\n\\n### 5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\n|  Model | BLEU |   | Training Cost (FLOPs)  |   |\\n| --- | --- | --- | --- | --- |\\n|   |  EN-DE | EN-FR | EN-DE | EN-FR  |\\n|  ByteNet [18] | 23.75 |  |  |   |\\n|  Deep-Att + PosUnk [39] |  | 39.2 |  | 1.0 · 1020  |\\n|  GNMT + RL [38] | 24.6 | 39.92 | 2.3 · 1019 | 1.4 · 1020  |\\n|  ConvS2S [9] | 25.16 | 40.46 | 9.6 · 1018 | 1.5 · 1020  |\\n|  MoE [32] | 26.03 | 40.56 | 2.0 · 1019 | 1.2 · 1020  |\\n|  Deep-Att + PosUnk Ensemble [39] |  | 40.4 |  | 8.0 · 1020  |\\n|  GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 · 1020 | 1.1 · 1021  |\\n|  ConvS2S Ensemble [9] | 26.36 | 41.29 | 7.7 · 1019 | 1.2 · 1021  |\\n|  Transformer (base model) | 27.3 | 38.1 | 3.3 · 1018  |   |\\n|  Transformer (big) | 28.4 | 41.8 | 2.3 · 1019  |   |\\n\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of  $P_{drop} = 0.1$ .\\n\\nLabel Smoothing During training, we employed label smoothing of value  $\\\\epsilon_{ls} = 0.1$  [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n# 6 Results\\n\\n# 6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than  $1/4$  the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate  $P_{drop} = 0.1$ , instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty  $\\\\alpha = 0.6$  [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length  $+50$ , but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each  $\\\\mathrm{GPU}^5$ .\\n\\n# 6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\n|   | N | dmodel | dff | h | dk | dv | Pdrop | εls | train steps | PPL (dev) | BLEU (dev) | params ×106  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n|  base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65  |\\n|  (A) |  |  |  | 1 | 512 | 512 |  |  |  | 5.29 | 24.9 |   |\\n|   |   |  |  | 4 | 128 | 128 |  |  |  | 5.00 | 25.5 |   |\\n|   |   |  |  | 16 | 32 | 32 |  |  |  | 4.91 | 25.8 |   |\\n|   |   |  |  | 32 | 16 | 16 |  |  |  | 5.01 | 25.4 |   |\\n|  (B) |  |  |  |  | 16 |  |  |  |  | 5.16 | 25.1 | 58  |\\n|   |   |  |  |  | 32 |  |  |  |  | 5.01 | 25.4 | 60  |\\n|  (C) | 2 |  |  |  |  |  |  |  |  | 6.11 | 23.7 | 36  |\\n|   |  4 |  |  |  |  |  |  |  |  | 5.19 | 25.3 | 50  |\\n|   |  8 |  |  |  |  |  |  |  |  | 4.88 | 25.5 | 80  |\\n|   |   | 256 |  |  | 32 | 32 |  |  |  | 5.75 | 24.5 | 28  |\\n|   |   | 1024 |  |  | 128 | 128 |  |  |  | 4.66 | 26.0 | 168  |\\n|   |   |  | 1024 |  |  |  |  |  |  | 5.12 | 25.4 | 53  |\\n|   |   |  | 4096 |  |  |  |  |  |  | 4.75 | 26.2 | 90  |\\n|  (D) |  |  |  |  |  |  | 0.0 |  |  | 5.77 | 24.6 |   |\\n|   |   |  |  |  |  |  | 0.2 |  |  | 4.95 | 25.5 |   |\\n|   |   |  |  |  |  |  |  | 0.0 |  | 4.67 | 25.3 |   |\\n|   |   |  |  |  |  |  |  | 0.2 |  | 5.47 | 25.7 |   |\\n|  (E) | positional embedding instead of sinusoids |   |   |   |   |   |   |   |   | 4.92 | 25.7 |   |\\n|  big | 6 | 1024 | 4096 | 16 |  |  | 0.3 |  | 300K | 4.33 | 26.4 | 213  |\\n\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\\n\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size  $d_{k}$  hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\\n\\n# 6.3 English Constituency Parsing\\n\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\\n\\nWe trained a 4-layer transformer with  $d_{model} = 1024$  on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\\n\\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\\n\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\\n\\n|  Parser | Training | WSJ 23 F1  |\\n| --- | --- | --- |\\n|  Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3  |\\n|  Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4  |\\n|  Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4  |\\n|  Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7  |\\n|  Transformer (4 layers) | WSJ only, discriminative | 91.3  |\\n|  Zhu et al. (2013) [40] | semi-supervised | 91.3  |\\n|  Huang & Harper (2009) [14] | semi-supervised | 91.3  |\\n|  McClosky et al. (2006) [26] | semi-supervised | 92.1  |\\n|  Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1  |\\n|  Transformer (4 layers) | semi-supervised | 92.7  |\\n|  Luong et al. (2015) [23] | multi-task | 93.0  |\\n|  Dyer et al. (2016) [8] | generative | 93.3  |\\n\\nincreased the maximum output length to input length  $+300$ . We used a beam size of 21 and  $\\\\alpha = 0.3$  for both WSJ only and the semi-supervised setting.\\n\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\\n\\n# 7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\n# References\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\\n- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\\n- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n- [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\n- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\\n- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\\n- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\\n- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\\n- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n- [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n- [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\\n- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n- [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\\n- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\\n- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\\n- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\\n- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n- [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n\\n# Attention Visualizations\\n\\n![img-3.jpeg](img-3.jpeg)\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \\'making\\', completing the phrase \\'making...more difficult\\'. Attentions here shown only for the word \\'making\\'. Different colors represent different heads. Best viewed in color.\\n\\n![img-4.jpeg](img-4.jpeg)\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \\'its\\' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n![img-5.jpeg](img-5.jpeg)\\n\\n![img-6.jpeg](img-6.jpeg)\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.'), -0.04060459391591231)]\n",
      "  results = self._store.similarity_search_with_relevance_scores(query, **kwargs)  # type: ignore[arg-type]\n"
     ]
    }
   ],
   "source": [
    "# Pick a specific paper and search for content inside it.\n",
    "\n",
    "doc_ids = {doc.name: doc.id for doc in docs}\n",
    "attention_id = doc_ids[\"Attention Is All You Need.pdf\"]\n",
    "\n",
    "results = kb.search_doc_contents(attention_id, \"positional encoding\", top_k=3)\n",
    "for hit in results:\n",
    "    print(f\"[{hit.score:.3f}] {hit.line_range[0]}-{hit.line_range[1]}\")\n",
    "    print(f\"{hit.text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read specific excerpts\n",
    "\n",
    "Read exact line ranges from a document — useful for presenting context to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines 1-30 of 1739\n",
      "\n",
      "# Language Models are Few-Shot Learners\n",
      "\n",
      "|  Tom B. Brown* |   | Benjamin Mann* |   | Nick Ryder* |   | Melanie Subbiah*  |   |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "|  Jared Kaplan† | Prafulla Dhariwal | Arvind Neelakantan | Pranav Shyam | Girish Sastry |  |  |   |\n",
      "|  Amanda Askell | Sandhini Agarwal | Ariel Herbert-Voss | Gretchen Krueger | Tom Henighan |  |  |   |\n",
      "|  Rewon Child | Aditya Ramesh | Daniel M. Ziegler | Jeffrey Wu | Clemens Winter |  |  |   |\n",
      "|  Christopher Hesse | Mark Chen | Eric Sigler | Mateusz Litwin | Scott Gray |  |  |   |\n",
      "|  Benjamin Chess |   | Jack Clark |   | Christopher Berner |   |  |   |\n",
      "|  Sam McCandlish |   | Alec Radford | Ilya Sutskever | Dario Amodei |   |  |   |\n",
      "\n",
      "OpenAI\n",
      "\n",
      "# Abstract\n",
      "\n",
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters,  $10\\mathrm{x}$  more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n",
      "\n",
      "Author contributions listed at end of paper.\n",
      "\n",
      "2\n",
      "\n",
      "# Contents\n",
      "\n",
      "1  Introduction  3\n",
      "2  Approach  6\n",
      "2.1  Model and Architectures  8\n",
      "2.2  Training Dataset  8\n",
      "2.3  Training Process  9\n",
      "2.4  Evaluation  10\n",
      "3  Results  10\n"
     ]
    }
   ],
   "source": [
    "# Read the first 30 lines of a paper\n",
    "gpt3_id = doc_ids[\"Language Models are Few-Shot Learners.pdf\"]\n",
    "excerpt = kb.read_doc(gpt3_id, start_line=1, end_line=30)\n",
    "\n",
    "print(f\"Lines {excerpt.line_range[0]}-{excerpt.line_range[1]} of {excerpt.total_lines}\\n\")\n",
    "print(excerpt.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use table of contents to navigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of Contents — LORA\n",
      "\n",
      "- LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS [lines 1-16]\n",
      "- ABSTRACT [lines 17-20]\n",
      "- 1 INTRODUCTION [lines 21-86]\n",
      "      - Terminologies and Conventions [lines 41-44]\n",
      "  - 2 Problem Statement [lines 45-62]\n",
      "  - 3 Aren’t Existing Solutions Good Enough? [lines 63-86]\n",
      "      - Adapter Layers Introduce Inference Latency [lines 67-72]\n",
      "      - Directly Optimizing the Prompt is Hard [lines 73-86]\n",
      "- 4 OUR METHOD [lines 87-90]\n",
      "- 4.1 LOW-RANK-PARAMETRIZED UPDATE MATRICES [lines 91-173]\n",
      "    - 4.2 Applying LoRA to Transformer [lines 107-116]\n",
      "      - Practical Benefits and Limitations. [lines 111-116]\n",
      "  - 5 Empirical Experiments [lines 117-173]\n",
      "    - 5.1 Baselines [lines 121-173]\n",
      "- 5.2 ROBERTA BASE/LARGE [lines 174-177]\n",
      "- 5.3 DEBERTA XXL [lines 178-181]\n",
      "- 5.4 GPT-2 MEDIUM/LARGE [lines 182-199]\n",
      "- 5.5 SCALING UP TO GPT-3 175B [lines 200-210]\n",
      "- 6 RELATED WORKS [lines 211-238]\n",
      "      - Prompt Engineering and Fine-Tuning. [lines 217-220]\n",
      "      - Parameter-Efficient Adaptation. [lines 221-224]\n",
      "      - Low-Rank Structures in Deep Learning. [lines 225-228]\n",
      "  - 7 Understanding the Low-Rank Updates [lines 229-238]\n",
      "- 7.1 WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO? [lines 239-250]\n",
      "- 7.2 WHAT IS THE OPTIMAL RANK  $r$  FOR LORA? [lines 251-311]\n",
      "    - 7.3 How does the Adaptation Matrix $\\Delta W$ compare to $W$? [lines 291-311]\n",
      "- 8 CONCLUSION AND FUTURE WORK [lines 312-397]\n",
      "  - References [lines 320-386]\n",
      "  - Appendix A Large Language Models Still Need Parameter Updates [lines 387-397]\n",
      "- B INFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS [lines 398-406]\n",
      "- C DATASET DETAILS [lines 407-478]\n",
      "  - Appendix D Hyperparameters Used in Experiments [lines 423-478]\n",
      "    - D.1 RoBERTa [lines 425-428]\n",
      "    - D.2 DeBERTa [lines 429-478]\n",
      "- D.3 GPT-2 [lines 479-482]\n",
      "- D.4 GPT-3 [lines 483-525]\n",
      "- E COMBINING LORA WITH Prefix TUNING [lines 526-546]\n",
      "- F ADDITIONAL EMPIRICAL EXPERIMENTS [lines 547-548]\n",
      "- F.1 ADDITIONAL EXPERIMENTS ON GPT-2 [lines 549-592]\n",
      "- F.2 ADDITIONAL EXPERIMENTS ON GPT-3 [lines 593-596]\n",
      "- F.3 LOW-DATA REGIME [lines 597-602]\n",
      "- G MEASURING SIMILARITY BETWEEN SUBSPACES [lines 603-682]\n",
      "- H ADDITIONAL EXPERIMENTS ON LOW-RANK MATRICES [lines 683-686]\n",
      "- H.1 CORRELATION BETWEEN LORA MODULES [lines 687-690]\n",
      "- H.2 EFFECT OF  $r$  ON GPT-2 [lines 691-694]\n",
      "- H.3 CORRELATION BETWEEN  $W$  AND  $\\Delta W$ [lines 695-705]\n",
      "- H.4 AMPLIFICATION FACTOR [lines 706-734]\n"
     ]
    }
   ],
   "source": [
    "lora_id = doc_ids[\"LORA.pdf\"]\n",
    "lora_docs = [d for d in kb.list_docs() if d.id == lora_id]\n",
    "\n",
    "print(lora_docs[0].table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of Contents — LORA\n",
      "\n",
      "- An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale [lines 1-48]\n",
      "          - Abstract [lines 10-13]\n",
      "  - 1 Introduction [lines 14-27]\n",
      "  - 2 Related Work [lines 28-48]\n",
      "- 3 METHOD [lines 49-52]\n",
      "- 3.1 VISION TRANSFORMER (VIT) [lines 53-109]\n",
      "      - Inductive bias. [lines 70-73]\n",
      "      - Hybrid Architecture. [lines 74-77]\n",
      "    - 3.2 Fine-tuning and Higher Resolution [lines 78-81]\n",
      "  - 4 Experiments [lines 82-109]\n",
      "    - 4.1 Setup [lines 86-109]\n",
      "- 4.2 COMPARISON TO STATE OF THE ART [lines 110-137]\n",
      "- 4.3 PRE-TRAINING DATA REQUIREMENTS [lines 138-164]\n",
      "- 4.4 SCALING STUDY [lines 165-170]\n",
      "- 4.5 INSPECTING VISION TRANSFORMER [lines 171-183]\n",
      "- 4.6 SELF-SUPERVISION [lines 184-199]\n",
      "- 5 CONCLUSION [lines 200-205]\n",
      "- ACKNOWLEDGEMENTS [lines 206-209]\n",
      "- REFERENCES [lines 210-328]\n",
      "- APPENDIX [lines 329-330]\n",
      "- A MULTIHEAD SELF-ATTENTION [lines 331-352]\n",
      "- B EXPERIMENT DETAILS [lines 353-354]\n",
      "- B.1 TRAINING [lines 355-358]\n",
      "- B.1.1 FINE-TUNING [lines 359-381]\n",
      "- B.1.2 SELF-SUPERVISION [lines 382-389]\n",
      "- C ADDITIONAL RESULTS [lines 390-443]\n",
      "- D ADDITIONAL ANALYSES [lines 444-445]\n",
      "- D.1 SGD VS. ADAM FOR RESNETS [lines 446-470]\n",
      "- D.2 TRANSFORMER SHAPE [lines 471-474]\n",
      "- D.3 HEAD TYPE AND CLASSTOKEN [lines 475-496]\n",
      "- D.4 POSITIONAL EMBEDDING [lines 497-525]\n",
      "- D.5 EMPIRICAL COMPUTATIONAL COSTS [lines 526-540]\n",
      "- D.6 AXIAL ATTENTION [lines 541-557]\n",
      "- D.7 ATTENTION DISTANCE [lines 558-561]\n",
      "- D.8 ATTENTION MAPS [lines 562-565]\n",
      "- D.9 OBJECTNET RESULTS [lines 566-569]\n",
      "- D.10 VTAB BREAKDOWN [lines 570-587]\n"
     ]
    }
   ],
   "source": [
    "lora_id = doc_ids[\"ViT.pdf\"]\n",
    "lora_docs = [d for d in kb.list_docs() if d.id == lora_id]\n",
    "\n",
    "print(lora_docs[0].table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage tags\n",
    "\n",
    "Tags can be added or removed after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a tag\n",
    "kb.tag_doc(attention_id, {\"seminal\", \"google\"})\n",
    "\n",
    "# Remove a tag\n",
    "kb.untag_doc(attention_id, {\"google\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "The knowledge base persists in `STORAGE_DIR`. Delete it to start fresh:\n",
    "\n",
    "```python\n",
    "import shutil\n",
    "shutil.rmtree(STORAGE_DIR)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
